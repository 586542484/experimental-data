{"DBSCAN": "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander and Xiaowei Xu in 1996.\nIt is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\nDBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD. As of July 2020, the follow-up paper \"DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN\" appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.\n\n\n== History ==\nIn 1972, Robert F. Ling published a closely related algorithm in \"The Theory and Construction of k-Clusters\" in The Computer Journal with an estimated runtime complexity of O(n\u00b3). DBSCAN has a worst-case of O(n\u00b2), and the database-oriented range-query formulation of DBSCAN allows for index acceleration. The algorithms slightly differ in their handling of border points.\n\n\n== Preliminary ==\nConsider a set of points in some space to be clustered. Let \u03b5 be a parameter specifying the radius of a neighborhood with respect to some point. For the purpose of DBSCAN clustering, the points are classified as core points, (directly-) reachable points and outliers, as follows:\n\nA point p is a core point if at least minPts points are within distance \u03b5 of it (including p).\nA point q is directly reachable from p if point q is within distance \u03b5 from core point p. Points are only said to be directly reachable from core points.\nA point q is reachable from p if there is a path p1, ..., pn with p1 = p and pn = q, where each pi+1 is directly reachable from pi. Note that this implies that the initial point and all points on the path must be core points, with the possible exception of q.\nAll points not reachable from any other point are outliers or noise points.Now if p is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its \"edge\", since they cannot be used to reach more points.\n\nReachability is not a symmetric relation: by definition, only core points can reach non-core points. The opposite is not true, so a non-core point may be reachable, but nothing can be reached from it. Therefore, a further notion of connectedness is needed to formally define the extent of the clusters found by DBSCAN. Two points p and q are density-connected if there is a point o such that both p and q are reachable from o. Density-connectedness is symmetric.\nA cluster then satisfies two properties:\n\nAll points within the cluster are mutually density-connected.\nIf a point is density-reachable from some point of the cluster, it is part of the cluster as well.\n\n\n== Algorithm ==\n\n\n=== Original query-based algorithm ===\nDBSCAN requires two parameters: \u03b5 (eps) and the minimum number of points required to form a dense region (minPts). It starts with an arbitrary starting point that has not been visited. This point's \u03b5-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized \u03b5-environment of a different point and hence be made part of a cluster.\nIf a point is found to be a dense part of a cluster, its \u03b5-neighborhood is also part of that cluster. Hence, all points that are found within the \u03b5-neighborhood are added, as is their own \u03b5-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise.\nDBSCAN can be used with any distance function (as well as similarity functions or other predicates). The distance function (dist) can therefore be seen as an additional parameter.\nThe algorithm can be expressed in pseudocode as follows:\nDBSCAN(DB, distFunc, eps, minPts) {\n    C := 0                                                  /* Cluster counter */\n    for each point P in database DB {\n        if label(P) \u2260 undefined then continue               /* Previously processed in inner loop */\n        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */\n        if |N| < minPts then {                              /* Density check */\n            label(P) := Noise                               /* Label as Noise */\n            continue\n        }\n        C := C + 1                                          /* next cluster label */\n        label(P) := C                                       /* Label initial point */\n        SeedSet S := N \\ {P}                                /* Neighbors to expand */\n        for each point Q in S {                             /* Process every seed point Q */\n            if label(Q) = Noise then label(Q) := C          /* Change Noise to border point */\n            if label(Q) \u2260 undefined then continue           /* Previously processed (e.g., border point) */\n            label(Q) := C                                   /* Label neighbor */\n            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */\n            if |N| \u2265 minPts then {                          /* Density check (if Q is a core point) */\n                S := S \u222a N                                  /* Add new neighbors to seed set */\n            }\n        }\n    }\n}\n\nwhere RangeQuery can be implemented using a database index for better performance, or using a slow linear scan:\n\nRangeQuery(DB, distFunc, Q, eps) {\n    Neighbors N := empty list\n    for each point P in database DB {                      /* Scan all points in the database */\n        if distFunc(Q, P) \u2264 eps then {                     /* Compute distance and check epsilon */\n            N := N \u222a {P}                                   /* Add to result */\n        }\n    }\n    return N\n}\n\n\n=== Abstract algorithm ===\nThe DBSCAN algorithm can be abstracted into the following steps:\nFind the points in the \u03b5 (eps) neighborhood of every point, and identify the core points with more than minPts neighbors.\nFind the connected components of core points on the neighbor graph, ignoring all non-core points.\nAssign each non-core point to a nearby cluster if the cluster is an \u03b5 (eps) neighbor, otherwise assign it to noise.A naive implementation of this requires storing the neighborhoods in step 1, thus requiring substantial memory. The original DBSCAN algorithm does not require this by performing these steps for one point at a time.\n\n\n== Complexity ==\nDBSCAN visits each point of the database, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighborhood query in O(log n), an overall average runtime complexity of O(n log n) is obtained (if parameter \u03b5 is chosen in a meaningful way, i.e. such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g. all points within a distance less than \u03b5), the worst case run time complexity remains O(n\u00b2). The \n  \n    \n      \n        \n          \n            \n              \n                (\n              \n              \n                n\n                2\n              \n              \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle {\\binom {n}{2}}}\n   - n = (n\u00b2-n)/2-sized upper triangle of the distance matrix can be materialized to avoid distance recomputations, but this needs O(n\u00b2) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.\n\n\n== Advantages ==\nDBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.\nDBSCAN can find arbitrarily-shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.\nDBSCAN has a notion of noise, and is robust to outliers.\nDBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)\nDBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.\nThe parameters minPts and \u03b5 can be set by a domain expert, if the data is well understood.\n\n\n== Disadvantages ==\nDBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data are processed. For most data sets and domains, this situation does not arise often and has little impact on the clustering result: both on core points and noise points, DBSCAN is deterministic. DBSCAN* is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.\nThe quality of DBSCAN depends on the distance measure used in the function regionQuery(P,\u03b5). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called \"Curse of dimensionality\", making it difficult to find an appropriate value for \u03b5. This effect, however, is also present in any other algorithm based on Euclidean distance.\nDBSCAN cannot cluster data sets well with large differences in densities, since the minPts-\u03b5 combination cannot then be chosen appropriately for all clusters.\nIf the data and scale are not well understood, choosing a meaningful distance threshold \u03b5 can be difficult.See the section below on extensions for algorithmic modifications to handle these issues.\n\n\n== Parameter estimation ==\nEvery data mining task has the problem of parameters. Every parameter influences the algorithm in specific ways. For DBSCAN, the parameters \u03b5 and minPts are needed. The parameters must be specified by the user. Ideally, the value of \u03b5 is given by the problem to solve (e.g. a physical distance), and minPts is then the desired minimum cluster size.\nMinPts: As a rule of thumb, a minimum minPts can be derived from the number of dimensions D in the data set, as minPts \u2265 D + 1. The low value of minPts = 1 does not make sense, as then every point is a core point by definition. With minPts \u2264 2, the result will be the same as of hierarchical clustering with the single link metric, with the dendrogram cut at height \u03b5. Therefore, minPts must be chosen at least 3. However, larger values are usually better for data sets with noise and will yield more significant clusters. As a rule of thumb, minPts = 2\u00b7dim can be used, but it may be necessary to choose larger values for very large data, for noisy data or for data that contains many duplicates.\n\u03b5: The value for \u03b5 can then be chosen by using a k-distance graph, plotting the distance to the k = minPts-1 nearest neighbor ordered from the largest to the smallest value. Good values of \u03b5 are where this plot shows an \"elbow\": if \u03b5 is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of \u03b5, clusters will merge and the majority of objects will be in the same cluster. In general, small values of \u03b5 are preferable, and as a rule of thumb only a small fraction of points should be within this distance of each other. Alternatively, an OPTICS plot can be used to choose \u03b5, but then the OPTICS algorithm itself can be used to cluster the data.\nDistance function: The choice of distance function is tightly coupled to the choice of \u03b5, and has a major impact on the results. In general, it will be necessary to first identify a reasonable measure of similarity for the data set, before the parameter \u03b5 can be chosen. There is no estimation for this parameter, but the distance functions needs to be chosen appropriately for the data set. For example, on geographic data, the great-circle distance is often a good choice.OPTICS can be seen as a generalization of DBSCAN that replaces the \u03b5 parameter with a maximum value that mostly affects performance. MinPts then essentially becomes the minimum cluster size to find. While the algorithm is much easier to parameterize than DBSCAN, the results are a bit more difficult to use, as it will usually produce a hierarchical clustering instead of the simple data partitioning that DBSCAN produces.\nRecently, one of the original authors of DBSCAN has revisited DBSCAN and OPTICS, and published a refined version of hierarchical DBSCAN (HDBSCAN*), which no longer has the notion of border points. Instead, only the core points form the cluster.\n\n\n== Relationship to spectral clustering ==\nA spectral implementation of DBSCAN is related to spectral clustering in the trivial case of determining connected graph components \u2014 the optimal clusters with no edges cut. However, it can be computationally intensive, up to \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3})}\n  . Additionally, one has to choose the number of eigenvectors to compute. For performance reasons, the original DBSCAN algorithm remains preferable to its spectral implementation.\n\n\n== Extensions ==\nGeneralized DBSCAN (GDBSCAN) is a generalization by the same authors to arbitrary \"neighborhood\" and \"dense\" predicates. The \u03b5 and minPts parameters are removed from the original algorithm and moved to the predicates. For example, on polygon data, the \"neighborhood\" could be any intersecting polygon, whereas the density predicate uses the polygon areas instead of just the object count.\nVarious extensions to the DBSCAN algorithm have been proposed, including methods for parallelization, parameter estimation, and support for uncertain data. The basic idea has been extended to hierarchical clustering by the OPTICS algorithm. DBSCAN is also used as part of subspace clustering algorithms like PreDeCon and SUBCLU. HDBSCAN is a hierarchical version of DBSCAN which is also faster than OPTICS, from which a flat partition consisting of the most prominent clusters can be extracted from the hierarchy.\n\n\n== Availability ==\nDifferent implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 1.4 seconds, the slowest taking 13803 seconds. The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration.\n\nApache Commons Math contains a Java implementation of the algorithm running in quadratic time.\nELKI offers an implementation of DBSCAN as well as GDBSCAN and other variants. This implementation can use various index structures for sub-quadratic runtime and supports arbitrary distance functions and arbitrary data types, but it may be outperformed by low-level optimized (and specialized) implementations on small data sets.\nMATLAB includes an implementation of DBSCAN in its \"Statistics and Machine Learning Toolbox\" since release R2019a.\nmlpack includes an implementation of DBSCAN accelerated with dual-tree range search techniques.\nPostGIS includes ST_ClusterDBSCAN \u2013 a 2D implementation of DBSCAN that uses R-tree index. Any geometry type is supported, e.g. Point, LineString, Polygon, etc.\nR contains implementations of DBSCAN in the packages dbscan and fpc. Both packages support arbitrary distance functions via distance matrices. The package fpc does not have index support (and thus has quadratic runtime and memory complexity) and is rather slow due to the R interpreter. The package dbscan provides a fast C++ implementation using k-d trees (for Euclidean distance only) and also includes implementations of DBSCAN*, HDBSCAN*, OPTICS, OPTICSXi, and other related methods.\nscikit-learn includes a Python implementation of DBSCAN for arbitrary Minkowski metrics, which can be accelerated using k-d trees and ball trees but which uses worst-case quadratic memory. A contribution to scikit-learn provides an implementation of the HDBSCAN* algorithm.\npyclustering library includes a Python and C++ implementation of DBSCAN for Euclidean distance only as well as OPTICS algorithm.\nSPMF includes an implementation of the DBSCAN algorithm with k-d tree support for Euclidean distance only.\nWeka contains (as an optional package in latest versions) a basic implementation of DBSCAN that runs in quadratic time and linear memory.\n\n\n== Notes ==\n\n\n== References ==", "OPTICS algorithm": "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and J\u00f6rg Sander.\nIts basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\n\n\n== Basic idea ==\nLike DBSCAN, OPTICS requires two parameters: \u03b5, which describes the maximum distance (radius) to consider, and MinPts, describing the number of points required to form a cluster. A point p is a core point if at least MinPts points are found within its \u03b5-neighborhood \n  \n    \n      \n        \n          N\n          \n            \u03b5\n          \n        \n        (\n        p\n        )\n      \n    \n    {\\displaystyle N_{\\varepsilon }(p)}\n   (including point p itself). In contrast to DBSCAN, OPTICS also considers points that are part of a more densely packed cluster, so each point is assigned a core distance that describes the distance to the MinPtsth closest point:\n\n  \n    \n      \n        \n          \n            core-dist\n          \n          \n            \n              \u03b5\n              ,\n              M\n              i\n              n\n              P\n              t\n              s\n            \n          \n        \n        (\n        p\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    UNDEFINED\n                  \n                \n                \n                  \n                    if \n                  \n                  \n                    |\n                  \n                  \n                    N\n                    \n                      \u03b5\n                    \n                  \n                  (\n                  p\n                  )\n                  \n                    |\n                  \n                  <\n                  \n                    \n                      M\n                      i\n                      n\n                      P\n                      t\n                      s\n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      M\n                      i\n                      n\n                      P\n                      t\n                      s\n                    \n                  \n                  \n                    -th smallest distance in \n                  \n                  \n                    N\n                    \n                      \u03b5\n                    \n                  \n                  (\n                  p\n                  )\n                \n                \n                  \n                    otherwise\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{core-dist}}_{\\mathit {\\varepsilon ,MinPts}}(p)={\\begin{cases}{\\text{UNDEFINED}}&{\\text{if }}|N_{\\varepsilon }(p)|<{\\mathit {MinPts}}\\\\{\\mathit {MinPts}}{\\text{-th smallest distance in }}N_{\\varepsilon }(p)&{\\text{otherwise}}\\end{cases}}}\n  The reachability-distance of another point o from a point p is either the distance between o and p, or the core distance of p, whichever is bigger:\n\n  \n    \n      \n        \n          \n            reachability-dist\n          \n          \n            \n              \u03b5\n              ,\n              M\n              i\n              n\n              P\n              t\n              s\n            \n          \n        \n        (\n        o\n        ,\n        p\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    UNDEFINED\n                  \n                \n                \n                  \n                    if \n                  \n                  \n                    |\n                  \n                  \n                    N\n                    \n                      \u03b5\n                    \n                  \n                  (\n                  p\n                  )\n                  \n                    |\n                  \n                  <\n                  \n                    \n                      M\n                      i\n                      n\n                      P\n                      t\n                      s\n                    \n                  \n                \n              \n              \n                \n                  max\n                  (\n                  \n                    \n                      core-dist\n                    \n                    \n                      \n                        \u03b5\n                        ,\n                        M\n                        i\n                        n\n                        P\n                        t\n                        s\n                      \n                    \n                  \n                  (\n                  p\n                  )\n                  ,\n                  \n                    dist\n                  \n                  (\n                  p\n                  ,\n                  o\n                  )\n                  )\n                \n                \n                  \n                    otherwise\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{reachability-dist}}_{\\mathit {\\varepsilon ,MinPts}}(o,p)={\\begin{cases}{\\text{UNDEFINED}}&{\\text{if }}|N_{\\varepsilon }(p)|<{\\mathit {MinPts}}\\\\\\max({\\text{core-dist}}_{\\mathit {\\varepsilon ,MinPts}}(p),{\\text{dist}}(p,o))&{\\text{otherwise}}\\end{cases}}}\n  If p and o are nearest neighbors, this is the \n  \n    \n      \n        \n          \u03b5\n          \u2032\n        \n        <\n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon '<\\varepsilon }\n   we need to assume to have p and o belong to the same cluster.\nBoth core-distance and reachability-distance are undefined if no sufficiently dense cluster (w.r.t. \u03b5) is available. Given a sufficiently large \u03b5, this never happens, but then every \u03b5-neighborhood query returns the entire database, resulting in \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n   runtime. Hence, the \u03b5 parameter is required to cut off the density of clusters that are no longer interesting, and to speed up the algorithm.\nThe parameter \u03b5 is, strictly speaking, not necessary. It can simply be set to the maximum possible value. When a spatial index is available, however, it does play a practical role with regards to complexity. OPTICS abstracts from DBSCAN by removing this parameter, at least to the extent of only having to give the maximum value.\n\n\n== Pseudocode ==\nThe basic approach of OPTICS is similar to DBSCAN, but instead of maintaining known, but so far unprocessed cluster members in a set, they are maintained in a priority queue (e.g. using an indexed heap).\n\nfunction OPTICS(DB, \u03b5, MinPts) is\n    for each point p of DB do\n        p.reachability-distance = UNDEFINED\n    for each unprocessed point p of DB do\n        N = getNeighbors(p, \u03b5)\n        mark p as processed\n        output p to the ordered list\n        if core-distance(p, \u03b5, MinPts) != UNDEFINED then\n            Seeds = empty priority queue\n            update(N, p, Seeds, \u03b5, MinPts)\n            for each next q in Seeds do\n                N' = getNeighbors(q, \u03b5)\n                mark q as processed\n                output q to the ordered list\n                if core-distance(q, \u03b5, MinPts) != UNDEFINED do\n                    update(N', q, Seeds, \u03b5, MinPts)\n\nIn update(), the priority queue Seeds is updated with the \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  -neighborhood of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  , respectively:\n\nfunction update(N, p, Seeds, \u03b5, MinPts) is\n    coredist = core-distance(p, \u03b5, MinPts)\n    for each o in N\n        if o is not processed then\n            new-reach-dist = max(coredist, dist(p,o))\n            if o.reachability-distance == UNDEFINED then // o is not in Seeds\n                o.reachability-distance = new-reach-dist\n                Seeds.insert(o, new-reach-dist)\n            else               // o in Seeds, check for improvement\n                if new-reach-dist < o.reachability-distance then\n                    o.reachability-distance = new-reach-dist\n                    Seeds.move-up(o, new-reach-dist)\n\nOPTICS hence outputs the points in a particular ordering, annotated with their smallest reachability distance (in the original algorithm, the core distance is also exported, but this is not required for further processing).\n\n\n== Extracting the clusters ==\n\nUsing a reachability-plot (a special kind of dendrogram), the hierarchical structure of the clusters can be obtained easily. It is a 2D plot, with the ordering of the points as processed by OPTICS on the x-axis and the reachability distance on the y-axis. Since points belonging to a cluster have a low reachability distance to their nearest neighbor, the clusters show up as valleys in the reachability plot. The deeper the valley, the denser the cluster.\nThe image above illustrates this concept. In its upper left area, a synthetic example data set is shown. The upper right part visualizes the spanning tree produced by OPTICS, and the lower part shows the reachability plot as computed by OPTICS. Colors in this plot are labels, and not computed by the algorithm; but it is well visible how the valleys in the plot correspond to the clusters in above data set. The yellow points in this image are considered noise, and no valley is found in their reachability plot. They are usually not assigned to clusters, except the omnipresent \"all data\" cluster in a hierarchical result.\nExtracting clusters from this plot can be done manually by selecting a range on the x-axis after visual inspection, by selecting a threshold on the y-axis (the result is then similar to a DBSCAN clustering result with the same \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   and minPts parameters; here a value of 0.1 may yield good results), or by different algorithms that try to detect the valleys by steepness, knee detection, or local maxima. Clusterings obtained this way usually are hierarchical, and cannot be achieved by a single DBSCAN run.\n\n\n== Complexity ==\nLike DBSCAN, OPTICS processes each point once, and performs one \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  -neighborhood query during this processing. Given a spatial index that grants a neighborhood query in \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n   runtime, an overall runtime of \n  \n    \n      \n        O\n        (\n        n\n        \u22c5\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle O(n\\cdot \\log n)}\n   is obtained. The authors of the original OPTICS paper report an actual constant slowdown factor of 1.6 compared to DBSCAN. Note that the value of \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   might heavily influence the cost of the algorithm, since a value too large might raise the cost of a neighborhood query to linear complexity.\nIn particular, choosing \n  \n    \n      \n        \u03b5\n        >\n        \n          max\n          \n            x\n            ,\n            y\n          \n        \n        d\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle \\varepsilon >\\max _{x,y}d(x,y)}\n   (larger than the maximum distance in the data set) is possible, but leads to quadratic complexity, since every neighborhood query returns the full data set. Even when no spatial index is available, this comes at additional cost in managing the heap. Therefore, \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   should be chosen appropriately for the data set.\n\n\n== Extensions ==\nOPTICS-OF is an outlier detection algorithm based on OPTICS. The main use is the extraction of outliers from an existing run of OPTICS at low cost compared to using a different outlier detection method. The better known version LOF is based on the same concepts.\nDeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   parameter and offering performance improvements over OPTICS.\nHiSC is a hierarchical subspace clustering (axis-parallel) method based on OPTICS.\nHiCO is a hierarchical correlation clustering algorithm based on OPTICS.\nDiSH is an improvement over HiSC that can find more complex hierarchies.\nFOPTICS is a faster implementation using random projections.\nHDBSCAN* is based on a refinement of DBSCAN, excluding border-points from the clusters and thus following more strictly the basic definition of density-levels by Hartigan.\n\n\n== Availability ==\nJava implementations of OPTICS, OPTICS-OF, DeLi-Clu, HiSC, HiCO and DiSH are available in the ELKI data mining framework (with index acceleration for several distance functions, and with automatic cluster extraction using the \u03be extraction method). Other Java implementations include the Weka extension (no support for \u03be cluster extraction).\nThe R package \"dbscan\" includes a C++ implementation of OPTICS (with both traditional dbscan-like and \u03be cluster extraction) using a k-d tree for index acceleration for Euclidean distance only.\nPython implementations of OPTICS are available in the PyClustering library and in scikit-learn. HDBSCAN* is available in the hdbscan library.\n\n\n== References ==", "Confidence interval": "In frequentist statistics, a confidence interval (CI) is a range of estimates for an unknown parameter. A confidence interval is computed at a designated confidence level; the 95% confidence level is most common, but other levels, such as 90% or 99%, are sometimes used. The confidence level represents the long-run proportion of CIs (at the given confidence level) that theoretically contain the true value of the parameter. For example, out of all intervals computed at the 95% level, 95% of them should contain the parameter's true value.Factors affecting the width of the CI include the sample size, the variability in the sample, and the confidence level. All else being the same, a larger sample produces a narrower confidence interval, greater variability in the sample produces a wider confidence interval, and a higher confidence level produces a wider confidence interval.\n\n\n== Definition ==\nLet X be a random sample from a probability distribution with statistical parameter \u03b8, which is a quantity to be estimated, and \u03c6, representing quantities that are not of immediate interest. A confidence interval for the parameter \u03b8, with confidence level or coefficient \u03b3, is an interval \n  \n    \n      \n         \n        (\n         \n        u\n        (\n        X\n        )\n        ,\n        v\n        (\n        X\n        )\n         \n        )\n         \n      \n    \n    {\\displaystyle \\ (\\ u(X),v(X)\\ )\\ }\n   determined by random variables \n  \n    \n      \n         \n        u\n        (\n        X\n        )\n         \n      \n    \n    {\\displaystyle \\ u(X)\\ }\n   and \n  \n    \n      \n         \n        v\n        (\n        X\n        )\n         \n      \n    \n    {\\displaystyle \\ v(X)\\ }\n   with the property:\n\n  \n    \n      \n        Pr\n        {\n         \n        u\n        (\n        X\n        )\n        <\n        \u03b8\n        <\n        v\n        (\n        X\n        )\n         \n        }\n         \n        =\n         \n        \u03b3\n        \n        \n           for every \n        \n        (\n        \u03b8\n        ,\n        \u03c6\n        )\n         \n        .\n      \n    \n    {\\displaystyle \\Pr\\{\\ u(X)<\\theta <v(X)\\ \\}\\ =\\ \\gamma \\quad {\\text{ for every }}(\\theta ,\\varphi )~.}\n  The number \u03b3, whose typical value is close to but not greater than 1, is sometimes given in the form \n  \n    \n      \n         \n        1\n        \u2212\n        \u03b1\n         \n      \n    \n    {\\displaystyle \\ 1-\\alpha \\ }\n   (or as a percentage \n  \n    \n      \n         \n        100\n        %\n        \u22c5\n        (\n        1\n        \u2212\n        \u03b1\n        )\n         \n      \n    \n    {\\displaystyle \\ 100\\%\\cdot (1-\\alpha )\\ }\n  ), where \n  \n    \n      \n         \n        \u03b1\n         \n      \n    \n    {\\displaystyle \\ \\alpha \\ }\n   is a small positive number, often 0.05 .\nIt is important for the bounds \n  \n    \n      \n         \n        u\n        (\n        X\n        )\n         \n      \n    \n    {\\displaystyle \\ u(X)\\ }\n   and \n  \n    \n      \n         \n        v\n        (\n        X\n        )\n         \n      \n    \n    {\\displaystyle \\ v(X)\\ }\n   to be specified in such a way that as long as X is collected randomly, every time we compute a confidence interval, there is probability \u03b3 that it would contain \u03b8, the true value of the parameter being estimated. This should hold true for any actual \u03b8 and \u03c6.\n\n\n=== Approximate confidence intervals ===\nIn many applications, confidence intervals that have exactly the required confidence level are hard to construct, but approximate intervals can be computed. The rule for constructing the interval may be accepted as providing a confidence interval at level \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   if\n\n  \n    \n      \n        Pr\n        {\n         \n        u\n        (\n        X\n        )\n        <\n        \u03b8\n        <\n        v\n        (\n        X\n        )\n         \n        }\n         \n        \u2248\n         \n        \u03b3\n        \n        \n           for every \n        \n        (\n        \u03b8\n        ,\n        \u03c6\n        )\n      \n    \n    {\\displaystyle \\Pr\\{\\ u(X)<\\theta <v(X)\\ \\}\\ \\approx \\ \\gamma \\quad {\\text{ for every }}(\\theta ,\\varphi )}\n  to an acceptable level of approximation. Alternatively, some authors simply require that\n\n  \n    \n      \n        Pr\n        {\n         \n        u\n        (\n        X\n        )\n        <\n        \u03b8\n        <\n        v\n        (\n        X\n        )\n         \n        }\n         \n        \u2265\n         \n        \u03b3\n        \n        \n           for every \n        \n        (\n        \u03b8\n        ,\n        \u03c6\n        )\n         \n        ,\n      \n    \n    {\\displaystyle \\Pr\\{\\ u(X)<\\theta <v(X)\\ \\}\\ \\geq \\ \\gamma \\quad {\\text{ for every }}(\\theta ,\\varphi )~,}\n  which is useful if the probabilities are only partially identified or imprecise, and also when dealing with discrete distributions. Confidence limits of form\n\n  \n    \n      \n        Pr\n        {\n         \n        u\n        (\n        X\n        )\n        <\n        \u03b8\n         \n        }\n         \n        \u2265\n         \n        \u03b3\n         \n      \n    \n    {\\displaystyle \\Pr\\{\\ u(X)<\\theta \\ \\}\\ \\geq \\ \\gamma ~}\n   and \n  \n    \n      \n        Pr\n        {\n         \n        \u03b8\n        <\n        v\n        (\n        X\n        )\n         \n        }\n        \u2265\n        \u03b3\n         \n      \n    \n    {\\displaystyle \\Pr\\{\\ \\theta <v(X)\\ \\}\\geq \\gamma ~}\n  are called conservative;(p\u202f210) accordingly, one speaks of conservative confidence intervals and, in general, regions.\n\n\n=== Desired properties ===\nWhen applying standard statistical procedures, there will often be standard ways of constructing confidence intervals. These will have been devised so as to meet certain desirable properties, which will hold given that the assumptions on which the procedure relies are true. These desirable properties may be described as: validity, optimality, and invariance.\nOf the three, \"validity\" is most important, followed closely by \"optimality\". \"Invariance\" may be considered as a property of the method of derivation of a confidence interval, rather than of the rule for constructing the interval. In non-standard applications, these same desirable properties would be sought:\n\n\n==== Validity ====\nThis means that the nominal coverage probability (confidence level) of the confidence interval should hold, either exactly or to a good approximation.\n\n\n==== Optimality ====\nThis means that the rule for constructing the confidence interval should make as much use of the information in the data-set as possible.\nRecall that one could throw away half of a dataset and still be able to derive a valid confidence interval. One way of assessing optimality is by the length of the interval so that a rule for constructing a confidence interval is judged better than another if it leads to intervals whose lengths are typically shorter.\n\n\n==== Invariance ====\nIn many applications, the quantity being estimated might not be tightly defined as such.\nFor example, a survey might result in an estimate of the median income in a population, but it might equally be considered as providing an estimate of the logarithm of the median income, given that this is a common scale for presenting graphical results. It would be desirable that the method used for constructing a confidence interval for the median income would give equivalent results when applied to constructing a confidence interval for the logarithm of the median income: Specifically the values at the ends of the latter interval would be the logarithms of the values at the ends of former interval.\n\n\n=== Methods of derivation ===\nFor non-standard applications, there are several routes that might be taken to derive a rule for the construction of confidence intervals. Established rules for standard procedures might be justified or explained via several of these routes. Typically a rule for constructing confidence intervals is closely tied to a particular way of finding a point estimate of the quantity being considered.\n\n\n==== Summary statistics ====\n\nThis is closely related to the method of moments for estimation. A simple example arises where the quantity to be estimated is the population mean, in which case a natural estimate is the sample mean. Similarly, the sample variance can be used to estimate the population variance. A confidence interval for the true mean can be constructed centered on the sample mean with a width which is a multiple of the square root of the sample variance.\n\n\n==== Likelihood theory ====\nEstimates can be constructed using the maximum likelihood principle, the likelihood theory for this provides two ways of constructing confidence intervals or confidence regions for the estimates.\n\n\n==== Estimating equations ====\nThe estimation approach here can be considered as both a generalization of the method of moments and a generalization of the maximum likelihood approach. There are corresponding generalizations of the results of maximum likelihood theory that allow confidence intervals to be constructed based on estimates derived from estimating equations.\n\n\n==== Hypothesis testing ====\nIf hypothesis tests are available for general values of a parameter, then confidence intervals/regions can be constructed by including in the  100 p %  confidence region all those points for which the hypothesis test of the null hypothesis that the true value is the given value is not rejected at a significance level of  (1 \u2212 p) .(\u00a7\u202f7.2\u202f(iii))\n\n\n==== Bootstrapping ====\n\nIn situations where the distributional assumptions for the above methods are uncertain or violated, resampling methods allow construction of confidence intervals or prediction intervals. The observed data distribution and the internal correlations are used as the surrogate for the correlations in the wider population. \n\n\n==== Central limit theorem ====\n\nThe central limit theorem is a refinement of the law of large numbers. For a large number of independent identically distributed random variables \n  \n    \n      \n         \n        \n          X\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          X\n          \n            n\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\ X_{1},...,X_{n}\\ ,}\n   with finite variance, the average \n  \n    \n      \n         \n        \n          \n            \n              X\n              \u00af\n            \n          \n          \n            n\n          \n        \n         \n      \n    \n    {\\displaystyle \\ {\\overline {X}}_{n}\\ }\n   approximately has a normal distribution, no matter what the distribution of the \n  \n    \n      \n         \n        \n          X\n          \n            i\n          \n        \n         \n      \n    \n    {\\displaystyle \\ X_{i}\\ }\n   is, with the approximation roughly improving in proportion to \n  \n    \n      \n         \n        \n          \n            n\n             \n          \n        \n        .\n      \n    \n    {\\displaystyle \\ {\\sqrt {n\\ }}.}\n  \n\n\n== Example ==\nSuppose {X1, \u2026, Xn} is an independent sample from a normally distributed population with unknown parameters mean \u03bc and variance \u03c32. Let\n\n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n        =\n        (\n        \n          X\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          X\n          \n            n\n          \n        \n        )\n        \n          /\n        \n        n\n        \n        ,\n      \n    \n    {\\displaystyle {\\bar {X}}=(X_{1}+\\cdots +X_{n})/n\\,,}\n  \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            \n              n\n              \u2212\n              1\n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          X\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle S^{2}={\\frac {1}{n-1}}\\sum _{i=1}^{n}(X_{i}-{\\bar {X}}\\,)^{2}.}\n  Where X is the sample mean, and S2 is the sample variance. Then\n\n  \n    \n      \n        T\n        =\n        \n          \n            \n              \n                \n                  \n                    X\n                    \u00af\n                  \n                \n              \n              \u2212\n              \u03bc\n            \n            \n              S\n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle T={\\frac {{\\bar {X}}-\\mu }{S/{\\sqrt {n}}}}}\n  has a Student's t distribution with n \u2212 1 degrees of freedom. Note that the distribution of T does not depend on the values of the unobservable parameters \u03bc and \u03c32; i.e., it is a pivotal quantity. Suppose we wanted to calculate a 95% confidence interval for \u03bc. Then, denoting c as the 97.5th percentile of this distribution,\n\n  \n    \n      \n        Pr\n        (\n        \u2212\n        c\n        \u2264\n        T\n        \u2264\n        c\n        )\n        =\n        0.95\n      \n    \n    {\\displaystyle \\Pr(-c\\leq T\\leq c)=0.95}\n  Note that \"97.5th\" and \"0.95\" are correct in the preceding expressions. There is a 2.5% chance that \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   will be less than \n  \n    \n      \n        \u2212\n        c\n      \n    \n    {\\displaystyle -c}\n   and a 2.5% chance that it will be larger than \n  \n    \n      \n        +\n        c\n      \n    \n    {\\displaystyle +c}\n  . Thus, the probability that \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   will be between \n  \n    \n      \n        \u2212\n        c\n      \n    \n    {\\displaystyle -c}\n   and \n  \n    \n      \n        +\n        c\n      \n    \n    {\\displaystyle +c}\n   is 95%.\nConsequently,\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            \n              \n                \n                  X\n                  \u00af\n                \n              \n            \n            \u2212\n            \n              \n                \n                  c\n                  S\n                \n                \n                  n\n                \n              \n            \n            \u2264\n            \u03bc\n            \u2264\n            \n              \n                \n                  X\n                  \u00af\n                \n              \n            \n            +\n            \n              \n                \n                  c\n                  S\n                \n                \n                  n\n                \n              \n            \n          \n          )\n        \n        =\n        0.95\n        \n      \n    \n    {\\displaystyle \\Pr \\left({\\bar {X}}-{\\frac {cS}{\\sqrt {n}}}\\leq \\mu \\leq {\\bar {X}}+{\\frac {cS}{\\sqrt {n}}}\\right)=0.95\\,}\n  and we have a theoretical (stochastic) 95% confidence interval for \u03bc.\nAfter observing the sample we find values x for X and s for S, from which we compute the confidence interval\n\n  \n    \n      \n        \n          [\n          \n            \n              \n                \n                  x\n                  \u00af\n                \n              \n            \n            \u2212\n            \n              \n                \n                  c\n                  s\n                \n                \n                  n\n                \n              \n            \n            ,\n            \n              \n                \n                  x\n                  \u00af\n                \n              \n            \n            +\n            \n              \n                \n                  c\n                  s\n                \n                \n                  n\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\left[{\\bar {x}}-{\\frac {cs}{\\sqrt {n}}},{\\bar {x}}+{\\frac {cs}{\\sqrt {n}}}\\right].}\n  \n\n\n== Interpretation ==\nVarious interpretations of a confidence interval can be given (taking the 95% confidence interval as an example in the following).\n\nThe confidence interval can be expressed in terms of a long-run frequency in repeated samples (or in resampling): \"Were this procedure to be repeated on numerous samples, the proportion of calculated 95% confidence intervals that encompassed the true value of the population parameter would tend toward 95%.\"\nThe confidence interval can be expressed in terms of probability with respect to a single theoretical (yet to be realized) sample: \"There is a 95% probability that the 95% confidence interval calculated from a given future sample will cover the true value of the population parameter.\"  This essentially reframes the \"repeated samples\" interpretation as a probability rather than a frequency. See Neyman construction.\nThe confidence interval can be expressed in terms of statistical significance, e.g.: \"The 95% confidence interval represents values that are not statistically significantly different from the point estimate at the .05 level\".\n\n\n=== Common misunderstandings ===\n\nConfidence intervals and levels are frequently misunderstood, and published studies have shown that even professional scientists often misinterpret them.\nA 95% confidence level does not mean that for a given realized interval there is a 95% probability that the population parameter lies within the interval (i.e., a 95% probability that the interval covers the population parameter). According to the strict frequentist interpretation, once an interval is calculated, this interval either covers the parameter value or it does not; it is no longer a matter of probability. The 95% probability relates to the reliability of the estimation procedure, not to a specific calculated interval. Neyman himself (the original proponent of confidence intervals) made this point in his original paper:It will be noticed that in the above description, the probability statements refer to the problems of estimation with which the statistician will be concerned in the future. In fact, I have repeatedly stated that the frequency of correct results will tend to \u03b1. Consider now the case when a sample is already drawn, and the calculations have given [particular limits]. Can we say that in this particular case the probability of the true value [falling between these limits] is equal to \u03b1? The answer is obviously in the negative. The parameter is an unknown constant, and no probability statement concerning its value may be made...Deborah Mayo expands on this further as follows:It must be stressed, however, that having seen the value [of the data], Neyman\u2013Pearson theory never permits one to conclude that the specific confidence interval formed covers the true value of 0 with either (1 \u2212 \u03b1)100% probability or (1 \u2212 \u03b1)100% degree of confidence. Seidenfeld's remark seems rooted in a (not uncommon) desire for Neyman\u2013Pearson confidence intervals to provide something which they cannot legitimately provide; namely, a measure of the degree of probability, belief, or support that an unknown parameter value lies in a specific interval. Following Savage (1962), the probability that a parameter lies in a specific interval may be referred to as a measure of final precision. While a measure of final precision may seem desirable, and while confidence levels are often (wrongly) interpreted as providing such a measure, no such interpretation is warranted. Admittedly, such a misinterpretation is encouraged by the word 'confidence'.A 95% confidence level does not mean that 95% of the sample data lie within the confidence interval.\nA confidence interval is not a definitive range of plausible values for the sample parameter, though it is often heuristically taken as a range of plausible values.\nA particular confidence level of 95% calculated from an experiment does not mean that there is a 95% probability of a sample parameter from a repeat of the experiment falling within this interval.\n\n\n== Counterexamples ==\nSince confidence interval theory was proposed, a number of counter-examples to the theory have been developed to show how the interpretation of confidence intervals can be problematic, at least if one interprets them na\u00efvely.\n\n\n=== Confidence procedure for uniform location ===\nWelch presented an example which clearly shows the difference between the theory of confidence intervals and other theories of interval estimation (including Fisher's fiducial intervals and objective Bayesian intervals). Robinson called this example \"[p]ossibly the best known counterexample for Neyman's version of confidence interval theory.\" To Welch, it showed the superiority of confidence interval theory; to critics of the theory, it shows a deficiency. Here we present a simplified version.\nSuppose that \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2}}\n   are independent observations from a Uniform(\u03b8 \u2212 1/2, \u03b8 + 1/2) distribution. Then the optimal 50% confidence procedure for \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is\n\n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n        \u00b1\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      \n                        \n                          \n                            |\n                          \n                          \n                            X\n                            \n                              1\n                            \n                          \n                          \u2212\n                          \n                            X\n                            \n                              2\n                            \n                          \n                          \n                            |\n                          \n                        \n                        2\n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  \n                    |\n                  \n                  \n                    X\n                    \n                      1\n                    \n                  \n                  \u2212\n                  \n                    X\n                    \n                      2\n                    \n                  \n                  \n                    |\n                  \n                  <\n                  1\n                  \n                    /\n                  \n                  2\n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          1\n                          \u2212\n                          \n                            |\n                          \n                          \n                            X\n                            \n                              1\n                            \n                          \n                          \u2212\n                          \n                            X\n                            \n                              2\n                            \n                          \n                          \n                            |\n                          \n                        \n                        2\n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  \n                    |\n                  \n                  \n                    X\n                    \n                      1\n                    \n                  \n                  \u2212\n                  \n                    X\n                    \n                      2\n                    \n                  \n                  \n                    |\n                  \n                  \u2265\n                  1\n                  \n                    /\n                  \n                  2.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}\\pm {\\begin{cases}{\\dfrac {|X_{1}-X_{2}|}{2}}&{\\text{if }}|X_{1}-X_{2}|<1/2\\\\[8pt]{\\dfrac {1-|X_{1}-X_{2}|}{2}}&{\\text{if }}|X_{1}-X_{2}|\\geq 1/2.\\end{cases}}}\n  A fiducial or objective Bayesian argument can be used to derive the interval estimate\n\n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n        \u00b1\n        \n          \n            \n              1\n              \u2212\n              \n                |\n              \n              \n                X\n                \n                  1\n                \n              \n              \u2212\n              \n                X\n                \n                  2\n                \n              \n              \n                |\n              \n            \n            4\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\bar {X}}\\pm {\\frac {1-|X_{1}-X_{2}|}{4}},}\n  which is also a 50% confidence procedure. Welch showed that the first confidence procedure dominates the second, according to desiderata from confidence interval theory; for every \n  \n    \n      \n        \n          \u03b8\n          \n            1\n          \n        \n        \u2260\n        \u03b8\n      \n    \n    {\\displaystyle \\theta _{1}\\neq \\theta }\n  , the probability that the first procedure contains \n  \n    \n      \n        \n          \u03b8\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n   is less than or equal to the probability that the second procedure contains \n  \n    \n      \n        \n          \u03b8\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  . The average width of the intervals from the first procedure is less than that of the second. Hence, the first procedure is preferred under classical confidence interval theory.\nHowever, when \n  \n    \n      \n        \n          |\n        \n        \n          X\n          \n            1\n          \n        \n        \u2212\n        \n          X\n          \n            2\n          \n        \n        \n          |\n        \n        \u2265\n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle |X_{1}-X_{2}|\\geq 1/2}\n  , intervals from the first procedure are guaranteed to contain the true value \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  : Therefore, the nominal 50% confidence coefficient is unrelated to the uncertainty we should have that a specific interval contains the true value. The second procedure does not have this property.\nMoreover, when the first procedure generates a very short interval, this indicates that \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2}}\n   are very close together and hence only offer the information in a single data point. Yet the first interval will exclude almost all reasonable values of the parameter due to its short width. The second procedure does not have this property.\nThe two counter-intuitive properties of the first procedure\u2014100% coverage when \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2}}\n   are far apart and almost 0% coverage when \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2}}\n   are close together\u2014balance out to yield 50% coverage on average. However, despite the first procedure being optimal, its intervals offer neither an assessment of the precision of the estimate nor an assessment of the uncertainty one should have that the interval contains the true value.\nThis counter-example is used to argue against na\u00efve interpretations of confidence intervals. If a confidence procedure is asserted to have properties beyond that of the nominal coverage (such as relation to precision, or a relationship with Bayesian inference), those properties must be proved; they do not follow from the fact that a procedure is a confidence procedure.\n\n\n=== Confidence procedure for \u03c92 ===\nSteiger suggested a number of confidence procedures for common effect size measures in ANOVA. Morey et al. point out that several of these confidence procedures, including the one for \u03c92, have the property that as the F statistic becomes increasingly small\u2014indicating misfit with all possible values of \u03c92\u2014the confidence interval shrinks and can even contain only the single value \u03c92 = 0; that is, the CI is infinitesimally narrow (this occurs when \n  \n    \n      \n        p\n        \u2265\n        1\n        \u2212\n        \u03b1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle p\\geq 1-\\alpha /2}\n   for a \n  \n    \n      \n        100\n        (\n        1\n        \u2212\n        \u03b1\n        )\n        %\n      \n    \n    {\\displaystyle 100(1-\\alpha )\\%}\n   CI).\nThis behavior is consistent with the relationship between the confidence procedure and significance testing: as F becomes so small that the group means are much closer together than we would expect by chance, a significance test might indicate rejection for most or all values of \u03c92. Hence the interval will be very narrow or even empty (or, by a convention suggested by Steiger, containing only 0). However, this does not indicate that the estimate of \u03c92 is very precise. In a sense, it indicates the opposite: that the trustworthiness of the results themselves may be in doubt. This is contrary to the common interpretation of confidence intervals that they reveal the precision of the estimate.\n\n\n== History ==\nConfidence intervals were introduced by Jerzy Neyman in 1937. Statisticians quickly took to the idea, but adoption by scientists was more gradual. Some authors in medical journals promoted confidence intervals as early as the 1970s. Despite this, confidence intervals were rarely used until the following decade, when they quickly became standard. By the late 1980s, medical journals began to require the reporting of confidence intervals.\n\n\n== See also ==\nCLs upper limits (particle physics)\n68\u201395\u201399.7 rule\nConfidence band, an interval estimate for a curve\nConfidence distribution\nConfidence region, a higher dimensional generalization\nCredence (statistics) \u2013 measure of belief strength used in statisticsPages displaying wikidata descriptions as a fallback\nCredible interval, a Bayesian alternative for interval estimation\nCumulative distribution function-based nonparametric confidence interval\nError bar \u2013 Graphical representations of the variability of data\nEstimation statistics \u2013 Data analysis approach in frequentist statistics\nMargin of error, the CI halfwidth\np-value \u2013 Function of the observed sample results\nPrediction interval, an interval estimate for a random variable\nProbable error\nRobust confidence intervals \u2013 Statistical indicators of the deviation of a samplePages displaying short descriptions of redirect targets\n\n\n=== Confidence interval for specific distributions ===\nConfidence interval for binomial distribution\nConfidence interval for exponent of the power law distribution\nConfidence interval for mean of the exponential distribution\nConfidence interval for mean of the Poisson distribution\nConfidence intervals for mean and variance of the normal distribution\n\n\n== References ==\n\n\n== Bibliography ==\n\nMehta, S. (2014) Statistics Topics ISBN 978-1-4992-7353-3\n\"Confidence estimation\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nMorey, R. D.; Hoekstra, R.; Rouder, J. N.; Lee, M. D.; Wagenmakers, E.-J. (2016). \"The fallacy of placing confidence in confidence intervals\". Psychonomic Bulletin & Review. 23 (1): 103\u2013123. doi:10.3758/s13423-015-0947-8. PMC 4742505. PMID 26450628.\n\n\n=== External links ===\n\nThe Exploratory Software for Confidence Intervals tutorial programs that run under Excel\nConfidence interval calculators for R-Squares, Regression Coefficients, and Regression Intercepts\nWeisstein, Eric W. \"Confidence Interval\". MathWorld.\nCAUSEweb.org Many resources for teaching statistics including Confidence Intervals.\nAn interactive introduction to Confidence Intervals\nConfidence Intervals: Confidence Level, Sample Size, and Margin of Error by Eric Schulz, the Wolfram Demonstrations Project.\nConfidence Intervals in Public Health. Straightforward description with examples and what to do about small sample sizes or rates near 0.", "Prior probability": "A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable.\nIn Bayesian statistics, Bayes' rule prescribes how to update the prior with new information to obtain the posterior probability distribution, which is the conditional distribution of the uncertain quantity given new data. Historically, the choice of priors was often constrained to a conjugate family of a given likelihood function, for that it would result in a tractable posterior of the same family. The widespread availability of Markov chain Monte Carlo methods, however, has made this less of a concern.\nThere are many ways to construct a prior distribution. In some cases, a prior may be determined from past information, such as previous experiments. A prior can also be elicited from the purely subjective assessment of an experienced expert. When no information is available, an uninformative prior may be adopted as justified by the principle of indifference. In modern applications, priors are also often chosen for their mechanical properties, such as regularization and feature selection.The prior distributions of model parameters will often depend on parameters of their own. Uncertainty about these hyperparameters can, in turn, be expressed as hyperprior probability distributions. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:\n\np is a parameter of the underlying system (Bernoulli distribution), and\n\u03b1 and \u03b2 are parameters of the prior distribution (beta distribution); hence hyperparameters.In principle, priors can be decomposed into many conditional levels of distributions, so-called hierarchical priors.\n\n\n== Informative priors ==\nAn informative prior expresses specific, definite information about a variable.\nAn example is a prior distribution for the temperature at noon tomorrow.\nA reasonable approach is to make the prior a normal distribution with expected value equal to today's noontime temperature, with variance equal to the day-to-day variance of atmospheric temperature,\nor a distribution of the temperature for that day of the year.\nThis example has a property in common with many priors,\nnamely, that the posterior from one problem (today's temperature) becomes the prior for another problem (tomorrow's temperature); pre-existing evidence which has already been taken into account is part of the prior and, as more evidence accumulates, the posterior is determined largely by the evidence rather than any original assumption, provided that the original assumption admitted the possibility of what the evidence is suggesting. The terms \"prior\" and \"posterior\" are generally relative to a specific datum or observation.\n\n\n== Weakly informative priors ==\nA weakly informative prior expresses partial information about a variable.  An example is, when setting the prior distribution for the temperature at noon tomorrow in St. Louis, to use a normal distribution with mean 50 degrees Fahrenheit and standard deviation 40 degrees, which very loosely constrains the temperature to the range (10 degrees, 90 degrees) with a small chance of being below -30 degrees or above 130 degrees.  The purpose of a weakly informative prior is for regularization, that is, to keep inferences in a reasonable range.\n\n\n== Uninformative priors ==\nAn uninformative, flat, or diffuse prior expresses vague or general information about a variable. The term \"uninformative prior\" is somewhat of a misnomer. Such a prior might also be called a not very informative prior, or an objective prior, i.e. one that's not subjectively elicited.\nUninformative priors can express \"objective\" information such as \"the variable is positive\" or \"the variable is less than some limit\". The simplest and oldest rule for determining a non-informative prior is the principle of indifference, which assigns equal probabilities to all possibilities. In parameter estimation problems, the use of an uninformative prior typically yields results which are not too different from conventional statistical analysis, as the likelihood function often yields more information than the uninformative prior.\nSome attempts have been made at finding a priori probabilities, i.e. probability distributions in some sense logically required by the nature of one's state of uncertainty; these are a subject of philosophical controversy, with Bayesians being roughly divided into two schools: \"objective Bayesians\", who believe such priors exist in many useful situations, and \"subjective Bayesians\" who believe that in practice priors usually represent subjective judgements of opinion that cannot be rigorously justified (Williamson 2010).  Perhaps the strongest arguments for objective Bayesianism were given by Edwin T. Jaynes, based mainly on the consequences of symmetries and on the principle of maximum entropy.\nAs an example of an a priori prior, due to Jaynes (2003), consider a situation in which one knows a ball has been hidden under one of three cups, A, B, or C, but no other information is available about its location.  In this case a uniform prior of p(A) = p(B) = p(C) = 1/3 seems intuitively like the only reasonable choice.  More formally, we can see that the problem remains the same if we swap around the labels (\"A\", \"B\" and \"C\") of the cups.  It would therefore be odd to choose a prior for which a permutation of the labels would cause a change in our predictions about which cup the ball will be found under; the uniform prior is the only one which preserves this invariance.  If one accepts this invariance principle then one can see that the uniform prior is the logically correct prior to represent this state of knowledge. This prior is \"objective\" in the sense of being the correct choice to represent a particular state of knowledge, but it is not objective in the sense of being an observer-independent feature of the world: in reality the ball exists under a particular cup, and it only makes sense to speak of probabilities in this situation if there is an observer with limited knowledge about the system.As a more contentious example, Jaynes published an argument based on the invariance of the prior under a change of parameters that suggests that the prior representing complete uncertainty about a probability should be the Haldane prior p\u22121(1 \u2212 p)\u22121.  The example Jaynes gives is of finding a chemical in a lab and asking whether it will dissolve in water in repeated experiments.  The Haldane prior gives by far the most weight to \n  \n    \n      \n        p\n        =\n        0\n      \n    \n    {\\displaystyle p=0}\n   and \n  \n    \n      \n        p\n        =\n        1\n      \n    \n    {\\displaystyle p=1}\n  , indicating that the sample will either dissolve every time or never dissolve, with equal probability.  However, if one has observed samples of the chemical to dissolve in one experiment and not to dissolve in another experiment then this prior is updated to the uniform distribution on the interval [0, 1].  This is obtained by applying Bayes' theorem to the data set consisting of one observation of dissolving and one of not dissolving, using the above prior.  The Haldane prior is an improper prior distribution (meaning that it has an infinite mass).  Harold Jeffreys devised a systematic way for designing uninformative priors as e.g., Jeffreys prior p\u22121/2(1 \u2212 p)\u22121/2 for the Bernoulli random variable.\nPriors can be constructed which are proportional to the Haar measure if the parameter space X carries a natural group structure which leaves invariant our Bayesian state of knowledge. This can be seen as a generalisation of the invariance principle used to justify the uniform prior over the three cups in the example above.  For example, in physics we might expect that an experiment will give the same results regardless of our choice of the origin of a coordinate system. This induces the group structure of the translation group on X, which determines the prior probability as a constant improper prior. Similarly, some measurements are naturally invariant to the choice of an arbitrary scale (e.g., whether centimeters or inches are used, the physical results should be equal). In such a case, the scale group is the natural group structure, and the corresponding prior on X is proportional to 1/x. It sometimes matters whether we use the left-invariant or right-invariant Haar measure. For example, the left and right invariant Haar measures on the affine group are not equal. Berger (1985, p. 413) argues that the right-invariant Haar measure is the correct choice.\nAnother idea, championed by Edwin T. Jaynes, is to use the principle of maximum entropy (MAXENT). The motivation is that the Shannon entropy of a probability distribution measures the amount of information contained in the distribution. The larger the entropy, the less information is provided by the distribution. Thus, by maximizing the entropy over a suitable set of probability distributions on X, one finds the distribution that is least informative in the sense that it contains the least amount of information consistent with the constraints that define the set. For example, the maximum entropy prior on a discrete space, given only that the probability is normalized to 1, is the prior that assigns equal probability to each state. And in the continuous case, the maximum entropy prior given that the density is normalized with mean zero and unit variance is the standard normal distribution.  The principle of minimum cross-entropy generalizes MAXENT to the case of \"updating\" an arbitrary prior distribution with suitable constraints in the maximum-entropy sense.\nA related idea, reference priors, was introduced by Jos\u00e9-Miguel Bernardo. Here, the idea is to maximize the expected Kullback\u2013Leibler divergence of the posterior distribution relative to the prior. This maximizes the expected posterior information about X when the prior density is p(x); thus, in some sense, p(x) is the \"least informative\" prior about X. The reference prior is defined in the asymptotic limit, i.e., one considers the limit of the priors so obtained as the number of data points goes to infinity.  In the present case, the KL divergence between the prior and posterior distributions is given by\n\nHere, \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is a sufficient statistic for some parameter \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .  The inner integral is the KL divergence between the posterior \n  \n    \n      \n        p\n        (\n        x\n        \u2223\n        t\n        )\n      \n    \n    {\\displaystyle p(x\\mid t)}\n   and prior \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n   distributions and the result is the weighted mean over all values of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  .   Splitting the logarithm into two parts, reversing the order of integrals in the second part and noting that  does not depend on \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   yields\n\nThe inner integral in the second part is the integral over \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   of the joint density \n  \n    \n      \n        p\n        (\n        x\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle p(x,t)}\n  .  This is the marginal distribution \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n  , so we have\n\nNow we use the concept of entropy which, in the case of probability distributions, is the negative expected value of the logarithm of the probability mass or density function or \n  \n    \n      \n        H\n        (\n        x\n        )\n        =\n        \u2212\n        \u222b\n        p\n        (\n        x\n        )\n        log\n        \u2061\n        [\n        p\n        (\n        x\n        )\n        ]\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle H(x)=-\\int p(x)\\log[p(x)]\\,dx.}\n    Using this in the last equation yields\n\nIn words, KL is the negative expected value over \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   of the entropy of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   conditional on \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   plus the marginal (i.e. unconditional) entropy of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .  In the limiting case where the sample size tends to infinity, the Bernstein-von Mises theorem states that the distribution of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   conditional on a given observed value of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is normal with a variance equal to the reciprocal of the Fisher information at the 'true' value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .  The entropy of a normal density function is equal to half the logarithm of \n  \n    \n      \n        2\n        \u03c0\n        e\n        v\n      \n    \n    {\\displaystyle 2\\pi ev}\n   where \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   is the variance of the distribution.  In this case therefore  where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the arbitrarily large sample size (to which Fisher information is proportional) and \n  \n    \n      \n        x\n        \u2217\n      \n    \n    {\\displaystyle x*}\n   is the 'true' value.  Since this does not depend on \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   it can be taken out of the integral, and as this integral is over a probability space it equals one.  Hence we can write the asymptotic form of KL as\n\nwhere \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is proportional to the (asymptotically large) sample size.  We do not know the value of \n  \n    \n      \n        x\n        \u2217\n      \n    \n    {\\displaystyle x*}\n  .  Indeed, the very idea goes against the philosophy of Bayesian inference in which 'true' values of parameters are replaced by prior and posterior distributions.  So we remove \n  \n    \n      \n        x\n        \u2217\n      \n    \n    {\\displaystyle x*}\n   by replacing it with \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and taking the expected value of the normal entropy, which we obtain by multiplying by \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n   and integrating over \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .  This allows us to combine the logarithms yielding\n\nThis is a quasi-KL divergence (\"quasi\" in the sense that the square root of the Fisher information may be the kernel of an improper distribution).  Due to the minus sign, we need to minimise this in order to maximise the KL divergence with which we started.  The minimum value of the last equation occurs where the two distributions in the logarithm argument, improper or not, do not diverge.  This in turn occurs when the prior distribution is proportional to the square root of the Fisher information of the likelihood function.  Hence in the single parameter case, reference priors and Jeffreys priors are identical, even though Jeffreys has a very different rationale.\nReference priors are often the objective prior of choice in multivariate problems, since other rules (e.g., Jeffreys' rule) may result in priors with problematic behavior.Objective prior distributions may also be derived from other principles, such as information or coding theory (see e.g. minimum description length) or frequentist statistics (so-called probability matching priors). Such methods are used in Solomonoff's theory of inductive inference. Constructing objective priors have been recently introduced in bioinformatics, and specially inference in cancer systems biology, where sample size is limited and a vast amount of prior knowledge is available. In these methods, either an information theory based criterion, such as KL divergence or log-likelihood function for binary supervised learning problems and mixture model problems.Philosophical problems associated with uninformative priors are associated with the choice of an appropriate metric, or measurement scale. Suppose we want a prior for the running speed of a runner who is unknown to us. We could specify, say, a normal distribution as the prior for his speed, but alternatively we could specify a normal prior for the time he takes to complete 100 metres, which is proportional to the reciprocal of the first prior. These are very different priors, but it is not clear which is to be preferred.  Jaynes' often-overlooked method of transformation groups can answer this question in some situations.Similarly, if asked to estimate an unknown proportion between 0 and 1, we might say that all proportions are equally likely, and use a uniform prior. Alternatively, we might say that all orders of magnitude for the proportion are equally likely, the logarithmic prior, which is the uniform prior on the logarithm of proportion. The Jeffreys prior attempts to solve this problem by computing a prior which expresses the same belief no matter which metric is used. The Jeffreys prior for an unknown proportion p is p\u22121/2(1 \u2212 p)\u22121/2, which differs from Jaynes' recommendation.\nPriors based on notions of algorithmic probability are used in inductive inference as a basis for induction in very general settings.\nPractical problems associated with uninformative priors include the requirement that the posterior distribution be proper. The usual uninformative priors on continuous, unbounded variables are improper. This need not be a problem if the posterior distribution is proper. Another issue of importance is that if an uninformative prior is to be used routinely, i.e., with many different data sets, it should have good frequentist properties. Normally a Bayesian would not be concerned with such issues, but it can be important in this situation. For example, one would want any decision rule based on the posterior distribution to be admissible under the adopted loss function. Unfortunately, admissibility is often difficult to check, although some results are known (e.g., Berger and Strawderman 1996). The issue is particularly acute with hierarchical Bayes models; the usual priors (e.g., Jeffreys' prior) may give badly inadmissible decision rules if employed at the higher levels of the hierarchy.\n\n\n== Improper priors ==\nLet events \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        ,\n        \n          A\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          A\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle A_{1},A_{2},\\ldots ,A_{n}}\n   be mutually exclusive and exhaustive. If Bayes' theorem is written as\n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            i\n          \n        \n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2223\n              \n                A\n                \n                  i\n                \n              \n              )\n              P\n              (\n              \n                A\n                \n                  i\n                \n              \n              )\n            \n            \n              \n                \u2211\n                \n                  j\n                \n              \n              P\n              (\n              B\n              \u2223\n              \n                A\n                \n                  j\n                \n              \n              )\n              P\n              (\n              \n                A\n                \n                  j\n                \n              \n              )\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle P(A_{i}\\mid B)={\\frac {P(B\\mid A_{i})P(A_{i})}{\\sum _{j}P(B\\mid A_{j})P(A_{j})}}\\,,}\n  then it is clear that the same result would be obtained if all the prior probabilities P(Ai) and P(Aj) were multiplied by a given constant; the same would be true for a continuous random variable.  If the summation in the denominator converges, the posterior probabilities will still sum (or integrate) to 1 even if the prior values do not, and so the priors may only need to be specified in the correct proportion. Taking this idea further, in many cases the sum or integral of the prior values may not even need to be finite to get sensible answers for the posterior probabilities.  When this is the case, the prior is called an improper prior.  However, the posterior distribution need not be a proper distribution if the prior is improper. This is clear from the case where event B is independent of all of the Aj.\nStatisticians sometimes use improper priors as uninformative priors.  For example, if they need a prior distribution for the mean and variance of a random variable, they may assume p(m, v) ~ 1/v (for v > 0) which would suggest that any value for the mean is \"equally likely\" and that a value for the positive variance becomes \"less likely\" in inverse proportion to its value.  Many authors (Lindley, 1973; De Groot, 1937; Kass and Wasserman, 1996) warn against the danger of over-interpreting those priors since they are not probability densities. The only relevance they have is found in the corresponding posterior, as long as it is well-defined for all observations. (The Haldane prior is a typical counterexample.)\nBy contrast, likelihood functions do not need to be integrated, and a likelihood function that is uniformly 1 corresponds to the absence of data (all models are equally likely, given no data): Bayes' rule multiplies a prior by the likelihood, and an empty product is just the constant likelihood 1. However, without starting with a prior probability distribution, one does not end up getting a posterior probability distribution, and thus cannot integrate or compute expected values or loss. See Likelihood function \u00a7 Non-integrability for details.\n\n\n=== Examples ===\nExamples of improper priors include:\n\nThe uniform distribution on an infinite interval (i.e., a half-line or the entire real line).\nBeta(0,0), the beta distribution for \u03b1=0, \u03b2=0 (uniform distribution on log-odds scale).\nThe logarithmic prior on the positive reals (uniform distribution on log scale).These functions, interpreted as uniform distributions, can also be interpreted as the likelihood function in the absence of data, but are not proper priors.\n\n\n== Prior probability in statistical mechanics ==\nThe a priori probability has an important application in statistical mechanics. The classical version is defined as the ratio of the number of elementary events (e.g. the number of times a die is thrown) to the total number of events\u2014and these considered purely deductively, i.e. without any experimenting. In the case of the die if we look at it on the table without throwing it, each elementary event is reasoned deductively to have the same probability\u2014thus the probability of each outcome of an imaginary throwing of the (perfect) die or simply by counting the number of faces is 1/6. Each face of the die appears with equal probability\u2014probability being a measure defined for each elementary event. The result is different if we throw the die twenty times and ask how many times (out of 20) the number 6 appears on the upper face. In this case time comes into play and we have a different type of probability depending on time or the number of times the die is thrown. On the other hand, the a priori probability is independent of time\u2014you can look at the die on the table as long as you like without touching it and you deduce the probability for the number 6 to appear on the upper face is 1/6.  \nIn statistical mechanics, e.g. that of a gas contained in a finite volume \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  , both the spatial coordinates \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n   and the momentum coordinates \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n   of the individual gas elements (atoms or molecules) are finite in the phase space spanned by these coordinates. In analogy to the case of the die, the a priori probability is here (in the case of a continuum) proportional to the phase space volume element \n  \n    \n      \n        \u0394\n        q\n        \u0394\n        p\n      \n    \n    {\\displaystyle \\Delta q\\Delta p}\n   divided by \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  , and is the number of standing waves (i.e. states) therein, where \n  \n    \n      \n        \u0394\n        q\n      \n    \n    {\\displaystyle \\Delta q}\n   is the range of the variable \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n    and \n  \n    \n      \n        \u0394\n        p\n      \n    \n    {\\displaystyle \\Delta p}\n   is the range of the variable \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   (here for simplicity considered in one dimension). In 1 dimension (length \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  ) this number or statistical weight or a priori weighting  is \n  \n    \n      \n        L\n        \u0394\n        p\n        \n          /\n        \n        h\n      \n    \n    {\\displaystyle L\\Delta p/h}\n  . In customary 3 dimensions (volume \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  ) the corresponding number can be calculated to be \n  \n    \n      \n        V\n        4\n        \u03c0\n        \n          p\n          \n            2\n          \n        \n        \u0394\n        p\n        \n          /\n        \n        \n          h\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle V4\\pi p^{2}\\Delta p/h^{3}}\n  . In order to understand this quantity as giving a number of states in quantum (i.e. wave) mechanics, recall that in quantum mechanics every particle is associated with a matter wave which is the solution of a Schr\u00f6dinger equation. In the case of free particles (of energy \n  \n    \n      \n        \u03f5\n        =\n        \n          \n            \n              p\n            \n          \n          \n            2\n          \n        \n        \n          /\n        \n        2\n        m\n      \n    \n    {\\displaystyle \\epsilon ={\\bf {p}}^{2}/2m}\n  ) like those of a gas in a box of volume \n  \n    \n      \n        V\n        =\n        \n          L\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle V=L^{3}}\n   such a matter wave is explicitly\n\n,where \n  \n    \n      \n        l\n        ,\n        m\n        ,\n        n\n      \n    \n    {\\displaystyle l,m,n}\n   are integers. The number of different  \n  \n    \n      \n        (\n        l\n        ,\n        m\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle (l,m,n)}\n   values and hence states in the region between \n  \n    \n      \n        p\n        ,\n        p\n        +\n        d\n        p\n        ,\n        \n          p\n          \n            2\n          \n        \n        =\n        \n          \n            \n              p\n            \n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle p,p+dp,p^{2}={\\bf {p}}^{2},}\n   is then found to be the above expression \n  \n    \n      \n        V\n        4\n        \u03c0\n        \n          p\n          \n            2\n          \n        \n        d\n        p\n        \n          /\n        \n        \n          h\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle V4\\pi p^{2}dp/h^{3}}\n   by considering the area covered by these points. \nMoreover, in view of the uncertainty relation, which in 1 spatial dimension is\n\n,these states are indistinguishable (i.e. these states do not carry labels). An important consequence is a result known as Liouville's theorem, i.e. the time independence of this phase space volume element and thus of the a priori probability. A time dependence of this quantity would imply known information about the dynamics of the system, and hence would not be an a priori probability. Thus the region\n\nwhen differentiated with respect to time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   yields zero (with the help of Hamilton's equations): The volume at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is the same as at time zero. One describes this also as conservation of information. \nIn the full quantum theory one has an analogous conservation law. In this case, the phase space region is replaced by a subspace of the space of states expressed in terms of a projection operator \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  , and instead of the probability in phase space, one has the probability density\n\nwhere \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the dimensionality of the subspace. The conservation law in this case is expressed by the unitarity of the S-matrix. In either case, the considerations assume a closed isolated system. This closed isolated system is a system with (1) a fixed energy \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   and (2) a fixed number of particles \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   in (c) a state of equilibrium. If one considers a huge number of replicas of this system, one obtains what is called a microcanonical ensemble. It is for this system that one postulates in quantum statistics the \"fundamental postulate of equal a priori probabilities of an isolated system.\" This says that the isolated system in equilibrium occupies each of its accessible states with the same probability. This fundamental postulate therefore allows us to equate the a priori probability to the degeneracy of a system, i.e. to the number of different states with the same energy.\n\n\n=== Example ===\nThe following example illustrates the a priori probability (or a priori weighting) in (a) classical and (b) quantal contexts.\n(a) Classical a priori probability\nConsider the rotational  energy E of a diatomic molecule with moment of inertia I in spherical polar coordinates \n  \n    \n      \n        \u03b8\n        ,\n        \u03d5\n      \n    \n    {\\displaystyle \\theta ,\\phi }\n   (this means \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   above is here \n  \n    \n      \n        \u03b8\n        ,\n        \u03d5\n      \n    \n    {\\displaystyle \\theta ,\\phi }\n  ), i.e.\n\nThe -curve for constant E and \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is an ellipse of area \n\n.By integrating over \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   and \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   the total volume of phase space  covered for constant energy E is \n\n,and hence the classical a priori weighting in the energy range \n  \n    \n      \n        d\n        E\n      \n    \n    {\\displaystyle dE}\n   is\n\n  \n    \n      \n        \u03a9\n        \u221d\n      \n    \n    {\\displaystyle \\Omega \\propto }\n    (phase space volume at \n  \n    \n      \n        E\n        +\n        d\n        E\n      \n    \n    {\\displaystyle E+dE}\n  ) minus  (phase space volume at \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  ) is given by \n  \n    \n      \n        8\n        \n          \n            \u03c0\n          \n          \n            2\n          \n        \n        I\n        d\n        E\n        .\n      \n    \n    {\\displaystyle 8{\\pi }^{2}IdE.}\n  (b) Quantum a priori probability\nAssuming that the number of quantum states in a range \n  \n    \n      \n        \u0394\n        q\n        \u0394\n        p\n      \n    \n    {\\displaystyle \\Delta q\\Delta p}\n   for each direction of motion is given, per element, by a factor \n  \n    \n      \n        \u0394\n        q\n        \u0394\n        p\n        \n          /\n        \n        h\n      \n    \n    {\\displaystyle \\Delta q\\Delta p/h}\n  , the number of states in the energy range dE is, as seen under (a) \n  \n    \n      \n        8\n        \n          \u03c0\n          \n            2\n          \n        \n        I\n        d\n        E\n        \n          /\n        \n        \n          h\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 8\\pi ^{2}IdE/h^{2}}\n   for the rotating diatomic molecule. From wave mechanics it is known that the energy levels of a\nrotating diatomic molecule are given by\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        \n          \n            \n              n\n              (\n              n\n              +\n              1\n              )\n              \n                h\n                \n                  2\n                \n              \n            \n            \n              8\n              \n                \u03c0\n                \n                  2\n                \n              \n              I\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle E_{n}={\\frac {n(n+1)h^{2}}{8\\pi ^{2}I}},}\n  each such level being (2n+1)-fold degenerate. By evaluating \n  \n    \n      \n        d\n        n\n        \n          /\n        \n        d\n        \n          E\n          \n            n\n          \n        \n        =\n        1\n        \n          /\n        \n        (\n        d\n        \n          E\n          \n            n\n          \n        \n        \n          /\n        \n        d\n        n\n        )\n      \n    \n    {\\displaystyle dn/dE_{n}=1/(dE_{n}/dn)}\n  \none obtains\n\nThus by comparison with \n  \n    \n      \n        \u03a9\n      \n    \n    {\\displaystyle \\Omega }\n   above, one finds that the approximate number of states in the range dE is given by the degeneracy, i.e.\n\n  \n    \n      \n        \u03a3\n        \u221d\n        (\n        2\n        n\n        +\n        1\n        )\n        d\n        n\n        .\n      \n    \n    {\\displaystyle \\Sigma \\propto (2n+1)dn.}\n  Thus the a priori weighting in the classical context (a) corresponds to the a priori weighting here in the quantal context (b).\nIn the case of the one-dimensional simple harmonic oscillator of natural frequency \n  \n    \n      \n        \u03bd\n      \n    \n    {\\displaystyle \\nu }\n   one finds correspondingly: (a) , and (b) \n  \n    \n      \n        \u03a3\n        \u221d\n        d\n        n\n      \n    \n    {\\displaystyle \\Sigma \\propto dn}\n   (no degeneracy).\nThus in quantum mechanics the a priori probability is effectively a measure of the degeneracy, i.e. the number of states having the same energy. \nIn the case of the hydrogen atom or Coulomb potential (where the evaluation of the phase space volume for constant energy is more complicated) one knows that the quantum mechanical degeneracy is \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n   with \n  \n    \n      \n        E\n        \u221d\n        1\n        \n          /\n        \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E\\propto 1/n^{2}}\n  . Thus in this case \n  \n    \n      \n        \u03a3\n        \u221d\n        \n          n\n          \n            2\n          \n        \n        d\n        n\n      \n    \n    {\\displaystyle \\Sigma \\propto n^{2}dn}\n  .\n\n\n=== Priori probability and distribution functions ===\nIn statistical mechanics (see any book) one derives the so-called distribution functions \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for various statistics. In the case of Fermi\u2013Dirac statistics and Bose\u2013Einstein statistics these functions are \nrespectively\n\nThese functions are derived for (1) a system in dynamic equilibrium (i.e. under steady, uniform conditions) with (2) total (and huge) number of particles \n  \n    \n      \n        N\n        =\n        \n          \u03a3\n          \n            i\n          \n        \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle N=\\Sigma _{i}n_{i}}\n   (this condition determines the constant \n  \n    \n      \n        \n          \u03f5\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{0}}\n  ), and (3) total energy \n  \n    \n      \n        E\n        =\n        \n          \u03a3\n          \n            i\n          \n        \n        \n          n\n          \n            i\n          \n        \n        \n          \u03f5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle E=\\Sigma _{i}n_{i}\\epsilon _{i}}\n  , i.e. with each of the \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   particles having the energy \n  \n    \n      \n        \n          \u03f5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{i}}\n  . An important aspect in the derivation is the taking into account of the indistinguishability of particles and states in quantum statistics, i.e. there particles and states do not have labels. In the case of fermions, like electrons, obeying the Pauli principle (only one particle per state or none allowed), one has therefore\n\nThus \n  \n    \n      \n        \n          f\n          \n            i\n          \n          \n            F\n            D\n          \n        \n      \n    \n    {\\displaystyle f_{i}^{FD}}\n   is a measure of the fraction of states actually occupied by electrons at energy \n  \n    \n      \n        \n          \u03f5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{i}}\n   and temperature \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  . On the other hand, the a priori probability \n  \n    \n      \n        \n          g\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle g_{i}}\n   is a measure of the number of wave mechanical states available. Hence\n\n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n        =\n        \n          f\n          \n            i\n          \n        \n        \n          g\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle n_{i}=f_{i}g_{i}.}\n  Since \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   is constant under uniform conditions (as many particles as flow out of a volume element also flow in steadily, so that the situation in the element appears static), i.e. independent of time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  , and \n  \n    \n      \n        \n          g\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle g_{i}}\n   is also independent of time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   as shown earlier, we obtain\n\nExpressing this equation in terms of its partial derivatives, one obtains the Boltzmann transport equation. How do coordinates \n  \n    \n      \n        \n          \n            r\n          \n        \n      \n    \n    {\\displaystyle {\\bf {r}}}\n   etc. appear here suddenly? Above no mention was made of electric or other fields. Thus with no such fields present we have the Fermi-Dirac distribution as above. But with such fields present we have this additional dependence of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  .\n\n\n== See also ==\nBase rate\nBayesian epistemology\nStrong prior\n\n\n== Notes ==\n\n\n== References ==\nBauwens, Luc; Lubrano, Michel; Richard, Jean-Fran\u00e7ois (1999). \"Prior Densities for the Regression Model\". Bayesian Inference in Dynamic Econometric Models. Oxford University Press. pp. 94\u2013128. ISBN 0-19-877313-7.\nRubin, Donald B.; Gelman, Andrew; John B. Carlin; Stern, Hal (2003). Bayesian Data Analysis (2nd ed.). Boca Raton: Chapman & Hall/CRC. ISBN 978-1-58488-388-3. MR 2027492.\nBerger, James O. (1985). Statistical decision theory and Bayesian analysis. Berlin: Springer-Verlag. ISBN 978-0-387-96098-2. MR 0804611.\nBerger, James O.; Strawderman, William E. (1996). \"Choice of hierarchical priors: admissibility in estimation of normal means\". Annals of Statistics. 24 (3): 931\u2013951. doi:10.1214/aos/1032526950. MR 1401831. Zbl 0865.62004.\nBernardo, Jose M. (1979). \"Reference Posterior Distributions for Bayesian Inference\". Journal of the Royal Statistical Society, Series B. 41 (2): 113\u2013147. JSTOR 2985028. MR 0547240.\nJames O. Berger; Jos\u00e9 M. Bernardo; Dongchu Sun (2009). \"The formal definition of reference priors\". Annals of Statistics. 37 (2): 905\u2013938. arXiv:0904.0156. Bibcode:2009arXiv0904.0156B. doi:10.1214/07-AOS587. S2CID 3221355.\nJaynes, Edwin T. (2003). Probability Theory: The Logic of Science. Cambridge University Press. ISBN 978-0-521-59271-0.\nWilliamson, Jon (2010). \"review of Bruno di Finetti. Philosophical Lectures on Probability\" (PDF). Philosophia Mathematica. 18 (1): 130\u2013135. doi:10.1093/philmat/nkp019. Archived from the original (PDF) on 2011-06-09. Retrieved 2010-07-02.", "Decision tree model": "A decision tree  is a decision support hierarchical model that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n\n\n== Overview ==\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\nIn decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.\nA decision tree consists of three types of nodes:\nDecision nodes \u2013 typically represented by squares\nChance nodes \u2013 typically represented by circles\nEnd nodes \u2013 typically represented by trianglesDecision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.\nDecision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods.\n\n\n== Decision-tree building blocks ==\n\n\n=== Decision-tree elements ===\n\nDrawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). So used manually they can grow very big and are then often hard to draw fully by hand. Traditionally, decision trees have been created manually \u2013 as the aside example shows \u2013 although increasingly, specialized software is employed.\n\n\n=== Decision rules ===\nThe decision tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form:\n\nif condition1 and condition2 and condition3 then outcome.Decision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.\n\n\n=== Decision tree using flowchart symbols ===\nCommonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. Note there is a conceptual error in the \"Proceed\" calculation of the tree shown below; the error relates to the calculation of \"costs\" awarded in a legal action.\n\n\n=== Analysis example ===\nAnalysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example:\n\nThe basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400K\u2014in that range of risk aversion, the company would need to model a third strategy, \"Neither A nor B\").\nAnother example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the \"Life's a Beach\" example). The example describes two beaches with lifeguards to be distributed on each beach. There is maximum budget B that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach.\n\nIn this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1.\n\nThe decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. But if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings.\n\n\n=== Influence diagram ===\nMuch of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.\n\n\n== Association rule induction ==\n\nDecision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or \"questions\"). Several algorithms to generate such optimal trees have been devised, such as ID3/4/5, CLS, ASSISTANT, and CART.\n\n\n== Advantages and disadvantages ==\nAmong decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:\n\nAre simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\nHave value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\nHelp determine worst, best, and expected values for different scenarios.\nUse a white box model. If a given result is provided by a model.\nCan be combined with other decision techniques.\nThe action of more than one decision-maker can be considered.Disadvantages of decision trees:\n\nThey are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.\nThey are often relatively inaccurate.  Many other predictors perform better with similar data.  This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\nFor data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels.\nCalculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n\n\n== Optimizing a decision tree ==\nA few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. Note that these things are not the only things to consider but only some.\nIncreasing the number of levels of the tree\nThe accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree\u2019s leaves are pure nodes. When a node is pure, it means that all the data in that node belongs to a single class. For example, if the classes in the data set are Cancer and Non-Cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. It is important to note that a deeper tree is not always better when optimizing the decision tree. A deeper tree can influence the runtime in a negative way. If a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm is significantly slower. There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. If the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. Occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. To summarize, observe the points below, we will define the number D as the depth of the tree.\nPossible advantages of increasing the number D:\n\nAccuracy of the decision-tree classification model increases.Possible disadvantages of increasing D\n\n Runtime issues\nDecrease in accuracy in general\nPure node splits while going deeper can cause issues.The ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.\nThe choice of node-splitting functions\nThe node splitting function used can have an impact on improving the accuracy of the decision tree. For example, using the information-gain function may yield better results than using the phi function. The phi function is known as a measure of \u201cgoodness\u201d of a candidate split  at a node in the decision tree. The information gain function is known as a measure of the \u201creduction in entropy\u201d. In the following, we will build two decision trees. One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes.\nThe main advantages and disadvantages of information gain and  phi function\n\nOne major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values.\nAn advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree. It is a very good measure for deciding the relevance of some features.\nThe phi function is also a good measure for deciding the relevance of some features based on \"goodness\".This is the information gain function formula. The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree. \n\n  \n    \n      \n        I\n        g\n        a\n        i\n        n\n        s\n        (\n        s\n        )\n        =\n        H\n        (\n        t\n        )\n        \u2212\n        H\n        (\n        s\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle Igains(s)=H(t)-H(s,t)}\n  \nThis is the phi function formula. The phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.\n\n  \n    \n      \n        \u03a6\n        (\n        s\n        ,\n        t\n        )\n        =\n        (\n        2\n        \u2217\n        \n          P\n          \n            L\n          \n        \n        \u2217\n        \n          P\n          \n            R\n          \n        \n        )\n        \u2217\n        Q\n        (\n        s\n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle \\Phi (s,t)=(2*P_{L}*P_{R})*Q(s|t)}\n  \nWe will set D, which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of cancer and non-cancer samples and the mutation features that the samples either have or do not have. If a sample has a feature mutation then the sample is positive for that mutation, and it will be represented by one. If a sample does not have a feature mutation then the sample is negative for that mutation, and it will be represented by zero.   \nTo summarize, C stands for cancer and NC stands for non-cancer. The letter M stands for mutation, and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.\n\nNow, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced. The first thing to be done is to select the root node. In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. Now assume that M1  has the highest phi function value and M4 has the highest information gain value. The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree. You can observe the root nodes below \n\nNow, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B. For example, if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4, NC3, NC1, C1 in group B.\nDisregarding the mutation chosen for the root node, proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.\n\nNow assume the classification results from both trees are given using a confusion matrix.\nInformation gain confusion matrix:\n\nPhi function confusion matrix:\n\nThe tree using information gain has  the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. For the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below. The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree.\nOther techniques\nThe above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build. One of the techniques is making our decision tree model from a bootstrapped dataset. The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. There are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible.\n\n\n== Evaluating a decision tree ==\nIt is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. All these measurements are derived from the number of true positives, false positives, True negatives, and false negatives obtained when running a set of samples through the decision tree classification model. Also, a confusion matrix can be made to display these results. All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. For example, A low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples.\nLet us take the confusion matrix below. The confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.\n\nWe will now calculate the values accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. \nAccuracy:\n\n  \n    \n      \n        A\n        c\n        c\n        u\n        r\n        a\n        c\n        y\n        =\n        (\n        T\n        P\n        +\n        T\n        N\n        )\n        \n          /\n        \n        (\n        T\n        P\n        +\n        T\n        N\n        +\n        F\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle Accuracy=(TP+TN)/(TP+TN+FP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        +\n        104\n        )\n        \u00f7\n        162\n        =\n        71.60\n        %\n      \n    \n    {\\displaystyle (11+104)\\div 162=71.60\\%}\n  \nSensitivity (TPR \u2013 true positive tate):\n  \n    \n      \n        T\n        P\n        R\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle TPR=TP/(TP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        )\n        \u00f7\n        (\n        11\n        +\n        45\n        )\n        =\n        19.64\n        %\n      \n    \n    {\\displaystyle (11)\\div (11+45)=19.64\\%}\n  \nSpecificity (TNR \u2013 true negative rate):\n\n  \n    \n      \n        T\n        N\n        R\n        =\n        T\n        N\n        \n          /\n        \n        (\n        T\n        N\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle TNR=TN/(TN+FP)}\n  \n\n  \n    \n      \n        105\n        \u00f7\n        (\n        105\n        +\n        1\n        )\n        =\n        99.06\n        %\n      \n    \n    {\\displaystyle 105\\div (105+1)=99.06\\%}\n  \nPrecision (PPV \u2013 positive predictive value):\n\n  \n    \n      \n        P\n        P\n        V\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle PPV=TP/(TP+FP)}\n  \n\n  \n    \n      \n        11\n        \n          /\n        \n        (\n        11\n        +\n        1\n        )\n        =\n        91.66\n        %\n      \n    \n    {\\displaystyle 11/(11+1)=91.66\\%}\n  \nMiss Rate (FNR \u2013 false negative rate):\n\n  \n    \n      \n        F\n        N\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FNR=FN/(FN+TP)}\n  \n\n  \n    \n      \n        45\n        \u00f7\n        (\n        45\n        +\n        11\n        )\n        =\n        80.35\n        %\n      \n    \n    {\\displaystyle 45\\div (45+11)=80.35\\%}\n  \nFalse discovery rate (FDR):\n\n  \n    \n      \n        F\n        D\n        R\n        =\n        F\n        P\n        \n          /\n        \n        (\n        F\n        P\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FDR=FP/(FP+TP)}\n  \n\n  \n    \n      \n        1\n        \u00f7\n        (\n        1\n        +\n        11\n        )\n        =\n        8.30\n        %\n      \n    \n    {\\displaystyle 1\\div (1+11)=8.30\\%}\n  \nFalse omission rate (FOR):\n\n  \n    \n      \n        F\n        O\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        N\n        )\n      \n    \n    {\\displaystyle FOR=FN/(FN+TN)}\n  \n\n  \n    \n      \n        45\n        \u00f7\n        (\n        45\n        +\n        105\n        )\n        =\n        30.00\n        %\n      \n    \n    {\\displaystyle 45\\div (45+105)=30.00\\%}\n  \nOnce we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%. The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. The sensitivity value of 19.64% means that out of everyone who was actually positive for cancer tested positive. If we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative. When it comes to sensitivity and specificity it is important to have a balance between the two values ,so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial. These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nExtensive Decision Tree tutorials and examples\nGallery of example decision trees\nGradient Boosted Decision Trees", "Boosting (machine learning)": "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\nRobert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [\u2026] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\n\n\n== Boosting algorithms ==\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\n\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation) and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious G\u00f6del Prize.\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\n\n\n== Object categorization in computer vision ==\n\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\n\n\n=== Problem of object categorization ===\nObject categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc.  Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks.  However, research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well.\n\n\n=== Status quo for object categorization ===\nThe recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large.  This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well.  Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc.  Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories) have been developed.  One means is by feature sharing and boosting.\n\n\n=== Boosting for binary categorization ===\nAdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows:\n\nForm a large set of simple features\nInitialize weights for training images\nFor T rounds\nNormalize the weights\nFor available features from the set, train a classifier using a single feature and evaluate the training error\nChoose the classifier with the lowest error\nUpdate the weights of the training images: increase if classified wrongly by this classifier, decrease if correctly\nForm the final strong classifier as the linear combination of the T classifiers (coefficient larger if training error is small)After boosting, a classifier constructed from 200 features could yield a 95% detection rate under a \n  \n    \n      \n        \n          10\n          \n            \u2212\n            5\n          \n        \n      \n    \n    {\\displaystyle 10^{-5}}\n   false positive rate.Another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance. This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework.\n\n\n=== Boosting for multi-class categorization ===\nCompared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time.  They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance.\nThe main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest), or by introducing a penalty error from the categories that do not have the feature of the classifier.In the paper \"Sharing visual features for multiclass and multiview object detection\", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper \"Incremental learning of object detectors using a visual shape alphabet\", yet the authors used AdaBoost for boosting.\n\n\n== Convex vs. non-convex boosting algorithms ==\nBoosting algorithms can be based on convex or non-convex optimization algorithms.  Convex algorithms, such as AdaBoost and LogitBoost, can be \"defeated\" by random  noise such that they can't learn basic and learnable combinations of weak hypotheses. This limitation was pointed out by Long & Servedio in 2008.  However, by 2009, multiple authors demonstrated that  boosting algorithms based on non-convex optimization, such as BrownBoost, can learn from noisy datasets and can specifically learn the underlying classifier of the Long\u2013Servedio dataset.\n\n\n== See also ==\n\n\n== Implementations ==\nscikit-learn, an open source machine learning library for Python\nOrange, a free data mining software suite, module Orange.ensemble\nWeka is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost\nR package GBM (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.\njboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees\nR package adabag: Applies Multiclass AdaBoost.M1, AdaBoost-SAMME and Bagging\nR package xgboost: An implementation of gradient boosting for linear and tree-based models.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nYoav Freund and Robert E. Schapire (1997); A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting, Journal of Computer and System Sciences, 55(1):119-139\nRobert E. Schapire and Yoram Singer (1999); Improved Boosting Algorithms Using Confidence-Rated Predictors, Machine Learning, 37(3):297-336\n\n\n== External links ==\nRobert E. Schapire (2003); The Boosting Approach to Machine Learning: An Overview, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification\nZhou Zhi-Hua (2014) Boosting 25 years, CCL 2014 Keynote.\nZhou, Zhihua (2008). \"On the margin explanation of boosting algorithm\" (PDF). In: Proceedings of the 21st Annual Conference on Learning Theory (COLT'08): 479\u2013490.\nZhou, Zhihua (2013). \"On the doubt about margin explanation of boosting\" (PDF). Artificial Intelligence. 203: 1\u201318. arXiv:1009.3613. doi:10.1016/j.artint.2013.07.002. S2CID 2828847.", "Naive Bayes classifier": "In statistics, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,:\u200a718\u200a which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\nIn the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.\n\n\n== Introduction ==\nNaive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter.  A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.\nIn many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods.\nDespite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers. Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification.\n\n\n== Probabilistic model ==\nAbstractly, naive Bayes is a conditional probability model: it assigns probabilities \n  \n    \n      \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        \u2223\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle p(C_{k}\\mid x_{1},\\ldots ,x_{n})}\n   for each of the K possible outcomes or classes \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n   given a problem instance to be classified, represented by a vector \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {x} =(x_{1},\\ldots ,x_{n})}\n   encoding some n features (independent variables).The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. The model must therefore be reformulated to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as:\n\n  \n    \n      \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        \u2223\n        \n          x\n        \n        )\n        =\n        \n          \n            \n              p\n              (\n              \n                C\n                \n                  k\n                \n              \n              )\n               \n              p\n              (\n              \n                x\n              \n              \u2223\n              \n                C\n                \n                  k\n                \n              \n              )\n            \n            \n              p\n              (\n              \n                x\n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}{p(\\mathbf {x} )}}\\,}\n  In plain English, using Bayesian probability terminology, the above equation can be written as\n\n  \n    \n      \n        \n          posterior\n        \n        =\n        \n          \n            \n              \n                prior\n              \n              \u00d7\n              \n                likelihood\n              \n            \n            evidence\n          \n        \n        \n      \n    \n    {\\displaystyle {\\text{posterior}}={\\frac {{\\text{prior}}\\times {\\text{likelihood}}}{\\text{evidence}}}\\,}\n  In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   and the values of the features \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   are given, so that the denominator is effectively constant.\nThe numerator is equivalent to the joint probability model\n\n  \n    \n      \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle p(C_{k},x_{1},\\ldots ,x_{n})\\,}\n  which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                )\n              \n              \n                \n                =\n                p\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                p\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                p\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    2\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    3\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    3\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \u22ef\n              \n            \n            \n              \n              \n                \n                =\n                p\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    2\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    3\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                \u22ef\n                p\n                (\n                \n                  x\n                  \n                    n\n                    \u2212\n                    1\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    n\n                  \n                \n                ,\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    n\n                  \n                \n                \u2223\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}p(C_{k},x_{1},\\ldots ,x_{n})&=p(x_{1},\\ldots ,x_{n},C_{k})\\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2},\\ldots ,x_{n},C_{k})\\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2}\\mid x_{3},\\ldots ,x_{n},C_{k})\\ p(x_{3},\\ldots ,x_{n},C_{k})\\\\&=\\cdots \\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2}\\mid x_{3},\\ldots ,x_{n},C_{k})\\cdots p(x_{n-1}\\mid x_{n},C_{k})\\ p(x_{n}\\mid C_{k})\\ p(C_{k})\\\\\\end{aligned}}}\n  Now the \"naive\" conditional independence assumptions come into play: assume that all features in \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   are mutually independent, conditional on the category \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n  . Under this assumption,\n\n  \n    \n      \n        p\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \n          x\n          \n            i\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          C\n          \n            k\n          \n        \n        )\n        =\n        p\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle p(x_{i}\\mid x_{i+1},\\ldots ,x_{n},C_{k})=p(x_{i}\\mid C_{k})\\,}\n  .Thus, the joint model can be expressed as\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                \u2223\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                )\n              \n              \n                \n                \u221d\n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \u221d\n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                \u2223\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    2\n                  \n                \n                \u2223\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                p\n                (\n                \n                  x\n                  \n                    3\n                  \n                \n                \u2223\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                 \n                \u22ef\n              \n            \n            \n              \n              \n                \n                \u221d\n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                \n                  \u220f\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                p\n                (\n                \n                  x\n                  \n                    i\n                  \n                \n                \u2223\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}p(C_{k}\\mid x_{1},\\ldots ,x_{n})&\\varpropto p(C_{k},x_{1},\\ldots ,x_{n})\\\\&\\varpropto p(C_{k})\\ p(x_{1}\\mid C_{k})\\ p(x_{2}\\mid C_{k})\\ p(x_{3}\\mid C_{k})\\ \\cdots \\\\&\\varpropto p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})\\,,\\end{aligned}}}\n  where \n  \n    \n      \n        \u221d\n      \n    \n    {\\displaystyle \\varpropto }\n   denotes proportionality.\nThis means that under the above independence assumptions, the conditional distribution over the class variable \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is:\n\n  \n    \n      \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        \u2223\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            1\n            Z\n          \n        \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        )\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        p\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle p(C_{k}\\mid x_{1},\\ldots ,x_{n})={\\frac {1}{Z}}p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})}\n  where the evidence \n  \n    \n      \n        Z\n        =\n        p\n        (\n        \n          x\n        \n        )\n        =\n        \n          \u2211\n          \n            k\n          \n        \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        )\n         \n        p\n        (\n        \n          x\n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle Z=p(\\mathbf {x} )=\\sum _{k}p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}\n   is a scaling factor dependent only on \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n  , that is, a constant if the values of the feature variables are known.\n\n\n=== Constructing a classifier from the probability model ===\nThe discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable so as to minimize the probability of misclassification; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}=C_{k}}\n   for some k as follows:\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \n            argmax\n            \n              k\n              \u2208\n              {\n              1\n              ,\n              \u2026\n              ,\n              K\n              }\n            \n          \n        \n         \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        )\n        \n          \n            \u220f\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          p\n          (\n          \n            x\n            \n              i\n            \n          \n          \u2223\n          \n            C\n            \n              k\n            \n          \n          )\n          .\n        \n      \n    \n    {\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\ldots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(x_{i}\\mid C_{k}).}\n  \n\n\n== Parameter estimation and event models ==\nA class's prior may be calculated by assuming equiprobable classes, i.e., \n  \n    \n      \n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        )\n        =\n        \n          \n            1\n            K\n          \n        \n      \n    \n    {\\displaystyle p(C_{k})={\\frac {1}{K}}}\n  , or by calculating an estimate for the class probability from the training set:\n\n  \n    \n      \n        \n          prior for a given class\n        \n        =\n        \n          \n            no. of samples in that class\n            total no. of samples\n          \n        \n        \n      \n    \n    {\\displaystyle {\\text{prior for a given class}}={\\frac {\\text{no. of samples in that class}}{\\text{total no. of samples}}}\\,}\n  To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set.The assumptions on distributions of features are called the \"event model\" of the naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.\n\n\n=== Gaussian naive Bayes ===\nWhen dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  . The data is first segmented by the class, and then the mean and variance of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is computed in each class. Let \n  \n    \n      \n        \n          \u03bc\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\mu _{k}}\n   be the mean of the values in \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   associated with class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n  , and let \n  \n    \n      \n        \n          \u03c3\n          \n            k\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{k}^{2}}\n   be the Bessel corrected variance of the values in \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   associated with class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n  . Suppose one has collected some observation value \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  . Then, the probability density of \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   given a class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n  , i.e., \n  \n    \n      \n        p\n        (\n        x\n        =\n        v\n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle p(x=v\\mid C_{k})}\n  , can be computed by plugging \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   into the equation for a normal distribution parameterized by \n  \n    \n      \n        \n          \u03bc\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\mu _{k}}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            k\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{k}^{2}}\n  . Formally,\n\n  \n    \n      \n        p\n        (\n        x\n        =\n        v\n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n              \n                \u03c3\n                \n                  k\n                \n                \n                  2\n                \n              \n            \n          \n        \n        \n        \n          e\n          \n            \u2212\n            \n              \n                \n                  (\n                  v\n                  \u2212\n                  \n                    \u03bc\n                    \n                      k\n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      k\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle p(x=v\\mid C_{k})={\\frac {1}{\\sqrt {2\\pi \\sigma _{k}^{2}}}}\\,e^{-{\\frac {(v-\\mu _{k})^{2}}{2\\sigma _{k}^{2}}}}}\n  Another common technique for handling continuous values is to use binning to discretize the feature values and obtain a new set of Bernoulli-distributed features. Some literature suggests that this is required in order to use naive Bayes, but it is not true, as the discretization may throw away discriminative information.Sometimes the distribution of class-conditional marginal densities is far from normal. In these cases, kernel density estimation can be used for a more realistic estimate of the marginal densities of each class. This method, which was introduced by John and Langley, can boost the accuracy of the classifier considerably.\n\n\n=== Multinomial naive Bayes ===\nWith a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial \n  \n    \n      \n        (\n        \n          p\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (p_{1},\\dots ,p_{n})}\n   where \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n   is the probability that event i occurs (or K such multinomials in the multiclass case). A feature vector \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {x} =(x_{1},\\dots ,x_{n})}\n   is then a histogram, with \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram x is given by:\n\n  \n    \n      \n        p\n        (\n        \n          x\n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n        =\n        \n          \n            \n              (\n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              )\n              !\n            \n            \n              \n                \u220f\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              !\n            \n          \n        \n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            \n              p\n              \n                k\n                i\n              \n            \n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p(\\mathbf {x} \\mid C_{k})={\\frac {(\\sum _{i=1}^{n}x_{i})!}{\\prod _{i=1}^{n}x_{i}!}}\\prod _{i=1}^{n}{p_{ki}}^{x_{i}}}\n   where \n  \n    \n      \n        \n          p\n          \n            k\n            i\n          \n        \n        :=\n        p\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle p_{ki}:=p(x_{i}\\mid C_{k})}\n  .The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:\n\n  \n    \n      \n        \n          \n            \n              \n                log\n                \u2061\n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                \u2223\n                \n                  x\n                \n                )\n              \n              \n                \n                \u221d\n                log\n                \u2061\n                \n                  (\n                  \n                    p\n                    (\n                    \n                      C\n                      \n                        k\n                      \n                    \n                    )\n                    \n                      \u220f\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        n\n                      \n                    \n                    \n                      \n                        \n                          p\n                          \n                            k\n                            i\n                          \n                        \n                      \n                      \n                        \n                          x\n                          \n                            i\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                log\n                \u2061\n                p\n                (\n                \n                  C\n                  \n                    k\n                  \n                \n                )\n                +\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  x\n                  \n                    i\n                  \n                \n                \u22c5\n                log\n                \u2061\n                \n                  p\n                  \n                    k\n                    i\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                b\n                +\n                \n                  \n                    w\n                  \n                  \n                    k\n                  \n                  \n                    \u22a4\n                  \n                \n                \n                  x\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\log p(C_{k}\\mid \\mathbf {x} )&\\varpropto \\log \\left(p(C_{k})\\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\\right)\\\\&=\\log p(C_{k})+\\sum _{i=1}^{n}x_{i}\\cdot \\log p_{ki}\\\\&=b+\\mathbf {w} _{k}^{\\top }\\mathbf {x} \\end{aligned}}}\n  where \n  \n    \n      \n        b\n        =\n        log\n        \u2061\n        p\n        (\n        \n          C\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle b=\\log p(C_{k})}\n   and \n  \n    \n      \n        \n          w\n          \n            k\n            i\n          \n        \n        =\n        log\n        \u2061\n        \n          p\n          \n            k\n            i\n          \n        \n      \n    \n    {\\displaystyle w_{ki}=\\log p_{ki}}\n  .\nIf a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.\nRennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf\u2013idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.\n\n\n=== Bernoulli naive Bayes ===\nIn the multivariate Bernoulli event model, features are independent Booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   is a boolean expressing the occurrence or absence of the i'th term from the vocabulary, then the likelihood of a document given a class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n   is given by:\n\n  \n    \n      \n        p\n        (\n        \n          x\n        \n        \u2223\n        \n          C\n          \n            k\n          \n        \n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          p\n          \n            k\n            i\n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n        (\n        1\n        \u2212\n        \n          p\n          \n            k\n            i\n          \n        \n        \n          )\n          \n            (\n            1\n            \u2212\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle p(\\mathbf {x} \\mid C_{k})=\\prod _{i=1}^{n}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}}\n  where \n  \n    \n      \n        \n          p\n          \n            k\n            i\n          \n        \n      \n    \n    {\\displaystyle p_{ki}}\n   is the probability of class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n   generating the term \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  . This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.\n\n\n=== Semi-supervised parameter estimation ===\nGiven a way to train a naive Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop:\nGiven a collection \n  \n    \n      \n        D\n        =\n        L\n        \u228e\n        U\n      \n    \n    {\\displaystyle D=L\\uplus U}\n   of labeled samples L and unlabeled samples U, start by training a naive Bayes classifier on L.\nUntil convergence, do:\nPredict class probabilities \n  \n    \n      \n        P\n        (\n        C\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle P(C\\mid x)}\n   for all examples x in \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  .\nRe-train the model based on the probabilities (not the labels) predicted in the previous step.Convergence is determined based on improvement to the model likelihood \n  \n    \n      \n        P\n        (\n        D\n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle P(D\\mid \\theta )}\n  , where \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   denotes the parameters of the naive Bayes model.\nThis training algorithm is an instance of the more general expectation\u2013maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem.\n\n\n== Discussion ==\nDespite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is predicted as more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.\n\n\n=== Relation to logistic regression ===\nIn the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood \n  \n    \n      \n        p\n        (\n        C\n        ,\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle p(C,\\mathbf {x} )}\n  , while logistic regression fits the same probability model to optimize the conditional \n  \n    \n      \n        p\n        (\n        C\n        \u2223\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle p(C\\mid \\mathbf {x} )}\n  .More formally, we have the following:\n\nThe link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as \"predict class \n  \n    \n      \n        \n          C\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle C_{1}}\n   if the odds of \n  \n    \n      \n        p\n        (\n        \n          C\n          \n            1\n          \n        \n        \u2223\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle p(C_{1}\\mid \\mathbf {x} )}\n   exceed those of \n  \n    \n      \n        p\n        (\n        \n          C\n          \n            2\n          \n        \n        \u2223\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle p(C_{2}\\mid \\mathbf {x} )}\n  \". Expressing this in log-space gives:\n\n  \n    \n      \n        log\n        \u2061\n        \n          \n            \n              p\n              (\n              \n                C\n                \n                  1\n                \n              \n              \u2223\n              \n                x\n              \n              )\n            \n            \n              p\n              (\n              \n                C\n                \n                  2\n                \n              \n              \u2223\n              \n                x\n              \n              )\n            \n          \n        \n        =\n        log\n        \u2061\n        p\n        (\n        \n          C\n          \n            1\n          \n        \n        \u2223\n        \n          x\n        \n        )\n        \u2212\n        log\n        \u2061\n        p\n        (\n        \n          C\n          \n            2\n          \n        \n        \u2223\n        \n          x\n        \n        )\n        >\n        0\n      \n    \n    {\\displaystyle \\log {\\frac {p(C_{1}\\mid \\mathbf {x} )}{p(C_{2}\\mid \\mathbf {x} )}}=\\log p(C_{1}\\mid \\mathbf {x} )-\\log p(C_{2}\\mid \\mathbf {x} )>0}\n  The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two \"discrete\" event models, it can be reparametrised as a linear function \n  \n    \n      \n        b\n        +\n        \n          \n            w\n          \n          \n            \u22a4\n          \n        \n        x\n        >\n        0\n      \n    \n    {\\displaystyle b+\\mathbf {w} ^{\\top }x>0}\n  . Obtaining the probabilities is then a matter of applying the logistic function to \n  \n    \n      \n        b\n        +\n        \n          \n            w\n          \n          \n            \u22a4\n          \n        \n        x\n      \n    \n    {\\displaystyle b+\\mathbf {w} ^{\\top }x}\n  , or in the multiclass case, the softmax function.\nDiscriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster.\n\n\n== Examples ==\n\n\n=== Person classification ===\nProblem: classify whether a given person is a male or a female based on the measured features.\nThe features include height, weight, and foot size. Although with NB classifier we treat them as independent, they are not in reality.\n\n\n==== Training ====\nExample training set below.\n\nThe classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances):\n\nThe following example assumes equiprobable classes so that P(male)= P(female) = 0.5. This prior probability distribution might be based on prior knowledge of frequencies in the larger population or in the training set.\n\n\n==== Testing ====\nBelow is a sample to be classified as male or female.\n\nIn order to classify the sample, one has to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n  \n    \n      \n        \n          posterior (male)\n        \n        =\n        \n          \n            \n              P\n              (\n              \n                male\n              \n              )\n              \n              p\n              (\n              \n                height\n              \n              \u2223\n              \n                male\n              \n              )\n              \n              p\n              (\n              \n                weight\n              \n              \u2223\n              \n                male\n              \n              )\n              \n              p\n              (\n              \n                foot size\n              \n              \u2223\n              \n                male\n              \n              )\n            \n            \n              e\n              v\n              i\n              d\n              e\n              n\n              c\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{posterior (male)}}={\\frac {P({\\text{male}})\\,p({\\text{height}}\\mid {\\text{male}})\\,p({\\text{weight}}\\mid {\\text{male}})\\,p({\\text{foot size}}\\mid {\\text{male}})}{evidence}}}\n  For the classification as female the posterior is given by\n\n  \n    \n      \n        \n          posterior (female)\n        \n        =\n        \n          \n            \n              P\n              (\n              \n                female\n              \n              )\n              \n              p\n              (\n              \n                height\n              \n              \u2223\n              \n                female\n              \n              )\n              \n              p\n              (\n              \n                weight\n              \n              \u2223\n              \n                female\n              \n              )\n              \n              p\n              (\n              \n                foot size\n              \n              \u2223\n              \n                female\n              \n              )\n            \n            \n              e\n              v\n              i\n              d\n              e\n              n\n              c\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{posterior (female)}}={\\frac {P({\\text{female}})\\,p({\\text{height}}\\mid {\\text{female}})\\,p({\\text{weight}}\\mid {\\text{female}})\\,p({\\text{foot size}}\\mid {\\text{female}})}{evidence}}}\n  The evidence (also termed normalizing constant) may be calculated:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  evidence\n                \n                =\n                P\n                (\n                \n                  male\n                \n                )\n                \n                p\n                (\n                \n                  height\n                \n                \u2223\n                \n                  male\n                \n                )\n                \n                p\n                (\n                \n                  weight\n                \n                \u2223\n                \n                  male\n                \n                )\n                \n                p\n                (\n                \n                  foot size\n                \n                \u2223\n                \n                  male\n                \n                )\n              \n            \n            \n              \n                +\n                P\n                (\n                \n                  female\n                \n                )\n                \n                p\n                (\n                \n                  height\n                \n                \u2223\n                \n                  female\n                \n                )\n                \n                p\n                (\n                \n                  weight\n                \n                \u2223\n                \n                  female\n                \n                )\n                \n                p\n                (\n                \n                  foot size\n                \n                \u2223\n                \n                  female\n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{evidence}}=P({\\text{male}})\\,p({\\text{height}}\\mid {\\text{male}})\\,p({\\text{weight}}\\mid {\\text{male}})\\,p({\\text{foot size}}\\mid {\\text{male}})\\\\+P({\\text{female}})\\,p({\\text{height}}\\mid {\\text{female}})\\,p({\\text{weight}}\\mid {\\text{female}})\\,p({\\text{foot size}}\\mid {\\text{female}})\\end{aligned}}}\n  However, given the sample, the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored.  The probability distribution for the sex of the sample can now be determined:\n\n  \n    \n      \n        P\n        (\n        \n          male\n        \n        )\n        =\n        0.5\n      \n    \n    {\\displaystyle P({\\text{male}})=0.5}\n  \n\n  \n    \n      \n        p\n        (\n        \n          height\n        \n        \u2223\n        \n          male\n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \n              \n                \u2212\n                (\n                6\n                \u2212\n                \u03bc\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                2\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        \u2248\n        1.5789\n      \n    \n    {\\displaystyle p({\\text{height}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(6-\\mu )^{2}}{2\\sigma ^{2}}}\\right)\\approx 1.5789}\n  ,where \n  \n    \n      \n        \u03bc\n        =\n        5.855\n      \n    \n    {\\displaystyle \\mu =5.855}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n        =\n        3.5033\n        \u22c5\n        \n          10\n          \n            \u2212\n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}=3.5033\\cdot 10^{-2}}\n   are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here \u2013 it is a probability density rather than a probability, because height is a continuous variable.\n\n  \n    \n      \n        p\n        (\n        \n          weight\n        \n        \u2223\n        \n          male\n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \n              \n                \u2212\n                (\n                130\n                \u2212\n                \u03bc\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                2\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        5.9881\n        \u22c5\n        \n          10\n          \n            \u2212\n            6\n          \n        \n      \n    \n    {\\displaystyle p({\\text{weight}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(130-\\mu )^{2}}{2\\sigma ^{2}}}\\right)=5.9881\\cdot 10^{-6}}\n  \n\n  \n    \n      \n        p\n        (\n        \n          foot size\n        \n        \u2223\n        \n          male\n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \n              \n                \u2212\n                (\n                8\n                \u2212\n                \u03bc\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                2\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        1.3112\n        \u22c5\n        \n          10\n          \n            \u2212\n            3\n          \n        \n      \n    \n    {\\displaystyle p({\\text{foot size}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(8-\\mu )^{2}}{2\\sigma ^{2}}}\\right)=1.3112\\cdot 10^{-3}}\n  \n\n  \n    \n      \n        \n          posterior numerator (male)\n        \n        =\n        \n          their product\n        \n        =\n        6.1984\n        \u22c5\n        \n          10\n          \n            \u2212\n            9\n          \n        \n      \n    \n    {\\displaystyle {\\text{posterior numerator (male)}}={\\text{their product}}=6.1984\\cdot 10^{-9}}\n  \n  \n    \n      \n        P\n        (\n        \n          female\n        \n        )\n        =\n        0.5\n      \n    \n    {\\displaystyle P({\\text{female}})=0.5}\n  \n\n  \n    \n      \n        p\n        (\n        \n          height\n        \n        \u2223\n        \n          female\n        \n        )\n        =\n        2.23\n        \u22c5\n        \n          10\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle p({\\text{height}}\\mid {\\text{female}})=2.23\\cdot 10^{-1}}\n  \n\n  \n    \n      \n        p\n        (\n        \n          weight\n        \n        \u2223\n        \n          female\n        \n        )\n        =\n        1.6789\n        \u22c5\n        \n          10\n          \n            \u2212\n            2\n          \n        \n      \n    \n    {\\displaystyle p({\\text{weight}}\\mid {\\text{female}})=1.6789\\cdot 10^{-2}}\n  \n\n  \n    \n      \n        p\n        (\n        \n          foot size\n        \n        \u2223\n        \n          female\n        \n        )\n        =\n        2.8669\n        \u22c5\n        \n          10\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle p({\\text{foot size}}\\mid {\\text{female}})=2.8669\\cdot 10^{-1}}\n  \n\n  \n    \n      \n        \n          posterior numerator (female)\n        \n        =\n        \n          their product\n        \n        =\n        5.3778\n        \u22c5\n        \n          10\n          \n            \u2212\n            4\n          \n        \n      \n    \n    {\\displaystyle {\\text{posterior numerator (female)}}={\\text{their product}}=5.3778\\cdot 10^{-4}}\n  Since posterior numerator is greater in the female case, the prediction is that the sample is female.\n\n\n=== Document classification ===\nHere is a worked example of naive Bayesian classification to the document classification problem.\nConsider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as\n\n  \n    \n      \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        C\n        )\n        \n      \n    \n    {\\displaystyle p(w_{i}\\mid C)\\,}\n  (For this treatment, things are further simplified by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)\nThen the probability that a given document D contains all of the words \n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}}\n  , given a class C, is\n\n  \n    \n      \n        p\n        (\n        D\n        \u2223\n        C\n        )\n        =\n        \n          \u220f\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        C\n        )\n        \n      \n    \n    {\\displaystyle p(D\\mid C)=\\prod _{i}p(w_{i}\\mid C)\\,}\n  The question that has to be answered is: \"what is the probability that a given document D belongs to a given class C?\" In other words, what is \n  \n    \n      \n        p\n        (\n        C\n        \u2223\n        D\n        )\n        \n      \n    \n    {\\displaystyle p(C\\mid D)\\,}\n  ?\nNow by definition\n\n  \n    \n      \n        p\n        (\n        D\n        \u2223\n        C\n        )\n        =\n        \n          \n            \n              p\n              (\n              D\n              \u2229\n              C\n              )\n            \n            \n              p\n              (\n              C\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(D\\mid C)={p(D\\cap C) \\over p(C)}}\n  and\n\n  \n    \n      \n        p\n        (\n        C\n        \u2223\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              D\n              \u2229\n              C\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(C\\mid D)={p(D\\cap C) \\over p(D)}}\n  Bayes' theorem manipulates these into a statement of probability in terms of likelihood.\n\n  \n    \n      \n        p\n        (\n        C\n        \u2223\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              C\n              )\n              \n              p\n              (\n              D\n              \u2223\n              C\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(C\\mid D)={\\frac {p(C)\\,p(D\\mid C)}{p(D)}}}\n  Assume for the moment that there are only two mutually exclusive classes, S and \u00acS (e.g. spam and not spam), such that every element (email) is in either one or the other;\n\n  \n    \n      \n        p\n        (\n        D\n        \u2223\n        S\n        )\n        =\n        \n          \u220f\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        S\n        )\n        \n      \n    \n    {\\displaystyle p(D\\mid S)=\\prod _{i}p(w_{i}\\mid S)\\,}\n  and\n\n  \n    \n      \n        p\n        (\n        D\n        \u2223\n        \u00ac\n        S\n        )\n        =\n        \n          \u220f\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        \u00ac\n        S\n        )\n        \n      \n    \n    {\\displaystyle p(D\\mid \\neg S)=\\prod _{i}p(w_{i}\\mid \\neg S)\\,}\n  Using the Bayesian result above, one can write:\n\n  \n    \n      \n        p\n        (\n        S\n        \u2223\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n        \n        \n          \u220f\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        S\n        )\n      \n    \n    {\\displaystyle p(S\\mid D)={p(S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\mid S)}\n  \n  \n    \n      \n        p\n        (\n        \u00ac\n        S\n        \u2223\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              \u00ac\n              S\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n        \n        \n          \u220f\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        \u00ac\n        S\n        )\n      \n    \n    {\\displaystyle p(\\neg S\\mid D)={p(\\neg S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\mid \\neg S)}\n  Dividing one by the other gives:\n\n  \n    \n      \n        \n          \n            \n              p\n              (\n              S\n              \u2223\n              D\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              \u2223\n              D\n              )\n            \n          \n        \n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n              \n              \n                \u220f\n                \n                  i\n                \n              \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              S\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              )\n              \n              \n                \u220f\n                \n                  i\n                \n              \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              \u00ac\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {p(S\\mid D) \\over p(\\neg S\\mid D)}={p(S)\\,\\prod _{i}p(w_{i}\\mid S) \\over p(\\neg S)\\,\\prod _{i}p(w_{i}\\mid \\neg S)}}\n  Which can be re-factored as:\n\n  \n    \n      \n        \n          \n            \n              p\n              (\n              S\n              \u2223\n              D\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              \u2223\n              D\n              )\n            \n          \n        \n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              )\n            \n          \n        \n        \n        \n          \u220f\n          \n            i\n          \n        \n        \n          \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              S\n              )\n            \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              \u00ac\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {p(S\\mid D) \\over p(\\neg S\\mid D)}={p(S) \\over p(\\neg S)}\\,\\prod _{i}{p(w_{i}\\mid S) \\over p(w_{i}\\mid \\neg S)}}\n  Thus, the probability ratio p(S | D) / p(\u00acS | D) can be expressed in terms of a series of likelihood ratios.\nThe actual probability p(S | D) can be easily computed from log (p(S | D) / p(\u00acS | D)) based on the observation that p(S | D) + p(\u00acS | D) = 1.\nTaking the logarithm of all these ratios, one obtains:\n\n  \n    \n      \n        ln\n        \u2061\n        \n          \n            \n              p\n              (\n              S\n              \u2223\n              D\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              \u2223\n              D\n              )\n            \n          \n        \n        =\n        ln\n        \u2061\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              )\n            \n          \n        \n        +\n        \n          \u2211\n          \n            i\n          \n        \n        ln\n        \u2061\n        \n          \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              S\n              )\n            \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              \u2223\n              \u00ac\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\ln {p(S\\mid D) \\over p(\\neg S\\mid D)}=\\ln {p(S) \\over p(\\neg S)}+\\sum _{i}\\ln {p(w_{i}\\mid S) \\over p(w_{i}\\mid \\neg S)}}\n  (This technique of \"log-likelihood ratios\" is a common technique in statistics.\nIn the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)\nFinally, the document can be classified as follows.  It is spam if \n  \n    \n      \n        p\n        (\n        S\n        \u2223\n        D\n        )\n        >\n        p\n        (\n        \u00ac\n        S\n        \u2223\n        D\n        )\n      \n    \n    {\\displaystyle p(S\\mid D)>p(\\neg S\\mid D)}\n   (i. e., \n  \n    \n      \n        ln\n        \u2061\n        \n          \n            \n              p\n              (\n              S\n              \u2223\n              D\n              )\n            \n            \n              p\n              (\n              \u00ac\n              S\n              \u2223\n              D\n              )\n            \n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle \\ln {p(S\\mid D) \\over p(\\neg S\\mid D)}>0}\n  ), otherwise it is not spam.\n\n\n== See also ==\nAODE\nBayes classifier\nBayesian spam filtering\nBayesian network\nRandom naive Bayes\nLinear classifier\nLogistic regression\nPerceptron\nTake-the-best heuristic\n\n\n== References ==\n\n\n== Further reading ==\nDomingos, Pedro; Pazzani, Michael (1997). \"On the optimality of the simple Bayesian classifier under zero-one loss\". Machine Learning. 29 (2/3): 103\u2013137. doi:10.1023/A:1007413511361.\nWebb, G. I.; Boughton, J.; Wang, Z. (2005). \"Not So Naive Bayes: Aggregating One-Dependence Estimators\". Machine Learning. 58 (1): 5\u201324. doi:10.1007/s10994-005-4258-6.\nMozina, M.; Demsar, J.; Kattan, M.; Zupan, B. (2004). Nomograms for Visualization of Naive Bayesian Classifier (PDF). Proc. PKDD-2004. pp. 337\u2013348.\nMaron, M. E. (1961). \"Automatic Indexing: An Experimental Inquiry\". Journal of the ACM. 8 (3): 404\u2013417. doi:10.1145/321075.321084. hdl:2027/uva.x030748531. S2CID 6692916.\nMinsky, M. (1961). Steps toward Artificial Intelligence. Proc. IRE. Vol. 49. pp. 8\u201330.\n\n\n== External links ==\nBook Chapter: Naive Bayes text classification, Introduction to Information Retrieval\nNaive Bayes for Text Classification with Unbalanced Classes\nBenchmark results of Naive Bayes implementations\nHierarchical Naive Bayes Classifiers for uncertain data (an extension of the Naive Bayes classifier).SoftwareNaive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout, Mallet, NLTK, Orange, scikit-learn and Weka.\nIMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier.\nAn interactive Microsoft Excel spreadsheet Naive Bayes implementation using VBA (requires enabled macros) with viewable source code.\njBNC - Bayesian Network Classifier Toolbox\nStatistical Pattern Recognition Toolbox for Matlab.\nifile - the first freely available (Naive) Bayesian mail/spam filter\nNClassifier - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J.\nClassifier4J - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier.\nJNBC Naive Bayes Classifier running in-memory or using fast key-value stores (MapDB, LevelDB or RocksDB).\nBlayze - Blayze is a minimal JVM library for Naive Bayes classification written in Kotlin.", "Feedforward neural network": "A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from its  descendant: recurrent neural networks.\nThe feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction\u2014forward\u2014from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.\n\n\n== Linear neural network ==\nThe simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\n\n\n== Single-layer perceptron ==\n\nThe single-layer perceptron combines a linear neural network with a threshold function. If the output value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are often called  linear threshold units. In the literature the term perceptron often refers to networks consisting of just one of these units. Similar \"neurons\" were described in physics by Ernst Ising and Wilhelm Lenz for the Ising model in the 1920s, and by Warren McCulloch and Walter Pitts in the 1940s.\nA perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two.\nPerceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\nSingle-layer perceptrons are only capable of learning linearly separable patterns; in 1969 in a famous monograph titled Perceptrons, Marvin Minsky and Seymour Papert showed that it was impossible for a single-layer perceptron network to learn an XOR function. Nonetheless, it was known that multi-layer perceptrons (MLPs) are capable of producing any possible boolean function. For example, already in 1967, Shun'ichi Amari trained an MLP by stochastic gradient descent.Although a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can approximate any continuous function from a compact interval of the real numbers into the interval [-1,1]. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass \"A learning rule for very simple universal approximators consisting of a single layer of perceptrons\".A single-layer neural network can compute a continuous output instead of a step function. A common choice is the so-called logistic function:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                e\n                \n                  \u2212\n                  x\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{1+e^{-x}}}}\n  With this choice, the single-layer network is identical to the logistic regression model, widely used in statistical modeling. The logistic function is one of the family of functions called sigmoid functions because their S-shaped graphs resemble the final-letter lower case of the Greek letter Sigma. It has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\n\n  \n    \n      \n        \n          f\n          \u2032\n        \n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n        (\n        1\n        \u2212\n        f\n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle f'(x)=f(x)(1-f(x))}\n  .(The fact that \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   satisfies the differential equation above can easily be shown by applying the chain rule.)\nIf single-layer neural network activation function is modulo 1, then this network can solve XOR problem with a single neuron.\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        x\n        \n        mod\n        \n        \n        1\n      \n    \n    {\\displaystyle f(x)=x\\mod 1}\n  \n\n  \n    \n      \n        \n          f\n          \u2032\n        \n        (\n        x\n        )\n        =\n        1\n      \n    \n    {\\displaystyle f'(x)=1}\n  \n\n\n== Multi-layer perceptron ==\n\nThis class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a sigmoid function as an activation function. However sigmoidal activation functions have very small derivative values outside a small range and do not work well in deep neural networks due to the vanishing gradient problem.\nThe universal approximation theorem for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds for a wide range of activation functions, e.g. for the sigmoidal functions.\nMulti-layer networks use a variety of learning techniques. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965. They trained their MLP layer by layer, incrementally adding layers until the remaining error was acceptable, continually pruning superfluous hidden units with the help of a separate validation set.The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations required to classify non-linearily separable pattern classes.Today, the most popular method for training MLPs is back-propagation. The terminology \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. Modern backpropagation is actually Seppo Linnainmaa's general reverse mode of automatic differentiation (1970) for discrete connected networks of nested differentiable functions. It is an efficient application of the chain rule (derived by Gottfried Wilhelm Leibniz in 1673) to networks of differentiable nodes. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985, David E. Rumelhart et al. published an experimental analysis of the technique. Many improvements have been implemented in subsequent decades.During backpropagation, the output values are compared with the correct answer to compute the value of some predefined error-function. The error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has learned a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent, due to Augustin-Louis Cauchy, who first suggested it in 1847. \nFor this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.\nIn general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available. The danger is that the network overfits the training data and fails to capture the true statistical process generating the data.  Computational learning theory is concerned with training classifiers on a limited amount of data.  In the context of neural networks a simple heuristic, called early stopping, often ensures that the network will generalize well to examples not in the training set.\nOther typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a local minimum of the error function. Today, there are practical methods that make back-propagation in multi-layer perceptrons the tool of choice for many machine learning tasks.\nOne also can use a series of independent neural networks moderated by some intermediary, a similar behavior that happens in brain. These neurons can perform separably and handle a large task, and the results can be finally combined.\n\n\n== Other feedforward networks ==\nMore generally, any directed acyclic graph may be used for a feedforward network, with some nodes (with no parents) designated as inputs, and some nodes (with no children) designated as outputs. These can be viewed as multilayer networks where some edges skip layers, either counting layers backwards from the outputs or forwards from the inputs. Various activation functions can be used, and there can be relations between weights, as in convolutional neural networks.\nExamples of other feedforward networks include radial basis function networks, which use a different activation function.\nSometimes multi-layer perceptron is used loosely to refer to any feedforward neural network, while in other cases it is restricted to specific ones (e.g., with specific activation functions, or with fully connected layers, or trained by the perceptron algorithm).\n\n\n== See also ==\nHopfield network\nConvolutional neural network\nFeed-forward\nBackpropagation\nRprop\n\n\n== References ==\n\n\n== External links ==\nFeedforward neural networks tutorial\nFeedforward Neural Network: Example\nFeedforward Neural Networks: An Introduction", "Data mining": "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics\u2014or, when referring to actual methods, artificial intelligence and machine learning\u2014are more appropriate.\nThe actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.\nThe difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n\n\n== Etymology ==\nIn the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).\nThe term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, a phrase \"database mining\"\u2122, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities. Currently, the terms data mining and knowledge discovery are used interchangeably.\n\n\n== Background ==\nThe manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.\n\n\n== Process ==\nThe knowledge discovery in databases (KDD) process is commonly defined with the stages:\n\nSelection\nPre-processing\nTransformation\nData mining\nInterpretation/evaluation.It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:\n\nBusiness understanding\nData understanding\nData preparation\nModeling\nEvaluation\nDeploymentor a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3\u20134 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.\n\n\n=== Pre-processing ===\nBefore data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\n\n\n=== Data mining ===\nData mining involves six common classes of tasks:\nAnomaly detection (outlier/change/deviation detection) \u2013 The identification of unusual data records, that might be interesting or data errors that require further investigation.\nAssociation rule learning (dependency modeling) \u2013 Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.\nClustering \u2013 is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\nClassification \u2013 is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\nRegression \u2013 attempts to find a function that models the data with the least error that is, for estimating the relationships among data or datasets.\nSummarization \u2013 providing a more compact representation of the data set, including visualization and report generation.\n\n\n=== Results validation ===\nData mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split\u2014when applicable at all\u2014may not be sufficient to prevent this from happening.The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.\nIf the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\n\n\n== Research ==\nThe premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".Computer science conferences on data mining include:\n\nCIKM Conference \u2013 ACM Conference on Information and Knowledge Management\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases\nKDD Conference \u2013 ACM SIGKDD Conference on Knowledge Discovery and Data MiningData mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases.\n\n\n== Standards ==\nThere have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\nFor exchanging the extracted models\u2014in particular for use in predictive analytics\u2014the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.\n\n\n== Notable uses ==\n\nData mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.\n\n\n== Privacy concerns and ethics ==\nWhile the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise).The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). This is not data mining per se, but a result of the preparation of data before\u2014and for the purposes of\u2014the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.It is recommended to be aware of the following before data are collected:\nThe purpose of the data collection and any (known) data mining projects.\nHow the data will be used.\nWho will be able to mine the data and use the data and their derivatives.\nThe status of security surrounding access to the data.\nHow collected data can be updated.Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even \"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\nemotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\nprescription information to data mining companies who in turn provided the data\nto pharmaceutical companies.\n\n\n=== Situation in Europe ===\nEurope has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.\u2013E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.\n\n\n=== Situation in the United States ===\nIn the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\" This underscores the necessity for data anonymity in data aggregation and mining practices.\nU.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\n\n\n== Copyright law ==\n\n\n=== Situation in Europe ===\nUnder European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright\u2014but database rights may exist, so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception. The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020.The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.\n\n\n=== Situation in the United States ===\nUS copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed\u2014one being text and data mining.\n\n\n== Software ==\n\n\n=== Free open-source data mining software and applications ===\nThe following applications are available under free/open-source licenses. Public access to application source code is also available.\n\nCarrot2: Text and search results clustering framework.\nChemicalize.org: A chemical structure miner and web search engine.\nELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.\nGATE: a natural language processing and language engineering tool.\nKNIME: The Konstanz Information Miner, a user-friendly and comprehensive data analytics framework.\nMassive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.\nMEPX: cross-platform tool for regression and classification problems based on a Genetic Programming variant.\nmlpack: a collection of ready-to-use machine learning algorithms written in the C++ language.\nNLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.\nOpenNN: Open neural networks library.\nOrange: A component-based data mining and machine learning software suite written in the Python language.\nPSPP: Data mining and statistics software under the GNU Project similar to SPSS\nR: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.\nscikit-learn: An open-source machine learning library for the Python programming language;\nTorch: An open-source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.\nUIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video \u2013 originally developed by IBM.\nWeka: A suite of machine learning software applications written in the Java programming language.\n\n\n=== Proprietary data-mining software and applications ===\nThe following applications are available under proprietary licenses.\n\nAngoss KnowledgeSTUDIO: data mining tool\nLIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.\nPolyAnalyst: data and text mining software by Megaputer Intelligence.\nMicrosoft Analysis Services: data mining software provided by Microsoft.\nNetOwl: suite of multilingual text and entity analytics products that enable data mining.\nOracle Data Mining: data mining software by Oracle Corporation.\nPSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.\nQlucore Omics Explorer: data mining software.\nRapidMiner: An environment for machine learning and data mining experiments.\nSAS Enterprise Miner: data mining software provided by the SAS Institute.\nSPSS Modeler: data mining software provided by IBM.\nSTATISTICA Data Miner: data mining software provided by StatSoft.\nTanagra: Visualisation-oriented data mining software, also for teaching.\nVertica: data mining software provided by Hewlett-Packard.\nGoogle Cloud Platform: automated custom ML models managed by Google.\nAmazon SageMaker: managed service provided by Amazon for creating & productionising custom ML models.\n\n\n== See also ==\nMethods\nApplication domains\nApplication examples\n\nRelated topicsFor more information about extracting information out of data (as opposed to analyzing data), see:\n\nOther resourcesInternational Journal of Data Warehousing and Mining\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nKnowledge Discovery Software at Curlie\nData Mining Tool Vendors at Curlie", "Interquartile range": "In descriptive statistics, the interquartile range (IQR) is a measure of statistical dispersion, which is the spread of the data. The IQR may also be called the midspread, middle 50%, fourth spread, or H\u2011spread. It is defined as the difference between the 75th and 25th percentiles of the data. To calculate the IQR, the data set is divided into quartiles, or four rank-ordered even parts via linear interpolation. These quartiles are denoted by Q1 (also called the lower quartile), Q2 (the median), and Q3 (also called the upper quartile). The lower quartile corresponds with the 25th percentile and the upper quartile corresponds with the 75th percentile, so IQR = Q3 \u2212  Q1.The IQR is an example of a trimmed estimator, defined as the 25% trimmed range, which enhances the accuracy of dataset statistics by dropping lower contribution, outlying points. It is also used as a robust measure of scale It can be clearly visualized by the box on a box plot.\n\n\n== Use ==\nUnlike total range, the interquartile range has a breakdown point of 25%, and is thus often preferred to the total range.\nThe IQR is used to build box plots, simple graphical representations of a probability distribution.\nThe IQR is used in businesses as a marker for their income rates.\nFor a symmetric distribution (where the median equals the midhinge, the average of the first and third quartiles), half the IQR equals the median absolute deviation (MAD).\nThe median is the corresponding measure of central tendency.\nThe IQR can be used to identify outliers (see below). The IQR also may indicate the skewness of the dataset. The quartile deviation or semi-interquartile range is defined as half the IQR.\n\n\n== Algorithm ==\nThe IQR of a set of values is calculated as the difference between the upper and lower quartiles, Q3 and Q1. Each quartile is a median calculated as follows.\nGiven an even 2n or odd 2n+1 number of values\n\nfirst quartile Q1 = median of the n smallest values\nthird quartile Q3 = median of the n largest valuesThe second quartile Q2 is the same as the ordinary median.\n\n\n== Examples ==\n\n\n=== Data set in a table ===\nThe following table has 13 rows, and follows the rules for the odd number of entries.\n\nFor the data in this table the interquartile range is IQR = Q3 \u2212 Q1 = 119 - 31 = 88.\n\n\n=== Data set in a plain-text box plot ===\n                    \n                             +\u2212\u2212\u2212\u2212\u2212+\u2212+     \n               * |\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212|     | |\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212|\n                             +\u2212\u2212\u2212\u2212\u2212+\u2212+    \n                    \n +\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212\u2212\u2212+   number line\n 0   1   2   3   4   5   6   7   8   9   10  11  12\n  \n\nFor the data set in this box plot:\n\nlower (first) quartile Q1 = 7\nmedian (second quartile) Q2 = 8.5\nupper (third) quartile Q3 = 9\ninterquartile range, IQR = Q3 - Q1 = 2\nlower 1.5*IQR whisker = Q1 - 1.5 * IQR = 7 - 3 = 4. (If there is no data point at 4, then the lowest point greater than 4.)\nupper 1.5*IQR whisker = Q3 + 1.5 * IQR = 9 + 3 = 12. (If there is no data point at 12, then the highest point less than 12.)This means the 1.5*IQR whiskers can be uneven in lengths. The median, minimum, maximum, and the first and third quartile constitute the Five-number summary.\n\n\n== Distributions ==\nThe interquartile range of a continuous distribution can be calculated by integrating the probability density function (which yields the cumulative distribution function\u2014any other means of calculating the CDF will also work). The lower quartile, Q1, is a number such that integral of the PDF from -\u221e to Q1 equals 0.25, while the upper quartile, Q3, is such a number that the integral from -\u221e to Q3 equals 0.75; in terms of the CDF, the quartiles can be defined as follows:\n\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        =\n        \n          \n            CDF\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        0.25\n        )\n        ,\n      \n    \n    {\\displaystyle Q_{1}={\\text{CDF}}^{-1}(0.25),}\n  \n  \n    \n      \n        \n          Q\n          \n            3\n          \n        \n        =\n        \n          \n            CDF\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        0.75\n        )\n        ,\n      \n    \n    {\\displaystyle Q_{3}={\\text{CDF}}^{-1}(0.75),}\n  where CDF\u22121 is the quantile function.\nThe interquartile range and median of some common distributions are shown below\n\n\n=== Interquartile range test for normality of distribution ===\nThe IQR, mean, and standard deviation of a population P can be used in a simple test of whether or not P is normally distributed, or Gaussian. If P is normally distributed, then the standard score of the first quartile, z1, is \u22120.67, and the standard score of the third quartile, z3, is +0.67. Given mean = \n  \n    \n      \n        \n          \n            \n              P\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {P}}}\n   and standard deviation = \u03c3 for P, if P is normally distributed, the first quartile\n\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        =\n        (\n        \u03c3\n        \n        \n          z\n          \n            1\n          \n        \n        )\n        +\n        \n          \n            \n              P\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle Q_{1}=(\\sigma \\,z_{1})+{\\bar {P}}}\n  and the third quartile\n\n  \n    \n      \n        \n          Q\n          \n            3\n          \n        \n        =\n        (\n        \u03c3\n        \n        \n          z\n          \n            3\n          \n        \n        )\n        +\n        \n          \n            \n              P\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle Q_{3}=(\\sigma \\,z_{3})+{\\bar {P}}}\n  If the actual values of the first or third quartiles differ substantially from the calculated values, P is not normally distributed. However, a normal distribution can be trivially perturbed to maintain its Q1 and Q2 std. scores at 0.67 and \u22120.67 and not be normally distributed (so the above test would produce a false positive). A better test of normality, such as Q\u2013Q plot would be indicated here.\n\n\n== Outliers ==\n\nThe interquartile range is often used to find outliers in data. Outliers here are defined as observations that fall below Q1 \u2212 1.5 IQR or above Q3 + 1.5 IQR. In a boxplot, the highest and lowest occurring value within this limit are indicated by whiskers of the box (frequently with an additional bar at the end of the whisker) and any outliers as individual points.\n\n\n== See also ==\nInterdecile range\nMidhinge\nProbable error\nRobust measures of scale\n\n\n== References ==\n\n\n== External links ==\n Media related to Interquartile range at Wikimedia Commons", "Cluster analysis": "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\nCluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.\nBesides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek \u03b2\u03cc\u03c4\u03c1\u03c5\u03c2 \"grape\"), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.\nCluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.\n\n\n== Definition ==\nThe notion of a \"cluster\" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these \"cluster models\" is key to understanding the differences between the various algorithms. Typical cluster models include:\n\nConnectivity models: for example, hierarchical clustering builds models based on distance connectivity.\nCentroid models: for example, the k-means algorithm represents each cluster by a single mean vector.\nDistribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the expectation-maximization algorithm.\nDensity models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.\nSubspace models: in biclustering (also known as co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.\nGroup models: some algorithms do not provide a refined model for their results and just provide the grouping information.\nGraph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.\nSigned graph models: Every path in a signed graph has a sign from the product of the signs on the edges. Under the assumptions of balance theory, edges may change sign and result in a bifurcated graph. The weaker \"clusterability axiom\" (no cycle has exactly one negative edge) yields results with more than two clusters, or subgraphs with only positive edges.\nNeural models: the most well known unsupervised neural network is the self-organizing map and these models can usually be characterized as similar to one or more of the above models, and including subspace models when neural networks implement a form of Principal Component Analysis or Independent Component Analysis.A \"clustering\" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:\n\nHard clustering: each object belongs to a cluster or not\nSoft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster)There are also finer distinctions possible, for example:\n\nStrict partitioning clustering: each object belongs to exactly one cluster\nStrict partitioning clustering with outliers: objects can also belong to no cluster, in which case they are considered outliers\nOverlapping clustering (also: alternative clustering, multi-view clustering): objects may belong to more than one cluster; usually involving hard clusters\nHierarchical clustering: objects that belong to a child cluster also belong to the parent cluster\nSubspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap\n\n\n== Algorithms ==\n\nAs listed above, clustering algorithms can be categorized based on their cluster model. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.\nThere is no objectively \"correct\" clustering algorithm, but as it was noted, \"clustering is in the eye of the beholder.\" The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model. For example, k-means cannot find non-convex clusters.\n\n\n=== Connectivity-based clustering (hierarchical clustering) ===\n\nConnectivity-based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect \"objects\" to form \"clusters\" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name \"hierarchical clustering\" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.\nConnectivity-based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances), and UPGMA or WPGMA (\"Unweighted or Weighted Pair Group Method with Arithmetic Mean\", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).\nThese methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as \"chaining phenomenon\", in particular with single-linkage clustering). In the general case, the complexity is \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{3})}\n   for agglomerative clustering and \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          2\n          \n            n\n            \u2212\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(2^{n-1})}\n   for divisive clustering, which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{2})}\n  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering.\n\nLinkage clustering examples\n\t\t\n\t\t\n\n\n=== Centroid-based clustering ===\n\nIn centroid-based clustering, each cluster is represented by a central vector, which is not necessarily a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.\nThe optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximate method is Lloyd's algorithm, often just referred to as \"k-means algorithm\" (although another algorithm introduced this name). It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means).\nMost k-means-type algorithms require the number of clusters \u2013 k \u2013 to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders of clusters (which is not surprising since the algorithm optimizes cluster centers, not cluster borders).\nK-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based clustering, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.\n\nk-means clustering examples\n\t\t\n\t\t\nCentroid-based clustering problems such as k-means and k-medoids are special cases of the uncapacitated, metric facility location problem, a canonical problem in the operations research and computational geometry communities. In a basic facility location problem (of which there are numerous variants that model more elaborate settings), the task is to find the best warehouse locations to optimally service a given set of consumers. One may view \"warehouses\" as cluster centroids and \"consumer locations\" as the data to be clustered. This makes it possible to apply the well-developed algorithmic solutions from the facility location literature to the presently considered centroid-based clustering problem.\n\n\n=== Distribution-based clustering ===\nThe clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.\nWhile the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.\nOne prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modeled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.\nDistribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).\n\nGaussian mixture model clustering examples\n\t\t\n\t\t\n\n\n=== Density-based clustering ===\nIn density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas \u2013 that are required to separate clusters \u2013 are usually considered to be noise and border points.\nThe most popular density based clustering method is DBSCAN. In contrast to many newer methods, it features a well-defined cluster model called \"density-reachability\". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low \u2013 it requires a linear number of range queries on the database \u2013 and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  , and produces a hierarchical result related to that of linkage clustering. DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   parameter entirely and offering performance improvements over OPTICS by using an R-tree index.\nThe key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions \u2013 a common use case in artificial data \u2013 the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.\nMean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.\nDensity-based clustering examples\n\t\t\n\t\t\n\t\t\n\n\n=== Grid-based clustering ===\nThe grid-based technique is used for a multi-dimensional data set. In this technique, we create a grid structure, and the comparison is performed on grids (also known as cells). The grid-based technique is fast and has low computational complexity. There are two types of grid-based clustering methods: STING and CLIQUE. Steps involved in grid-based clustering algorithm are:\n\nDivide data space into a finite number of cells.\nRandomly select a cell \u2018c\u2019, where c should not be traversed beforehand.\nCalculate the density of \u2018c\u2019\nIf the density of \u2018c\u2019 greater than threshold density\nMark cell \u2018c\u2019 as a new cluster\nCalculate the density of all the neighbors of \u2018c\u2019\nIf the density of a neighboring cell is greater than threshold density then, add the cell in the cluster and repeat steps 4.2 and 4.3 till there is no neighbor with a density greater than threshold density.\nRepeat steps 2,3 and 4 till all the cells are traversed.\nStop.\n\n\n=== Recent developments ===\nIn recent years, considerable effort has been put into improving the performance of existing algorithms. Among them are CLARANS, and BIRCH. With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting \"clusters\" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering.\nFor high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (\"correlated\") subspace clusters that can be modeled by giving a correlation of their attributes. Examples for such clustering algorithms are CLIQUE and SUBCLU.Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using \"correlation connectivity\" and ERiC exploring hierarchical density-based correlation clusters).\nSeveral different clustering systems based on mutual information have been proposed. One is Marina Meil\u0103's variation of information metric; another provides hierarchical clustering. Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information. Also belief propagation, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms.\n\n\n== Evaluation and assessment ==\nEvaluation (or \"validation\") of clustering results is as difficult as the clustering itself. Popular approaches involve \"internal\" evaluation, where the clustering is summarized to a single quality score, \"external\" evaluation, where the clustering is compared to an existing \"ground truth\" classification, \"manual\" evaluation by a human expert, and \"indirect\" evaluation by evaluating the utility of the clustering in its intended application.Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective. For example, one could cluster the data set by the Silhouette coefficient; except that there is no known efficient algorithm for this. By using such an internal measure for evaluation, one rather compares the similarity of the optimization problems, and not necessarily how useful the clustering is.\nExternal evaluation has similar problems: if we have such \"ground truth\" labels, then we would not need to cluster; and in practical applications we usually do not have such labels. On the other hand, the labels only reflect one possible partitioning of the data set, which does not imply that there does not exist a different, and maybe even better, clustering.\nNeither of these approaches can therefore ultimately judge the actual quality of a clustering, but this needs human evaluation, which is highly subjective. Nevertheless, such statistics can be quite informative in identifying bad clusterings, but one should not dismiss subjective human evaluation.\n\n\n=== Internal evaluation ===\n\nWhen a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications. Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.\nTherefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another. Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion. For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.\nMore than a dozen of internal evaluation measures exist, usually based on the intuition that items in the same cluster should be more similar than items in different clusters.:\u200a115\u2013121\u200a For example, the following methods can be used to assess the quality of clustering algorithms based on internal criterion:\n\nDavies\u2013Bouldin indexThe Davies\u2013Bouldin index can be calculated by the following formula:\n\n  \n    \n      \n        D\n        B\n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          max\n          \n            j\n            \u2260\n            i\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \u03c3\n                  \n                    i\n                  \n                \n                +\n                \n                  \u03c3\n                  \n                    j\n                  \n                \n              \n              \n                d\n                (\n                \n                  c\n                  \n                    i\n                  \n                \n                ,\n                \n                  c\n                  \n                    j\n                  \n                \n                )\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle DB={\\frac {1}{n}}\\sum _{i=1}^{n}\\max _{j\\neq i}\\left({\\frac {\\sigma _{i}+\\sigma _{j}}{d(c_{i},c_{j})}}\\right)}\n  \nwhere n is the number of clusters, \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   is the centroid of cluster \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  , \n  \n    \n      \n        \n          \u03c3\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}}\n   is the average distance of all elements in cluster \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   to centroid \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n  , and \n  \n    \n      \n        d\n        (\n        \n          c\n          \n            i\n          \n        \n        ,\n        \n          c\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle d(c_{i},c_{j})}\n   is the distance between centroids \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   and \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  . Since algorithms that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity) will have a low Davies\u2013Bouldin index, the clustering algorithm that produces a collection of clusters with the smallest Davies\u2013Bouldin index is considered the best algorithm based on this criterion.Dunn indexThe Dunn index aims to identify dense and well-separated clusters. It is defined as the ratio between the minimal inter-cluster distance to maximal intra-cluster distance. For each cluster partition, the Dunn index can be calculated by the following formula:\n  \n    \n      \n        D\n        =\n        \n          \n            \n              \n                min\n                \n                  1\n                  \u2264\n                  i\n                  <\n                  j\n                  \u2264\n                  n\n                \n              \n              d\n              (\n              i\n              ,\n              j\n              )\n            \n            \n              \n                max\n                \n                  1\n                  \u2264\n                  k\n                  \u2264\n                  n\n                \n              \n              \n                d\n                \n                  \u2032\n                \n              \n              (\n              k\n              )\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle D={\\frac {\\min _{1\\leq i<j\\leq n}d(i,j)}{\\max _{1\\leq k\\leq n}d^{\\prime }(k)}}\\,,}\n  \nwhere d(i,j) represents the distance between clusters i and j, and d '(k) measures the intra-cluster distance of cluster k. The inter-cluster distance d(i,j) between two clusters may be any number of distance measures, such as the distance between the centroids of the clusters. Similarly, the intra-cluster distance d '(k) may be measured in a variety ways, such as the maximal distance between any pair of elements in cluster k. Since internal criterion seek clusters with high intra-cluster similarity and low inter-cluster similarity, algorithms that produce clusters with high Dunn index are more desirable.Silhouette coefficientThe silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters.\n\n\n=== External evaluation ===\nIn external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation. These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies. Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result. In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.A number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.As with internal evaluation, several external evaluation measures exist,:\u200a125\u2013129\u200a for example:\n\nPurity: Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster. Now take the sum over all clusters and divide by the total number of data points. Formally, given some set of clusters \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   and some set of classes \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  , both partitioning \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   data points, purity can be defined as:\n  \n    \n      \n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            m\n            \u2208\n            M\n          \n        \n        \n          max\n          \n            d\n            \u2208\n            D\n          \n        \n        \n          \n            |\n          \n          m\n          \u2229\n          d\n          \n            |\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{N}}\\sum _{m\\in M}\\max _{d\\in D}{|m\\cap d|}}\n  \nThis measure doesn't penalize having many clusters, and more clusters will make it easier to produce a high purity. A purity score of 1 is always possible by putting each data point in its own cluster. Also, purity doesn't work well for imbalanced data, where even poorly performing clustering algorithms will give a high purity value. For example, if a size 1000 dataset consists of two classes, one containing 999 points and the other containing 1 point, then every possible partition will have a purity of at least 99.9%.Rand indexThe Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. It can be computed using the following formula:\n\n  \n    \n      \n        R\n        I\n        =\n        \n          \n            \n              T\n              P\n              +\n              T\n              N\n            \n            \n              T\n              P\n              +\n              F\n              P\n              +\n              F\n              N\n              +\n              T\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle RI={\\frac {TP+TN}{TP+FP+FN+TN}}}\n  \nwhere  \n  \n    \n      \n        T\n        P\n      \n    \n    {\\displaystyle TP}\n   is the number of true positives, \n  \n    \n      \n        T\n        N\n      \n    \n    {\\displaystyle TN}\n   is the number of true negatives, \n  \n    \n      \n        F\n        P\n      \n    \n    {\\displaystyle FP}\n   is the number of false positives, and \n  \n    \n      \n        F\n        N\n      \n    \n    {\\displaystyle FN}\n   is the number of false negatives. The instances being counted here are the number of correct pairwise assignments. That is, \n  \n    \n      \n        T\n        P\n      \n    \n    {\\displaystyle TP}\n   is the number of pairs of points that are clustered together in the predicted partition and in the ground truth partition, \n  \n    \n      \n        F\n        P\n      \n    \n    {\\displaystyle FP}\n   is the number of pairs of points that are clustered together in the predicted partition but not in the ground truth partition etc. If the dataset is of size N, then \n  \n    \n      \n        T\n        P\n        +\n        T\n        N\n        +\n        F\n        P\n        +\n        F\n        N\n        =\n        \n          \n            \n              (\n            \n            \n              N\n              2\n            \n            \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle TP+TN+FP+FN={\\binom {N}{2}}}\n  .One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern, as does the chance-corrected adjusted Rand index.\n\nF-measureThe F-measure can be used to balance the contribution of false negatives by weighting recall through a parameter \n  \n    \n      \n        \u03b2\n        \u2265\n        0\n      \n    \n    {\\displaystyle \\beta \\geq 0}\n  . Let precision and recall (both external evaluation measures in themselves) be defined as follows:\n\n  \n    \n      \n        P\n        =\n        \n          \n            \n              T\n              P\n            \n            \n              T\n              P\n              +\n              F\n              P\n            \n          \n        \n      \n    \n    {\\displaystyle P={\\frac {TP}{TP+FP}}}\n  \n\n  \n    \n      \n        R\n        =\n        \n          \n            \n              T\n              P\n            \n            \n              T\n              P\n              +\n              F\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle R={\\frac {TP}{TP+FN}}}\n  \nwhere \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   is the precision rate and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n   is the recall rate. We can calculate the F-measure by using the following formula:\n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n        =\n        \n          \n            \n              (\n              \n                \u03b2\n                \n                  2\n                \n              \n              +\n              1\n              )\n              \u22c5\n              P\n              \u22c5\n              R\n            \n            \n              \n                \u03b2\n                \n                  2\n                \n              \n              \u22c5\n              P\n              +\n              R\n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }={\\frac {(\\beta ^{2}+1)\\cdot P\\cdot R}{\\beta ^{2}\\cdot P+R}}}\n  \nWhen \n  \n    \n      \n        \u03b2\n        =\n        0\n      \n    \n    {\\displaystyle \\beta =0}\n  , \n  \n    \n      \n        \n          F\n          \n            0\n          \n        \n        =\n        P\n      \n    \n    {\\displaystyle F_{0}=P}\n  . In other words, recall has no impact on the F-measure when \n  \n    \n      \n        \u03b2\n        =\n        0\n      \n    \n    {\\displaystyle \\beta =0}\n  , and increasing \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   allocates an increasing amount of weight to recall in the final F-measure.\nAlso \n  \n    \n      \n        T\n        N\n      \n    \n    {\\displaystyle TN}\n   is not taken into account and can vary from 0 upward without bound.Jaccard indexThe Jaccard index is used to quantify the similarity between two datasets. The Jaccard index takes on a value between 0 and 1. An index of 1 means that the two dataset are identical, and an index of 0 indicates that the datasets have no common elements. The Jaccard index is defined by the following formula:\n\n  \n    \n      \n        J\n        (\n        A\n        ,\n        B\n        )\n        =\n        \n          \n            \n              \n                |\n              \n              A\n              \u2229\n              B\n              \n                |\n              \n            \n            \n              \n                |\n              \n              A\n              \u222a\n              B\n              \n                |\n              \n            \n          \n        \n        =\n        \n          \n            \n              T\n              P\n            \n            \n              T\n              P\n              +\n              F\n              P\n              +\n              F\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle J(A,B)={\\frac {|A\\cap B|}{|A\\cup B|}}={\\frac {TP}{TP+FP+FN}}}\n  \nThis is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets.\nNote that \n  \n    \n      \n        T\n        N\n      \n    \n    {\\displaystyle TN}\n   is not taken into account.Dice indexThe Dice symmetric measure doubles the weight on \n  \n    \n      \n        T\n        P\n      \n    \n    {\\displaystyle TP}\n   while still ignoring \n  \n    \n      \n        T\n        N\n      \n    \n    {\\displaystyle TN}\n  :\n\n  \n    \n      \n        D\n        S\n        C\n        =\n        \n          \n            \n              2\n              T\n              P\n            \n            \n              2\n              T\n              P\n              +\n              F\n              P\n              +\n              F\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle DSC={\\frac {2TP}{2TP+FP+FN}}}\n  Fowlkes\u2013Mallows indexThe Fowlkes\u2013Mallows index computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications. The higher the value of the Fowlkes\u2013Mallows index the more similar the clusters and the benchmark classifications are. It can be computed using the following formula:\n\n  \n    \n      \n        F\n        M\n        =\n        \n          \n            \n              \n                \n                  T\n                  P\n                \n                \n                  T\n                  P\n                  +\n                  F\n                  P\n                \n              \n            \n            \u22c5\n            \n              \n                \n                  T\n                  P\n                \n                \n                  T\n                  P\n                  +\n                  F\n                  N\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle FM={\\sqrt {{\\frac {TP}{TP+FP}}\\cdot {\\frac {TP}{TP+FN}}}}}\n  \nwhere  \n  \n    \n      \n        T\n        P\n      \n    \n    {\\displaystyle TP}\n   is the number of true positives, \n  \n    \n      \n        F\n        P\n      \n    \n    {\\displaystyle FP}\n   is the number of false positives, and \n  \n    \n      \n        F\n        N\n      \n    \n    {\\displaystyle FN}\n   is the number of false negatives. The \n  \n    \n      \n        F\n        M\n      \n    \n    {\\displaystyle FM}\n   index is the geometric mean of the precision and recall \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  , and is thus also known as the G-measure, while the F-measure is their harmonic mean. Moreover, precision and recall are also known as Wallace's indices \n  \n    \n      \n        \n          B\n          \n            I\n          \n        \n      \n    \n    {\\displaystyle B^{I}}\n   and \n  \n    \n      \n        \n          B\n          \n            I\n            I\n          \n        \n      \n    \n    {\\displaystyle B^{II}}\n  . Chance normalized versions of recall, precision and G-measure correspond to Informedness, Markedness and Matthews Correlation and relate strongly to Kappa.Chi Index is a external validation index that measure the clustering results by applying the chi-squared statistic. This index scores positively the fact that the labels are as sparse as possible across the clusters, i.e., that each cluster has as few different labels as possible. The higher the value of the Chi Index the greater the relationship between the resulting clusters and the label used.\nThe mutual information is an information theoretic measure of how much information is shared between a clustering and a ground-truth classification that can detect a non-linear similarity between two clusterings. Normalized mutual information is a family of corrected-for-chance variants of this that has a reduced bias for varying cluster numbers.\nConfusion matrixA confusion matrix can be used to quickly visualize the results of a classification (or clustering) algorithm. It shows how different a cluster is from the gold standard cluster.\n\n\n=== Cluster tendency ===\nTo measure cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering. One way to do this is to compare the data against random data. On average, random data should not have clusters.\n\nHopkins statisticThere are multiple formulations of the Hopkins statistic. A typical one is as follows. Let \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   be the set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   data points in \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   dimensional space. Consider a random sample (without replacement) of \n  \n    \n      \n        m\n        \u226a\n        n\n      \n    \n    {\\displaystyle m\\ll n}\n   data points with members \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  . Also generate a set \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   of \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   uniformly randomly distributed data points. Now define two distance measures, \n  \n    \n      \n        \n          u\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle u_{i}}\n   to be the distance of \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \u2208\n        Y\n      \n    \n    {\\displaystyle y_{i}\\in Y}\n   from its nearest neighbor in X and \n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}}\n   to be the distance of \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{i}\\in X}\n   from its nearest neighbor in X. We then define the Hopkins statistic as:\n\n  \n    \n      \n        H\n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  m\n                \n              \n              \n                \n                  u\n                  \n                    i\n                  \n                  \n                    d\n                  \n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  m\n                \n              \n              \n                \n                  u\n                  \n                    i\n                  \n                  \n                    d\n                  \n                \n              \n              +\n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  m\n                \n              \n              \n                \n                  w\n                  \n                    i\n                  \n                  \n                    d\n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle H={\\frac {\\sum _{i=1}^{m}{u_{i}^{d}}}{\\sum _{i=1}^{m}{u_{i}^{d}}+\\sum _{i=1}^{m}{w_{i}^{d}}}}\\,,}\n  \nWith this definition, uniform random data should tend to have values near to 0.5, and clustered data should tend to have values nearer to 1.\nHowever, data containing just a single Gaussian will also score close to 1, as this statistic measures deviation from a uniform distribution, not multimodality, making this statistic largely useless in application (as real data never is remotely uniform).\n\n\n== Applications ==\n\n\n=== Biology, computational biology and bioinformatics ===\n\nPlant and animal ecology\nCluster analysis is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments. It is also used in plant systematics to generate artificial phylogenies or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes.\nTranscriptomics\nClustering is used to build groups of genes with related expression patterns (also known as coexpressed genes) as in HCS clustering algorithm. Often such groups contain functionally related proteins, such as enzymes for a specific pathway, or genes that are co-regulated. High throughput experiments using expressed sequence tags (ESTs) or DNA microarrays can be a powerful tool for genome annotation \u2013 a general aspect of genomics.\nSequence analysis\nSequence clustering is used to group homologous sequences into gene families. This is a very important concept in bioinformatics, and evolutionary biology in general. See evolution by gene duplication.\nHigh-throughput genotyping platforms\nClustering algorithms are used to automatically assign genotypes.\nHuman genetic clustering\nThe similarity of genetic data is used in clustering to infer population structures.\n\n\n=== Medicine ===\nMedical imaging\nOn PET scans, cluster analysis can be used to differentiate between different types of tissue in a three-dimensional image for many different purposes.\nAnalysis of antimicrobial activity\nCluster analysis can be used to analyse patterns of antibiotic resistance, to classify antimicrobial compounds according to their mechanism of action, to classify antibiotics according to their antibacterial activity.\nIMRT segmentation\nClustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC-based Radiation Therapy.\n\n\n=== Business and marketing ===\nMarket research\nCluster analysis is widely used in market research when working with multivariate data from surveys and test panels. Market researchers use cluster analysis to partition the general population of consumers into market segments and to better understand the relationships between different groups of consumers/potential customers, and for use in market segmentation, product positioning, new product development and selecting test markets.\nGrouping of shopping items\nClustering can be used to group all the shopping items available on the web into a set of unique products. For example, all the items on eBay can be grouped into unique products (eBay does not have the concept of a SKU).\n\n\n=== World Wide Web ===\nSocial network analysis\nIn the study of social networks, clustering may be used to recognize communities within large groups of people.\nSearch result grouping\nIn the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like Google. There are currently a number of web-based clustering tools such as Clusty. It also may be used to return a more comprehensive set of results in cases where a search term could refer to vastly different things. Each distinct use of the term corresponds to a unique cluster of results, allowing a ranking algorithm to return comprehensive results by picking the top result from each cluster.\nSlippy map optimization\nFlickr's map of photos and other map sites use clustering to reduce the number of markers on a map. This makes it both faster and reduces the amount of visual clutter.\n\n\n=== Computer science ===\nSoftware evolution\nClustering is useful in software evolution as it helps to reduce legacy properties in code by reforming functionality that has become dispersed. It is a form of restructuring and hence is a way of direct preventative maintenance.\nImage segmentation\nClustering can be used to divide a digital image into distinct regions for border detection or object recognition.\nEvolutionary algorithms\nClustering may be used to identify different niches within the population of an evolutionary algorithm so that reproductive opportunity can be distributed more evenly amongst the evolving species or subspecies.\nRecommender systems\nRecommender systems are designed to recommend new items based on a user's tastes.  They sometimes use clustering algorithms to predict a user's preferences based on the preferences of other users in the user's cluster.\nMarkov chain Monte Carlo methods\nClustering is often utilized to locate and characterize extrema in the target distribution.\nAnomaly detection\nAnomalies/outliers are typically \u2013 be it explicitly or implicitly \u2013 defined with respect to clustering structure in data.\nNatural language processing\nClustering can be used to resolve lexical ambiguity.\nDevOps\nClustering has been used to analyse the effectiveness of DevOps teams.\n\n\n=== Social science ===\nSequence analysis in social sciences\nCluster analysis is used to identify patterns of family life trajectories, professional careers, and daily or weekly time use for example.\nCrime analysis\nCluster analysis can be used to identify areas where there are greater incidences of particular types of crime. By identifying these distinct areas or \"hot spots\" where a similar crime has happened over a period of time, it is possible to manage law enforcement resources more effectively.\nEducational data mining\nCluster analysis is for example used to identify groups of schools or students with similar properties.\nTypologies\nFrom poll data, projects such as those undertaken by the Pew Research Center use cluster analysis to discern typologies of opinions, habits, and demographics that may be useful in politics and marketing.\n\n\n=== Others ===\nField robotics\nClustering algorithms are used for robotic situational awareness to track objects and detect outliers in sensor data.Mathematical chemistry\nTo find structural similarity, etc., for example, 3000 chemical compounds were clustered in the space of 90 topological indices.Climatology\nTo find weather regimes or preferred sea level pressure atmospheric patterns.Finance\nCluster analysis has been used to cluster stocks into sectors.Petroleum geology\nCluster analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties.Geochemistry\nThe clustering of chemical properties in different sample locations.\n\n\n== See also ==\n\n\n=== Specialized types of cluster analysis ===\nAutomatic clustering algorithms\nBalanced clustering\nClustering high-dimensional data\nConceptual clustering\nConsensus clustering\nConstrained clustering\nCommunity detection\nData stream clustering\nHCS clustering\nSequence clustering\nSpectral clustering\n\n\n=== Techniques used in cluster analysis ===\nArtificial neural network (ANN)\nNearest neighbor search\nNeighbourhood components analysis\nLatent class analysis\nAffinity propagation\n\n\n=== Data projection and preprocessing ===\nDimension reduction\nPrincipal component analysis\nMultidimensional scaling\n\n\n=== Other ===\nCluster-weighted modeling\nCurse of dimensionality\nDetermining the number of clusters in a data set\nParallel coordinates\nStructured data analysis\n\n\n== References ==", "Bayesian network": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n\n== Graphical model ==\nFormally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   parent nodes represent \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   Boolean variables, then the probability function could be represented by a table of \n  \n    \n      \n        \n          2\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle 2^{m}}\n   entries, one entry for each of the \n  \n    \n      \n        \n          2\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle 2^{m}}\n   possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.\n\n\n== Example ==\n\nLet us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).\nThe joint probability function is, by the chain rule of probability,\n\n  \n    \n      \n        Pr\n        (\n        G\n        ,\n        S\n        ,\n        R\n        )\n        =\n        Pr\n        (\n        G\n        \u2223\n        S\n        ,\n        R\n        )\n        Pr\n        (\n        S\n        \u2223\n        R\n        )\n        Pr\n        (\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G,S,R)=\\Pr(G\\mid S,R)\\Pr(S\\mid R)\\Pr(R)}\n  where G = \"Grass wet (true/false)\", S = \"Sprinkler turned on (true/false)\", and R = \"Raining (true/false)\".\nThe model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using the conditional probability formula and summing over all nuisance variables:\n\n  \n    \n      \n        Pr\n        (\n        R\n        =\n        T\n        \u2223\n        G\n        =\n        T\n        )\n        =\n        \n          \n            \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              R\n              =\n              T\n              )\n            \n            \n              Pr\n              (\n              G\n              =\n              T\n              )\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  x\n                  \u2208\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              S\n              =\n              x\n              ,\n              R\n              =\n              T\n              )\n            \n            \n              \n                \u2211\n                \n                  x\n                  ,\n                  y\n                  \u2208\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              Pr\n              (\n              G\n              =\n              T\n              ,\n              S\n              =\n              x\n              ,\n              R\n              =\n              y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr(R=T\\mid G=T)={\\frac {\\Pr(G=T,R=T)}{\\Pr(G=T)}}={\\frac {\\sum _{x\\in \\{T,F\\}}\\Pr(G=T,S=x,R=T)}{\\sum _{x,y\\in \\{T,F\\}}\\Pr(G=T,S=x,R=y)}}}\n  Using the expansion for the joint probability function \n  \n    \n      \n        Pr\n        (\n        G\n        ,\n        S\n        ,\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G,S,R)}\n   and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                G\n                =\n                T\n                ,\n                S\n                =\n                T\n                ,\n                R\n                =\n                T\n                )\n              \n              \n                \n                =\n                Pr\n                (\n                G\n                =\n                T\n                \u2223\n                S\n                =\n                T\n                ,\n                R\n                =\n                T\n                )\n                Pr\n                (\n                S\n                =\n                T\n                \u2223\n                R\n                =\n                T\n                )\n                Pr\n                (\n                R\n                =\n                T\n                )\n              \n            \n            \n              \n              \n                \n                =\n                0.99\n                \u00d7\n                0.01\n                \u00d7\n                0.2\n              \n            \n            \n              \n              \n                \n                =\n                0.00198.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr(G=T,S=T,R=T)&=\\Pr(G=T\\mid S=T,R=T)\\Pr(S=T\\mid R=T)\\Pr(R=T)\\\\&=0.99\\times 0.01\\times 0.2\\\\&=0.00198.\\end{aligned}}}\n  Then the numerical results (subscripted by the associated variable values) are\n\n  \n    \n      \n        Pr\n        (\n        R\n        =\n        T\n        \u2223\n        G\n        =\n        T\n        )\n        =\n        \n          \n            \n              \n                0.00198\n                \n                  T\n                  T\n                  T\n                \n              \n              +\n              \n                0.1584\n                \n                  T\n                  F\n                  T\n                \n              \n            \n            \n              \n                0.00198\n                \n                  T\n                  T\n                  T\n                \n              \n              +\n              \n                0.288\n                \n                  T\n                  T\n                  F\n                \n              \n              +\n              \n                0.1584\n                \n                  T\n                  F\n                  T\n                \n              \n              +\n              \n                0.0\n                \n                  T\n                  F\n                  F\n                \n              \n            \n          \n        \n        =\n        \n          \n            891\n            2491\n          \n        \n        \u2248\n        35.77\n        %\n        .\n      \n    \n    {\\displaystyle \\Pr(R=T\\mid G=T)={\\frac {0.00198_{TTT}+0.1584_{TFT}}{0.00198_{TTT}+0.288_{TTF}+0.1584_{TFT}+0.0_{TFF}}}={\\frac {891}{2491}}\\approx 35.77\\%.}\n  To answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function\n\n  \n    \n      \n        Pr\n        (\n        S\n        ,\n        R\n        \u2223\n        \n          do\n        \n        (\n        G\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        S\n        \u2223\n        R\n        )\n        Pr\n        (\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(S,R\\mid {\\text{do}}(G=T))=\\Pr(S\\mid R)\\Pr(R)}\n  obtained by removing the factor \n  \n    \n      \n        Pr\n        (\n        G\n        \u2223\n        S\n        ,\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(G\\mid S,R)}\n   from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:\n\n  \n    \n      \n        Pr\n        (\n        R\n        \u2223\n        \n          do\n        \n        (\n        G\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        R\n        )\n        .\n      \n    \n    {\\displaystyle \\Pr(R\\mid {\\text{do}}(G=T))=\\Pr(R).}\n  To predict the impact of turning the sprinkler on:\n\n  \n    \n      \n        Pr\n        (\n        R\n        ,\n        G\n        \u2223\n        \n          do\n        \n        (\n        S\n        =\n        T\n        )\n        )\n        =\n        Pr\n        (\n        R\n        )\n        Pr\n        (\n        G\n        \u2223\n        R\n        ,\n        S\n        =\n        T\n        )\n      \n    \n    {\\displaystyle \\Pr(R,G\\mid {\\text{do}}(S=T))=\\Pr(R)\\Pr(G\\mid R,S=T)}\n  with the term \n  \n    \n      \n        Pr\n        (\n        S\n        =\n        T\n        \u2223\n        R\n        )\n      \n    \n    {\\displaystyle \\Pr(S=T\\mid R)}\n   removed, showing that the action affects the grass but not the rain.\nThese predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action \n  \n    \n      \n        \n          do\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\text{do}}(x)}\n   can still be predicted, however, whenever the back-door criterion is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then\n\n  \n    \n      \n        Pr\n        (\n        Y\n        ,\n        Z\n        \u2223\n        \n          do\n        \n        (\n        x\n        )\n        )\n        =\n        \n          \n            \n              Pr\n              (\n              Y\n              ,\n              Z\n              ,\n              X\n              =\n              x\n              )\n            \n            \n              Pr\n              (\n              X\n              =\n              x\n              \u2223\n              Z\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\Pr(Y,Z\\mid {\\text{do}}(x))={\\frac {\\Pr(Y,Z,X=x)}{\\Pr(X=x\\mid Z)}}.}\n  A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S \u2190 R \u2192 G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious\n(apparent dependence arising from a common cause, R). (see Simpson's paradox)\nTo determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\" and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for \n  \n    \n      \n        \n          2\n          \n            10\n          \n        \n        =\n        1024\n      \n    \n    {\\displaystyle 2^{10}=1024}\n   values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most \n  \n    \n      \n        10\n        \u22c5\n        \n          2\n          \n            3\n          \n        \n        =\n        80\n      \n    \n    {\\displaystyle 10\\cdot 2^{3}=80}\n   values.\nOne advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.\n\n\n== Inference and learning ==\nBayesian networks perform three main inference tasks:\n\n\n=== Inferring unobserved variables ===\nBecause a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.\nThe most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space\u2013time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.\n\n\n=== Parameter learning ===\nIn order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)\nOften these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.\nA more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.\n\n\n=== Structure learning ===\nIn the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.\nAutomatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl and rests on the distinction between the three possible patterns allowed in a 3-node DAG:\n\nThe first 2 represent the same dependencies (\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   are independent given \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  ) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.\nA particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Such method can handle problems with up to 100 variables.\nIn order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.\n\n\n== Statistical introduction ==\n\nGiven data \n  \n    \n      \n        x\n        \n        \n      \n    \n    {\\displaystyle x\\,\\!}\n   and parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , a simple Bayesian analysis starts with a prior probability (prior) \n  \n    \n      \n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(\\theta )}\n   and likelihood \n  \n    \n      \n        p\n        (\n        x\n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(x\\mid \\theta )}\n   to compute a posterior probability \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        x\n        )\n        \u221d\n        p\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(\\theta \\mid x)\\propto p(x\\mid \\theta )p(\\theta )}\n  .\nOften the prior on \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   depends in turn on other parameters \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   that are not mentioned in the likelihood. So, the prior \n  \n    \n      \n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(\\theta )}\n   must be replaced by a likelihood \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        \u03c6\n        )\n      \n    \n    {\\displaystyle p(\\theta \\mid \\varphi )}\n  , and a prior \n  \n    \n      \n        p\n        (\n        \u03c6\n        )\n      \n    \n    {\\displaystyle p(\\varphi )}\n   on the newly introduced parameters \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   is required, resulting in a posterior probability\n\n  \n    \n      \n        p\n        (\n        \u03b8\n        ,\n        \u03c6\n        \u2223\n        x\n        )\n        \u221d\n        p\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        p\n        (\n        \u03b8\n        \u2223\n        \u03c6\n        )\n        p\n        (\n        \u03c6\n        )\n        .\n      \n    \n    {\\displaystyle p(\\theta ,\\varphi \\mid x)\\propto p(x\\mid \\theta )p(\\theta \\mid \\varphi )p(\\varphi ).}\n  This is the simplest example of a hierarchical Bayes model.\nThe process may be repeated; for example, the parameters \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   may depend in turn on additional parameters \n  \n    \n      \n        \u03c8\n        \n        \n      \n    \n    {\\displaystyle \\psi \\,\\!}\n  , which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.\n\n\n=== Introductory examples ===\nGiven the measured quantities \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        \n        \n      \n    \n    {\\displaystyle x_{1},\\dots ,x_{n}\\,\\!}\n  each with normally distributed errors of known standard deviation \n  \n    \n      \n        \u03c3\n        \n        \n      \n    \n    {\\displaystyle \\sigma \\,\\!}\n  ,\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \u223c\n        N\n        (\n        \n          \u03b8\n          \n            i\n          \n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle x_{i}\\sim N(\\theta _{i},\\sigma ^{2})}\n  Suppose we are interested in estimating the \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n  . An approach would be to estimate the \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n   using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply\n\n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\theta _{i}=x_{i}.}\n  However, if the quantities are related, so that for example the individual \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n  have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \u223c\n        N\n        (\n        \n          \u03b8\n          \n            i\n          \n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle x_{i}\\sim N(\\theta _{i},\\sigma ^{2}),}\n  \n\n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n        \u223c\n        N\n        (\n        \u03c6\n        ,\n        \n          \u03c4\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\theta _{i}\\sim N(\\varphi ,\\tau ^{2}),}\n  with improper priors \n  \n    \n      \n        \u03c6\n        \u223c\n        \n          flat\n        \n      \n    \n    {\\displaystyle \\varphi \\sim {\\text{flat}}}\n  , \n  \n    \n      \n        \u03c4\n        \u223c\n        \n          flat\n        \n        \u2208\n        (\n        0\n        ,\n        \u221e\n        )\n      \n    \n    {\\displaystyle \\tau \\sim {\\text{flat}}\\in (0,\\infty )}\n  . When \n  \n    \n      \n        n\n        \u2265\n        3\n      \n    \n    {\\displaystyle n\\geq 3}\n  , this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n   will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.\n\n\n=== Restrictions on priors ===\nSome care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable \n  \n    \n      \n        \u03c4\n        \n        \n      \n    \n    {\\displaystyle \\tau \\,\\!}\n   in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.\n\n\n== Definitions and concepts ==\n\nSeveral equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v \u2208 V be a set of random variables indexed by V.\n\n\n=== Factorization definition ===\nX is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:\n\n  \n    \n      \n        p\n        (\n        x\n        )\n        =\n        \n          \u220f\n          \n            v\n            \u2208\n            V\n          \n        \n        p\n        \n          (\n          \n            \n              x\n              \n                v\n              \n            \n            \n            \n              \n                |\n              \n            \n            \n            \n              x\n              \n                pa\n                \u2061\n                (\n                v\n                )\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle p(x)=\\prod _{v\\in V}p\\left(x_{v}\\,{\\big |}\\,x_{\\operatorname {pa} (v)}\\right)}\n  where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).\nFor any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:\n\n  \n    \n      \n        P\n        \u2061\n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          \u220f\n          \n            v\n            =\n            1\n          \n          \n            n\n          \n        \n        P\n        \u2061\n        \n          (\n          \n            \n              X\n              \n                v\n              \n            \n            =\n            \n              x\n              \n                v\n              \n            \n            \u2223\n            \n              X\n              \n                v\n                +\n                1\n              \n            \n            =\n            \n              x\n              \n                v\n                +\n                1\n              \n            \n            ,\n            \u2026\n            ,\n            \n              X\n              \n                n\n              \n            \n            =\n            \n              x\n              \n                n\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\operatorname {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{v=1}^{n}\\operatorname {P} \\left(X_{v}=x_{v}\\mid X_{v+1}=x_{v+1},\\ldots ,X_{n}=x_{n}\\right)}\n  Using the definition above, this can be written as:\n\n  \n    \n      \n        P\n        \u2061\n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          \u220f\n          \n            v\n            =\n            1\n          \n          \n            n\n          \n        \n        P\n        \u2061\n        (\n        \n          X\n          \n            v\n          \n        \n        =\n        \n          x\n          \n            v\n          \n        \n        \u2223\n        \n          X\n          \n            j\n          \n        \n        =\n        \n          x\n          \n            j\n          \n        \n        \n           for each \n        \n        \n          X\n          \n            j\n          \n        \n        \n        \n           that is a parent of \n        \n        \n          X\n          \n            v\n          \n        \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{v=1}^{n}\\operatorname {P} (X_{v}=x_{v}\\mid X_{j}=x_{j}{\\text{ for each }}X_{j}\\,{\\text{ that is a parent of }}X_{v}\\,)}\n  The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.\n\n\n=== Local Markov property ===\nX is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:\n\n  \n    \n      \n        \n          X\n          \n            v\n          \n        \n        \u22a5\n        \n        \n        \n        \u22a5\n        \n          X\n          \n            V\n            \n            \u2216\n            \n            de\n            \u2061\n            (\n            v\n            )\n          \n        \n        \u2223\n        \n          X\n          \n            pa\n            \u2061\n            (\n            v\n            )\n          \n        \n        \n        \n          for all \n        \n        v\n        \u2208\n        V\n      \n    \n    {\\displaystyle X_{v}\\perp \\!\\!\\!\\perp X_{V\\,\\smallsetminus \\,\\operatorname {de} (v)}\\mid X_{\\operatorname {pa} (v)}\\quad {\\text{for all }}v\\in V}\n  where de(v) is the set of descendants and V \\ de(v) is the set of non-descendants of v.\nThis can be expressed in terms similar to the first definition, as\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \u2061\n                (\n                \n                  X\n                  \n                    v\n                  \n                \n                =\n                \n                  x\n                  \n                    v\n                  \n                \n                \u2223\n                \n                  X\n                  \n                    i\n                  \n                \n                =\n                \n                  x\n                  \n                    i\n                  \n                \n                \n                   for each \n                \n                \n                  X\n                  \n                    i\n                  \n                \n                \n                   that is not a descendant of \n                \n                \n                  X\n                  \n                    v\n                  \n                \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                (\n                \n                  X\n                  \n                    v\n                  \n                \n                =\n                \n                  x\n                  \n                    v\n                  \n                \n                \u2223\n                \n                  X\n                  \n                    j\n                  \n                \n                =\n                \n                  x\n                  \n                    j\n                  \n                \n                \n                   for each \n                \n                \n                  X\n                  \n                    j\n                  \n                \n                \n                   that is a parent of \n                \n                \n                  X\n                  \n                    v\n                  \n                \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\operatorname {P} (X_{v}=x_{v}\\mid X_{i}=x_{i}{\\text{ for each }}X_{i}{\\text{ that is not a descendant of }}X_{v}\\,)\\\\[6pt]={}&P(X_{v}=x_{v}\\mid X_{j}=x_{j}{\\text{ for each }}X_{j}{\\text{ that is a parent of }}X_{v}\\,)\\end{aligned}}}\n  The set of parents is a subset of the set of non-descendants because the graph is acyclic.\n\n\n=== Developing Bayesian networks ===\nDeveloping a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.\n\n\n=== Markov blanket ===\nThe Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.\n\n\n==== d-separation ====\nThis definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional. We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that.\nLet P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:\n\nP contains (but does not need to be entirely) a directed chain, \n  \n    \n      \n        u\n        \u22ef\n        \u2190\n        m\n        \u2190\n        \u22ef\n        v\n      \n    \n    {\\displaystyle u\\cdots \\leftarrow m\\leftarrow \\cdots v}\n   or \n  \n    \n      \n        u\n        \u22ef\n        \u2192\n        m\n        \u2192\n        \u22ef\n        v\n      \n    \n    {\\displaystyle u\\cdots \\rightarrow m\\rightarrow \\cdots v}\n  , such that the middle node m is in Z,\nP contains a fork, \n  \n    \n      \n        u\n        \u22ef\n        \u2190\n        m\n        \u2192\n        \u22ef\n        v\n      \n    \n    {\\displaystyle u\\cdots \\leftarrow m\\rightarrow \\cdots v}\n  , such that the middle node m is in Z, or\nP contains an inverted fork (or collider), \n  \n    \n      \n        u\n        \u22ef\n        \u2192\n        m\n        \u2190\n        \u22ef\n        v\n      \n    \n    {\\displaystyle u\\cdots \\rightarrow m\\leftarrow \\cdots v}\n  , such that the middle node m is not in Z and no descendant of m is in Z.The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.\nX is a Bayesian network with respect to G if, for any two nodes u, v:\n\n  \n    \n      \n        \n          X\n          \n            u\n          \n        \n        \u22a5\n        \n        \n        \n        \u22a5\n        \n          X\n          \n            v\n          \n        \n        \u2223\n        \n          X\n          \n            Z\n          \n        \n      \n    \n    {\\displaystyle X_{u}\\perp \\!\\!\\!\\perp X_{v}\\mid X_{Z}}\n  where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)\n\n\n=== Causal networks ===\nAlthough Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:\n\n  \n    \n      \n        a\n        \u2192\n        b\n        \u2192\n        c\n        \n        \n          and\n        \n        \n        a\n        \u2190\n        b\n        \u2190\n        c\n      \n    \n    {\\displaystyle a\\rightarrow b\\rightarrow c\\qquad {\\text{and}}\\qquad a\\leftarrow b\\leftarrow c}\n  are equivalent: that is they impose exactly the same conditional independence requirements.\nA causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x. Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.\n\n\n== Inference complexity and approximation algorithms ==\nIn 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error \u025b < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error \u025b < 1/2 with confidence probability greater than 1/2.\nAt about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1\u2212\u025b for every \u025b > 0, even for Bayesian networks with restricted architecture, is NP-hard.In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as na\u00efve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by \n  \n    \n      \n        1\n        \n          /\n        \n        p\n        (\n        n\n        )\n      \n    \n    {\\displaystyle 1/p(n)}\n   where \n  \n    \n      \n        p\n        (\n        n\n        )\n      \n    \n    {\\displaystyle p(n)}\n   was any polynomial of the number of nodes in the network, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  .\n\n\n== Software ==\nNotable software for Bayesian networks include:\n\nJust another Gibbs sampler (JAGS) \u2013 Open-source alternative to WinBUGS. Uses Gibbs sampling.\nOpenBUGS \u2013 Open-source development of WinBUGS.\nSPSS Modeler \u2013 Commercial software that includes an implementation for Bayesian networks.\nStan (software) \u2013 Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo.\nPyMC3 \u2013 A Python library implementing an embedded domain specific language to represent bayesian networks, and a variety of samplers (including NUTS)\nWinBUGS \u2013 One of the first computational implementations of MCMC samplers. No longer maintained.\n\n\n== History ==\nThe term Bayesian network was coined by Judea Pearl in 1985 to emphasize:\nthe often subjective nature of the input information\nthe reliance on Bayes' conditioning as the basis for updating information\nthe distinction between causal and evidential modes of reasoningIn the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems and Neapolitan's Probabilistic Reasoning in Expert Systems summarized their properties and established them as a field of study.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nConrady S, Jouffe L (2015-07-01). Bayesian Networks and BayesiaLab \u2013 A practical introduction for researchers. Franklin, Tennessee: Bayesian USA. ISBN 978-0-9965333-0-0.\nCharniak E (Winter 1991). \"Bayesian networks without tears\" (PDF). AI Magazine.\nKruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P (2013). Computational Intelligence A Methodological Introduction. London: Springer-Verlag. ISBN 978-1-4471-5012-1.\nBorgelt C, Steinbrecher M, Kruse R (2009). Graphical Models \u2013 Representations for Learning, Reasoning and Data Mining (Second ed.). Chichester: Wiley. ISBN 978-0-470-74956-2.\n\n\n== External links ==\nAn Introduction to Bayesian Networks and their Contemporary Applications\nOn-line Tutorial on Bayesian nets and probability\nWeb-App to create Bayesian nets and run it with a Monte Carlo method\nContinuous Time Bayesian Networks\nBayesian Networks: Explanation and Analogy\nA live tutorial on learning Bayesian networks\nA hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples.\nHierarchical Naive Bayes Model for handling sample uncertainty Archived 2007-09-28 at the Wayback Machine, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.", "Sensitivity and specificity": "Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. If individuals who have the condition are considered \"positive\" and those who don't are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\n\nSensitivity (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\nSpecificity (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.If the true status of the condition cannot be known, sensitivity and specificity can be defined relative to a \"gold standard test\" which is assumed correct. For all testing, both diagnostic and screening, there is usually a trade-off between sensitivity and specificity, such that higher sensitivities will mean lower specificities and vice versa.\nA test which reliably detects the presence of a condition, resulting in a high number of true positives and low number of false negatives, will have a high sensitivity. This is especially important when the consequence of failing to treat the condition is serious and/or the treatment is very effective and has minimal side effects.\nA test which reliably excludes individuals who do not have the condition, resulting in a high number of true negatives and low number of false positives, will have a high specificity. This is especially important when people who are identified as having a condition may be subjected to more testing, expense, stigma, anxiety, etc.\n\nThe terms \"sensitivity\" and \"specificity\" were introduced by American biostatistician Jacob Yerushalmy in 1947.\n\n\n== Application to screening study ==\nImagine a study evaluating a test that screens people for a disease. Each person taking the test either has or does not have the disease. The test outcome can be positive (classifying the person as having the disease) or negative (classifying the person as not having the disease). The test results for each subject may or may not match the subject's actual status. In that setting:\n\nTrue positive: Sick people correctly identified as sick\nFalse positive: Healthy people incorrectly identified as sick\nTrue negative: Healthy people correctly identified as healthy\nFalse negative: Sick people incorrectly identified as healthyAfter getting the numbers of true positives, false positives, true negatives, and false negatives, the sensitivity and specificity for the test can be calculated. If it turns out that the sensitivity is high then any person who has the disease is likely to be classified as positive by the test. On the other hand, if the specificity is high, any person who does not have the disease is likely to be classified as negative by the test.  An NIH web site has a discussion of how these ratios are calculated.\n\n\n== Definition ==\n\n\n=== Sensitivity ===\nConsider the example of a medical test for diagnosing a condition. Sensitivity (sometimes also named the detection rate in a clinical setting) refers to the test's ability to correctly detect ill patients out of those who do have the condition. Mathematically, this can be expressed as:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  sensitivity\n                \n              \n              \n                \n                =\n                \n                  \n                    number of true positives\n                    \n                      \n                        number of true positives\n                      \n                      +\n                      \n                        number of false negatives\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    number of true positives\n                    total number of sick individuals in population\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  probability of a positive test given that the patient has the disease\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{sensitivity}}&={\\frac {\\text{number of true positives}}{{\\text{number of true positives}}+{\\text{number of false negatives}}}}\\\\[8pt]&={\\frac {\\text{number of true positives}}{\\text{total number of sick individuals in population}}}\\\\[8pt]&={\\text{probability of a positive test given that the patient has the disease}}\\end{aligned}}}\n  A negative result in a test with high sensitivity can be useful for \"ruling out\" disease, since it rarely misdiagnoses those who do have the disease. A test with 100% sensitivity will recognize all patients with the disease by testing positive. In this case, a negative test result would definitively rule out the presence of the disease in a patient. However, a positive result in a test with high sensitivity is not necessarily useful for \"ruling in\" disease. Suppose a 'bogus' test kit is designed to always give a positive reading. When used on diseased patients, all patients test positive, giving the test 100% sensitivity. However, sensitivity does not take into account false positives. The bogus test also returns positive on all healthy patients, giving it a false positive rate of 100%, rendering it useless for detecting or \"ruling in\" the disease.\nThe calculation of sensitivity does not take into account indeterminate test results.\nIf a test cannot be repeated, indeterminate samples either should be excluded from the analysis (the number of exclusions should be stated when quoting sensitivity) or can be treated as false negatives (which gives the worst-case value for sensitivity and may therefore underestimate it).\nA test with a higher sensitivity has a lower type II error rate.\n\n\n=== Specificity ===\nConsider the example of a medical test for diagnosing a disease. Specificity refers to the test's ability to correctly reject healthy patients without a condition. Mathematically, this can be written as:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  specificity\n                \n              \n              \n                \n                =\n                \n                  \n                    number of true negatives\n                    \n                      \n                        number of true negatives\n                      \n                      +\n                      \n                        number of false positives\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    number of true negatives\n                    total number of well individuals in population\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  probability of a negative test given that the patient is well\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{specificity}}&={\\frac {\\text{number of true negatives}}{{\\text{number of true negatives}}+{\\text{number of false positives}}}}\\\\[8pt]&={\\frac {\\text{number of true negatives}}{\\text{total number of well individuals in population}}}\\\\[8pt]&={\\text{probability of a negative test given that the patient is well}}\\end{aligned}}}\n  A positive result in a test with high specificity can be useful for \"ruling in\" disease, since the test rarely gives positive results in healthy patients. A test with 100% specificity will recognize all patients without the disease by testing negative, so a positive test result would definitively rule in the presence of the disease. However, a negative result from a test with high specificity is not necessarily useful for \"ruling out\" disease. For example, a test that always returns a negative test result will have a specificity of 100% because specificity does not consider false negatives. A test like that would return negative for patients with the disease, making it useless for \"ruling out\" the disease.\nA test with a higher specificity has a lower type I error rate.\n\n\n== Graphical illustration ==\n\n\t\t\n\t\t\nThe above graphical illustration is meant to show the relationship between sensitivity and specificity. The black, dotted line in the center of the graph is where the sensitivity and specificity are the same. As one moves to the left of the black dotted line, the sensitivity increases, reaching its maximum value of 100% at line A, and the specificity decreases. The sensitivity at line A is 100% because at that point there are zero false negatives, meaning that all the negative test results are true negatives. When moving to the right, the opposite applies, the specificity increases until it reaches the B line and becomes 100% and the sensitivity decreases. The specificity at line B is 100% because the number of false positives is zero at that line, meaning all the positive test results are true positives.\nThe middle solid line in both figures that show the level of sensitivity and specificity is the test cutoff point. As previously described, moving this line results in a trade-off between the level of sensitivity and specificity. The left-hand side of this line contains the data points that tests below the cut off point and are considered negative (the blue dots indicate the False Negatives (FN), the white dots True Negatives (TN)). The right-hand side of the line shows the data points that tests above the cut off point and are considered positive (red dots indicate False Positives (FP)). Each side contains 40 data points.\nFor the figure that shows high sensitivity and low specificity, there are 3 FN and 8 FP. Using the fact that positive results = true positives (TP) + FP, we get TP = positive results - FP, or TP = 40 - 8 = 32. The number of sick people in the data set is equal to TP + FN, or 32 + 3 = 35. The sensitivity is therefor 32 / 35 = 91.4%. Using the same method, we get TN = 40 - 3 = 37, and the number of healthy people 37 + 8 = 45, which results in a specificity of 37 / 45 = 82.2 %.\nFor the figure that shows low sensitivity and high specificity, there are 8 FN and 3 FP. Using the same method as the previous figure, we get TP = 40 - 3 = 37. The number of sick people is 37 + 8 = 45, which gives a sensitivity of 37 / 45 = 82.2 %. There are 40 - 8 = 32 TN. The specificity therefor comes out to 32 / 35 = 91.4%. \n\n\t\t\nThe red dot indicates the patient with the medical condition. The red background indicates the area where the test predicts the data point to be positive. The true positive in this figure is 6, and false negatives of 0 (because all positive condition is correctly predicted as positive). Therefore, the sensitivity is 100% (from 6 / (6 + 0)). This situation is also illustrated in the previous figure where the dotted line is at position A (the left-hand side is predicted as negative by the model, the right-hand side is predicted as positive by the model). When the dotted line, test cut-off line, is at position A, the test correctly predicts all the population of the true positive class, but it will fail to correctly identify the data point from the true negative class.\nSimilar to the previously explained figure, the red dot indicates the patient with the medical condition. However, in this case, the green background indicates that the test predicts that all patients are free of the medical condition. The number of data point that is true negative is then 26, and the number of false positives is 0. This result in 100% specificity (from 26 / (26 + 0)). Therefore, sensitivity or specificity alone cannot be used to measure the performance of the test.\n\n\n== Medical usage ==\nIn medical diagnosis, test sensitivity is the ability of a test to correctly identify those with the disease (true positive rate), whereas test specificity is the ability of the test to correctly identify those without the disease (true negative rate).\nIf 100 patients known to have a disease were tested, and 43 test positive, then the test has 43% sensitivity. If 100 with no disease are tested and 96 return a completely negative result, then the test has 96% specificity. Sensitivity and specificity are prevalence-independent test characteristics, as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest. Positive and negative predictive values, but not sensitivity or specificity, are values influenced by the prevalence of disease in the population that is being tested. These concepts are illustrated graphically in this applet Bayesian clinical diagnostic model which show the positive and negative predictive values as a function of the prevalence, sensitivity and specificity.\n\n\n=== Misconceptions ===\nIt is often claimed that a highly specific test is effective at ruling in a disease when positive, while a highly sensitive test is deemed effective at ruling out a disease when negative. This has led to the widely used mnemonics SPPIN and SNNOUT, according to which a highly specific test, when positive, rules in disease (SP-P-IN), and a highly sensitive test, when negative, rules out disease (SN-N-OUT). Both rules of thumb are, however, inferentially misleading, as the diagnostic power of any test is determined by both its sensitivity and its specificity.The tradeoff between specificity and sensitivity is explored in ROC analysis as a trade off between TPR and FPR (that is, recall and fallout). Giving them equal weight optimizes informedness = specificity + sensitivity \u2212 1 = TPR \u2212 FPR, the magnitude of which gives the probability of an informed decision between the two classes (> 0 represents appropriate use of information, 0 represents chance-level performance, < 0 represents perverse use of information).\n\n\n=== Sensitivity index ===\nThe sensitivity index or d\u2032 (pronounced \"dee-prime\") is a statistic used in signal detection theory. It provides the separation between the means of the signal and the noise distributions, compared against the standard deviation of the noise distribution. For normally distributed signal and noise with mean and standard deviations \n  \n    \n      \n        \n          \u03bc\n          \n            S\n          \n        \n      \n    \n    {\\displaystyle \\mu _{S}}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            S\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{S}}\n  , and \n  \n    \n      \n        \n          \u03bc\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mu _{N}}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{N}}\n  , respectively, d\u2032 is defined as:\n\n  \n    \n      \n        \n          d\n          \n            \u2032\n          \n        \n        =\n        \n          \n            \n              \n                \u03bc\n                \n                  S\n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  N\n                \n              \n            \n            \n              \n                \n                  1\n                  2\n                \n              \n              \n                (\n                \n                  \n                    \u03c3\n                    \n                      S\n                    \n                    \n                      2\n                    \n                  \n                  +\n                  \n                    \u03c3\n                    \n                      N\n                    \n                    \n                      2\n                    \n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle d^{\\prime }={\\frac {\\mu _{S}-\\mu _{N}}{\\sqrt {{\\frac {1}{2}}\\left(\\sigma _{S}^{2}+\\sigma _{N}^{2}\\right)}}}}\n  An estimate of d\u2032 can be also found from measurements of the hit rate and false-alarm rate. It is calculated as:\n\nd\u2032 = Z(hit rate) \u2212 Z(false alarm rate),where function Z(p), p \u2208 [0, 1], is the inverse of the cumulative Gaussian distribution.\nd\u2032 is a dimensionless statistic. A higher d\u2032 indicates that the signal can be more readily detected.\n\n\n== Confusion matrix ==\n\nThe relationship between sensitivity, specificity, and similar terms can be understood using the following table. Consider a group with P positive instances and N negative instances of some condition. The four outcomes can be formulated in a 2\u00d72 contingency table or confusion matrix, as well as derivations of several metrics using the four outcomes, as follows:\n\nA worked example\nA diagnostic test with sensitivity 67% and specificity 91% is applied to 2030 people to look for a disorder with a population prevalence of 1.48%Related calculations\n\nFalse positive rate (\u03b1) = type I error = 1 \u2212 specificity = FP / (FP + TN) = 180 / (180 + 1820) = 9%\nFalse negative rate (\u03b2) = type II error = 1 \u2212 sensitivity = FN / (TP + FN) = 10 / (20 + 10) \u2248 33%\nPower = sensitivity = 1 \u2212 \u03b2\nPositive likelihood ratio = sensitivity / (1 \u2212 specificity) \u2248 0.67 / (1 \u2212 0.91) \u2248 7.4\nNegative likelihood ratio = (1 \u2212 sensitivity) / specificity \u2248 (1 \u2212 0.67) / 0.91 \u2248 0.37\nPrevalence threshold = \n  \n    \n      \n        P\n        T\n        =\n        \n          \n            \n              \n                \n                  T\n                  P\n                  R\n                  (\n                  \u2212\n                  T\n                  N\n                  R\n                  +\n                  1\n                  )\n                \n              \n              +\n              T\n              N\n              R\n              \u2212\n              1\n            \n            \n              (\n              T\n              P\n              R\n              +\n              T\n              N\n              R\n              \u2212\n              1\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle PT={\\frac {{\\sqrt {TPR(-TNR+1)}}+TNR-1}{(TPR+TNR-1)}}}\n   \u2248 0.2686 \u2248 26.9%This hypothetical screening test (fecal occult blood test) correctly identified two-thirds (66.7%) of patients with colorectal cancer. Unfortunately, factoring in prevalence rates reveals that this hypothetical test has a high false positive rate, and it does not reliably identify colorectal cancer in the overall population of asymptomatic people (PPV = 10%). \nOn the other hand, this hypothetical test demonstrates very accurate detection of cancer-free individuals (NPV \u2248 99.5%). Therefore, when used for routine colorectal cancer screening with asymptomatic adults, a negative result supplies important data for the patient and doctor, such as ruling out cancer as the cause of gastrointestinal symptoms or reassuring patients worried about developing colorectal cancer.\n\n\n== Estimation of errors in quoted sensitivity or specificity ==\nSensitivity and specificity values alone may be highly misleading.  The 'worst-case' sensitivity or specificity must be calculated in order to avoid reliance on experiments with few results. For example, a particular test may easily show 100% sensitivity if tested against the gold standard four times, but a single additional test against the gold standard that gave a poor result would imply a sensitivity of only 80%.  A common way to do this is to state the binomial proportion confidence interval, often calculated using a Wilson score interval.\nConfidence intervals for sensitivity and specificity can be calculated, giving the range of values within which the correct value lies at a given confidence  level (e.g., 95%).\n\n\n== Terminology in information retrieval ==\nIn information retrieval, the positive predictive value is called precision, and sensitivity is called recall. Unlike the Specificity vs Sensitivity tradeoff, these measures are both independent of the number of true negatives, which is generally unknown and much larger than the actual numbers of relevant and retrieved documents. This assumption of very large numbers of true negatives versus positives is rare in other applications.The F-score can be used as a single measure of performance of the test for the positive class. The F-score is the harmonic mean of precision and recall:\n\n  \n    \n      \n        F\n        =\n        2\n        \u00d7\n        \n          \n            \n              \n                precision\n              \n              \u00d7\n              \n                recall\n              \n            \n            \n              \n                precision\n              \n              +\n              \n                recall\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F=2\\times {\\frac {{\\text{precision}}\\times {\\text{recall}}}{{\\text{precision}}+{\\text{recall}}}}}\n  In the traditional language of statistical hypothesis testing, the sensitivity of a test is called the statistical power of the test, although the word power in that context has a more general usage that is not applicable in the present context.  A sensitive test will have fewer Type II errors.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nUIC Calculator\nVassar College's Sensitivity/Specificity Calculator\nMedCalc Free Online Calculator\nBayesian clinical diagnostic model applet", "K-medoids": "The k-medoids problem is a clustering problem similar to k-means. The name was coined by Leonard Kaufman and Peter J. Rousseeuw with their PAM algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses actual data points as centers (medoids or exemplars), and thereby allows for greater interpretability of the cluster centers than in k-means, where the center of a cluster is not necessarily one of the input data points (it is the average between the points in the cluster). Furthermore, k-medoids can be used with arbitrary dissimilarity measures, whereas k-means generally requires Euclidean distance for efficient solutions. Because k-medoids minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances, it is more robust to noise and outliers than k-means.\nk-medoids is a classical partitioning technique of clustering that splits the data set of n objects into k clusters, where the number k of clusters assumed known a priori (which implies that the programmer must specify k before the execution of a k-medoids algorithm). The \"goodness\" of the given value of k can be assessed with methods such as the silhouette method.\nThe medoid of a cluster is defined as the object in the cluster whose average dissimilarity to all the objects in the cluster is minimal, that is, it is a most centrally located point in the cluster.\n\n\n== Algorithms ==\n\nIn general, the k-medoids problem is NP-hard to solve exactly. As such, many heuristic solutions exist. \n\n\n=== Partitioning Around Medoids (PAM) ===\nPAM uses a greedy search which may not find the optimum solution, but it is faster than exhaustive search. It works as follows:\n\n(BUILD) Initialize: greedily select k of the n data points as the medoids to minimize the cost\nAssociate each data point to the closest medoid.\n(SWAP) While the cost of the configuration decreases:\nFor each medoid m, and for each non-medoid data point o:\nConsider the swap of m and o, and compute the cost change\nIf the cost change is the current best, remember this m and o combination\nPerform the best swap of \n  \n    \n      \n        \n          m\n          \n            best\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{best}}}\n   and \n  \n    \n      \n        \n          o\n          \n            best\n          \n        \n      \n    \n    {\\displaystyle o_{\\text{best}}}\n  , if it decreases the cost function. Otherwise, the algorithm terminates.The runtime complexity of the original PAM algorithm per iteration of (3) is \n  \n    \n      \n        O\n        (\n        k\n        (\n        n\n        \u2212\n        k\n        \n          )\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(k(n-k)^{2})}\n  , by only computing the change in cost. A naive implementation recomputing the entire cost function every time will be in \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        \n          k\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2}k^{2})}\n  . This runtime can be further reduced to \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  , by splitting the cost change into three parts such that computations can be shared or avoided (FastPAM). The runtime can further be reduced by eagerly performing swaps (FasterPAM), at which point a random initialization becomes a viable alternative to BUILD.\n\n\n=== Alternating Optimization ===\nAlgorithms other than PAM have also been suggested in the literature, including the following Voronoi iteration method known as the \"Alternating\" heuristic in literature, as it alternates between two optimization steps:\nSelect initial medoids randomly\nIterate while the cost decreases:\nIn each cluster, make the point that minimizes the sum of distances within the cluster the medoid\nReassign each point to the cluster defined by the closest medoid determined in the previous stepk-means-style Voronoi iteration tends to produce worse results, and exhibit \"erratic behavior\".:\u200a957\u200a Because it does not allow re-assigning points to other clusters while updating means it only explores a smaller search space. It can be shown that even in simple cases this heuristic finds inferior solutions the swap based methods can solve.\n\n\n=== Hierarchical Clustering ===\nMultiple variants of hierarchical clustering with a \"medoid linkage\" have been proposed. The Minimum Sum linkage criterion directly uses the objective of medoids, but the Minimum Sum Increase linkage was shown to produce better results (similar to how Ward linkage uses the increase in squared error). Earlier approaches simply used the distance of the cluster medoids of the previous medoids as linkage measure, but which tends to result in worse solutions, as the distance of two medoids does not ensure there exists a good medoid for the combination. These approaches have a run time complexity of \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3})}\n  , and when the dendrogram is cut at a particular number of clusters k, the results will typically be worse than the results found by PAM. Hence these methods are primarily of interest when a hierarchical tree structure is desired.\n\n\n=== Other Algorithms ===\nOther approximate algorithms such as CLARA and CLARANS trade quality for runtime. CLARA applies PAM on multiple subsamples, keeping the best result. CLARANS works on the entire data set, but only explores a subset of the possible swaps of medoids and non-medoids using sampling. BanditPAM uses the concept of multi-armed bandits to choose candidate swaps instead of uniform sampling as in CLARANS.\n\n\n== Software ==\nELKI includes several k-medoid variants, including a Voronoi-iteration k-medoids, the original PAM algorithm, Reynolds' improvements, and the O(n\u00b2) FastPAM and FasterPAM algorithms, CLARA, CLARANS, FastCLARA and FastCLARANS.\nJulia contains a k-medoid implementation of the k-means style algorithm (fast, but much worse result quality) in the JuliaStats/Clustering.jl package.\nKNIME includes a k-medoid implementation supporting a variety of efficient matrix distance measures, as well as a number of native (and integrated third-party) k-means implementations\nPython contains FasterPAM and other variants in the \"kmedoids\" package, additional implementations can be found in many other packages\nR contains PAM in the \"cluster\" package, including the FasterPAM improvements via the options variant = \"faster\" and medoids = \"random\". There also exists a \"fastkmedoids\" package.\nRapidMiner has an operator named KMedoids, but it does not implement any of above KMedoids algorithms. Instead, it is a k-means variant, that substitutes the mean with the closest data point (which is not the medoid), which combines the drawbacks of k-means (limited to coordinate data) with the additional cost of finding the nearest point to the mean.\nRust has a \"kmedoids\" crate that also includes the FasterPAM variant.\nMATLAB implements PAM, CLARA, and two other algorithms to solve the k-medoid clustering problem.\n\n\n== References ==", "Backpropagation": "In machine learning, backpropagation  is a widely used algorithm for training feedforward artificial neural networks or other parameterized networks with differentiable nodes. It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as \nthe reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nThe term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent. In 1986, David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.\n\n\n== Overview ==\nBackpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:\n\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  : input (vector of features)\n\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  : target output\nFor classification, output will be a vector of class probabilities (e.g., \n  \n    \n      \n        (\n        0.1\n        ,\n        0.7\n        ,\n        0.2\n        )\n      \n    \n    {\\displaystyle (0.1,0.7,0.2)}\n  , and target output is a specific class, encoded by the one-hot/dummy variable (e.g., \n  \n    \n      \n        (\n        0\n        ,\n        1\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (0,1,0)}\n  ).\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  : loss function or \"cost function\"For classification, this is usually cross entropy (XC, log loss), while for regression it is usually squared error loss (SEL).\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  : the number of layers\n\n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n        =\n        (\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle W^{l}=(w_{jk}^{l})}\n  : the weights between layer \n  \n    \n      \n        l\n        \u2212\n        1\n      \n    \n    {\\displaystyle l-1}\n   and \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  , where \n  \n    \n      \n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle w_{jk}^{l}}\n   is the weight between the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -th node in layer \n  \n    \n      \n        l\n        \u2212\n        1\n      \n    \n    {\\displaystyle l-1}\n   and the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th node in layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n\n  \n    \n      \n        \n          f\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle f^{l}}\n  : activation functions at layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \nFor classification the last layer is usually the logistic function for binary classification, and softmax (softargmax) for multi-class classification, while for the hidden layers this was traditionally a sigmoid function (logistic function or others) on each node (coordinate), but today is more varied, with rectifier (ramp, ReLU) being common.In the derivation of backpropagation, other intermediate quantities are used; they are introduced as needed below. Bias terms are not treated specially, as they correspond to a weight with a fixed input of 1. For the purpose of backpropagation, the specific loss function and activation functions do not matter, as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include but are not limited to sigmoid, tanh, and ReLU. Since, swish, mish, and other activation functions were proposed as well.\nThe overall network is a combination of function composition and matrix multiplication:\n\n  \n    \n      \n        g\n        (\n        x\n        )\n        :=\n        \n          f\n          \n            L\n          \n        \n        (\n        \n          W\n          \n            L\n          \n        \n        \n          f\n          \n            L\n            \u2212\n            1\n          \n        \n        (\n        \n          W\n          \n            L\n            \u2212\n            1\n          \n        \n        \u22ef\n        \n          f\n          \n            1\n          \n        \n        (\n        \n          W\n          \n            1\n          \n        \n        x\n        )\n        \u22ef\n        )\n        )\n      \n    \n    {\\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{1}(W^{1}x)\\cdots ))}\n  For a training set there will be a set of input\u2013output pairs, \n  \n    \n      \n        \n          {\n          \n            (\n            \n              x\n              \n                i\n              \n            \n            ,\n            \n              y\n              \n                i\n              \n            \n            )\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{(x_{i},y_{i})\\right\\}}\n  . For each input\u2013output pair \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{i},y_{i})}\n   in the training set, the loss of the model on that pair is the cost of the difference between the predicted output \n  \n    \n      \n        g\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle g(x_{i})}\n   and the target output \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  :\n\n  \n    \n      \n        C\n        (\n        \n          y\n          \n            i\n          \n        \n        ,\n        g\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle C(y_{i},g(x_{i}))}\n  Note the distinction: during model evaluation, the weights are fixed, while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training, the input\u2013output pair is fixed, while the weights vary, and the network ends with the loss function.\nBackpropagation computes the gradient for a fixed input\u2013output pair \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{i},y_{i})}\n  , where the weights \n  \n    \n      \n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle w_{jk}^{l}}\n   can vary. Each individual component of the gradient, \n  \n    \n      \n        \u2202\n        C\n        \n          /\n        \n        \u2202\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\partial C/\\partial w_{jk}^{l},}\n   can be computed by the chain rule; however, doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer \u2013 specifically, the gradient of the weighted input of each layer, denoted by \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   \u2013 from back to front.\nInformally, the key point is that since the only way a weight in \n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle W^{l}}\n   affects the loss is through its effect on the next layer, and it does so linearly, \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   are the only data you need to compute the gradients of the weights at layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  , and then you can compute the previous layer \n  \n    \n      \n        \n          \u03b4\n          \n            l\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l-1}}\n   and repeat recursively. This avoids inefficiency in two ways. Firstly, it avoids duplication because when computing the gradient at layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  , you do not need to recompute all the derivatives on later layers \n  \n    \n      \n        l\n        +\n        1\n        ,\n        l\n        +\n        2\n        ,\n        \u2026\n      \n    \n    {\\displaystyle l+1,l+2,\\ldots }\n   each time. Secondly, it avoids unnecessary intermediate calculations because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights \n  \n    \n      \n        \u2202\n        \n          a\n          \n            \n              j\n              \u2032\n            \n          \n          \n            \n              l\n              \u2032\n            \n          \n        \n        \n          /\n        \n        \u2202\n        \n          w\n          \n            j\n            k\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\partial a_{j'}^{l'}/\\partial w_{jk}^{l}}\n  .\nBackpropagation can be expressed for simple feedforward networks in terms of matrix multiplication, or more generally in terms of the adjoint graph.\n\n\n== Matrix multiplication ==\nFor the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication. Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left \u2013 \"backwards\" \u2013 with the gradient of the weights between each layer being a simple modification of the partial products (the \"backwards propagated error\").\nGiven an input\u2013output pair \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  , the loss is:\n\n  \n    \n      \n        C\n        (\n        y\n        ,\n        \n          f\n          \n            L\n          \n        \n        (\n        \n          W\n          \n            L\n          \n        \n        \n          f\n          \n            L\n            \u2212\n            1\n          \n        \n        (\n        \n          W\n          \n            L\n            \u2212\n            1\n          \n        \n        \u22ef\n        \n          f\n          \n            2\n          \n        \n        (\n        \n          W\n          \n            2\n          \n        \n        \n          f\n          \n            1\n          \n        \n        (\n        \n          W\n          \n            1\n          \n        \n        x\n        )\n        )\n        \u22ef\n        )\n        )\n        )\n      \n    \n    {\\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{2}(W^{2}f^{1}(W^{1}x))\\cdots )))}\n  To compute this, one starts with the input \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and works forward; denote the weighted input of each hidden layer as \n  \n    \n      \n        \n          z\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle z^{l}}\n   and the output of hidden layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   as the activation \n  \n    \n      \n        \n          a\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle a^{l}}\n  . For backpropagation, the activation \n  \n    \n      \n        \n          a\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle a^{l}}\n   as well as the derivatives \n  \n    \n      \n        (\n        \n          f\n          \n            l\n          \n        \n        \n          )\n          \u2032\n        \n      \n    \n    {\\displaystyle (f^{l})'}\n   (evaluated at \n  \n    \n      \n        \n          z\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle z^{l}}\n  ) must be cached for use during the backwards pass.\nThe derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  :\n\n  \n    \n      \n        \n          \n            \n              d\n              C\n            \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n          \n        \n        \u2218\n        \n          \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  L\n                \n              \n            \n          \n        \n        \u22c5\n        \n          \n            \n              d\n              \n                z\n                \n                  L\n                \n              \n            \n            \n              d\n              \n                a\n                \n                  L\n                  \u2212\n                  1\n                \n              \n            \n          \n        \n        \u2218\n        \n          \n            \n              d\n              \n                a\n                \n                  L\n                  \u2212\n                  1\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  L\n                  \u2212\n                  1\n                \n              \n            \n          \n        \n        \u22c5\n        \n          \n            \n              d\n              \n                z\n                \n                  L\n                  \u2212\n                  1\n                \n              \n            \n            \n              d\n              \n                a\n                \n                  L\n                  \u2212\n                  2\n                \n              \n            \n          \n        \n        \u2218\n        \u2026\n        \u2218\n        \n          \n            \n              d\n              \n                a\n                \n                  1\n                \n              \n            \n            \n              d\n              \n                z\n                \n                  1\n                \n              \n            \n          \n        \n        \u22c5\n        \n          \n            \n              \u2202\n              \n                z\n                \n                  1\n                \n              \n            \n            \n              \u2202\n              x\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {dC}{da^{L}}}\\circ {\\frac {da^{L}}{dz^{L}}}\\cdot {\\frac {dz^{L}}{da^{L-1}}}\\circ {\\frac {da^{L-1}}{dz^{L-1}}}\\cdot {\\frac {dz^{L-1}}{da^{L-2}}}\\circ \\ldots \\circ {\\frac {da^{1}}{dz^{1}}}\\cdot {\\frac {\\partial z^{1}}{\\partial x}},}\n  where \n  \n    \n      \n        \u2218\n      \n    \n    {\\displaystyle \\circ }\n   is a Hadamard product, that is an element-wise product.\nThese terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights:\n\n  \n    \n      \n        \n          \n            \n              d\n              C\n            \n            \n              d\n              \n                a\n                \n                  L\n                \n              \n            \n          \n        \n        \u2218\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          \u2032\n        \n        \u22c5\n        \n          W\n          \n            L\n          \n        \n        \u2218\n        (\n        \n          f\n          \n            L\n            \u2212\n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u22c5\n        \n          W\n          \n            L\n            \u2212\n            1\n          \n        \n        \u2218\n        \u22ef\n        \u2218\n        (\n        \n          f\n          \n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u22c5\n        \n          W\n          \n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dC}{da^{L}}}\\circ (f^{L})'\\cdot W^{L}\\circ (f^{L-1})'\\cdot W^{L-1}\\circ \\cdots \\circ (f^{1})'\\cdot W^{1}.}\n  The gradient \n  \n    \n      \n        \u2207\n      \n    \n    {\\displaystyle \\nabla }\n   is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:\n\n  \n    \n      \n        \n          \u2207\n          \n            x\n          \n        \n        C\n        =\n        (\n        \n          W\n          \n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        (\n        \n          f\n          \n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        \u2026\n        \u2218\n        (\n        \n          W\n          \n            L\n            \u2212\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        (\n        \n          f\n          \n            L\n            \u2212\n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        (\n        \n          W\n          \n            L\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        \n          \u2207\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n        .\n      \n    \n    {\\displaystyle \\nabla _{x}C=(W^{1})^{T}\\cdot (f^{1})'\\circ \\ldots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C.}\n  Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights isn't just a subexpression: there's an extra multiplication.\nIntroducing the auxiliary quantity \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   for the partial products (multiplying from right to left), interpreted as the \"error at level \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \" and defined as the gradient of the input values at level \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  :\n\n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n        :=\n        (\n        \n          f\n          \n            l\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        (\n        \n          W\n          \n            l\n            +\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u2218\n        \u22ef\n        \u2218\n        (\n        \n          W\n          \n            L\n            \u2212\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        (\n        \n          f\n          \n            L\n            \u2212\n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        (\n        \n          W\n          \n            L\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        (\n        \n          f\n          \n            L\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        \n          \u2207\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n        .\n      \n    \n    {\\displaystyle \\delta ^{l}:=(f^{l})'\\circ (W^{l+1})^{T}\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C.}\n  Note that \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   is a vector, of length equal to the number of nodes in level \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  ; each component is interpreted as the \"cost attributable to (the value of) that node\".\nThe gradient of the weights in layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   is then:\n\n  \n    \n      \n        \n          \u2207\n          \n            \n              W\n              \n                l\n              \n            \n          \n        \n        C\n        =\n        \n          \u03b4\n          \n            l\n          \n        \n        (\n        \n          a\n          \n            l\n            \u2212\n            1\n          \n        \n        \n          )\n          \n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle \\nabla _{W^{l}}C=\\delta ^{l}(a^{l-1})^{T}.}\n  The factor of \n  \n    \n      \n        \n          a\n          \n            l\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle a^{l-1}}\n   is because the weights \n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle W^{l}}\n   between level \n  \n    \n      \n        l\n        \u2212\n        1\n      \n    \n    {\\displaystyle l-1}\n   and \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   affect level \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   proportionally to the inputs (activations): the inputs are fixed, the weights vary.\nThe \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   can easily be computed recursively, going from right to left, as:\n\n  \n    \n      \n        \n          \u03b4\n          \n            l\n            \u2212\n            1\n          \n        \n        :=\n        (\n        \n          f\n          \n            l\n            \u2212\n            1\n          \n        \n        \n          )\n          \u2032\n        \n        \u2218\n        (\n        \n          W\n          \n            l\n          \n        \n        \n          )\n          \n            T\n          \n        \n        \u22c5\n        \n          \u03b4\n          \n            l\n          \n        \n        .\n      \n    \n    {\\displaystyle \\delta ^{l-1}:=(f^{l-1})'\\circ (W^{l})^{T}\\cdot \\delta ^{l}.}\n  The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.\nCompared with naively computing forwards (using the \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   for illustration):\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b4\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    1\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                (\n                \n                  W\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    2\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \u22ef\n                \u2218\n                (\n                \n                  W\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \n                  \u2207\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n                \n                  \u03b4\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    2\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \u22ef\n                \u2218\n                (\n                \n                  W\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \n                  \u2207\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n              \n                \n                \u22ee\n              \n            \n            \n              \n                \n                  \u03b4\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    L\n                    \u2212\n                    1\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                (\n                \n                  W\n                  \n                    L\n                  \n                \n                \n                  )\n                  \n                    T\n                  \n                \n                \u22c5\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \n                  \u2207\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n              \n            \n            \n              \n                \n                  \u03b4\n                  \n                    L\n                  \n                \n              \n              \n                \n                =\n                (\n                \n                  f\n                  \n                    L\n                  \n                \n                \n                  )\n                  \u2032\n                \n                \u2218\n                \n                  \u2207\n                  \n                    \n                      a\n                      \n                        L\n                      \n                    \n                  \n                \n                C\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\delta ^{1}&=(f^{1})'\\circ (W^{2})^{T}\\cdot (f^{2})'\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\\\delta ^{2}&=(f^{2})'\\circ \\cdots \\circ (W^{L-1})^{T}\\cdot (f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\&\\vdots \\\\\\delta ^{L-1}&=(f^{L-1})'\\circ (W^{L})^{T}\\cdot (f^{L})'\\circ \\nabla _{a^{L}}C\\\\\\delta ^{L}&=(f^{L})'\\circ \\nabla _{a^{L}}C,\\end{aligned}}}\n  there are two key differences with backpropagation:\n\nComputing \n  \n    \n      \n        \n          \u03b4\n          \n            l\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l-1}}\n   in terms of \n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n   avoids the obvious duplicate multiplication of layers \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   and beyond.\nMultiplying starting from \n  \n    \n      \n        \n          \u2207\n          \n            \n              a\n              \n                L\n              \n            \n          \n        \n        C\n      \n    \n    {\\displaystyle \\nabla _{a^{L}}C}\n   \u2013 propagating the error backwards \u2013 means that each step simply multiplies a vector (\n  \n    \n      \n        \n          \u03b4\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{l}}\n  ) by the matrices of weights \n  \n    \n      \n        (\n        \n          W\n          \n            l\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle (W^{l})^{T}}\n   and derivatives of activations \n  \n    \n      \n        (\n        \n          f\n          \n            l\n            \u2212\n            1\n          \n        \n        \n          )\n          \u2032\n        \n      \n    \n    {\\displaystyle (f^{l-1})'}\n  . By contrast, multiplying forwards, starting from the changes at an earlier layer, means that each multiplication multiplies a matrix by a matrix. This is much more expensive, and corresponds to tracking every possible path of a change in one layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   forward to changes in the layer \n  \n    \n      \n        l\n        +\n        2\n      \n    \n    {\\displaystyle l+2}\n   (for multiplying \n  \n    \n      \n        \n          W\n          \n            l\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle W^{l+1}}\n   by \n  \n    \n      \n        \n          W\n          \n            l\n            +\n            2\n          \n        \n      \n    \n    {\\displaystyle W^{l+2}}\n  , with additional multiplications for the derivatives of the activations), which unnecessarily computes the intermediate quantities of how weight changes affect the values of hidden nodes.\n\n\n== Adjoint graph ==\nFor more general graphs, and other advanced variations, backpropagation can be understood in terms of automatic differentiation, where backpropagation is a special case of reverse accumulation (or \"reverse mode\").\n\n\n== Intuition ==\n\n\n=== Motivation ===\nThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n\n\n=== Learning as an optimization problem ===\nTo understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input. \nInitially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consist of a set of tuples \n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle (x_{1},x_{2},t)}\n   where \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n   are the inputs to the network and t is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  , will compute an output y that likely differs from t (given random weights). A loss function \n  \n    \n      \n        L\n        (\n        t\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle L(t,y)}\n   is used for measuring the discrepancy between the target output t and the computed output y. For regression analysis problems the squared error can be used as a loss function, for classification the categorical crossentropy can be used.\nAs an example consider a regression problem using the square error as a loss:\n\n  \n    \n      \n        L\n        (\n        t\n        ,\n        y\n        )\n        =\n        (\n        t\n        \u2212\n        y\n        \n          )\n          \n            2\n          \n        \n        =\n        E\n        ,\n      \n    \n    {\\displaystyle L(t,y)=(t-y)^{2}=E,}\n  where E is the discrepancy or error.\n\nConsider the network on a single training case: \n  \n    \n      \n        (\n        1\n        ,\n        1\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (1,1,0)}\n  . Thus, the input \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n   are 1 and 1 respectively and the correct output, t is 0. Now if the relation is plotted between the network's output y on the horizontal axis and the error E on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output y which minimizes the error E. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output y that exactly matches the target output t. Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error. \nHowever, the output of a neuron depends on the weighted sum of all its inputs:\n\n  \n    \n      \n        y\n        =\n        \n          x\n          \n            1\n          \n        \n        \n          w\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          w\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}\n  where \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle w_{1}}\n   and \n  \n    \n      \n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{2}}\n   are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.\nIn this example, upon injecting the training data \n  \n    \n      \n        (\n        1\n        ,\n        1\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (1,1,0)}\n  , the loss function becomes\n\n  \n    \n      \n        E\n        =\n        (\n        t\n        \u2212\n        y\n        \n          )\n          \n            2\n          \n        \n        =\n        \n          y\n          \n            2\n          \n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        \n          w\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          w\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        (\n        \n          w\n          \n            1\n          \n        \n        +\n        \n          w\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}\n  \nThen, the loss function \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   takes the form of a parabolic cylinder with its base directed along \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        =\n        \u2212\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{1}=-w_{2}}\n  . Since all sets of weights that satisfy \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        =\n        \u2212\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{1}=-w_{2}}\n   minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.\nOne commonly used algorithm to find the set of weights that minimizes the error is gradient descent. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.\n\n\n== Derivation ==\nThe gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared error function is\n\n  \n    \n      \n        E\n        =\n        L\n        (\n        t\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle E=L(t,y)}\n  where\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   is the loss for the output \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and target value \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  ,\n\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is the target output for a training sample, and\n\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is the actual output of the output neuron.For each neuron \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  , its output \n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle o_{j}}\n   is defined as\n\n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n        =\n        \u03c6\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        =\n        \u03c6\n        \n          (\n          \n            \n              \u2211\n              \n                k\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                k\n                j\n              \n            \n            \n              x\n              \n                k\n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle o_{j}=\\varphi ({\\text{net}}_{j})=\\varphi \\left(\\sum _{k=1}^{n}w_{kj}x_{k}\\right),}\n  where the activation function \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   is non-linear and differentiable over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the logistic function:\n\n  \n    \n      \n        \u03c6\n        (\n        z\n        )\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                e\n                \n                  \u2212\n                  z\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\varphi (z)={\\frac {1}{1+e^{-z}}}}\n  which has a convenient derivative of:\n\n  \n    \n      \n        \n          \n            \n              d\n              \u03c6\n              (\n              z\n              )\n            \n            \n              d\n              z\n            \n          \n        \n        =\n        \u03c6\n        (\n        z\n        )\n        (\n        1\n        \u2212\n        \u03c6\n        (\n        z\n        )\n        )\n      \n    \n    {\\displaystyle {\\frac {d\\varphi (z)}{dz}}=\\varphi (z)(1-\\varphi (z))}\n  The input \n  \n    \n      \n        \n          \n            net\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\text{net}}_{j}}\n   to a neuron is the weighted sum of outputs \n  \n    \n      \n        \n          o\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle o_{k}}\n   of previous neurons. If the neuron is in the first layer after the input layer, the \n  \n    \n      \n        \n          o\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle o_{k}}\n   of the input layer are simply the inputs \n  \n    \n      \n        \n          x\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle x_{k}}\n   to the network. The number of input units to the neuron is \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . The variable \n  \n    \n      \n        \n          w\n          \n            k\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{kj}}\n   denotes the weight between neuron \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   of the previous layer and neuron \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   of the current layer.\n\n\n=== Finding the derivative of the error ===\n\nCalculating the partial derivative of the error with respect to a weight \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   is done using the chain rule twice:\n\nIn the last factor of the right-hand side of the above, only one term in the sum \n  \n    \n      \n        \n          \n            net\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\text{net}}_{j}}\n   depends on \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n  , so that\n\nIf the neuron is in the first layer after the input layer, \n  \n    \n      \n        \n          o\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle o_{i}}\n   is just \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  .\nThe derivative of the output of neuron \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   with respect to its input is simply the partial derivative of the activation function:\n\nwhich for the logistic activation function \n\n  \n    \n      \n        \n          \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \u2202\n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        \u03c6\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        =\n        \u03c6\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        (\n        1\n        \u2212\n        \u03c6\n        (\n        \n          \n            net\n          \n          \n            j\n          \n        \n        )\n        )\n        =\n        \n          o\n          \n            j\n          \n        \n        (\n        1\n        \u2212\n        \n          o\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\frac {\\partial }{\\partial {\\text{net}}_{j}}}\\varphi ({\\text{net}}_{j})=\\varphi ({\\text{net}}_{j})(1-\\varphi ({\\text{net}}_{j}))=o_{j}(1-o_{j})}\n  This is the reason why backpropagation requires that the activation function be differentiable. (Nevertheless, the ReLU activation function, which is non-differentiable at 0, has become quite popular, e.g. in AlexNet)\nThe first factor is straightforward to evaluate if the neuron is in the output layer, because then \n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n        =\n        y\n      \n    \n    {\\displaystyle o_{j}=y}\n   and\n\nIf half of the square error is used as loss function we can rewrite it as\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              y\n            \n          \n        \n        =\n        \n          \n            \u2202\n            \n              \u2202\n              y\n            \n          \n        \n        \n          \n            1\n            2\n          \n        \n        (\n        t\n        \u2212\n        y\n        \n          )\n          \n            2\n          \n        \n        =\n        y\n        \u2212\n        t\n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial o_{j}}}={\\frac {\\partial E}{\\partial y}}={\\frac {\\partial }{\\partial y}}{\\frac {1}{2}}(t-y)^{2}=y-t}\n  However, if \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   is in an arbitrary inner layer of the network, finding the derivative \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   with respect to \n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle o_{j}}\n   is less obvious.\nConsidering \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   as a function with the inputs being all neurons \n  \n    \n      \n        L\n        =\n        {\n        u\n        ,\n        v\n        ,\n        \u2026\n        ,\n        w\n        }\n      \n    \n    {\\displaystyle L=\\{u,v,\\dots ,w\\}}\n   receiving input from neuron \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  ,\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n              (\n              \n                o\n                \n                  j\n                \n              \n              )\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n              (\n              \n                \n                  n\n                  e\n                  t\n                \n                \n                  u\n                \n              \n              ,\n              \n                \n                  net\n                \n                \n                  v\n                \n              \n              ,\n              \u2026\n              ,\n              \n                \n                  n\n                  e\n                  t\n                \n                \n                  w\n                \n              \n              )\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E(o_{j})}{\\partial o_{j}}}={\\frac {\\partial E(\\mathrm {net} _{u},{\\text{net}}_{v},\\dots ,\\mathrm {net} _{w})}{\\partial o_{j}}}}\n  and taking the total derivative with respect to \n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle o_{j}}\n  , a recursive expression for the derivative is obtained:\n\nTherefore, the derivative with respect to \n  \n    \n      \n        \n          o\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle o_{j}}\n   can be calculated if all the derivatives with respect to the outputs \n  \n    \n      \n        \n          o\n          \n            \u2113\n          \n        \n      \n    \n    {\\displaystyle o_{\\ell }}\n   of the next layer \u2013 the ones closer to the output neuron \u2013 are known. [Note, if any of the neurons in set \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   were not connected to neuron \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  , they would be independent of \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   and the corresponding partial derivative under the summation would vanish to 0.]\nSubstituting Eq. 2, Eq. 3 Eq.4 and Eq. 5 in Eq. 1  we obtain:\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        \n          o\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}{\\frac {\\partial {\\text{net}}_{j}}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}o_{i}}\n  \n\n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \n          o\n          \n            i\n          \n        \n        \n          \u03b4\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}=o_{i}\\delta _{j}}\n  with\n\n  \n    \n      \n        \n          \u03b4\n          \n            j\n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      \n                        \u2202\n                        L\n                        (\n                        \n                          o\n                          \n                            j\n                          \n                        \n                        ,\n                        t\n                        )\n                      \n                      \n                        \u2202\n                        \n                          o\n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                  \n                    \n                      \n                        d\n                        \u03c6\n                        (\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                        )\n                      \n                      \n                        d\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an output neuron,\n                  \n                \n              \n              \n                \n                  (\n                  \n                    \u2211\n                    \n                      \u2113\n                      \u2208\n                      L\n                    \n                  \n                  \n                    w\n                    \n                      j\n                      \u2113\n                    \n                  \n                  \n                    \u03b4\n                    \n                      \u2113\n                    \n                  \n                  )\n                  \n                    \n                      \n                        d\n                        \u03c6\n                        (\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                        )\n                      \n                      \n                        d\n                        \n                          \n                            net\n                          \n                          \n                            j\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an inner neuron.\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\delta _{j}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\begin{cases}{\\frac {\\partial L(o_{j},t)}{\\partial o_{j}}}{\\frac {d\\varphi ({\\text{net}}_{j})}{d{\\text{net}}_{j}}}&{\\text{if }}j{\\text{ is an output neuron,}}\\\\(\\sum _{\\ell \\in L}w_{j\\ell }\\delta _{\\ell }){\\frac {d\\varphi ({\\text{net}}_{j})}{d{\\text{net}}_{j}}}&{\\text{if }}j{\\text{ is an inner neuron.}}\\end{cases}}}\n  if \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   is the logistic function, and the error is the square error:\n\n  \n    \n      \n        \n          \u03b4\n          \n            j\n          \n        \n        =\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n          \n        \n        \n          \n            \n              \u2202\n              \n                o\n                \n                  j\n                \n              \n            \n            \n              \u2202\n              \n                \n                  net\n                \n                \n                  j\n                \n              \n            \n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  (\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  \u2212\n                  \n                    t\n                    \n                      j\n                    \n                  \n                  )\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  (\n                  1\n                  \u2212\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  )\n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an output neuron,\n                  \n                \n              \n              \n                \n                  (\n                  \n                    \u2211\n                    \n                      \u2113\n                      \u2208\n                      L\n                    \n                  \n                  \n                    w\n                    \n                      j\n                      \u2113\n                    \n                  \n                  \n                    \u03b4\n                    \n                      \u2113\n                    \n                  \n                  )\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  (\n                  1\n                  \u2212\n                  \n                    o\n                    \n                      j\n                    \n                  \n                  )\n                \n                \n                  \n                    if \n                  \n                  j\n                  \n                     is an inner neuron.\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\delta _{j}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}={\\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&{\\text{if }}j{\\text{ is an output neuron,}}\\\\(\\sum _{\\ell \\in L}w_{j\\ell }\\delta _{\\ell })o_{j}(1-o_{j})&{\\text{if }}j{\\text{ is an inner neuron.}}\\end{cases}}}\n  To update the weight \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   using gradient descent, one must choose a learning rate, \n  \n    \n      \n        \u03b7\n        >\n        0\n      \n    \n    {\\displaystyle \\eta >0}\n  . The change in weight needs to reflect the impact on \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   of an increase or decrease in \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n  . If \n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}>0}\n  , an increase in \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   increases \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  ; conversely, if \n  \n    \n      \n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        <\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}<0}\n  , an increase in \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   decreases \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  . The new \n  \n    \n      \n        \u0394\n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{ij}}\n   is added to the old weight, and the product of the learning rate and the gradient, multiplied by \n  \n    \n      \n        \u2212\n        1\n      \n    \n    {\\displaystyle -1}\n   guarantees that \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   changes in a way that always decreases \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  . In other words, in the equation immediately below, \n  \n    \n      \n        \u2212\n        \u03b7\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle -\\eta {\\frac {\\partial E}{\\partial w_{ij}}}}\n   always changes \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   in such a way that \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   is decreased:\n\n  \n    \n      \n        \u0394\n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        \u2212\n        \u03b7\n        \n          \n            \n              \u2202\n              E\n            \n            \n              \u2202\n              \n                w\n                \n                  i\n                  j\n                \n              \n            \n          \n        \n        =\n        \u2212\n        \u03b7\n        \n          o\n          \n            i\n          \n        \n        \n          \u03b4\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{ij}=-\\eta {\\frac {\\partial E}{\\partial w_{ij}}}=-\\eta o_{i}\\delta _{j}}\n  \n\n\n== Second-order gradient descent ==\nUsing a Hessian matrix of second-order derivatives of the error function, the Levenberg-Marquardt algorithm often converges faster than first-order gradient descent, especially when the topology of the error function is complicated. It may also find solutions in smaller node counts for which other methods might not converge. The Hessian can be approximated by the Fisher information matrix.\n\n\n== Loss function ==\n\nThe loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.\n\n\n=== Assumptions ===\nThe mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation. The first is that it can be written as an average \n  \n    \n      \n        E\n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            x\n          \n        \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\textstyle E={\\frac {1}{n}}\\sum _{x}E_{x}}\n   over error functions \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\textstyle E_{x}}\n  , for \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   individual training examples, \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  . The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function.  The second assumption is that it can be written as a function of the outputs from the neural network.\n\n\n=== Example loss function ===\nLet \n  \n    \n      \n        y\n        ,\n        \n          y\n          \u2032\n        \n      \n    \n    {\\displaystyle y,y'}\n   be vectors in \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  .\nSelect an error function \n  \n    \n      \n        E\n        (\n        y\n        ,\n        \n          y\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle E(y,y')}\n   measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and \n  \n    \n      \n        \n          y\n          \u2032\n        \n      \n    \n    {\\displaystyle y'}\n  :The error function over \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n   training examples can then be written as an average of losses over individual examples:\n\n\n== Limitations ==\n\nGradient descent with backpropagation is not guaranteed to find the global minimum of the error function, but only a local minimum; also, it has trouble crossing plateaus in the error function landscape. This issue, caused by the non-convexity of error functions in neural networks, was long thought to be a major drawback, but Yann LeCun et al. argue that in many practical problems, it is not.\nBackpropagation learning does not require normalization of input vectors; however, normalization could improve performance.\nBackpropagation requires the derivatives of activation functions to be known at network design time.\n\n\n== History ==\n\nModern backpropagation is Seppo Linnainmaa's reverse mode of automatic differentiation (1970) for discrete connected networks of nested differentiable functions. It is an efficient application of the chain rule (derived by Gottfried Wilhelm Leibniz in 1673) to such networks. The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. The first deep learning multilayer perceptron (MLP) trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments, his five layer MLP with two modifiable layers learned  internal representations required to classify non-linearily separable pattern classes. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. \nIn 1985, David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.Kelley (1960) and Arthur E. Bryson (1961) used principles of dynamic programming to derive the above-mentioned continuous precursor of the method. In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. \nIn 1973, he adapted parameters of controllers in proportion to error gradients. Unlike Linnainmaa's 1970 method, these precursors used \"standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.\"In 1985, the method was also described by Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987. In 1993, Eric Wan won an international pattern recognition contest through backpropagation.During the 2000s it fell out of favour, but returned in the 2010s, benefitting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.).\nError backpropagation has been suggested to explain human brain ERP components like the N400 and P600.\n\n\n== See also ==\nArtificial neural network\nNeural circuit\nCatastrophic interference\nEnsemble learning\nAdaBoost\nOverfitting\nNeural backpropagation\nBackpropagation through time\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nGoodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). \"6.5 Back-Propagation and Other Differentiation Algorithms\". Deep Learning. MIT Press. pp. 200\u2013220. ISBN 9780262035613.\nNielsen, Michael A. (2015). \"How the backpropagation algorithm works\". Neural Networks and Deep Learning. Determination Press.\nMcCaffrey, James (October 2012). \"Neural Network Back-Propagation for Programmers\". MSDN Magazine.\nRojas, Ra\u00fal (1996). \"The Backpropagation Algorithm\" (PDF). Neural Networks : A Systematic Introduction. Berlin: Springer. ISBN 3-540-60505-3.\n\n\n== External links ==\nBackpropagation neural network tutorial at the Wikiversity\nBernacki, Mariusz; W\u0142odarczyk, Przemys\u0142aw (2004). \"Principles of training multi-layer neural network using backpropagation\".\nKarpathy, Andrej (2016). \"Lecture 4: Backpropagation, Neural Networks 1\". CS231n. Stanford University. Archived from the original on 2021-12-12 \u2013 via YouTube.\n\"What is Backpropagation Really Doing?\". 3Blue1Brown. November 3, 2017. Archived from the original on 2021-12-12 \u2013 via YouTube.\nPutta, Sudeep Raja (2022). \"Yet Another Derivation of Backpropagation in Matrix Form\".", "Random forest": "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.:\u200a587\u2013588\u200a Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered \"Random Forests\" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\nRandom forests are frequently used as black box models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.\n\n\n== History ==\nThe general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.The early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree.  The idea of random subspace selection from Ho was also influential in the design of random forests.  In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node.  Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Thomas G. Dietterich.The proper introduction of random forests was made in a paper\nby Leo Breiman.  This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging.  In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\nUsing out-of-bag error as an estimate of the generalization error.\nMeasuring variable importance through permutation.The report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\n\n== Algorithm ==\n\n\n=== Preliminaries: decision tree learning ===\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie et al., \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".:\u200a352\u200aIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.:\u200a587\u2013588\u200a This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\nForests are like the pulling together of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random tree. Though not quite similar, forests give the effects of a k-fold cross validation.\n\n\n=== Bagging ===\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nFor b = 1, ..., B:\nSample, with replacement, n training examples from X, Y; call these Xb, Yb.\nTrain a classification or regression tree fb on Xb, Yb.After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':\n\n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            B\n          \n        \n        \n          \u2211\n          \n            b\n            =\n            1\n          \n          \n            B\n          \n        \n        \n          f\n          \n            b\n          \n        \n        (\n        \n          x\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle {\\hat {f}}={\\frac {1}{B}}\\sum _{b=1}^{B}f_{b}(x')}\n  or by taking the majority vote in the case of classification trees.\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on x':\n\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            \n              \n                \n                  \u2211\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                (\n                \n                  f\n                  \n                    b\n                  \n                \n                (\n                \n                  x\n                  \u2032\n                \n                )\n                \u2212\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                B\n                \u2212\n                1\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\frac {\\sum _{b=1}^{B}(f_{b}(x')-{\\hat {f}})^{2}}{B-1}}}.}\n  The number of samples/trees, B, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\n\n=== From bagging to random forests ===\n\nThe above procedure describes the original bagging algorithm for trees. Random forests also include another type of bagging scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.Typically, for a classification problem with p features, \u221ap (rounded down) features are used in each split.:\u200a592\u200a  For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.:\u200a592\u200a In practice, the best values for these parameters should be tuned on a case-to-case basis for every problem.:\u200a592\u200a\n\n\n=== ExtraTrees ===\nAdding one further step of randomization yields extremely randomized trees, or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally optimal cut-point for each feature under consideration (based on, e.g., information gain or the Gini impurity), a random cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are \n  \n    \n      \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {p}}}\n   for classification and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   for regression, where \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is the number of features in the model.\n\n\n== Properties ==\n\n\n=== Variable importance ===\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper and is implemented in the R package randomForest.The first step in measuring the variable importance in a data set \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n        =\n        {\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          Y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{n}=\\{(X_{i},Y_{i})\\}_{i=1}^{n}}\n   is to fit a random forest to the data.  During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\nTo measure the importance of the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th feature after training, the values of the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.  The importance score for the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.\nFeatures which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu et al.This method of determining variable importance has some drawbacks.  For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased trees can be used to solve the problem.  If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\n\n=== Relationship to nearest neighbors ===\nA relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set \n  \n    \n      \n        {\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{(x_{i},y_{i})\\}_{i=1}^{n}}\n   that make predictions \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}}\n   for new points x' by looking at the \"neighborhood\" of the point, formalized by a weight function W:\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \u2032\n        \n        )\n        \n        \n          y\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {y}}=\\sum _{i=1}^{n}W(x_{i},x')\\,y_{i}.}\n  Here, \n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle W(x_{i},x')}\n   is the non-negative weight of the i'th training point relative to the new point x' in the same tree. For any particular x', the weights for points \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   must sum to one. Weight functions are given as follows:\n\nIn k-NN, the weights are \n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \u2032\n        \n        )\n        =\n        \n          \n            1\n            k\n          \n        \n      \n    \n    {\\displaystyle W(x_{i},x')={\\frac {1}{k}}}\n   if xi is one of the k points closest to x', and zero otherwise.\nIn a tree, \n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \u2032\n        \n        )\n        =\n        \n          \n            1\n            \n              k\n              \u2032\n            \n          \n        \n      \n    \n    {\\displaystyle W(x_{i},x')={\\frac {1}{k'}}}\n   if xi is one of the k' points in the same leaf as x', and zero otherwise.Since a forest averages the predictions of a set of m trees with individual weight functions \n  \n    \n      \n        \n          W\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle W_{j}}\n  , its predictions are\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          W\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \u2032\n        \n        )\n        \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                1\n                m\n              \n            \n            \n              \u2211\n              \n                j\n                =\n                1\n              \n              \n                m\n              \n            \n            \n              W\n              \n                j\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            ,\n            \n              x\n              \u2032\n            \n            )\n          \n          )\n        \n        \n        \n          y\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {y}}={\\frac {1}{m}}\\sum _{j=1}^{m}\\sum _{i=1}^{n}W_{j}(x_{i},x')\\,y_{i}=\\sum _{i=1}^{n}\\left({\\frac {1}{m}}\\sum _{j=1}^{m}W_{j}(x_{i},x')\\right)\\,y_{i}.}\n  This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of x' in this interpretation are the points \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   sharing the same leaf in any tree \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  . In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\n\n== Unsupervised learning with random forests ==\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the \"observed\" data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\n\n== Variants ==\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers. In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.\n\n\n== Kernel random forest ==\nIn machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\n\n=== History ===\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\n\n=== Notations and definitions ===\n\n\n==== Preliminaries: Centered forests ====\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is built, where \n  \n    \n      \n        k\n        \u2208\n        \n          N\n        \n      \n    \n    {\\displaystyle k\\in \\mathbb {N} }\n   is a parameter of the algorithm.\n\n\n==== Uniform forest ====\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\n\n==== From random forest to KeRF ====\nGiven a training sample  \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n        =\n        {\n        (\n        \n          \n            X\n          \n          \n            i\n          \n        \n        ,\n        \n          Y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{n}=\\{(\\mathbf {X} _{i},Y_{i})\\}_{i=1}^{n}}\n   of \n  \n    \n      \n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            p\n          \n        \n        \u00d7\n        \n          R\n        \n      \n    \n    {\\displaystyle [0,1]^{p}\\times \\mathbb {R} }\n  -valued independent random variables distributed as the independent prototype pair \n  \n    \n      \n        (\n        \n          X\n        \n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle (\\mathbf {X} ,Y)}\n  , where \n  \n    \n      \n        E\n        \u2061\n        [\n        \n          Y\n          \n            2\n          \n        \n        ]\n        <\n        \u221e\n      \n    \n    {\\displaystyle \\operatorname {E} [Y^{2}]<\\infty }\n  . We aim at predicting the response \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , associated with the random variable \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  , by estimating the regression function \n  \n    \n      \n        m\n        (\n        \n          x\n        \n        )\n        =\n        E\n        \u2061\n        [\n        Y\n        \u2223\n        \n          X\n        \n        =\n        \n          x\n        \n        ]\n      \n    \n    {\\displaystyle m(\\mathbf {x} )=\\operatorname {E} [Y\\mid \\mathbf {X} =\\mathbf {x} ]}\n  . A random regression forest is an ensemble of \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   randomized regression trees. Denote \n  \n    \n      \n        \n          m\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \n            \u0398\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle m_{n}(\\mathbf {x} ,\\mathbf {\\Theta } _{j})}\n   the predicted value at point \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   by the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th tree, where \n  \n    \n      \n        \n          \n            \u0398\n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            \u0398\n          \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\Theta } _{1},\\ldots ,\\mathbf {\\Theta } _{M}}\n   are independent random variables, distributed as a generic random variable \n  \n    \n      \n        \n          \u0398\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Theta } }\n  , independent of the sample \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{n}}\n  . This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate \n  \n    \n      \n        \n          m\n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n        =\n        \n          \n            1\n            M\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          m\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle m_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{M}}\\sum _{j=1}^{M}m_{n}(\\mathbf {x} ,\\Theta _{j})}\n  .\nFor regression trees, we have \n  \n    \n      \n        \n          m\n          \n            n\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            \n              \n                Y\n                \n                  i\n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    \n                      X\n                    \n                    \n                      i\n                    \n                  \n                  \u2208\n                  \n                    A\n                    \n                      n\n                    \n                  \n                  (\n                  \n                    x\n                  \n                  ,\n                  \n                    \u0398\n                    \n                      j\n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                N\n                \n                  n\n                \n              \n              (\n              \n                x\n              \n              ,\n              \n                \u0398\n                \n                  j\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle m_{n}=\\sum _{i=1}^{n}{\\frac {Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}{N_{n}(\\mathbf {x} ,\\Theta _{j})}}}\n  , where \n  \n    \n      \n        \n          A\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle A_{n}(\\mathbf {x} ,\\Theta _{j})}\n   is the cell containing \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  , designed with randomness \n  \n    \n      \n        \n          \u0398\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Theta _{j}}\n   and dataset \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{n}}\n  , and \n  \n    \n      \n        \n          N\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            j\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            1\n          \n          \n            \n              \n                X\n              \n              \n                i\n              \n            \n            \u2208\n            \n              A\n              \n                n\n              \n            \n            (\n            \n              x\n            \n            ,\n            \n              \u0398\n              \n                j\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle N_{n}(\\mathbf {x} ,\\Theta _{j})=\\sum _{i=1}^{n}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}\n  .\nThus random forest estimates satisfy, for all \n  \n    \n      \n        \n          x\n        \n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} \\in [0,1]^{d}}\n  , \n  \n    \n      \n        \n          m\n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n        =\n        \n          \n            1\n            M\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          (\n          \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              \n                \n                  \n                    Y\n                    \n                      i\n                    \n                  \n                  \n                    \n                      1\n                    \n                    \n                      \n                        \n                          X\n                        \n                        \n                          i\n                        \n                      \n                      \u2208\n                      \n                        A\n                        \n                          n\n                        \n                      \n                      (\n                      \n                        x\n                      \n                      ,\n                      \n                        \u0398\n                        \n                          j\n                        \n                      \n                      )\n                    \n                  \n                \n                \n                  \n                    N\n                    \n                      n\n                    \n                  \n                  (\n                  \n                    x\n                  \n                  ,\n                  \n                    \u0398\n                    \n                      j\n                    \n                  \n                  )\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle m_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{M}}\\sum _{j=1}^{M}\\left(\\sum _{i=1}^{n}{\\frac {Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}{N_{n}(\\mathbf {x} ,\\Theta _{j})}}\\right)}\n  . Random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\n  \n    \n      \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              \n                \u2211\n                \n                  j\n                  =\n                  1\n                \n                \n                  M\n                \n              \n              \n                N\n                \n                  n\n                \n              \n              (\n              \n                x\n              \n              ,\n              \n                \u0398\n                \n                  j\n                \n              \n              )\n            \n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          Y\n          \n            i\n          \n        \n        \n          \n            1\n          \n          \n            \n              \n                X\n              \n              \n                i\n              \n            \n            \u2208\n            \n              A\n              \n                n\n              \n            \n            (\n            \n              x\n            \n            ,\n            \n              \u0398\n              \n                j\n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{\\sum _{j=1}^{M}N_{n}(\\mathbf {x} ,\\Theta _{j})}}\\sum _{j=1}^{M}\\sum _{i=1}^{n}Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})},}\n  which is equal to the mean of the \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n  's falling in the cells containing \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   in the forest. If we define the connection function of the \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   finite forest as \n  \n    \n      \n        \n          K\n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          z\n        \n        )\n        =\n        \n          \n            1\n            M\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          \n            1\n          \n          \n            \n              z\n            \n            \u2208\n            \n              A\n              \n                n\n              \n            \n            (\n            \n              x\n            \n            ,\n            \n              \u0398\n              \n                j\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle K_{M,n}(\\mathbf {x} ,\\mathbf {z} )={\\frac {1}{M}}\\sum _{j=1}^{M}\\mathbf {1} _{\\mathbf {z} \\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}\n  , i.e. the proportion of cells shared between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   and \n  \n    \n      \n        \n          z\n        \n      \n    \n    {\\displaystyle \\mathbf {z} }\n  , then almost surely we have \n  \n    \n      \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                Y\n                \n                  i\n                \n              \n              \n                K\n                \n                  M\n                  ,\n                  n\n                \n              \n              (\n              \n                x\n              \n              ,\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              )\n            \n            \n              \n                \u2211\n                \n                  \u2113\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                K\n                \n                  M\n                  ,\n                  n\n                \n              \n              (\n              \n                x\n              \n              ,\n              \n                \n                  x\n                \n                \n                  \u2113\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {\\sum _{i=1}^{n}Y_{i}K_{M,n}(\\mathbf {x} ,\\mathbf {x} _{i})}{\\sum _{\\ell =1}^{n}K_{M,n}(\\mathbf {x} ,\\mathbf {x} _{\\ell })}}}\n  , which defines the KeRF.\n\n\n==== Centered KeRF ====\nThe construction of Centered KeRF of level \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is the same as for centered forest, except that predictions are made by \n  \n    \n      \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})}\n  , the corresponding kernel function, or connection function is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  K\n                  \n                    k\n                  \n                  \n                    c\n                    c\n                  \n                \n                (\n                \n                  x\n                \n                ,\n                \n                  z\n                \n                )\n                =\n                \n                  \u2211\n                  \n                    \n                      k\n                      \n                        1\n                      \n                    \n                    ,\n                    \u2026\n                    ,\n                    \n                      k\n                      \n                        d\n                      \n                    \n                    ,\n                    \n                      \u2211\n                      \n                        j\n                        =\n                        1\n                      \n                      \n                        d\n                      \n                    \n                    \n                      k\n                      \n                        j\n                      \n                    \n                    =\n                    k\n                  \n                \n              \n              \n                \n                  \n                    \n                      k\n                      !\n                    \n                    \n                      \n                        k\n                        \n                          1\n                        \n                      \n                      !\n                      \u22ef\n                      \n                        k\n                        \n                          d\n                        \n                      \n                      !\n                    \n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        1\n                        d\n                      \n                    \n                    )\n                  \n                  \n                    k\n                  \n                \n                \n                  \u220f\n                  \n                    j\n                    =\n                    1\n                  \n                  \n                    d\n                  \n                \n                \n                  \n                    1\n                  \n                  \n                    \u2308\n                    \n                      2\n                      \n                        \n                          k\n                          \n                            j\n                          \n                        \n                      \n                    \n                    \n                      x\n                      \n                        j\n                      \n                    \n                    \u2309\n                    =\n                    \u2308\n                    \n                      2\n                      \n                        \n                          k\n                          \n                            j\n                          \n                        \n                      \n                    \n                    \n                      z\n                      \n                        j\n                      \n                    \n                    \u2309\n                  \n                \n                ,\n              \n            \n            \n              \n              \n                \n                   for all \n                \n                \n                  x\n                \n                ,\n                \n                  z\n                \n                \u2208\n                [\n                0\n                ,\n                1\n                \n                  ]\n                  \n                    d\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}K_{k}^{cc}(\\mathbf {x} ,\\mathbf {z} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}&{\\frac {k!}{k_{1}!\\cdots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{j=1}^{d}\\mathbf {1} _{\\lceil 2^{k_{j}}x_{j}\\rceil =\\lceil 2^{k_{j}}z_{j}\\rceil },\\\\&{\\text{ for all }}\\mathbf {x} ,\\mathbf {z} \\in [0,1]^{d}.\\end{aligned}}}\n  \n\n\n==== Uniform KeRF ====\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by \n  \n    \n      \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \u0398\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u0398\n          \n            M\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})}\n  , the corresponding kernel function, or connection function is\n\n  \n    \n      \n        \n          K\n          \n            k\n          \n          \n            u\n            f\n          \n        \n        (\n        \n          0\n        \n        ,\n        \n          x\n        \n        )\n        =\n        \n          \u2211\n          \n            \n              k\n              \n                1\n              \n            \n            ,\n            \u2026\n            ,\n            \n              k\n              \n                d\n              \n            \n            ,\n            \n              \u2211\n              \n                j\n                =\n                1\n              \n              \n                d\n              \n            \n            \n              k\n              \n                j\n              \n            \n            =\n            k\n          \n        \n        \n          \n            \n              k\n              !\n            \n            \n              \n                k\n                \n                  1\n                \n              \n              !\n              \u2026\n              \n                k\n                \n                  d\n                \n              \n              !\n            \n          \n        \n        \n          \n            (\n            \n              \n                1\n                d\n              \n            \n            )\n          \n          \n            k\n          \n        \n        \n          \u220f\n          \n            m\n            =\n            1\n          \n          \n            d\n          \n        \n        \n          (\n          \n            1\n            \u2212\n            \n              |\n            \n            \n              x\n              \n                m\n              \n            \n            \n              |\n            \n            \n              \u2211\n              \n                j\n                =\n                0\n              \n              \n                \n                  k\n                  \n                    m\n                  \n                \n                \u2212\n                1\n              \n            \n            \n              \n                \n                  (\n                  \u2212\n                  ln\n                  \u2061\n                  \n                    |\n                  \n                  \n                    x\n                    \n                      m\n                    \n                  \n                  \n                    |\n                  \n                  \n                    )\n                    \n                      j\n                    \n                  \n                \n                \n                  j\n                  !\n                \n              \n            \n          \n          )\n        \n        \n           for all \n        \n        \n          x\n        \n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n        .\n      \n    \n    {\\displaystyle K_{k}^{uf}(\\mathbf {0} ,\\mathbf {x} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}{\\frac {k!}{k_{1}!\\ldots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{m=1}^{d}\\left(1-|x_{m}|\\sum _{j=0}^{k_{m}-1}{\\frac {(-\\ln |x_{m}|)^{j}}{j!}}\\right){\\text{ for all }}\\mathbf {x} \\in [0,1]^{d}.}\n  \n\n\n=== Properties ===\n\n\n==== Relation between KeRF and random forest ====\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\n\nAssume that there exist sequences \n  \n    \n      \n        (\n        \n          a\n          \n            n\n          \n        \n        )\n        ,\n        (\n        \n          b\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (a_{n}),(b_{n})}\n   such that, almost surely,\n\n  \n    \n      \n        \n          a\n          \n            n\n          \n        \n        \u2264\n        \n          N\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \u0398\n        )\n        \u2264\n        \n          b\n          \n            n\n          \n        \n        \n           and \n        \n        \n          a\n          \n            n\n          \n        \n        \u2264\n        \n          \n            1\n            M\n          \n        \n        \n          \u2211\n          \n            m\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          N\n          \n            n\n          \n        \n        \n          \n            x\n          \n          ,\n          \n            \u0398\n            \n              m\n            \n          \n        \n        \u2264\n        \n          b\n          \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle a_{n}\\leq N_{n}(\\mathbf {x} ,\\Theta )\\leq b_{n}{\\text{ and }}a_{n}\\leq {\\frac {1}{M}}\\sum _{m=1}^{M}N_{n}{\\mathbf {x} ,\\Theta _{m}}\\leq b_{n}.}\n  Then almost surely,\n\n  \n    \n      \n        \n          |\n        \n        \n          m\n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        \u2212\n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        \n          |\n        \n        \u2264\n        \n          \n            \n              \n                b\n                \n                  n\n                \n              \n              \u2212\n              \n                a\n                \n                  n\n                \n              \n            \n            \n              a\n              \n                n\n              \n            \n          \n        \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            M\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        .\n      \n    \n    {\\displaystyle |m_{M,n}(\\mathbf {x} )-{\\tilde {m}}_{M,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{M,n}(\\mathbf {x} ).}\n  \n\n\n==== Relation between infinite KeRF and infinite random forest ====\nWhen the number of trees \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\n\nAssume that there exist sequences \n  \n    \n      \n        (\n        \n          \u03b5\n          \n            n\n          \n        \n        )\n        ,\n        (\n        \n          a\n          \n            n\n          \n        \n        )\n        ,\n        (\n        \n          b\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\varepsilon _{n}),(a_{n}),(b_{n})}\n   such that, almost surely\n\n  \n    \n      \n        E\n        \u2061\n        [\n        \n          N\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \u0398\n        )\n        ]\n        \u2265\n        1\n        ,\n      \n    \n    {\\displaystyle \\operatorname {E} [N_{n}(\\mathbf {x} ,\\Theta )]\\geq 1,}\n  \n\n  \n    \n      \n        P\n        \u2061\n        [\n        \n          a\n          \n            n\n          \n        \n        \u2264\n        \n          N\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \u0398\n        )\n        \u2264\n        \n          b\n          \n            n\n          \n        \n        \u2223\n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n        ]\n        \u2265\n        1\n        \u2212\n        \n          \u03b5\n          \n            n\n          \n        \n        \n          /\n        \n        2\n        ,\n      \n    \n    {\\displaystyle \\operatorname {P} [a_{n}\\leq N_{n}(\\mathbf {x} ,\\Theta )\\leq b_{n}\\mid {\\mathcal {D}}_{n}]\\geq 1-\\varepsilon _{n}/2,}\n  \n\n  \n    \n      \n        P\n        \u2061\n        [\n        \n          a\n          \n            n\n          \n        \n        \u2264\n        \n          E\n          \n            \u0398\n          \n        \n        \u2061\n        [\n        \n          N\n          \n            n\n          \n        \n        (\n        \n          x\n        \n        ,\n        \u0398\n        )\n        ]\n        \u2264\n        \n          b\n          \n            n\n          \n        \n        \u2223\n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n        ]\n        \u2265\n        1\n        \u2212\n        \n          \u03b5\n          \n            n\n          \n        \n        \n          /\n        \n        2\n        ,\n      \n    \n    {\\displaystyle \\operatorname {P} [a_{n}\\leq \\operatorname {E} _{\\Theta }[N_{n}(\\mathbf {x} ,\\Theta )]\\leq b_{n}\\mid {\\mathcal {D}}_{n}]\\geq 1-\\varepsilon _{n}/2,}\n  Then almost surely,\n\n  \n    \n      \n        \n          |\n        \n        \n          m\n          \n            \u221e\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        \u2212\n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            \u221e\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        \n          |\n        \n        \u2264\n        \n          \n            \n              \n                b\n                \n                  n\n                \n              \n              \u2212\n              \n                a\n                \n                  n\n                \n              \n            \n            \n              a\n              \n                n\n              \n            \n          \n        \n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            \u221e\n            ,\n            n\n          \n        \n        (\n        \n          x\n        \n        )\n        +\n        n\n        \n          \u03b5\n          \n            n\n          \n        \n        \n          (\n          \n            \n              max\n              \n                1\n                \u2264\n                i\n                \u2264\n                n\n              \n            \n            \n              Y\n              \n                i\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle |m_{\\infty ,n}(\\mathbf {x} )-{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )+n\\varepsilon _{n}\\left(\\max _{1\\leq i\\leq n}Y_{i}\\right).}\n  \n\n\n=== Consistency results ===\nAssume that \n  \n    \n      \n        Y\n        =\n        m\n        (\n        \n          X\n        \n        )\n        +\n        \u03b5\n      \n    \n    {\\displaystyle Y=m(\\mathbf {X} )+\\varepsilon }\n  , where \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   is a centered Gaussian noise, independent of \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  , with finite variance \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n        <\n        \u221e\n      \n    \n    {\\displaystyle \\sigma ^{2}<\\infty }\n  . Moreover, \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   is uniformly distributed on \n  \n    \n      \n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle [0,1]^{d}}\n   and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\n\n==== Consistency of centered KeRF ====\nProviding \n  \n    \n      \n        k\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle k\\rightarrow \\infty }\n   and \n  \n    \n      \n        n\n        \n          /\n        \n        \n          2\n          \n            k\n          \n        \n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle n/2^{k}\\rightarrow \\infty }\n  , there exists a constant \n  \n    \n      \n        \n          C\n          \n            1\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle C_{1}>0}\n   such that, for all \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  ,\n\n  \n    \n      \n        \n          E\n        \n        [\n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            n\n          \n          \n            c\n            c\n          \n        \n        (\n        \n          X\n        \n        )\n        \u2212\n        m\n        (\n        \n          X\n        \n        )\n        \n          ]\n          \n            2\n          \n        \n        \u2264\n        \n          C\n          \n            1\n          \n        \n        \n          n\n          \n            \u2212\n            1\n            \n              /\n            \n            (\n            3\n            +\n            d\n            log\n            \u2061\n            2\n            )\n          \n        \n        (\n        log\n        \u2061\n        n\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [{\\tilde {m}}_{n}^{cc}(\\mathbf {X} )-m(\\mathbf {X} )]^{2}\\leq C_{1}n^{-1/(3+d\\log 2)}(\\log n)^{2}}\n  .\n\n\n==== Consistency of uniform KeRF ====\nProviding \n  \n    \n      \n        k\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle k\\rightarrow \\infty }\n   and \n  \n    \n      \n        n\n        \n          /\n        \n        \n          2\n          \n            k\n          \n        \n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle n/2^{k}\\rightarrow \\infty }\n  , there exists a constant \n  \n    \n      \n        C\n        >\n        0\n      \n    \n    {\\displaystyle C>0}\n   such that,\n\n  \n    \n      \n        \n          E\n        \n        [\n        \n          \n            \n              \n                m\n                ~\n              \n            \n          \n          \n            n\n          \n          \n            u\n            f\n          \n        \n        (\n        \n          X\n        \n        )\n        \u2212\n        m\n        (\n        \n          X\n        \n        )\n        \n          ]\n          \n            2\n          \n        \n        \u2264\n        C\n        \n          n\n          \n            \u2212\n            2\n            \n              /\n            \n            (\n            6\n            +\n            3\n            d\n            log\n            \u2061\n            2\n            )\n          \n        \n        (\n        log\n        \u2061\n        n\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [{\\tilde {m}}_{n}^{uf}(\\mathbf {X} )-m(\\mathbf {X} )]^{2}\\leq Cn^{-2/(6+3d\\log 2)}(\\log n)^{2}}\n  .\n\n\n== Disadvantages ==\nWhile random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the most desirable qualities of decision trees. It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model. For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal \"born-again\" decision tree that faithfully reproduces the same decision function. If it is established that the predictive attributes are linearly correlated with the target variable, using random forest may not enhance the accuracy of the base learner. Furthermore, in problems with multiple categorical variables, random forest may not be able to increase the accuracy of the base learner.\n\n\n== See also ==\nBoosting \u2013 Method in machine learning\nDecision tree learning \u2013 Machine learning algorithm\nEnsemble learning \u2013 Statistics and machine learning technique\nGradient boosting \u2013 Machine learning technique\nNon-parametric statistics \u2013 Branch of statistics that is not based solely on parametrized families of probability distributionsPages displaying short descriptions of redirect targets\nRandomized algorithm \u2013 Algorithm that employs a degree of randomness as part of its logic or procedure\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nRandom Forests classifier description (Leo Breiman's site)\nLiaw, Andy & Wiener, Matthew \"Classification and Regression by randomForest\" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R)", "K-means clustering": "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n\n\n== Description ==\nGiven a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (\u2264 n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:where \u03bci is the mean (also called centroid) of points in \n  \n    \n      \n        \n          S\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle S_{i}}\n  , i.e.\n\n  \n    \n      \n        \n          |\n        \n        \n          S\n          \n            i\n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle |S_{i}|}\n   is the size of \n  \n    \n      \n        \n          S\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle S_{i}}\n  , and \n  \n    \n      \n        \u2016\n        \u22c5\n        \u2016\n      \n    \n    {\\displaystyle \\|\\cdot \\|}\n   is the usual L2 norm . This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:The equivalence can be deduced from identity \n  \n    \n      \n        \n          |\n        \n        \n          S\n          \n            i\n          \n        \n        \n          |\n        \n        \n          \u2211\n          \n            \n              x\n            \n            \u2208\n            \n              S\n              \n                i\n              \n            \n          \n        \n        \n          \n            \u2016\n            \n              \n                x\n              \n              \u2212\n              \n                \n                  \u03bc\n                \n                \n                  i\n                \n              \n            \n            \u2016\n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        \n          \u2211\n          \n            \n              x\n            \n            ,\n            \n              y\n            \n            \u2208\n            \n              S\n              \n                i\n              \n            \n          \n        \n        \n          \n            \u2016\n            \n              \n                x\n              \n              \u2212\n              \n                y\n              \n            \n            \u2016\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |S_{i}|\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\frac {1}{2}}\\sum _{\\mathbf {x} ,\\mathbf {y} \\in S_{i}}\\left\\|\\mathbf {x} -\\mathbf {y} \\right\\|^{2}}\n  . Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS). This deterministic relationship is also related to the law of total variance in probability theory.\n\n\n== History ==\nThe term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd\u2013Forgy algorithm.\n\n\n== Algorithms ==\n\n\n=== Standard algorithm (naive k-means) ===\n\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the k-means algorithm\"; it is also referred to as Lloyd's algorithm, particularly in the computer science community. It is sometimes also referred to as \"na\u00efve k-means\", because there exist much faster alternatives.Given an initial set of k means m1(1), ..., mk(1) (see below), the algorithm proceeds by alternating between two steps:\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\n\n  \n    \n      \n        \n          S\n          \n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          {\n          \n            \n              x\n              \n                p\n              \n            \n            :\n            \n              \n                \u2016\n                \n                  \n                    x\n                    \n                      p\n                    \n                  \n                  \u2212\n                  \n                    m\n                    \n                      i\n                    \n                    \n                      (\n                      t\n                      )\n                    \n                  \n                \n                \u2016\n              \n              \n                2\n              \n            \n            \u2264\n            \n              \n                \u2016\n                \n                  \n                    x\n                    \n                      p\n                    \n                  \n                  \u2212\n                  \n                    m\n                    \n                      j\n                    \n                    \n                      (\n                      t\n                      )\n                    \n                  \n                \n                \u2016\n              \n              \n                2\n              \n            \n             \n            \u2200\n            j\n            ,\n            1\n            \u2264\n            j\n            \u2264\n            k\n          \n          }\n        \n        ,\n      \n    \n    {\\displaystyle S_{i}^{(t)}=\\left\\{x_{p}:\\left\\|x_{p}-m_{i}^{(t)}\\right\\|^{2}\\leq \\left\\|x_{p}-m_{j}^{(t)}\\right\\|^{2}\\ \\forall j,1\\leq j\\leq k\\right\\},}\n  \nwhere each \n  \n    \n      \n        \n          x\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle x_{p}}\n   is assigned to exactly one \n  \n    \n      \n        \n          S\n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle S^{(t)}}\n  , even if it could be assigned to two or more of them.\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\n\n  \n    \n      \n        \n          m\n          \n            i\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            1\n            \n              |\n              \n                S\n                \n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              |\n            \n          \n        \n        \n          \u2211\n          \n            \n              x\n              \n                j\n              \n            \n            \u2208\n            \n              S\n              \n                i\n              \n              \n                (\n                t\n                )\n              \n            \n          \n        \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle m_{i}^{(t+1)}={\\frac {1}{\\left|S_{i}^{(t)}\\right|}}\\sum _{x_{j}\\in S_{i}^{(t)}}x_{j}}\n  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\n\n\n==== Initialization methods ====\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\n\nDemonstration of the standard algorithm\n\t\t\n\t\t\n\t\t\n\t\t\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2\u03a9(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.\n\n\n=== Complexity ===\nFinding the optimal solution to the k-means clustering problem for observations in d dimensions is:\n\nNP-hard in general Euclidean space (of d dimensions) even for two clusters,\nNP-hard for a general number of clusters k even in the plane,\nif k and d (the dimension) are fixed, the problem can be exactly solved in time \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            d\n            k\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{dk+1})}\n  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.\nThe running time of Lloyd's algorithm (and most variants) is \n  \n    \n      \n        O\n        (\n        n\n        k\n        d\n        i\n        )\n      \n    \n    {\\displaystyle O(nkdi)}\n  , where:\n\nn is the number of d-dimensional vectors (to be clustered)\nk the number of clusters\ni the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.\nIn the worst-case, Lloyd's algorithm needs \n  \n    \n      \n        i\n        =\n        \n          2\n          \n            \u03a9\n            (\n            \n              \n                n\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle i=2^{\\Omega ({\\sqrt {n}})}}\n   iterations, so that the worst-case complexity of Lloyd's algorithm is superpolynomial.\nLloyd's k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle [0,1]^{d}}\n  , if each point is independently perturbed by a normal distribution with mean 0 and variance \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  , then the expected running time of k-means algorithm is bounded by \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            34\n          \n        \n        \n          k\n          \n            34\n          \n        \n        \n          d\n          \n            8\n          \n        \n        \n          log\n          \n            4\n          \n        \n        \u2061\n        (\n        n\n        )\n        \n          /\n        \n        \n          \u03c3\n          \n            6\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{34}k^{34}d^{8}\\log ^{4}(n)/\\sigma ^{6})}\n  , which is a polynomial in n, k, d and \n  \n    \n      \n        1\n        \n          /\n        \n        \u03c3\n      \n    \n    {\\displaystyle 1/\\sigma }\n  .\nBetter bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by \n  \n    \n      \n        O\n        (\n        d\n        \n          n\n          \n            4\n          \n        \n        \n          M\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(dn^{4}M^{2})}\n   for n points in an integer lattice \n  \n    \n      \n        {\n        1\n        ,\n        \u2026\n        ,\n        M\n        \n          }\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\{1,\\dots ,M\\}^{d}}\n  .Lloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the na\u00efve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.\n\n\n=== Variations ===\nJenks natural breaks optimization: k-means applied to univariate data\nk-medians clustering uses the median in each dimension instead of the mean, and this way minimizes \n  \n    \n      \n        \n          L\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle L_{1}}\n   norm (Taxicab geometry).\nk-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.\nFuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\nGaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.\nk-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.\nThe filtering algorithm uses kd-trees to speed up each k-means step.\nSome methods attempt to speed up each k-means step using the triangle inequality.\nEscape local optima by swapping points between clusters.\nThe Spherical k-means clustering algorithm is suitable for textual data.\nHierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.\nInternal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.\nMinkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.\nMini-batch k-means: k-means variation using \"mini batch\" samples for data sets that do not fit into memory.\nOtsu's method\n\n\n=== Hartigan\u2013Wong method ===\nHartigan and Wong's method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\nLet \n  \n    \n      \n        \u03c6\n        (\n        \n          S\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varphi (S_{j})}\n   be the individual cost of \n  \n    \n      \n        \n          S\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle S_{j}}\n   defined by \n  \n    \n      \n        \n          \u2211\n          \n            x\n            \u2208\n            \n              S\n              \n                j\n              \n            \n          \n        \n        (\n        x\n        \u2212\n        \n          \u03bc\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sum _{x\\in S_{j}}(x-\\mu _{j})^{2}}\n  , with \n  \n    \n      \n        \n          \u03bc\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\mu _{j}}\n   the center of the cluster.\nAssignment step: Hartigan and Wong's method starts by partitioning the points into random clusters \n  \n    \n      \n        {\n        \n          S\n          \n            j\n          \n        \n        \n          }\n          \n            j\n            \u2208\n            {\n            1\n            ,\n            \u22ef\n            k\n            }\n          \n        \n      \n    \n    {\\displaystyle \\{S_{j}\\}_{j\\in \\{1,\\cdots k\\}}}\n  .\nUpdate step: Next it determines the \n  \n    \n      \n        n\n        ,\n        m\n        \u2208\n        {\n        1\n        ,\n        \u2026\n        ,\n        k\n        }\n      \n    \n    {\\displaystyle n,m\\in \\{1,\\ldots ,k\\}}\n   and \n  \n    \n      \n        x\n        \u2208\n        \n          S\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x\\in S_{n}}\n   for which the following function reaches a maximum\n\n  \n    \n      \n        \u0394\n        (\n        m\n        ,\n        n\n        ,\n        x\n        )\n        =\n        \u03c6\n        (\n        \n          S\n          \n            n\n          \n        \n        )\n        +\n        \u03c6\n        (\n        \n          S\n          \n            m\n          \n        \n        )\n        \u2212\n        \u03c6\n        (\n        \n          S\n          \n            n\n          \n        \n        \u2216\n        {\n        x\n        }\n        )\n        \u2212\n        \u03c6\n        (\n        \n          S\n          \n            m\n          \n        \n        \u222a\n        {\n        x\n        }\n        )\n        .\n      \n    \n    {\\displaystyle \\Delta (m,n,x)=\\varphi (S_{n})+\\varphi (S_{m})-\\varphi (S_{n}\\smallsetminus \\{x\\})-\\varphi (S_{m}\\cup \\{x\\}).}\n  For the \n  \n    \n      \n        x\n        ,\n        n\n        ,\n        m\n      \n    \n    {\\displaystyle x,n,m}\n   that reach this maximum, \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   moves from the cluster \n  \n    \n      \n        \n          S\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle S_{n}}\n   to the cluster \n  \n    \n      \n        \n          S\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle S_{m}}\n  .\nTermination: The algorithm terminates once \n  \n    \n      \n        \u0394\n        (\n        m\n        ,\n        n\n        ,\n        x\n        )\n      \n    \n    {\\displaystyle \\Delta (m,n,x)}\n   is less than zero for all \n  \n    \n      \n        x\n        ,\n        n\n        ,\n        m\n      \n    \n    {\\displaystyle x,n,m}\n  .\nDifferent move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function \n  \n    \n      \n        \u0394\n      \n    \n    {\\displaystyle \\Delta }\n   used to calculate the result of a relocation can also be efficiently evaluated by using equality\n\n  \n    \n      \n        \u0394\n        (\n        x\n        ,\n        n\n        ,\n        m\n        )\n        =\n        \n          \n            \n              \u2223\n              \n                S\n                \n                  n\n                \n              \n              \u2223\n            \n            \n              \u2223\n              \n                S\n                \n                  n\n                \n              \n              \u2223\n              \u2212\n              1\n            \n          \n        \n        \u22c5\n        \u2016\n        \n          \u03bc\n          \n            n\n          \n        \n        \u2212\n        x\n        \n          \u2016\n          \n            2\n          \n        \n        \u2212\n        \n          \n            \n              \u2223\n              \n                S\n                \n                  m\n                \n              \n              \u2223\n            \n            \n              \u2223\n              \n                S\n                \n                  m\n                \n              \n              \u2223\n              +\n              1\n            \n          \n        \n        \u22c5\n        \u2016\n        \n          \u03bc\n          \n            m\n          \n        \n        \u2212\n        x\n        \n          \u2016\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\Delta (x,n,m)={\\frac {\\mid S_{n}\\mid }{\\mid S_{n}\\mid -1}}\\cdot \\lVert \\mu _{n}-x\\rVert ^{2}-{\\frac {\\mid S_{m}\\mid }{\\mid S_{m}\\mid +1}}\\cdot \\lVert \\mu _{m}-x\\rVert ^{2}.}\n  \n\n\n=== Global optimization and metaheuristics ===\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced \u2018\u2019provenly optimal\u2019\u2019 solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood search and genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\n\n\n== Discussion ==\n\nThree key features of k-means that make it efficient are often regarded as its biggest drawbacks:\n\nEuclidean distance is used as a metric and variance is used as a measure of cluster scatter.\nThe number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\nConvergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of \n  \n    \n      \n        k\n        =\n        3\n      \n    \n    {\\displaystyle k=3}\n   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With \n  \n    \n      \n        k\n        =\n        2\n      \n    \n    {\\displaystyle k=2}\n  , the two visible clusters (one containing two species) will be discovered, whereas with \n  \n    \n      \n        k\n        =\n        3\n      \n    \n    {\\displaystyle k=3}\n   one of the two clusters will be split into two even parts. In fact, \n  \n    \n      \n        k\n        =\n        2\n      \n    \n    {\\displaystyle k=2}\n   is more appropriate for this data set, despite the data set's containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\nThe result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.\n\n\n== Applications ==\nk-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\n\n\n=== Vector quantization ===\n\nk-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\n\n\n=== Cluster analysis ===\nIn cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\nHowever, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\n\n\n=== Feature learning ===\nk-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\n\n\n== Relation to other algorithms ==\n\n\n=== Gaussian mixture model ===\n\nThe slow \"standard algorithm\" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.:\u200a850\u200a Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of \"hard\" Gaussian mixture modelling.:\u200a354,\u200a11.4.2.5\u200a This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.:\u200a849\u200a\n\n\n=== k-SVD ===\n\nAnother generalization of the k-means algorithm is the k-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\n\n\n=== Principal component analysis ===\n\nThe relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\n\n\n=== Mean shift clustering ===\n\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\n\n\n=== Independent component analysis ===\n\nUnder sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.\n\n\n=== Bilateral filtering ===\n\nk-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\n\n\n== Similar problems ==\nThe set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.\n\n\n== Software implementations ==\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\n\n\n=== Free Software/Open Source ===\nThe following implementations are available under Free/Open Source Software licenses, with publicly available source code.\n\nAccord.NET contains C# implementations for k-means, k-means++ and k-modes.\nALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.\nAOSP contains a Java implementation for k-means.\nCrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.\nELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.\nSmile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).\nJulia contains a k-means implementation in the JuliaStats Clustering package.\nKNIME contains nodes for k-means and k-medoids.\nMahout contains a MapReduce based k-means.\nmlpack contains a C++ implementation of k-means.\nOctave contains k-means.\nOpenCV contains a k-means implementation.\nOrange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.\nPSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.\nR contains three k-means variations.\nSciPy and scikit-learn contain multiple k-means implementations.\nSpark MLlib implements a distributed k-means algorithm.\nTorch contains an unsup package that provides k-means clustering.\nWeka contains k-means and x-means.\n\n\n=== Proprietary ===\nThe following implementations are available under proprietary license terms, and may not have publicly available source code.\n\n\n== See also ==\nBFR algorithm\nCentroidal Voronoi tessellation\nHead/tail Breaks\nk q-flats\nk-means++\nLinde\u2013Buzo\u2013Gray algorithm\nSelf-organizing map\n\n\n== References ==", "Mutation (genetic algorithm)": "Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\nThe classic example of a mutation operator of a binary coded genetic algorithm (GA) involves a probability that an arbitrary bit in a genetic sequence will be flipped from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be flipped. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types of mutation operators are commonly used for representations other than binary, such as floating-point encodings or representations for combinatorial problems.\nThe purpose of mutation in EAs is to introduce diversity into the sampled population. Mutation operators are used in an attempt to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping convergence to the global optimum. This reasoning also leads most EAs to avoid only taking the fittest of the population in generating the next generation, but rather selecting a random (or semi-random) set with a weighting toward those that are fitter.The following requirements apply to all mutation operators used in an EA:\nevery point in the search space must be reachable by one or more mutations.\nthere must be no preference for parts or directions in the search space (no drift).\nsmall mutations should be more probable than large ones.For different genome types, different mutation types are suitable. An overview and more operators than those presented below can be found in the introductory book by Eiben and Smith or in.\n\n\n== Bit string mutation ==\nThe mutation of bit strings ensue through bit flips at random positions.\nExample:\n\nThe probability of a mutation of a bit is \n  \n    \n      \n        \n          \n            1\n            l\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{l}}}\n  , where \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   is the length of the binary vector. Thus, a mutation rate of \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   per mutation and individual selected for mutation is reached.\n\n\n== Mutation of real numbers ==\nMany EAs, such as the evolution strategy or the real-coded genetic algorithms, work with real numbers instead of bit strings. This is due to the good experiences that have been made with this type of coding.The value of a real-valued gene can either be changed or redetermined. A mutation that implements the latter should only ever be used in conjunction with the value-changing mutations and then only with comparatively low probability, as it can lead to large changes.\nIn practical applications, the decision variables to be changed of the optimisation problem to be solved are usually limited. Accordingly, the values of the associated genes are each restricted to an interval \n  \n    \n      \n        [\n        \n          x\n          \n            min\n          \n        \n        ,\n        \n          x\n          \n            max\n          \n        \n        ]\n      \n    \n    {\\displaystyle [x_{\\min },x_{\\max }]}\n  . Mutations may or may not take these restrictions into account. In the latter case, suitable post-treatment is then required as described below.\n\n\n=== Mutation without consideration of restrictions ===\n\nA real number \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   can be mutated using normal distribution \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \u03c3\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(0,\\sigma )}\n   by adding the generated random value to the old value of the gene, resulting in the mutated value \n  \n    \n      \n        \n          x\n          \u2032\n        \n      \n    \n    {\\displaystyle x'}\n  :\n  \n    \n      \n        \n          x\n          \u2032\n        \n        =\n        x\n        +\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \u03c3\n        )\n      \n    \n    {\\displaystyle x'=x+{\\mathcal {N}}(0,\\sigma )}\n  In the case of genes with a restricted range of values, it is a good idea to choose the step size of the mutation \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   so that it reasonably fits the range \n  \n    \n      \n        [\n        \n          x\n          \n            min\n          \n        \n        ,\n        \n          x\n          \n            max\n          \n        \n        ]\n      \n    \n    {\\displaystyle [x_{\\min },x_{\\max }]}\n   of the gene to be changed, e.g.:\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            \n              \n                x\n                \n                  max\n                \n              \n              \u2212\n              \n                x\n                \n                  min\n                \n              \n            \n            6\n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\frac {x_{\\text{max}}-x_{\\text{min}}}{6}}}\n  The step size can also be adjusted to the smaller permissible change range depending on the current value. In any case, however, it is likely that the new value \n  \n    \n      \n        \n          x\n          \u2032\n        \n      \n    \n    {\\displaystyle x'}\n   of the gene will be outside the permissible range of values. Such a case must be considered a lethal mutation, since the obvious repair by using the respective violated limit as the new value of the gene would lead to a drift. This is because the limit value would then be selected with the entire probability of the values beyond the limit of the value range. \nThe evolution strategy works with real numbers and mutation based on normal distribution. The step sizes are part of the chromosome and are subject to evolution together with the actual decision variables.\n\n\n=== Mutation with consideration of restrictions ===\nOne possible form of changing the value of a gene while taking its value range \n  \n    \n      \n        [\n        \n          x\n          \n            min\n          \n        \n        ,\n        \n          x\n          \n            max\n          \n        \n        ]\n      \n    \n    {\\displaystyle [x_{\\min },x_{\\max }]}\n   into account is the mutation relative parameter change of the evolutionary algorithm GLEAM (General Learning Evolutionary Algorithm and Method), in which, as with the mutation presented earlier, small changes are more likely than large ones.\n\nFirst, an equally distributed decision is made as to whether the current value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   should be increased or decreased and then the corresponding total change interval is determined. Without loss of generality, an increase is assumed for the explanation and the total change interval is then \n  \n    \n      \n        [\n        x\n        ,\n        \n          x\n          \n            max\n          \n        \n        ]\n      \n    \n    {\\displaystyle [x,x_{\\max }]}\n  . It is divided into \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   sub-areas of equal size with the width \n  \n    \n      \n        \u03b4\n      \n    \n    {\\displaystyle \\delta }\n  , from which \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   sub-change intervals of different size are formed:\n\n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th sub-change interval: \n  \n    \n      \n        [\n        x\n        ,\n        x\n        +\n        \u03b4\n        \u22c5\n        i\n        ]\n      \n    \n    {\\displaystyle [x,x+\\delta \\cdot i]}\n   with\n\n  \n    \n      \n        \u03b4\n        =\n        \n          \n            \n              (\n              \n                x\n                \n                  max\n                \n              \n              \u2212\n              x\n              )\n            \n            k\n          \n        \n      \n    \n    {\\displaystyle \\delta ={\\frac {(x_{\\text{max}}-x)}{k}}}\n   and \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        k\n      \n    \n    {\\displaystyle i=1,\\dots ,k}\n  Subsequently, one of the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   sub-change intervals is selected in equal distribution and a random number, also equally distributed, is drawn from it as the new value \n  \n    \n      \n        \n          x\n          \u2032\n        \n      \n    \n    {\\displaystyle x'}\n   of the gene. The resulting summed probabilities of the sub-change intervals result in the probability distribution of the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   sub-areas for the exemplary case of \n  \n    \n      \n        k\n        =\n        10\n      \n    \n    {\\displaystyle k=10}\n   shown in the adjacent figure. This is not a normal distribution as before, but this distribution also clearly favours small changes over larger ones. \nThis mutation for larger values of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , such as 10, is less well suited for tasks where the optimum lies on one of the value range boundaries. This can be remedied by significantly reducing \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   when a gene value approaches its limits very closely.\n\n\n== Mutation of permutations ==\nMutations of permutations are specially designed for genomes that are themselves permutations of a set. These are often used to solve combinatorial tasks. In the two mutations presented, parts of the genome are rotated or inverted.\n\n\n=== Rotation to the right ===\nThe presentation of the procedure is illustrated by an example on the right:\n\n\n=== Inversion ===\nThe presentation of the procedure is illustrated by an example on the right:\n\n\n=== Variants with preference for smaller changes ===\nThe requirement raised at the beginning for mutations, according to which small changes should be more probable than large ones, is only inadequately fulfilled by the two permutation mutations presented, since the lengths of the partial lists and the number of shift positions are determined in an equally distributed manner. However, the longer the partial list and the shift, the greater the change in gene order.\nThis can be remedied by the following modifications. The end index \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   of the partial lists is determined as the distance \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   to the start index \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  :\n\n  \n    \n      \n        j\n        =\n        (\n        i\n        +\n        d\n        )\n        \n          mod\n          \n            \n              |\n              \n                P\n                \n                  0\n                \n              \n              |\n            \n          \n        \n      \n    \n    {\\displaystyle j=(i+d){\\bmod {\\left|P_{0}\\right|}}}\n  where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   is determined randomly according to one of the two procedures for the mutation of real numbers from the interval \n  \n    \n      \n        \n          [\n          \n            0\n            ,\n            \n              |\n              \n                P\n                \n                  0\n                \n              \n              |\n            \n            \u2212\n            1\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\left[0,\\left|P_{0}\\right|-1\\right]}\n   and rounded.\nFor the rotation, \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is determined similarly to the distance \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  , but the value \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   is forbidden.\nFor the inversion, note that \n  \n    \n      \n        i\n        \u2260\n        j\n      \n    \n    {\\displaystyle i\\neq j}\n   must hold, so for \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   the value \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   must be excluded.\n\n\n== See also ==\nEvolutionary algorithms\nGenetic algorithms\n\n\n== References ==\n\n\n== Bibliography ==\nJohn Holland (1975). Adaptation in Natural and Artificial Systems, PhD thesis, University of Michigan Press, Ann Arbor, Michigan. ISBN 0-262-58111-6.\nSchwefel, Hans-Paul (1995). Evolution and Optimum Seeking. New York: John Wiley & Sons. ISBN 0-471-57148-2.\nDavis, Lawrence (1991). Handbook of genetic algorithms. New York: Van Nostrand Reinhold. ISBN 0-442-00173-8. OCLC 23081440.\nEiben, A.E.; Smith, J.E. (2015). Introduction to Evolutionary Computing. Natural Computing Series. Berlin, Heidelberg: Springer. doi:10.1007/978-3-662-44874-8. ISBN 978-3-662-44873-1.\nYu, Xinjie; Gen, Mitsuo (2010). Introduction to Evolutionary Algorithms. Decision Engineering. London: Springer. doi:10.1007/978-1-84996-129-5. ISBN 978-1-84996-128-8.\nDe Jong, Kenneth A. (2006). Evolutionary computation : a unified approach. Cambridge, Mass.: MIT Press. ISBN 978-0-262-25598-1. OCLC 69652176.\nFogel, David B.; B\u00e4ck, Thomas; Michalewicz, Zbigniew, eds. (1999). Evolutionary computation. Vol. 1, Basic algorithms and operators. Bristol: Institute of Physics Pub. ISBN 0-585-30560-9. OCLC 45730387.", "Support vector machine": "In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \n\n\n== Motivation ==\n\nClassifying data is a common task in machine learning.\nSuppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  -dimensional vector (a list of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   numbers), and we want to know whether we can separate such points with a \n  \n    \n      \n        (\n        p\n        \u2212\n        1\n        )\n      \n    \n    {\\displaystyle (p-1)}\n  -dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.\n\nWhereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function \n  \n    \n      \n        k\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle k(x,y)}\n   selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters \n  \n    \n      \n        \n          \u03b1\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n   of images of feature vectors \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   that occur in the data base. With this choice of a hyperplane, the points \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   in the feature space that are mapped into the hyperplane are defined by the relation \n  \n    \n      \n        \n          \n            \u2211\n            \n              i\n            \n          \n          \n            \u03b1\n            \n              i\n            \n          \n          k\n          (\n          \n            x\n            \n              i\n            \n          \n          ,\n          x\n          )\n          =\n          \n            constant\n          \n          .\n        \n      \n    \n    {\\displaystyle \\textstyle \\sum _{i}\\alpha _{i}k(x_{i},x)={\\text{constant}}.}\n    Note that if \n  \n    \n      \n        k\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle k(x,y)}\n   becomes small as \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   grows further away from \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , each term in the sum measures the degree of closeness of the test point \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   to the corresponding data base point \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  . In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.\n\n\n== Applications ==\nSVMs can be used to solve various real-world problems:\n\nSVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines.\nClassification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.\nClassification of satellite data like SAR data using supervised SVM.\nHand-written characters can be recognized using SVM.\nThe SVM algorithm has been widely applied in the biological and other sciences.  They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.\n\n\n== History ==\nThe original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\n\n\n== Linear SVM ==\n\nWe are given a training dataset of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   points of the form\n\nwhere the \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   are either 1 or \u22121, each indicating the class to which the point \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   belongs. Each \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   is a \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  -dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   for which \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle y_{i}=1}\n   from the group of points for which \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \u2212\n        1\n      \n    \n    {\\displaystyle y_{i}=-1}\n  , which is defined so that the distance between the hyperplane and the nearest point \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   from either group is maximized.\nAny hyperplane can be written as the set of points \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   satisfying\n\nwhere \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is not necessarily a unit vector. The parameter \n  \n    \n      \n        \n          \n            \n              b\n              \n                \u2016\n                \n                  w\n                \n                \u2016\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}\n   determines the offset of the hyperplane from the origin along the normal vector \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n  .\n\n\n=== Hard-margin ===\nIf the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            \n              T\n            \n          \n        \n        \n          x\n        \n        \u2212\n        b\n        =\n        1\n      \n    \n    {\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b=1}\n   (anything on or above this boundary is of one class, with label 1)and\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            \n              T\n            \n          \n        \n        \n          x\n        \n        \u2212\n        b\n        =\n        \u2212\n        1\n      \n    \n    {\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b=-1}\n   (anything on or below this boundary is of the other class, with label \u22121).Geometrically, the distance between these two hyperplanes is \n  \n    \n      \n        \n          \n            \n              2\n              \n                \u2016\n                \n                  w\n                \n                \u2016\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {2}{\\|\\mathbf {w} \\|}}}\n  , so to maximize the distance between the planes we want to minimize \n  \n    \n      \n        \u2016\n        \n          w\n        \n        \u2016\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|}\n  . The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   either\n\nor\n\nThese constraints state that each data point must lie on the correct side of the margin.\nThis can be rewritten as\n\nWe can put this together to get the optimization problem:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    minimize\n                    \n                      \n                        w\n                      \n                      ,\n                      \n                      b\n                    \n                  \n                \n              \n              \n              \n                \n                \u2016\n                \n                  w\n                \n                \n                  \u2016\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                  subject to\n                \n              \n              \n              \n                \n                  y\n                  \n                    i\n                  \n                \n                (\n                \n                  \n                    w\n                  \n                  \n                    \u22a4\n                  \n                \n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                \u2212\n                b\n                )\n                \u2265\n                1\n                \n                \u2200\n                i\n                \u2208\n                {\n                1\n                ,\n                \u2026\n                ,\n                n\n                }\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b}{\\operatorname {minimize} }}&&\\|\\mathbf {w} \\|_{2}^{2}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}\n  \nThe \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   that solve this problem determine our classifier, \n  \n    \n      \n        \n          x\n        \n        \u21a6\n        sgn\n        \u2061\n        (\n        \n          \n            w\n          \n          \n            \n              T\n            \n          \n        \n        \n          x\n        \n        \u2212\n        b\n        )\n      \n    \n    {\\displaystyle \\mathbf {x} \\mapsto \\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b)}\n   where \n  \n    \n      \n        sgn\n        \u2061\n        (\n        \u22c5\n        )\n      \n    \n    {\\displaystyle \\operatorname {sgn}(\\cdot )}\n   is the sign function.\nAn important consequence of this geometric description is that the max-margin hyperplane is completely determined by those \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   that lie nearest to it. These \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   are called support vectors.\n\n\n=== Soft-margin ===\nTo extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful\n\nNote that \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   is the i-th target (i.e., in this case, 1 or \u22121), and \n  \n    \n      \n        \n          \n            w\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            x\n          \n          \n            i\n          \n        \n        \u2212\n        b\n      \n    \n    {\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b}\n   is the i-th output.\nThis function is zero if the constraint in (1) is satisfied, in other words, if \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.\nThe goal of the optimization then is to minimize\n\nwhere the parameter \n  \n    \n      \n        \u03bb\n        >\n        0\n      \n    \n    {\\displaystyle \\lambda >0}\n   determines the trade-off between increasing the margin size and ensuring that the \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   lie on the correct side of the margin. By deconstructing the hinge loss, this optimization problem can be massaged into the following:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    minimize\n                    \n                      \n                        w\n                      \n                      ,\n                      \n                      b\n                      ,\n                      \n                      \n                        \u03b6\n                      \n                    \n                  \n                \n              \n              \n              \n                \n                \u2016\n                \n                  w\n                \n                \n                  \u2016\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                +\n                C\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \u03b6\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n              \n                \n                  subject to\n                \n              \n              \n              \n                \n                  y\n                  \n                    i\n                  \n                \n                (\n                \n                  \n                    w\n                  \n                  \n                    \u22a4\n                  \n                \n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                \u2212\n                b\n                )\n                \u2265\n                1\n                \u2212\n                \n                  \u03b6\n                  \n                    i\n                  \n                \n                ,\n                \n                \n                  \u03b6\n                  \n                    i\n                  \n                \n                \u2265\n                0\n                \n                \u2200\n                i\n                \u2208\n                {\n                1\n                ,\n                \u2026\n                ,\n                n\n                }\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b,\\;\\mathbf {\\zeta } }{\\operatorname {minimize} }}&&\\|\\mathbf {w} \\|_{2}^{2}+C\\sum _{i=1}^{n}\\zeta _{i}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1-\\zeta _{i},\\quad \\zeta _{i}\\geq 0\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}\n  \nThus, for large values of \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not. (\n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   is inversely related to \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , e.g. in LIBSVM.)\n\n\n== Nonlinear Kernels ==\n\nThe original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.\nIt is noteworthy that working in a higher-dimensional feature space increases the generalization error of support vector machines, although given enough samples the algorithm still performs well.Some common kernels include:\n\nPolynomial (homogeneous): \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          \n            x\n          \n          \n            j\n          \n        \n        \n          )\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=(\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j})^{d}}\n  . Particularly, when \n  \n    \n      \n        d\n        =\n        1\n      \n    \n    {\\displaystyle d=1}\n  , this becomes the linear kernel.\nPolynomial (inhomogeneous): \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          \n            x\n          \n          \n            j\n          \n        \n        +\n        r\n        \n          )\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=(\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}+r)^{d}}\n  .\nGaussian radial basis function: \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \u03b3\n            \n              \n                \u2016\n                \n                  \n                    \n                      x\n                    \n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      x\n                    \n                    \n                      j\n                    \n                  \n                \n                \u2016\n              \n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\exp \\left(-\\gamma \\left\\|\\mathbf {x} _{i}-\\mathbf {x} _{j}\\right\\|^{2}\\right)}\n   for \n  \n    \n      \n        \u03b3\n        >\n        0\n      \n    \n    {\\displaystyle \\gamma >0}\n  . Sometimes parametrized using \n  \n    \n      \n        \u03b3\n        =\n        1\n        \n          /\n        \n        (\n        2\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\gamma =1/(2\\sigma ^{2})}\n  .\nSigmoid function (Hyperbolic tangent): \n  \n    \n      \n        k\n        (\n        \n          \n            x\n            \n              i\n            \n          \n        \n        ,\n        \n          \n            x\n            \n              j\n            \n          \n        \n        )\n        =\n        tanh\n        \u2061\n        (\n        \u03ba\n        \n          \n            x\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          \n            x\n          \n          \n            j\n          \n        \n        +\n        c\n        )\n      \n    \n    {\\displaystyle k(\\mathbf {x_{i}} ,\\mathbf {x_{j}} )=\\tanh(\\kappa \\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}+c)}\n   for some (not every) \n  \n    \n      \n        \u03ba\n        >\n        0\n      \n    \n    {\\displaystyle \\kappa >0}\n   and \n  \n    \n      \n        c\n        <\n        0\n      \n    \n    {\\displaystyle c<0}\n  .The kernel is related to the transform \n  \n    \n      \n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varphi (\\mathbf {x} _{i})}\n   by the equation \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n        \u22c5\n        \u03c6\n        (\n        \n          \n            x\n            \n              j\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\varphi (\\mathbf {x} _{i})\\cdot \\varphi (\\mathbf {x_{j}} )}\n  . The value w is also in the transformed space, with \n  \n    \n      \n        \n          w\n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03b1\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n      \n    \n    {\\textstyle \\mathbf {w} =\\sum _{i}\\alpha _{i}y_{i}\\varphi (\\mathbf {x} _{i})}\n  . Dot products with w for classification can again be computed by the kernel trick, i.e. \n  \n    \n      \n        \n          w\n        \n        \u22c5\n        \u03c6\n        (\n        \n          x\n        \n        )\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u03b1\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          x\n        \n        )\n      \n    \n    {\\textstyle \\mathbf {w} \\cdot \\varphi (\\mathbf {x} )=\\sum _{i}\\alpha _{i}y_{i}k(\\mathbf {x} _{i},\\mathbf {x} )}\n  .\n\n\n== Computing the SVM classifier ==\nComputing the (soft-margin) SVM classifier amounts to minimizing an expression of the form\n\nWe focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.\n\n\n=== Primal ===\nMinimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.\nFor each \n  \n    \n      \n        i\n        \u2208\n        {\n        1\n        ,\n        \n        \u2026\n        ,\n        \n        n\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,\\,\\ldots ,\\,n\\}}\n   we introduce a variable \n  \n    \n      \n        \n          \u03b6\n          \n            i\n          \n        \n        =\n        max\n        \n          (\n          \n            0\n            ,\n            1\n            \u2212\n            \n              y\n              \n                i\n              \n            \n            (\n            \n              \n                w\n              \n              \n                \n                  T\n                \n              \n            \n            \n              \n                x\n              \n              \n                i\n              \n            \n            \u2212\n            b\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\zeta _{i}=\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right)}\n  . Note that \n  \n    \n      \n        \n          \u03b6\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\zeta _{i}}\n   is the smallest nonnegative number satisfying \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          \n            w\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            x\n          \n          \n            i\n          \n        \n        \u2212\n        b\n        )\n        \u2265\n        1\n        \u2212\n        \n          \u03b6\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\geq 1-\\zeta _{i}.}\n  \nThus we can rewrite the optimization problem as follows\n\nThis is called the primal problem.\n\n\n=== Dual ===\nBy solving for the Lagrangian dual of the above problem, one obtains the simplified problem\n\nThis is called the dual problem. Since the dual maximization problem is a quadratic function of the \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.\nHere, the variables \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   are defined such that\n\nMoreover, \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle c_{i}=0}\n   exactly when \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   lies on the correct side of the margin, and \n  \n    \n      \n        0\n        <\n        \n          c\n          \n            i\n          \n        \n        <\n        (\n        2\n        n\n        \u03bb\n        \n          )\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}\n    when \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   lies on the margin's boundary. It follows that \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   can be written as a linear combination of the support vectors.\nThe offset, \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  , can be recovered by finding an \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   on the margin's boundary and solving\n\n(Note that \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2212\n            1\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{-1}=y_{i}}\n   since \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \u00b1\n        1\n      \n    \n    {\\displaystyle y_{i}=\\pm 1}\n  .)\n\n\n=== Kernel trick ===\n\nSuppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points \n  \n    \n      \n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\varphi (\\mathbf {x} _{i}).}\n   Moreover, we are given a kernel function \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   which satisfies \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n        \u22c5\n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\varphi (\\mathbf {x} _{i})\\cdot \\varphi (\\mathbf {x} _{j})}\n  .\nWe know the classification vector \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   in the transformed space satisfies\n\nwhere, the \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   are obtained by solving the optimization problem\n\nThe coefficients \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   can be solved for using quadratic programming, as before. Again, we can find some index \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   such that \n  \n    \n      \n        0\n        <\n        \n          c\n          \n            i\n          \n        \n        <\n        (\n        2\n        n\n        \u03bb\n        \n          )\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}\n  , so that \n  \n    \n      \n        \u03c6\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varphi (\\mathbf {x} _{i})}\n   lies on the boundary of the margin in the transformed space, and then solve\n\nFinally,\n\n\n=== Modern methods ===\nRecent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets\u2014sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.\n\n\n==== Sub-gradient descent ====\nSub-gradient descent algorithms for the SVM work directly with the expression\n\nNote that \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is a convex function of \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  . As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , the number of data points.\n\n\n==== Coordinate descent ====\nCoordinate descent algorithms for the SVM work from the dual problem\n\nFor each \n  \n    \n      \n        i\n        \u2208\n        {\n        1\n        ,\n        \n        \u2026\n        ,\n        \n        n\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,\\,\\ldots ,\\,n\\}}\n  , iteratively, the coefficient \n  \n    \n      \n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle c_{i}}\n   is adjusted in the direction of \n  \n    \n      \n        \u2202\n        f\n        \n          /\n        \n        \u2202\n        \n          c\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\partial f/\\partial c_{i}}\n  . Then, the resulting vector of coefficients \n  \n    \n      \n        (\n        \n          c\n          \n            1\n          \n          \u2032\n        \n        ,\n        \n        \u2026\n        ,\n        \n        \n          c\n          \n            n\n          \n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle (c_{1}',\\,\\ldots ,\\,c_{n}')}\n   is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.\n\n\n== Empirical risk minimization ==\nThe soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.\n\n\n=== Risk minimization ===\nIn supervised learning, one is given a set of training examples \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        \u2026\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1}\\ldots X_{n}}\n   with labels \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        \u2026\n        \n          y\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle y_{1}\\ldots y_{n}}\n  , and wishes to predict \n  \n    \n      \n        \n          y\n          \n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle y_{n+1}}\n   given \n  \n    \n      \n        \n          X\n          \n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{n+1}}\n  . To do so one forms a hypothesis, \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , such that \n  \n    \n      \n        f\n        (\n        \n          X\n          \n            n\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle f(X_{n+1})}\n   is a \"good\" approximation of \n  \n    \n      \n        \n          y\n          \n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle y_{n+1}}\n  . A \"good\" approximation is usually defined with the help of a loss function, \n  \n    \n      \n        \u2113\n        (\n        y\n        ,\n        z\n        )\n      \n    \n    {\\displaystyle \\ell (y,z)}\n  , which characterizes how bad \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n   is as a prediction of \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . We would then like to choose a hypothesis that minimizes the expected risk:\n\nIn most cases, we don't know the joint distribution of \n  \n    \n      \n        \n          X\n          \n            n\n            +\n            1\n          \n        \n        ,\n        \n        \n          y\n          \n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle X_{n+1},\\,y_{n+1}}\n   outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:\n\nUnder certain assumptions about the sequence of random variables \n  \n    \n      \n        \n          X\n          \n            k\n          \n        \n        ,\n        \n        \n          y\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle X_{k},\\,y_{k}}\n   (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   grows large. This approach is called empirical risk minimization, or ERM.\n\n\n=== Regularization and stability ===\nIn order for the minimization problem to have a well-defined solution, we have to place constraints on the set \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n   of hypotheses being considered. If \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n   is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   for which \n  \n    \n      \n        \u2016\n        f\n        \n          \u2016\n          \n            \n              H\n            \n          \n        \n        <\n        k\n      \n    \n    {\\displaystyle \\lVert f\\rVert _{\\mathcal {H}}<k}\n   . This is equivalent to imposing a regularization penalty \n  \n    \n      \n        \n          \n            R\n          \n        \n        (\n        f\n        )\n        =\n        \n          \u03bb\n          \n            k\n          \n        \n        \u2016\n        f\n        \n          \u2016\n          \n            \n              H\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}(f)=\\lambda _{k}\\lVert f\\rVert _{\\mathcal {H}}}\n  , and solving the new optimization problem\n\nThis approach is called Tikhonov regularization.\nMore generally, \n  \n    \n      \n        \n          \n            R\n          \n        \n        (\n        f\n        )\n      \n    \n    {\\displaystyle {\\mathcal {R}}(f)}\n   can be some measure of the complexity of the hypothesis \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  , so that simpler hypotheses are preferred.\n\n\n=== SVM and the hinge loss ===\nRecall that the (soft-margin) SVM classifier \n  \n    \n      \n        \n          \n            \n              \n                w\n              \n              ^\n            \n          \n        \n        ,\n        b\n        :\n        \n          x\n        \n        \u21a6\n        sgn\n        \u2061\n        (\n        \n          \n            \n              \n                \n                  w\n                \n                ^\n              \n            \n          \n          \n            \n              T\n            \n          \n        \n        \n          x\n        \n        \u2212\n        b\n        )\n      \n    \n    {\\displaystyle {\\hat {\\mathbf {w} }},b:\\mathbf {x} \\mapsto \\operatorname {sgn}({\\hat {\\mathbf {w} }}^{\\mathsf {T}}\\mathbf {x} -b)}\n   is chosen to minimize the following expression:\n\nIn light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss\n\nFrom this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  \n  \n    \n      \n        \n          \u2113\n          \n            s\n            q\n          \n        \n        (\n        y\n        ,\n        z\n        )\n        =\n        (\n        y\n        \u2212\n        z\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\ell _{sq}(y,z)=(y-z)^{2}}\n  ; logistic regression employs the log-loss,\n\n\n==== Target functions ====\nThe difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables \n  \n    \n      \n        X\n        ,\n        \n        y\n      \n    \n    {\\displaystyle X,\\,y}\n  .\nIn particular, let \n  \n    \n      \n        \n          y\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle y_{x}}\n   denote \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   conditional on the event that \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  .  In the classification setting, we have:\n\nThe optimal classifier is therefore:\n\nFor the square-loss, the target function is the conditional expectation function, \n  \n    \n      \n        \n          f\n          \n            s\n            q\n          \n        \n        (\n        x\n        )\n        =\n        \n          E\n        \n        \n          [\n          \n            y\n            \n              x\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle f_{sq}(x)=\\mathbb {E} \\left[y_{x}\\right]}\n  ; For the logistic loss, it's the logit function, \n  \n    \n      \n        \n          f\n          \n            log\n          \n        \n        (\n        x\n        )\n        =\n        ln\n        \u2061\n        \n          (\n          \n            \n              p\n              \n                x\n              \n            \n            \n              /\n            \n            (\n            \n              1\n              \u2212\n              \n                p\n                \n                  x\n                \n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle f_{\\log }(x)=\\ln \\left(p_{x}/({1-p_{x}})\\right)}\n  . While both of these target functions yield the correct classifier, as \n  \n    \n      \n        sgn\n        \u2061\n        (\n        \n          f\n          \n            s\n            q\n          \n        \n        )\n        =\n        sgn\n        \u2061\n        (\n        \n          f\n          \n            log\n          \n        \n        )\n        =\n        \n          f\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {sgn}(f_{sq})=\\operatorname {sgn}(f_{\\log })=f^{*}}\n  , they give us more information than we need. In fact, they give us enough information to completely describe the distribution of \n  \n    \n      \n        \n          y\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle y_{x}}\n  .\nOn the other hand, one can check that the target function for the hinge loss is exactly \n  \n    \n      \n        \n          f\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle f^{*}}\n  . Thus, in a sufficiently rich hypothesis space\u2014or equivalently, for an appropriately chosen kernel\u2014the SVM classifier will converge to the simplest function (in terms of \n  \n    \n      \n        \n          \n            R\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}}\n  ) that correctly classifies the data. This extends the geometric interpretation of SVM\u2014for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.\n\n\n== Properties ==\nSVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.\nA comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.\n\n\n=== Parameter selection ===\nThe effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  .\nA common choice is a Gaussian kernel, which has a single parameter \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n  . The best combination of \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   and \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   is often selected by a grid search with exponentially growing sequences of \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   and \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n  , for example, \n  \n    \n      \n        \u03bb\n        \u2208\n        {\n        \n          2\n          \n            \u2212\n            5\n          \n        \n        ,\n        \n          2\n          \n            \u2212\n            3\n          \n        \n        ,\n        \u2026\n        ,\n        \n          2\n          \n            13\n          \n        \n        ,\n        \n          2\n          \n            15\n          \n        \n        }\n      \n    \n    {\\displaystyle \\lambda \\in \\{2^{-5},2^{-3},\\dots ,2^{13},2^{15}\\}}\n  ; \n  \n    \n      \n        \u03b3\n        \u2208\n        {\n        \n          2\n          \n            \u2212\n            15\n          \n        \n        ,\n        \n          2\n          \n            \u2212\n            13\n          \n        \n        ,\n        \u2026\n        ,\n        \n          2\n          \n            1\n          \n        \n        ,\n        \n          2\n          \n            3\n          \n        \n        }\n      \n    \n    {\\displaystyle \\gamma \\in \\{2^{-15},2^{-13},\\dots ,2^{1},2^{3}\\}}\n  . Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   and \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.\n\n\n=== Issues ===\nPotential drawbacks of the SVM include the following aspects:\n\nRequires full labeling of input data\nUncalibrated class membership probabilities\u2014SVM stems from Vapnik's theory which avoids estimating probabilities on finite data\nThe SVM is only directly applicable for two-class tasks. Therefore, algorithms that reduce the multi-class task to several binary problems have to be applied; see the multi-class SVM section.\nParameters of a solved model are difficult to interpret.\n\n\n== Extensions ==\n\n\n=== Support vector clustering (SVC) ===\nSVC is a similar method that also builds on kernel functions but is appropriate for unsupervised learning.\n\n\n=== Multiclass SVM ===\nMulticlass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements.\nThe dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems. Common methods for such reduction include:\nBuilding binary classifiers that distinguish between one of the labels and the rest (one-versus-all) or between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest-output function assigns the class (it is important that the output functions be calibrated to produce comparable scores). For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification.\nDirected acyclic graph SVM (DAGSVM)\nError-correcting output codesCrammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems. See also Lee, Lin and Wahba and Van den Burg and Groenen.\n\n\n=== Transductive support vector machines ===\nTransductive support vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  , the learner is also given a set\n\nof test examples to be classified. Formally, a transductive support vector machine is defined by the following primal optimization problem:Minimize (in \n  \n    \n      \n        \n          w\n        \n        ,\n        b\n        ,\n        \n          \n            y\n          \n          \n            \u22c6\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} ,b,\\mathbf {y} ^{\\star }}\n  )\n\nsubject to (for any \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n   and any \n  \n    \n      \n        j\n        =\n        1\n        ,\n        \u2026\n        ,\n        k\n      \n    \n    {\\displaystyle j=1,\\dots ,k}\n  )\n\nand\n\nTransductive support vector machines were introduced by Vladimir N. Vapnik in 1998.\n\n\n=== Structured SVM ===\nSVMs have been generalized to structured SVMs, where the label space is structured and of possibly infinite size.\n\n\n=== Regression ===\n\nA version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola. This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.Training the original SVR means solving\nminimize \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        \u2016\n        w\n        \n          \u2016\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}\\|w\\|^{2}}\n  \nsubject to \n  \n    \n      \n        \n          |\n        \n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \u27e8\n        w\n        ,\n        \n          x\n          \n            i\n          \n        \n        \u27e9\n        \u2212\n        b\n        \n          |\n        \n        \u2264\n        \u03b5\n      \n    \n    {\\displaystyle |y_{i}-\\langle w,x_{i}\\rangle -b|\\leq \\varepsilon }\n  where \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   is a training sample with target value \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  . The inner product plus intercept \n  \n    \n      \n        \u27e8\n        w\n        ,\n        \n          x\n          \n            i\n          \n        \n        \u27e9\n        +\n        b\n      \n    \n    {\\displaystyle \\langle w,x_{i}\\rangle +b}\n   is the prediction for that sample, and \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   is a free parameter that serves as a threshold: all predictions have to be within an \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.\n\n\n=== Bayesian SVM ===\nIn 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation. In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.\n\n\n== Implementation ==\nThe parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.\nAnother approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush\u2013Kuhn\u2013Tucker conditions of the primal and dual problems.\nInstead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.\nAnother common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.The special case of linear support vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.\nThe general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.\nKernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.\nPreprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM.\n\n\n== See also ==\nIn situ adaptive tabulation\nKernel machines\nFisher kernel\nPlatt scaling\nPolynomial kernel\nPredictive analytics\nRegularization perspectives on support vector machines\nRelevance vector machine, a probabilistic sparse-kernel model identical in functional form to SVM\nSequential minimal optimization\nSpace mapping\nWinnow (algorithm)\n\n\n== References ==\n\n\n== Further reading ==\nBennett, Kristin P.; Campbell, Colin (2000). \"Support Vector Machines: Hype or Hallelujah?\" (PDF). SIGKDD Explorations. 2 (2): 1\u201313. doi:10.1145/380995.380999. S2CID 207753020.\nCristianini, Nello; Shawe-Taylor, John (2000). An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press. ISBN 0-521-78019-5.\nFradkin, Dmitriy; Muchnik, Ilya (2006). \"Support Vector Machines for Classification\" (PDF).  In Abello, J.; Carmode, G. (eds.). Discrete Methods in Epidemiology. DIMACS Series in Discrete Mathematics and Theoretical Computer Science. Vol. 70. pp. 13\u201320.\nJoachims, Thorsten (1998). \"Text categorization with Support Vector Machines: Learning with many relevant features\".  In N\u00e9dellec, Claire; Rouveirol, C\u00e9line (eds.). \"Machine Learning: ECML-98. Lecture Notes in Computer Science. Vol. 1398. Berlin, Heidelberg: Springer. p. 137-142. doi:10.1007/BFb0026683. ISBN 978-3-540-64417-0.\nIvanciuc, Ovidiu (2007). \"Applications of Support Vector Machines in Chemistry\" (PDF). Reviews in Computational Chemistry. 23: 291\u2013400. doi:10.1002/9780470116449.ch6. ISBN 9780470116449.\nJames, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2013). \"Support Vector Machines\" (PDF). An Introduction to Statistical Learning : with Applications in R. New York: Springer. pp. 337\u2013372. ISBN 978-1-4614-7137-0.\nSch\u00f6lkopf, Bernhard; Smola, Alexander J. (2002). Learning with Kernels. Cambridge, MA: MIT Press. ISBN 0-262-19475-9.\nSteinwart, Ingo; Christmann, Andreas (2008). Support Vector Machines. New York: Springer. ISBN 978-0-387-77241-7.\nTheodoridis, Sergios; Koutroumbas, Konstantinos (2009). Pattern Recognition (4th ed.). Academic Press. ISBN 978-1-59749-272-0.\n\n\n== External links ==\nlibsvm, LIBSVM is a popular library of SVM learners\nliblinear is a library for large linear classification including some SVMs\nSVM light is a collection of software tools for learning and classification using SVM\nSVMJS live demo is a GUI demo for JavaScript implementation of SVMs", "Gradient descent": "In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although all are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.\n\n\n== Description ==\n\nGradient descent is based on the observation that if the multi-variable function \n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {x} )}\n   is defined and differentiable in a neighborhood of a point \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n  , then \n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {x} )}\n   decreases fastest if one goes from \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n   in the direction of the negative gradient of \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   at \n  \n    \n      \n        \n          a\n        \n        ,\n        \u2212\n        \u2207\n        F\n        (\n        \n          a\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {a} ,-\\nabla F(\\mathbf {a} )}\n  . It follows that, if\n\n  \n    \n      \n        \n          \n            a\n          \n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            a\n          \n          \n            n\n          \n        \n        \u2212\n        \u03b3\n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\gamma \\nabla F(\\mathbf {a} _{n})}\n  for a small enough step size or learning rate \n  \n    \n      \n        \u03b3\n        \u2208\n        \n          \n            R\n          \n          \n            +\n          \n        \n      \n    \n    {\\displaystyle \\gamma \\in \\mathbb {R} _{+}}\n  , then  \n  \n    \n      \n        F\n        (\n        \n          \n            a\n            \n              n\n            \n          \n        \n        )\n        \u2265\n        F\n        (\n        \n          \n            a\n            \n              n\n              +\n              1\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {a_{n}} )\\geq F(\\mathbf {a_{n+1}} )}\n  . In other words, the term \n  \n    \n      \n        \u03b3\n        \u2207\n        F\n        (\n        \n          a\n        \n        )\n      \n    \n    {\\displaystyle \\gamma \\nabla F(\\mathbf {a} )}\n   is subtracted from \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n   because we want to move against the gradient, toward the local minimum. With this observation in mind, one starts with a guess \n  \n    \n      \n        \n          \n            x\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{0}}\n   for a local minimum of \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  , and considers the sequence \n  \n    \n      \n        \n          \n            x\n          \n          \n            0\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            2\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle \\mathbf {x} _{0},\\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots }\n   such that\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            x\n          \n          \n            n\n          \n        \n        \u2212\n        \n          \u03b3\n          \n            n\n          \n        \n        \u2207\n        F\n        (\n        \n          \n            x\n          \n          \n            n\n          \n        \n        )\n        ,\n         \n        n\n        \u2265\n        0.\n      \n    \n    {\\displaystyle \\mathbf {x} _{n+1}=\\mathbf {x} _{n}-\\gamma _{n}\\nabla F(\\mathbf {x} _{n}),\\ n\\geq 0.}\n  We have a monotonic sequence\n\n  \n    \n      \n        F\n        (\n        \n          \n            x\n          \n          \n            0\n          \n        \n        )\n        \u2265\n        F\n        (\n        \n          \n            x\n          \n          \n            1\n          \n        \n        )\n        \u2265\n        F\n        (\n        \n          \n            x\n          \n          \n            2\n          \n        \n        )\n        \u2265\n        \u22ef\n        ,\n      \n    \n    {\\displaystyle F(\\mathbf {x} _{0})\\geq F(\\mathbf {x} _{1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots ,}\n  so, hopefully, the sequence \n  \n    \n      \n        (\n        \n          \n            x\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\mathbf {x} _{n})}\n   converges to the desired local minimum. Note that the value of the step size \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   is allowed to change at every iteration. With certain assumptions on the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   (for example, \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   convex and \n  \n    \n      \n        \u2207\n        F\n      \n    \n    {\\displaystyle \\nabla F}\n   Lipschitz) and particular choices of \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   (e.g., chosen either via a line search that satisfies the Wolfe conditions, or the Barzilai-Borwein method shown as following),\n\n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n        =\n        \n          \n            \n              |\n              \n                \n                  \n                    (\n                    \n                      \n                        \n                          x\n                        \n                        \n                          n\n                        \n                      \n                      \u2212\n                      \n                        \n                          x\n                        \n                        \n                          n\n                          \u2212\n                          1\n                        \n                      \n                    \n                    )\n                  \n                  \n                    T\n                  \n                \n                \n                  [\n                  \n                    \u2207\n                    F\n                    (\n                    \n                      \n                        x\n                      \n                      \n                        n\n                      \n                    \n                    )\n                    \u2212\n                    \u2207\n                    F\n                    (\n                    \n                      \n                        x\n                      \n                      \n                        n\n                        \u2212\n                        1\n                      \n                    \n                    )\n                  \n                  ]\n                \n              \n              |\n            \n            \n              \n                \u2016\n                \n                  \u2207\n                  F\n                  (\n                  \n                    \n                      x\n                    \n                    \n                      n\n                    \n                  \n                  )\n                  \u2212\n                  \u2207\n                  F\n                  (\n                  \n                    \n                      x\n                    \n                    \n                      n\n                      \u2212\n                      1\n                    \n                  \n                  )\n                \n                \u2016\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}={\\frac {\\left|\\left(\\mathbf {x} _{n}-\\mathbf {x} _{n-1}\\right)^{T}\\left[\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})\\right]\\right|}{\\left\\|\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})\\right\\|^{2}}}}\n  convergence to a local minimum can be guaranteed. When the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.\nThis process is illustrated in the adjacent picture. Here, \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is assumed to be defined on the plane, and that its graph has a bowl shape.  The blue curves are the contour lines, that is, the regions on which the value of \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is orthogonal to the contour line going through that point. We see that gradient descent leads us to the bottom of the bowl, that is, to the point where the value of the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is minimal.\n\n\n=== An analogy for understanding gradient descent ===\n\nThe basic intuition behind gradient descent can be illustrated by a hypothetical scenario. A person is stuck in the mountains and is trying to get down (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minimum. They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to find the top of the mountain (i.e., the maximum), then they would proceed in the direction of steepest ascent (i.e., uphill). Using this method, they would eventually find their way down the mountain or possibly get stuck in some hole (i.e., local minimum or saddle point), like a mountain lake. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the person happens to have at the moment. It takes quite some time to measure the steepness of the hill with the instrument, thus they should minimize their use of the instrument if they wanted to get down the mountain before sunset. The difficulty then is choosing the frequency at which they should measure the steepness of the hill so not to go off track.\nIn this analogy, the person represents the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents the slope of the function at that point. The instrument used to measure steepness is differentiation. The direction they choose to travel in aligns with the gradient of the function at that point. The amount of time they travel before taking another measurement is the step size.\n\n\n=== Choosing the step size and descent direction ===\nSince using a step size \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   that is too small would slow convergence, and a \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   too large would lead to divergence, finding a good setting of \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   is an important practical problem. Philip Wolfe also advocated using \"clever choices of the [descent] direction\" in practice. Whilst using a direction that deviates from the steepest descent direction may seem counter-intuitive, the idea is that the smaller slope may be compensated for by being sustained over a much longer distance.\nTo reason about this mathematically, consider a direction \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and step size \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   and consider the more general update:\n\n  \n    \n      \n        \n          \n            a\n          \n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            a\n          \n          \n            n\n          \n        \n        \u2212\n        \n          \u03b3\n          \n            n\n          \n        \n        \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\gamma _{n}\\,\\mathbf {p} _{n}}\n  .Finding good settings of \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   requires some thought. First of all, we would like the update direction to point downhill. Mathematically, letting \n  \n    \n      \n        \n          \u03b8\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\theta _{n}}\n   denote the angle between \n  \n    \n      \n        \u2212\n        \u2207\n        F\n        (\n        \n          \n            a\n            \n              n\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle -\\nabla F(\\mathbf {a_{n}} )}\n   and \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n  , this requires that \n  \n    \n      \n        cos\n        \u2061\n        \n          \u03b8\n          \n            n\n          \n        \n        >\n        0.\n      \n    \n    {\\displaystyle \\cos \\theta _{n}>0.}\n   To say more, we need more information about the objective function that we are optimising. Under the fairly weak assumption that \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is continuously differentiable, we may prove that:\n\nThis inequality implies that the amount by which we can be sure the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is decreased depends on a trade off between the two terms in square brackets. The first term in square brackets measures the angle between the descent direction and the negative gradient. The second term measures how quickly the gradient changes along the descent direction.\nIn principle inequality (1) could be optimized over \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   to choose an optimal step size and direction. The problem is that evaluating the second term in square brackets requires evaluating \n  \n    \n      \n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        \u2212\n        t\n        \n          \u03b3\n          \n            n\n          \n        \n        \n          \n            p\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\nabla F(\\mathbf {a} _{n}-t\\gamma _{n}\\mathbf {p} _{n})}\n  , and extra gradient evaluations are generally expensive and undesirable. Some ways around this problem are:\n\nForgo the benefits of a clever descent direction by setting \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n        =\n        \u2207\n        F\n        (\n        \n          \n            a\n            \n              n\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {p} _{n}=\\nabla F(\\mathbf {a_{n}} )}\n  , and use line search to find a suitable step-size \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n  , such as one that satisfies the Wolfe conditions. A more economic way of choosing learning rates is backtracking line search, a method that has both good theoretical guarantees and experimental results. Note that one does not need to choose  \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   to be the gradient; any direction that has positive intersection product with the gradient will result in a reduction of the function value (for a sufficiently small value of \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n  ).\nAssuming that \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is twice-differentiable, use its Hessian \n  \n    \n      \n        \n          \u2207\n          \n            2\n          \n        \n        F\n      \n    \n    {\\displaystyle \\nabla ^{2}F}\n   to estimate \n  \n    \n      \n        \u2016\n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        \u2212\n        t\n        \n          \u03b3\n          \n            n\n          \n        \n        \n          \n            p\n          \n          \n            n\n          \n        \n        )\n        \u2212\n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        )\n        \n          \u2016\n          \n            2\n          \n        \n        \u2248\n        \u2016\n        t\n        \n          \u03b3\n          \n            n\n          \n        \n        \n          \u2207\n          \n            2\n          \n        \n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        )\n        \n          \n            p\n          \n          \n            n\n          \n        \n        \u2016\n        .\n      \n    \n    {\\displaystyle \\|\\nabla F(\\mathbf {a} _{n}-t\\gamma _{n}\\mathbf {p} _{n})-\\nabla F(\\mathbf {a} _{n})\\|_{2}\\approx \\|t\\gamma _{n}\\nabla ^{2}F(\\mathbf {a} _{n})\\mathbf {p} _{n}\\|.}\n  Then choose \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   by optimising inequality (1).\nAssuming that \n  \n    \n      \n        \u2207\n        F\n      \n    \n    {\\displaystyle \\nabla F}\n   is Lipschitz, use its Lipschitz constant \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   to bound \n  \n    \n      \n        \u2016\n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        \u2212\n        t\n        \n          \u03b3\n          \n            n\n          \n        \n        \n          \n            p\n          \n          \n            n\n          \n        \n        )\n        \u2212\n        \u2207\n        F\n        (\n        \n          \n            a\n          \n          \n            n\n          \n        \n        )\n        \n          \u2016\n          \n            2\n          \n        \n        \u2264\n        L\n        t\n        \n          \u03b3\n          \n            n\n          \n        \n        \u2016\n        \n          \n            p\n          \n          \n            n\n          \n        \n        \u2016\n        .\n      \n    \n    {\\displaystyle \\|\\nabla F(\\mathbf {a} _{n}-t\\gamma _{n}\\mathbf {p} _{n})-\\nabla F(\\mathbf {a} _{n})\\|_{2}\\leq Lt\\gamma _{n}\\|\\mathbf {p} _{n}\\|.}\n   Then choose \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   by optimising inequality (1).\nBuild a custom model of \n  \n    \n      \n        \n          max\n          \n            t\n            \u2208\n            [\n            0\n            ,\n            1\n            ]\n          \n        \n        \n          \n            \n              \u2016\n              \u2207\n              F\n              (\n              \n                \n                  a\n                \n                \n                  n\n                \n              \n              \u2212\n              t\n              \n                \u03b3\n                \n                  n\n                \n              \n              \n                \n                  p\n                \n                \n                  n\n                \n              \n              )\n              \u2212\n              \u2207\n              F\n              (\n              \n                \n                  a\n                \n                \n                  n\n                \n              \n              )\n              \n                \u2016\n                \n                  2\n                \n              \n            \n            \n              \u2016\n              \u2207\n              F\n              (\n              \n                \n                  a\n                \n                \n                  n\n                \n              \n              )\n              \n                \u2016\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\max _{t\\in [0,1]}{\\frac {\\|\\nabla F(\\mathbf {a} _{n}-t\\gamma _{n}\\mathbf {p} _{n})-\\nabla F(\\mathbf {a} _{n})\\|_{2}}{\\|\\nabla F(\\mathbf {a} _{n})\\|_{2}}}}\n   for \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  . Then choose \n  \n    \n      \n        \n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} _{n}}\n   and \n  \n    \n      \n        \n          \u03b3\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{n}}\n   by optimising inequality (1).\nUnder stronger assumptions on the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   such as convexity, more advanced techniques may be possible.Usually by following one of the recipes above, convergence to a local minimum can be guaranteed. When the function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.\n\n\n== Solution of a linear system ==\n\nGradient descent can be used to solve a system of linear equations\n\n  \n    \n      \n        A\n        \n          x\n        \n        \u2212\n        \n          b\n        \n        =\n        0\n      \n    \n    {\\displaystyle A\\mathbf {x} -\\mathbf {b} =0}\n  reformulated as a quadratic minimization problem.\nIf the system matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is real symmetric and positive-definite, an objective function is defined as the quadratic function, with minimization of\n\n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n        =\n        \n          \n            x\n          \n          \n            T\n          \n        \n        A\n        \n          x\n        \n        \u2212\n        2\n        \n          \n            x\n          \n          \n            T\n          \n        \n        \n          b\n        \n        ,\n      \n    \n    {\\displaystyle F(\\mathbf {x} )=\\mathbf {x} ^{T}A\\mathbf {x} -2\\mathbf {x} ^{T}\\mathbf {b} ,}\n  so that\n\n  \n    \n      \n        \u2207\n        F\n        (\n        \n          x\n        \n        )\n        =\n        2\n        (\n        A\n        \n          x\n        \n        \u2212\n        \n          b\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\nabla F(\\mathbf {x} )=2(A\\mathbf {x} -\\mathbf {b} ).}\n  For a general real matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , linear least squares define\n\n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n        =\n        \n          \n            \u2016\n            \n              A\n              \n                x\n              \n              \u2212\n              \n                b\n              \n            \n            \u2016\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle F(\\mathbf {x} )=\\left\\|A\\mathbf {x} -\\mathbf {b} \\right\\|^{2}.}\n  In traditional linear least squares for real \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        \n          b\n        \n      \n    \n    {\\displaystyle \\mathbf {b} }\n   the Euclidean norm is used, in which case\n\n  \n    \n      \n        \u2207\n        F\n        (\n        \n          x\n        \n        )\n        =\n        2\n        \n          A\n          \n            T\n          \n        \n        (\n        A\n        \n          x\n        \n        \u2212\n        \n          b\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\nabla F(\\mathbf {x} )=2A^{T}(A\\mathbf {x} -\\mathbf {b} ).}\n  The line search minimization, finding the locally optimal step size \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   on every iteration, can be performed analytically for quadratic functions, and explicit formulas for the locally optimal \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   are known.For example, for real symmetric and positive-definite matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , a simple algorithm can be as follows,\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  repeat in the loop:\n                \n              \n            \n            \n              \n              \n                \n                \n                  r\n                \n                :=\n                \n                  b\n                \n                \u2212\n                \n                  A\n                  x\n                \n              \n            \n            \n              \n              \n                \n                \u03b3\n                :=\n                \n                  \n                    \n                      r\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    r\n                  \n                \n                \n                  /\n                \n                \n                  \n                    \n                      r\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    A\n                    r\n                  \n                \n              \n            \n            \n              \n              \n                \n                \n                  x\n                \n                :=\n                \n                  x\n                \n                +\n                \u03b3\n                \n                  r\n                \n              \n            \n            \n              \n              \n                \n                \n                  \n                    if \n                  \n                \n                \n                  \n                    r\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  r\n                \n                \n                   is sufficiently small, then exit loop\n                \n              \n            \n            \n              \n              \n                \n                  end repeat loop\n                \n              \n            \n            \n              \n              \n                \n                  return \n                \n                \n                  x\n                \n                \n                   as the result\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\text{repeat in the loop:}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}}}\n  To avoid multiplying by \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   twice per iteration,\nwe note that \n  \n    \n      \n        \n          x\n        \n        :=\n        \n          x\n        \n        +\n        \u03b3\n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} }\n   implies \n  \n    \n      \n        \n          r\n        \n        :=\n        \n          r\n        \n        \u2212\n        \u03b3\n        \n          A\n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} :=\\mathbf {r} -\\gamma \\mathbf {Ar} }\n  , which gives the traditional algorithm,\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  r\n                \n                :=\n                \n                  b\n                \n                \u2212\n                \n                  A\n                  x\n                \n              \n            \n            \n              \n              \n                \n                  repeat in the loop:\n                \n              \n            \n            \n              \n              \n                \n                \u03b3\n                :=\n                \n                  \n                    \n                      r\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    r\n                  \n                \n                \n                  /\n                \n                \n                  \n                    \n                      r\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    A\n                    r\n                  \n                \n              \n            \n            \n              \n              \n                \n                \n                  x\n                \n                :=\n                \n                  x\n                \n                +\n                \u03b3\n                \n                  r\n                \n              \n            \n            \n              \n              \n                \n                \n                  \n                    if \n                  \n                \n                \n                  \n                    r\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  r\n                \n                \n                   is sufficiently small, then exit loop\n                \n              \n            \n            \n              \n              \n                \n                \n                  r\n                \n                :=\n                \n                  r\n                \n                \u2212\n                \u03b3\n                \n                  A\n                  r\n                \n              \n            \n            \n              \n              \n                \n                  end repeat loop\n                \n              \n            \n            \n              \n              \n                \n                  return \n                \n                \n                  x\n                \n                \n                   as the result\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&{\\text{repeat in the loop:}}\\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {r} -\\gamma \\mathbf {Ar} \\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}}}\n  The method is rarely used for solving linear equations, with the conjugate gradient method being one of the most popular alternatives. The number of gradient descent iterations is commonly proportional to the spectral condition number \n  \n    \n      \n        \u03ba\n        (\n        A\n        )\n      \n    \n    {\\displaystyle \\kappa (A)}\n   of the system matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   (the ratio of the maximum to minimum eigenvalues of \n  \n    \n      \n        \n          A\n          \n            T\n          \n        \n        A\n      \n    \n    {\\displaystyle A^{T}A}\n  ), while the convergence of conjugate gradient method is typically determined by a square root of the condition number, i.e., is much faster. Both methods can benefit from preconditioning, where gradient descent may require less assumptions on the preconditioner.\n\n\n== Solution of a non-linear system ==\nGradient descent can also be used to solve a system of nonlinear equations. Below is an example that shows how to use the gradient descent to solve for three unknown variables, x1, x2, and x3. This example shows one iteration of the gradient descent.\nConsider the nonlinear system of equations\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  3\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  \u2212\n                  cos\n                  \u2061\n                  (\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \n                    x\n                    \n                      3\n                    \n                  \n                  )\n                  \u2212\n                  \n                    \n                      \n                        3\n                        2\n                      \n                    \n                  \n                  =\n                  0\n                \n              \n              \n                \n                  4\n                  \n                    x\n                    \n                      1\n                    \n                    \n                      2\n                    \n                  \n                  \u2212\n                  625\n                  \n                    x\n                    \n                      2\n                    \n                    \n                      2\n                    \n                  \n                  +\n                  2\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \u2212\n                  1\n                  =\n                  0\n                \n              \n              \n                \n                  exp\n                  \u2061\n                  (\n                  \u2212\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  \n                    x\n                    \n                      2\n                    \n                  \n                  )\n                  +\n                  20\n                  \n                    x\n                    \n                      3\n                    \n                  \n                  +\n                  \n                    \n                      \n                        \n                          10\n                          \u03c0\n                          \u2212\n                          3\n                        \n                        3\n                      \n                    \n                  \n                  =\n                  0\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}3x_{1}-\\cos(x_{2}x_{3})-{\\tfrac {3}{2}}=0\\\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1=0\\\\\\exp(-x_{1}x_{2})+20x_{3}+{\\tfrac {10\\pi -3}{3}}=0\\end{cases}}}\n  Let us introduce the associated function\n\n  \n    \n      \n        G\n        (\n        \n          x\n        \n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  3\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  \u2212\n                  cos\n                  \u2061\n                  (\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \n                    x\n                    \n                      3\n                    \n                  \n                  )\n                  \u2212\n                  \n                    \n                      \n                        3\n                        2\n                      \n                    \n                  \n                \n              \n              \n                \n                  4\n                  \n                    x\n                    \n                      1\n                    \n                    \n                      2\n                    \n                  \n                  \u2212\n                  625\n                  \n                    x\n                    \n                      2\n                    \n                    \n                      2\n                    \n                  \n                  +\n                  2\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \u2212\n                  1\n                \n              \n              \n                \n                  exp\n                  \u2061\n                  (\n                  \u2212\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  \n                    x\n                    \n                      2\n                    \n                  \n                  )\n                  +\n                  20\n                  \n                    x\n                    \n                      3\n                    \n                  \n                  +\n                  \n                    \n                      \n                        \n                          10\n                          \u03c0\n                          \u2212\n                          3\n                        \n                        3\n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle G(\\mathbf {x} )={\\begin{bmatrix}3x_{1}-\\cos(x_{2}x_{3})-{\\tfrac {3}{2}}\\\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\\\\\\exp(-x_{1}x_{2})+20x_{3}+{\\tfrac {10\\pi -3}{3}}\\\\\\end{bmatrix}},}\n  where\n\n  \n    \n      \n        \n          x\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {x} ={\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\\\\\end{bmatrix}}.}\n  One might now define the objective function\n\n  \n    \n      \n        \n          \n            \n              \n                F\n                (\n                \n                  x\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    1\n                    2\n                  \n                \n                \n                  G\n                  \n                    \n                      T\n                    \n                  \n                \n                (\n                \n                  x\n                \n                )\n                G\n                (\n                \n                  x\n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    2\n                  \n                \n                \n                  [\n                  \n                    \n                      \n                        (\n                        \n                          3\n                          \n                            x\n                            \n                              1\n                            \n                          \n                          \u2212\n                          cos\n                          \u2061\n                          (\n                          \n                            x\n                            \n                              2\n                            \n                          \n                          \n                            x\n                            \n                              3\n                            \n                          \n                          )\n                          \u2212\n                          \n                            \n                              3\n                              2\n                            \n                          \n                        \n                        )\n                      \n                      \n                        2\n                      \n                    \n                    +\n                    \n                      \n                        (\n                        \n                          4\n                          \n                            x\n                            \n                              1\n                            \n                            \n                              2\n                            \n                          \n                          \u2212\n                          625\n                          \n                            x\n                            \n                              2\n                            \n                            \n                              2\n                            \n                          \n                          +\n                          2\n                          \n                            x\n                            \n                              2\n                            \n                          \n                          \u2212\n                          1\n                        \n                        )\n                      \n                      \n                        2\n                      \n                    \n                    +\n                  \n                  \n                \n              \n            \n            \n              \n              \n                \n\n                \n                \n                \n                  \n                  \n                    \n                      (\n                      \n                        exp\n                        \u2061\n                        (\n                        \u2212\n                        \n                          x\n                          \n                            1\n                          \n                        \n                        \n                          x\n                          \n                            2\n                          \n                        \n                        )\n                        +\n                        20\n                        \n                          x\n                          \n                            3\n                          \n                        \n                        +\n                        \n                          \n                            \n                              10\n                              \u03c0\n                              \u2212\n                              3\n                            \n                            3\n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                  ]\n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F(\\mathbf {x} )&={\\frac {1}{2}}G^{\\mathrm {T} }(\\mathbf {x} )G(\\mathbf {x} )\\\\&={\\frac {1}{2}}\\left[\\left(3x_{1}-\\cos(x_{2}x_{3})-{\\frac {3}{2}}\\right)^{2}+\\left(4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\\right)^{2}+\\right.\\\\&{}\\qquad \\left.\\left(\\exp(-x_{1}x_{2})+20x_{3}+{\\frac {10\\pi -3}{3}}\\right)^{2}\\right],\\end{aligned}}}\n  which we will attempt to minimize. As an initial guess, let us use\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            0\n            )\n          \n        \n        =\n        \n          0\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(0)}=\\mathbf {0} ={\\begin{bmatrix}0\\\\0\\\\0\\\\\\end{bmatrix}}.}\n  We know that\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        \n          0\n        \n        \u2212\n        \n          \u03b3\n          \n            0\n          \n        \n        \u2207\n        F\n        (\n        \n          0\n        \n        )\n        =\n        \n          0\n        \n        \u2212\n        \n          \u03b3\n          \n            0\n          \n        \n        \n          J\n          \n            G\n          \n        \n        (\n        \n          0\n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        G\n        (\n        \n          0\n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(1)}=\\mathbf {0} -\\gamma _{0}\\nabla F(\\mathbf {0} )=\\mathbf {0} -\\gamma _{0}J_{G}(\\mathbf {0} )^{\\mathrm {T} }G(\\mathbf {0} ),}\n  where the Jacobian matrix \n  \n    \n      \n        \n          J\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle J_{G}}\n   is given by\n\n  \n    \n      \n        \n          J\n          \n            G\n          \n        \n        (\n        \n          x\n        \n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  3\n                \n                \n                  sin\n                  \u2061\n                  (\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \n                    x\n                    \n                      3\n                    \n                  \n                  )\n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n                \n                  sin\n                  \u2061\n                  (\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  \n                    x\n                    \n                      3\n                    \n                  \n                  )\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  8\n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n                \n                  \u2212\n                  1250\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  +\n                  2\n                \n                \n                  0\n                \n              \n              \n                \n                  \u2212\n                  \n                    x\n                    \n                      2\n                    \n                  \n                  exp\n                  \u2061\n                  \n                    (\n                    \u2212\n                    \n                      x\n                      \n                        1\n                      \n                    \n                    \n                      x\n                      \n                        2\n                      \n                    \n                    )\n                  \n                \n                \n                  \u2212\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  exp\n                  \u2061\n                  (\n                  \u2212\n                  \n                    x\n                    \n                      1\n                    \n                  \n                  \n                    x\n                    \n                      2\n                    \n                  \n                  )\n                \n                \n                  20\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle J_{G}(\\mathbf {x} )={\\begin{bmatrix}3&\\sin(x_{2}x_{3})x_{3}&\\sin(x_{2}x_{3})x_{2}\\\\8x_{1}&-1250x_{2}+2&0\\\\-x_{2}\\exp {(-x_{1}x_{2})}&-x_{1}\\exp(-x_{1}x_{2})&20\\\\\\end{bmatrix}}.}\n  We calculate:\n\n  \n    \n      \n        \n          J\n          \n            G\n          \n        \n        (\n        \n          0\n        \n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  3\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  2\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  20\n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        G\n        (\n        \n          0\n        \n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  2.5\n                \n              \n              \n                \n                  \u2212\n                  1\n                \n              \n              \n                \n                  10.472\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle J_{G}(\\mathbf {0} )={\\begin{bmatrix}3&0&0\\\\0&2&0\\\\0&0&20\\end{bmatrix}},\\qquad G(\\mathbf {0} )={\\begin{bmatrix}-2.5\\\\-1\\\\10.472\\end{bmatrix}}.}\n  Thus\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        \n          0\n        \n        \u2212\n        \n          \u03b3\n          \n            0\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  7.5\n                \n              \n              \n                \n                  \u2212\n                  2\n                \n              \n              \n                \n                  209.44\n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(1)}=\\mathbf {0} -\\gamma _{0}{\\begin{bmatrix}-7.5\\\\-2\\\\209.44\\end{bmatrix}},}\n  and\n\n  \n    \n      \n        F\n        (\n        \n          0\n        \n        )\n        =\n        0.5\n        \n          (\n          \n            (\n            \u2212\n            2.5\n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            \u2212\n            1\n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            10.472\n            \n              )\n              \n                2\n              \n            \n          \n          )\n        \n        =\n        58.456.\n      \n    \n    {\\displaystyle F(\\mathbf {0} )=0.5\\left((-2.5)^{2}+(-1)^{2}+(10.472)^{2}\\right)=58.456.}\n  \nNow, a suitable \n  \n    \n      \n        \n          \u03b3\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\gamma _{0}}\n   must be found such that\n\n  \n    \n      \n        F\n        \n          (\n          \n            \n              x\n            \n            \n              (\n              1\n              )\n            \n          \n          )\n        \n        \u2264\n        F\n        \n          (\n          \n            \n              x\n            \n            \n              (\n              0\n              )\n            \n          \n          )\n        \n        =\n        F\n        (\n        \n          0\n        \n        )\n        .\n      \n    \n    {\\displaystyle F\\left(\\mathbf {x} ^{(1)}\\right)\\leq F\\left(\\mathbf {x} ^{(0)}\\right)=F(\\mathbf {0} ).}\n  This can be done with any of a variety of line search algorithms. One might also simply guess \n  \n    \n      \n        \n          \u03b3\n          \n            0\n          \n        \n        =\n        0.001\n        ,\n      \n    \n    {\\displaystyle \\gamma _{0}=0.001,}\n   which gives\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0.0075\n                \n              \n              \n                \n                  0.002\n                \n              \n              \n                \n                  \u2212\n                  0.20944\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(1)}={\\begin{bmatrix}0.0075\\\\0.002\\\\-0.20944\\\\\\end{bmatrix}}.}\n  Evaluating the objective function at this value, yields\n\n  \n    \n      \n        F\n        \n          (\n          \n            \n              x\n            \n            \n              (\n              1\n              )\n            \n          \n          )\n        \n        =\n        0.5\n        \n          (\n          \n            (\n            \u2212\n            2.48\n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            \u2212\n            1.00\n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            6.28\n            \n              )\n              \n                2\n              \n            \n          \n          )\n        \n        =\n        23.306.\n      \n    \n    {\\displaystyle F\\left(\\mathbf {x} ^{(1)}\\right)=0.5\\left((-2.48)^{2}+(-1.00)^{2}+(6.28)^{2}\\right)=23.306.}\n  The decrease from \n  \n    \n      \n        F\n        (\n        \n          0\n        \n        )\n        =\n        58.456\n      \n    \n    {\\displaystyle F(\\mathbf {0} )=58.456}\n   to the next step's value of\n\n  \n    \n      \n        F\n        \n          (\n          \n            \n              x\n            \n            \n              (\n              1\n              )\n            \n          \n          )\n        \n        =\n        23.306\n      \n    \n    {\\displaystyle F\\left(\\mathbf {x} ^{(1)}\\right)=23.306}\n  is a sizable decrease in the objective function. Further steps would reduce its value further until an approximate solution to the system was found.\n\n\n== Comments ==\nGradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case, the search space is typically a function space, and one calculates the Fr\u00e9chet derivative of the functional to be minimized to determine the descent direction.That gradient descent works in any number of dimensions (finite number at least) can be seen as a consequence of the Cauchy-Schwarz inequality. That article proves that the magnitude of the inner (dot) product of two vectors of any dimension is maximized when they are colinear. In the case of gradient descent, that would be when the vector of independent variable adjustments is proportional to the gradient vector of partial derivatives.\nThe gradient descent can take many iterations to compute a local minimum with a required accuracy, if the curvature in different directions is very different for the given function. For such functions, preconditioning, which changes the geometry of the space to shape the function level sets like concentric circles, cures the slow convergence. Constructing and applying preconditioning can be computationally expensive, however.\nThe gradient descent can be combined with a line search, finding the locally optimal step size \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   on every iteration. Performing the line search can be time-consuming. Conversely, using a fixed small \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   can yield poor convergence.\nMethods based on Newton's method and inversion of the Hessian using conjugate gradient techniques can be better alternatives. Generally, such methods converge in fewer iterations, but the cost of each iteration is higher. An example is the BFGS method which consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a \"better\" direction, combined with a more sophisticated line search algorithm, to find the \"best\" value of \n  \n    \n      \n        \u03b3\n        .\n      \n    \n    {\\displaystyle \\gamma .}\n   For extremely large problems, where the computer-memory issues dominate, a limited-memory method such as L-BFGS should be used instead of BFGS or the steepest descent. \nWhile it is sometimes possible to substitute gradient descent for a local search algorithm, gradient descent is not in the same family: although it is an iterative method for local optimization, it relies on an objective function\u2019s gradient rather than an explicit exploration of a solution space.\nGradient descent can be viewed as applying Euler's method for solving ordinary differential equations \n  \n    \n      \n        \n          x\n          \u2032\n        \n        (\n        t\n        )\n        =\n        \u2212\n        \u2207\n        f\n        (\n        x\n        (\n        t\n        )\n        )\n      \n    \n    {\\displaystyle x'(t)=-\\nabla f(x(t))}\n   to a gradient flow.  In turn, this equation may be derived as an optimal controller for the control system \n  \n    \n      \n        \n          x\n          \u2032\n        \n        (\n        t\n        )\n        =\n        u\n        (\n        t\n        )\n      \n    \n    {\\displaystyle x'(t)=u(t)}\n   with \n  \n    \n      \n        u\n        (\n        t\n        )\n      \n    \n    {\\displaystyle u(t)}\n   given in feedback form \n  \n    \n      \n        u\n        (\n        t\n        )\n        =\n        \u2212\n        \u2207\n        f\n        (\n        x\n        (\n        t\n        )\n        )\n      \n    \n    {\\displaystyle u(t)=-\\nabla f(x(t))}\n  .\nIt can be shown that there is a correspondence between neuroevolution and gradient descent.\n\n\n== Modifications ==\nGradient descent can converge to a local minimum and slow down in a neighborhood of a saddle point. Even for unconstrained quadratic minimization, gradient descent develops a zig-zag pattern of subsequent iterates as iterations progress, resulting in slow convergence. Multiple modifications of gradient descent have been proposed to address these deficiencies.\n\n\n=== Fast gradient methods ===\nYurii Nesterov has proposed a simple modification that enables faster convergence for convex problems and has been since further generalized. For unconstrained smooth problems the method is called the fast gradient method (FGM) or the accelerated gradient method (AGM). Specifically, if the differentiable function \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is convex and \n  \n    \n      \n        \u2207\n        F\n      \n    \n    {\\displaystyle \\nabla F}\n   is Lipschitz, and it is not assumed that \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is strongly convex, then the error in the objective value generated at each step \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   by the gradient descent method will be bounded by \n  \n    \n      \n        \n          \n            O\n          \n        \n        \n          (\n          \n            \n              \n                1\n                k\n              \n            \n          \n          )\n        \n      \n    \n    {\\textstyle {\\mathcal {O}}\\left({\\tfrac {1}{k}}\\right)}\n  . Using the Nesterov acceleration technique, the error decreases at \n  \n    \n      \n        \n          \n            O\n          \n        \n        \n          (\n          \n            \n              \n                1\n                \n                  k\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\textstyle {\\mathcal {O}}\\left({\\tfrac {1}{k^{2}}}\\right)}\n  . It is known that the rate \n  \n    \n      \n        \n          \n            O\n          \n        \n        \n          (\n          \n            \n              k\n              \n                \u2212\n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\mathcal {O}}\\left({k^{-2}}\\right)}\n   for the decrease of the cost function is optimal for first-order optimization methods. Nevertheless, there is the opportunity to improve the algorithm by reducing the constant factor. The optimized gradient method (OGM) reduces that constant by a factor of two and is an optimal first-order method for large-scale problems.For constrained or non-smooth problems, Nesterov's FGM is called the fast proximal gradient method (FPGM), an acceleration of the proximal gradient method.\n\n\n=== Momentum or heavy ball method ===\nTrying to break the zig-zag pattern of gradient descent, the momentum or heavy ball method uses a momentum term in analogy to a heavy ball sliding on the surface of values of the function being minimized, or to mass movement in Newtonian dynamics through a viscous medium in a conservative force field. Gradient descent with momentum remembers the solution update at each iteration, and determines the next update as a linear combination of the gradient and the previous update. For unconstrained quadratic minimization, a theoretical convergence rate bound of the heavy ball method is asymptotically the same as that for the optimal conjugate gradient method.This technique is used in stochastic gradient descent and as an extension to the backpropagation algorithms used to train artificial neural networks. In the direction of updating, stochastic gradient descent adds a stochastic property. The weights can be used to calculate the derivatives.\n\n\n== Extensions ==\nGradient descent can be extended to handle constraints by including a projection onto the set of constraints. This method is only feasible when the projection is efficiently computable on a computer. Under suitable assumptions, this method converges.  This method is a specific case of the forward-backward algorithm for monotone inclusions (which includes convex programming and variational inequalities).Gradient descent is a special case of mirror descent using the squared Euclidean distance as the given Bregman divergence.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nBoyd, Stephen; Vandenberghe, Lieven (2004). \"Unconstrained Minimization\" (PDF). Convex Optimization. New York: Cambridge University Press. pp. 457\u2013520. ISBN 0-521-83378-7.\nChong, Edwin K. P.; \u017bak, Stanislaw H. (2013). \"Gradient Methods\". An Introduction to Optimization (Fourth ed.). Hoboken: Wiley. pp. 131\u2013160. ISBN 978-1-118-27901-4.\nHimmelblau, David M. (1972). \"Unconstrained Minimization Procedures Using Derivatives\". Applied Nonlinear Programming. New York: McGraw-Hill. pp. 63\u2013132. ISBN 0-07-028921-2.\n\n\n== External links ==\n\nUsing gradient descent in C++, Boost, Ublas for linear regression\nSeries of Khan Academy videos discusses gradient ascent\nOnline book teaching gradient descent in deep neural network context\nArchived at Ghostarchive and the Wayback Machine: \"Gradient Descent, How Neural Networks Learn\". 3Blue1Brown. October 16, 2017 \u2013 via YouTube.\nHandbook of Convergence Theorems for (Stochastic) Gradient Methods", "Conditional probability": "In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. This particular method relies on event B occurring with some sort of relationship with another event A. In this event, the event B can be analyzed by a conditional probability with respect to A. If the event of interest is A and the event B is known or assumed to have occurred, \"the conditional probability of A given B\", or \"the probability of A under the condition B\", is usually written as P(A|B) or occasionally PB(A). This can also be understood as the fraction of probability B that intersects with A: \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}}\n  .For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person is sick, then they are much more likely to be coughing. For example, the conditional probability that someone unwell (sick) is coughing might be 75%, in which case we would have that P(Cough) = 5% and P(Cough|Sick) = 75 %. Although there is a relationship between A and B in this example, such a relationship or dependence between A and B is not necessary, nor do they have to occur simultaneously.\nP(A|B) may or may not be equal to P(A) (the unconditional probability of A). If P(A|B) = P(A), then events A and B are said to be independent: in such a case, knowledge about either event does not alter the likelihood of each other. P(A|B) (the conditional probability of A given B) typically differs from P(B|A). For example, if a person has dengue fever, the person might have a 90% chance of being tested as positive for the disease. In this case, what is being measured is that if event B (having dengue) has occurred, the probability of A (tested as positive) given that B occurred is 90%, simply writing P(A|B) = 90%. Alternatively, if a person is tested as positive for dengue fever, they may have only a 15% chance of actually having this rare disease due to high false positive rates. In this case, the probability of the event B (having dengue) given that the event A (testing positive) has occurred is 15% or P(B|A) = 15%. It should be apparent now that falsely equating the two probabilities can lead to various errors of reasoning, which is commonly seen through base rate fallacies.  \nWhile conditional probabilities can provide extremely useful information, limited information is often supplied or at hand. Therefore, it can be useful to reverse or convert a conditional probability using Bayes' theorem: \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2223\n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={{P(B\\mid A)P(A)} \\over {P(B)}}}\n  .  Another option is to display conditional probabilities in conditional probability table to illuminate the relationship between events.\n\n\n== Definition ==\n\n\n=== Conditioning on an event ===\n\n\n==== Kolmogorov definition ====\nGiven two events A and B from the sigma-field of a probability space, with the unconditional probability of B being greater than zero (i.e., P(B) > 0), the conditional probability of A given B (\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  ) is the probability of A occurring if B has or is assumed to have happened. A is assumed to be the set of all possible outcomes of an experiment or random trial that has a restricted or reduced sample space. The conditional probability can be found by the quotient of the probability of the joint intersection of events A and B (\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  )\u2014the probability at which A and B occur together, although not necessarily occurring at the same time\u2014and the probability of B:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}}\n  .For a sample space consisting of equal likelihood outcomes, the probability of the event A is understood as the fraction of the number of outcomes in A to the number of all outcomes in the sample space. Then, this equation is understood as the fraction of the set \n  \n    \n      \n        A\n        \u2229\n        B\n      \n    \n    {\\displaystyle A\\cap B}\n   to the set B. Note that the above equation is a definition, not just a theoretical result. We denote the quantity \n  \n    \n      \n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {P(A\\cap B)}{P(B)}}}\n   as \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   and call it the \"conditional probability of A given B.\"\n\n\n==== As an axiom of probability ====\nSome authors, such as de Finetti, prefer to introduce conditional probability as an axiom of probability:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        B\n        )\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A\\mid B)P(B)}\n  .This equation for a conditional probability, although mathematically equivalent, may be intuitively easier to understand. It can be interpreted as \"the probability of B occurring multiplied by the probability of A occurring, provided that B has occurred, is equal to the probability of the A and B occurrences together, although not necessarily occurring at the same time\". Additionally, this may be preferred philosophically; under major probability interpretations, such as the subjective theory, conditional probability is considered a primitive entity. Moreover, this \"multiplication rule\" can be practically useful in computing the probability of \n  \n    \n      \n        A\n        \u2229\n        B\n      \n    \n    {\\displaystyle A\\cap B}\n   and introduces a symmetry with the summation axiom for Poincar\u00e9 Formula:\n\n  \n    \n      \n        P\n        (\n        A\n        \u222a\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        \u2212\n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cup B)=P(A)+P(B)-P(A\\cap B)}\n  \nThus the equations can be combined to find a new representation of the :\n\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        \u2212\n        P\n        (\n        A\n        \u222a\n        B\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        B\n        )\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A)+P(B)-P(A\\cup B)=P(A\\mid B)P(B)}\n  \n\n  \n    \n      \n        P\n        (\n        A\n        \u222a\n        B\n        )\n        =\n        \n          P\n          (\n          A\n          )\n          +\n          P\n          (\n          B\n          )\n          \u2212\n          P\n          (\n          A\n          \u2223\n          B\n          )\n          \n            P\n            (\n            B\n            )\n          \n        \n      \n    \n    {\\displaystyle P(A\\cup B)={P(A)+P(B)-P(A\\mid B){P(B)}}}\n  \n\n\n==== As the probability of a conditional event ====\nConditional probability can be defined as the probability of a conditional event \n  \n    \n      \n        \n          A\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle A_{B}}\n  . The Goodman\u2013Nguyen\u2013Van Fraassen conditional event can be defined as:\n\n  \n    \n      \n        \n          A\n          \n            B\n          \n        \n        =\n        \n          \u22c3\n          \n            i\n            \u2265\n            1\n          \n        \n        \n          (\n          \n            \n              \u22c2\n              \n                j\n                <\n                i\n              \n            \n            \n              \n                \n                  B\n                  \u00af\n                \n              \n              \n                j\n              \n            \n            ,\n            \n              A\n              \n                i\n              \n            \n            \n              B\n              \n                i\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle A_{B}=\\bigcup _{i\\geq 1}\\left(\\bigcap _{j<i}{\\overline {B}}_{j},A_{i}B_{i}\\right)}\n  , where \n  \n    \n      \n        \n          A\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle A_{i}}\n   and \n  \n    \n      \n        \n          B\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle B_{i}}\n   represent states or elements of A  or B. It can be shown that\n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            B\n          \n        \n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A_{B})={\\frac {P(A\\cap B)}{P(B)}}}\n  which meets the Kolmogorov definition of conditional probability.\n\n\n=== Conditioning on an event of probability zero ===\nIf \n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(B)=0}\n  , then according to the definition, \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   is undefined.\nThe case of greatest interest is that of a random variable Y, conditioned on a continuous random variable X resulting in a particular outcome x. The event \n  \n    \n      \n        B\n        =\n        {\n        X\n        =\n        x\n        }\n      \n    \n    {\\displaystyle B=\\{X=x\\}}\n   has probability zero and, as such, cannot be conditioned on.\nInstead of conditioning on X being exactly x, we could condition on it being closer than distance \n  \n    \n      \n        \u03f5\n      \n    \n    {\\displaystyle \\epsilon }\n   away from x. The event \n  \n    \n      \n        B\n        =\n        {\n        x\n        \u2212\n        \u03f5\n        <\n        X\n        <\n        x\n        +\n        \u03f5\n        }\n      \n    \n    {\\displaystyle B=\\{x-\\epsilon <X<x+\\epsilon \\}}\n   will generally have nonzero probability and hence, can be conditioned on.\nWe can then take the limit\n\n  \n    \n      \n        \n          lim\n          \n            \u03f5\n            \u2192\n            0\n          \n        \n        P\n        (\n        A\n        \u2223\n        x\n        \u2212\n        \u03f5\n        <\n        X\n        <\n        x\n        +\n        \u03f5\n        )\n        .\n      \n    \n    {\\displaystyle \\lim _{\\epsilon \\to 0}P(A\\mid x-\\epsilon <X<x+\\epsilon ).}\n  For example, if two continuous random variables X and Y have a joint density \n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n  , then by L'H\u00f4pital's rule and Leibniz integral rule, upon differentiation with respect to \n  \n    \n      \n        \u03f5\n      \n    \n    {\\displaystyle \\epsilon }\n  :\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  lim\n                  \n                    \u03f5\n                    \u2192\n                    0\n                  \n                \n                P\n                (\n                Y\n                \u2208\n                U\n                \u2223\n                \n                  x\n                  \n                    0\n                  \n                \n                \u2212\n                \u03f5\n                <\n                X\n                <\n                \n                  x\n                  \n                    0\n                  \n                \n                +\n                \u03f5\n                )\n              \n              \n                \n                =\n                \n                  lim\n                  \n                    \u03f5\n                    \u2192\n                    0\n                  \n                \n                \n                  \n                    \n                      \n                        \u222b\n                        \n                          \n                            x\n                            \n                              0\n                            \n                          \n                          \u2212\n                          \u03f5\n                        \n                        \n                          \n                            x\n                            \n                              0\n                            \n                          \n                          +\n                          \u03f5\n                        \n                      \n                      \n                        \u222b\n                        \n                          U\n                        \n                      \n                      \n                        f\n                        \n                          X\n                          ,\n                          Y\n                        \n                      \n                      (\n                      x\n                      ,\n                      y\n                      )\n                      \n                        d\n                      \n                      y\n                      \n                        d\n                      \n                      x\n                    \n                    \n                      \n                        \u222b\n                        \n                          \n                            x\n                            \n                              0\n                            \n                          \n                          \u2212\n                          \u03f5\n                        \n                        \n                          \n                            x\n                            \n                              0\n                            \n                          \n                          +\n                          \u03f5\n                        \n                      \n                      \n                        \u222b\n                        \n                          \n                            R\n                          \n                        \n                      \n                      \n                        f\n                        \n                          X\n                          ,\n                          Y\n                        \n                      \n                      (\n                      x\n                      ,\n                      y\n                      )\n                      \n                        d\n                      \n                      y\n                      \n                        d\n                      \n                      x\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \u222b\n                        \n                          U\n                        \n                      \n                      \n                        f\n                        \n                          X\n                          ,\n                          Y\n                        \n                      \n                      (\n                      \n                        x\n                        \n                          0\n                        \n                      \n                      ,\n                      y\n                      )\n                      \n                        d\n                      \n                      y\n                    \n                    \n                      \n                        \u222b\n                        \n                          \n                            R\n                          \n                        \n                      \n                      \n                        f\n                        \n                          X\n                          ,\n                          Y\n                        \n                      \n                      (\n                      \n                        x\n                        \n                          0\n                        \n                      \n                      ,\n                      y\n                      )\n                      \n                        d\n                      \n                      y\n                    \n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\lim _{\\epsilon \\to 0}P(Y\\in U\\mid x_{0}-\\epsilon <X<x_{0}+\\epsilon )&=\\lim _{\\epsilon \\to 0}{\\frac {\\int _{x_{0}-\\epsilon }^{x_{0}+\\epsilon }\\int _{U}f_{X,Y}(x,y)\\mathrm {d} y\\mathrm {d} x}{\\int _{x_{0}-\\epsilon }^{x_{0}+\\epsilon }\\int _{\\mathbb {R} }f_{X,Y}(x,y)\\mathrm {d} y\\mathrm {d} x}}\\\\&={\\frac {\\int _{U}f_{X,Y}(x_{0},y)\\mathrm {d} y}{\\int _{\\mathbb {R} }f_{X,Y}(x_{0},y)\\mathrm {d} y}}.\\end{aligned}}}\n  The resulting limit is the conditional probability distribution of Y given X and exists when the denominator, the probability density \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f_{X}(x_{0})}\n  , is strictly positive.\nIt is tempting to define the undefined probability \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(A\\mid X=x)}\n   using this limit, but this cannot be done in a consistent manner. In particular, it is possible to find random variables X and W and values x, w such that the events \n  \n    \n      \n        {\n        X\n        =\n        x\n        }\n      \n    \n    {\\displaystyle \\{X=x\\}}\n   and \n  \n    \n      \n        {\n        W\n        =\n        w\n        }\n      \n    \n    {\\displaystyle \\{W=w\\}}\n   are identical but the resulting limits are not:\n\n  \n    \n      \n        \n          lim\n          \n            \u03f5\n            \u2192\n            0\n          \n        \n        P\n        (\n        A\n        \u2223\n        x\n        \u2212\n        \u03f5\n        \u2264\n        X\n        \u2264\n        x\n        +\n        \u03f5\n        )\n        \u2260\n        \n          lim\n          \n            \u03f5\n            \u2192\n            0\n          \n        \n        P\n        (\n        A\n        \u2223\n        w\n        \u2212\n        \u03f5\n        \u2264\n        W\n        \u2264\n        w\n        +\n        \u03f5\n        )\n        .\n      \n    \n    {\\displaystyle \\lim _{\\epsilon \\to 0}P(A\\mid x-\\epsilon \\leq X\\leq x+\\epsilon )\\neq \\lim _{\\epsilon \\to 0}P(A\\mid w-\\epsilon \\leq W\\leq w+\\epsilon ).}\n  The Borel\u2013Kolmogorov paradox demonstrates this with a geometrical argument.\n\n\n=== Conditioning on a discrete random variable ===\n\nLet X be a discrete random variable and its possible outcomes denoted V. For example, if X represents the value of a rolled die then V is the set \n  \n    \n      \n        {\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n        ,\n        5\n        ,\n        6\n        }\n      \n    \n    {\\displaystyle \\{1,2,3,4,5,6\\}}\n  . Let us assume for the sake of presentation that X is a discrete random variable, so that each value in V has a nonzero probability.\nFor a value x in V and an event A, the conditional probability\nis given by \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(A\\mid X=x)}\n  .\nWriting\n\n  \n    \n      \n        c\n        (\n        x\n        ,\n        A\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle c(x,A)=P(A\\mid X=x)}\n  for short, we see that it is a function of two variables, x and A.\nFor a fixed A, we can form the random variable \n  \n    \n      \n        Y\n        =\n        c\n        (\n        X\n        ,\n        A\n        )\n      \n    \n    {\\displaystyle Y=c(X,A)}\n  . It represents an outcome of \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(A\\mid X=x)}\n   whenever a value x of X is observed.\nThe conditional probability of A given X can thus be treated as a random variable  Y with outcomes in the interval \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  . From the law of total probability, its expected value is equal to the unconditional probability of A.\n\n\n=== Partial conditional probability ===\nThe partial conditional probability\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        \n          B\n          \n            1\n          \n        \n        \u2261\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          B\n          \n            m\n          \n        \n        \u2261\n        \n          b\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(A\\mid B_{1}\\equiv b_{1},\\ldots ,B_{m}\\equiv b_{m})}\n  \nis about the probability of event \n\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given that each of the condition events\n\n  \n    \n      \n        \n          B\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle B_{i}}\n   has occurred to a degree\n\n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle b_{i}}\n   (degree of belief, degree of experience) that might be different from 100%. Frequentistically, partial conditional probability makes sense, if the conditions are tested in experiment repetitions of appropriate length \n\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . Such \n\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -bounded partial conditional probability can be defined as the conditionally expected average occurrence of event\n\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   in testbeds of length \n\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   that adhere to all of the probability specifications\n\n  \n    \n      \n        \n          B\n          \n            i\n          \n        \n        \u2261\n        \n          b\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle B_{i}\\equiv b_{i}}\n  , i.e.:\n\n  \n    \n      \n        \n          P\n          \n            n\n          \n        \n        (\n        A\n        \u2223\n        \n          B\n          \n            1\n          \n        \n        \u2261\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          B\n          \n            m\n          \n        \n        \u2261\n        \n          b\n          \n            m\n          \n        \n        )\n        =\n        E\n        \u2061\n        (\n        \n          \n            \n              A\n              \u00af\n            \n          \n          \n            n\n          \n        \n        \u2223\n        \n          \n            \n              B\n              \u00af\n            \n          \n          \n            1\n          \n          \n            n\n          \n        \n        =\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            \n              B\n              \u00af\n            \n          \n          \n            m\n          \n          \n            n\n          \n        \n        =\n        \n          b\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P^{n}(A\\mid B_{1}\\equiv b_{1},\\ldots ,B_{m}\\equiv b_{m})=\\operatorname {E} ({\\overline {A}}^{n}\\mid {\\overline {B}}_{1}^{n}=b_{1},\\ldots ,{\\overline {B}}_{m}^{n}=b_{m})}\n  Based on that,  partial conditional probability can be defined as\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        \n          B\n          \n            1\n          \n        \n        \u2261\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          B\n          \n            m\n          \n        \n        \u2261\n        \n          b\n          \n            m\n          \n        \n        )\n        =\n        \n          lim\n          \n            n\n            \u2192\n            \u221e\n          \n        \n        \n          P\n          \n            n\n          \n        \n        (\n        A\n        \u2223\n        \n          B\n          \n            1\n          \n        \n        \u2261\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          B\n          \n            m\n          \n        \n        \u2261\n        \n          b\n          \n            m\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle P(A\\mid B_{1}\\equiv b_{1},\\ldots ,B_{m}\\equiv b_{m})=\\lim _{n\\to \\infty }P^{n}(A\\mid B_{1}\\equiv b_{1},\\ldots ,B_{m}\\equiv b_{m}),}\n  where \n  \n    \n      \n        \n          b\n          \n            i\n          \n        \n        n\n        \u2208\n        \n          N\n        \n      \n    \n    {\\displaystyle b_{i}n\\in \\mathbb {N} }\n  Jeffrey conditionalization\nis a special case of partial conditional probability, in which the condition events must form a partition:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        \n          B\n          \n            1\n          \n        \n        \u2261\n        \n          b\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          B\n          \n            m\n          \n        \n        \u2261\n        \n          b\n          \n            m\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          b\n          \n            i\n          \n        \n        P\n        (\n        A\n        \u2223\n        \n          B\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle P(A\\mid B_{1}\\equiv b_{1},\\ldots ,B_{m}\\equiv b_{m})=\\sum _{i=1}^{m}b_{i}P(A\\mid B_{i})}\n  \n\n\n== Example ==\nSuppose that somebody secretly rolls two fair six-sided dice, and we wish to compute the probability that the face-up value of the first one is 2, given the information that their sum is no greater than 5.\n\nLet D1 be the value rolled on die 1.\nLet D2 be the value rolled on die 2.Probability that D1 = 2\nTable 1 shows the sample space of 36 combinations of rolled values of the two dice, each of which occurs with probability 1/36, with the numbers displayed in the red and dark gray cells being D1 + D2.\nD1 = 2 in exactly 6 of the 36 outcomes; thus P(D1 = 2) = 6\u204436 = 1\u20446:\n\nProbability that D1 + D2 \u2264 5\nTable 2 shows that D1 + D2 \u2264 5 for exactly 10 of the 36 outcomes, thus P(D1 + D2 \u2264 5) = 10\u204436:\n\nProbability that D1 = 2 given that D1 + D2 \u2264 5 \nTable 3 shows that for 3 of these 10 outcomes, D1 = 2.\nThus, the conditional probability P(D1 = 2 | D1+D2 \u2264 5) = 3\u204410 = 0.3:\n\nHere, in the earlier notation for the definition of conditional probability, the conditioning event B is that D1 + D2 \u2264 5, and the event A is D1 = 2. We have \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              \n                P\n                (\n                A\n                \u2229\n                B\n                )\n              \n              \n                P\n                (\n                B\n                )\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                3\n                \n                  /\n                \n                36\n              \n              \n                10\n                \n                  /\n                \n                36\n              \n            \n          \n        \n        =\n        \n          \n            \n              3\n              10\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle P(A\\mid B)={\\tfrac {P(A\\cap B)}{P(B)}}={\\tfrac {3/36}{10/36}}={\\tfrac {3}{10}},}\n   as seen in the table.\n\n\n== Use in inference ==\nIn statistical inference, the conditional probability is an update of the probability of an event based on new information. The new information can be incorporated as follows:\nLet A, the event of interest, be in the sample space, say (X,P).\nThe occurrence of the event A knowing that event B has or will have occurred,  means the occurrence of  A as it is restricted to B, i.e. \n  \n    \n      \n        A\n        \u2229\n        B\n      \n    \n    {\\displaystyle A\\cap B}\n  .\nWithout the knowledge of the occurrence of B, the information about the occurrence of A would simply be P(A)\nThe probability of A knowing that event B has or will have occurred, will be the probability of \n  \n    \n      \n        A\n        \u2229\n        B\n      \n    \n    {\\displaystyle A\\cap B}\n   relative to P(B), the probability that B has occurred.\nThis results in \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        P\n        (\n        A\n        \u2229\n        B\n        )\n        \n          /\n        \n        P\n        (\n        B\n        )\n      \n    \n    {\\textstyle P(A\\mid B)=P(A\\cap B)/P(B)}\n   whenever P(B) > 0 and 0 otherwise.This approach results in a probability measure that is consistent with the original probability measure and satisfies all the Kolmogorov axioms.  This conditional probability measure also could have resulted by assuming that the relative magnitude of the probability of A with respect to X will be preserved with respect to B (cf. a Formal Derivation below).\nThe wording \"evidence\" or \"information\" is generally used in the Bayesian interpretation of probability. The conditioning event is interpreted as evidence for the conditioned event. That is, P(A) is the probability of A before accounting for evidence E, and P(A|E) is the probability of A after having accounted for evidence E or after having updated P(A). This is consistent with the frequentist interpretation, which is the first definition given above.\n\n\n=== Example ===\nWhen Morse code is transmitted, there is a certain probability that the \"dot\" or \"dash\" that was received is erroneous. This is often taken as interference in the transmission of a message.  Therefore, it is important to consider when sending a \"dot\", for example, the probability that a \"dot\" was received. This is represented by: \n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        \u2223\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n        =\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        \u2223\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        \n          \n            \n              P\n              (\n              d\n              o\n              t\n               \n              s\n              e\n              n\n              t\n              )\n            \n            \n              P\n              (\n              d\n              o\n              t\n               \n              r\n              e\n              c\n              e\n              i\n              v\n              e\n              d\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(dot\\ sent\\mid dot\\ received)=P(dot\\ received\\mid dot\\ sent){\\frac {P(dot\\ sent)}{P(dot\\ received)}}.}\n   In Morse code, the ratio of dots to dashes is 3:4 at the point of sending, so the probability of a \"dot\" and \"dash\" are \n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        =\n        \n          \n            3\n            7\n          \n        \n         \n        a\n        n\n        d\n         \n        P\n        (\n        d\n        a\n        s\n        h\n         \n        s\n        e\n        n\n        t\n        )\n        =\n        \n          \n            4\n            7\n          \n        \n      \n    \n    {\\displaystyle P(dot\\ sent)={\\frac {3}{7}}\\ and\\ P(dash\\ sent)={\\frac {4}{7}}}\n  . If it is assumed that the probability that a dot is transmitted as a dash is 1/10, and that the probability that a dash is transmitted as a dot is likewise 1/10, then Bayes's rule can be used to calculate \n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n      \n    \n    {\\displaystyle P(dot\\ received)}\n  .     \n\n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n        =\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n         \n        \u2229\n         \n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        +\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n         \n        \u2229\n         \n        d\n        a\n        s\n        h\n         \n        s\n        e\n        n\n        t\n        )\n      \n    \n    {\\displaystyle P(dot\\ received)=P(dot\\ received\\ \\cap \\ dot\\ sent)+P(dot\\ received\\ \\cap \\ dash\\ sent)}\n      \n\n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n        =\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        \u2223\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        P\n        (\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        +\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        \u2223\n        d\n        a\n        s\n        h\n         \n        s\n        e\n        n\n        t\n        )\n        P\n        (\n        d\n        a\n        s\n        h\n         \n        s\n        e\n        n\n        t\n        )\n      \n    \n    {\\displaystyle P(dot\\ received)=P(dot\\ received\\mid dot\\ sent)P(dot\\ sent)+P(dot\\ received\\mid dash\\ sent)P(dash\\ sent)}\n      \n\n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n        =\n        \n          \n            9\n            10\n          \n        \n        \u00d7\n        \n          \n            3\n            7\n          \n        \n        +\n        \n          \n            1\n            10\n          \n        \n        \u00d7\n        \n          \n            4\n            7\n          \n        \n        =\n        \n          \n            31\n            70\n          \n        \n      \n    \n    {\\displaystyle P(dot\\ received)={\\frac {9}{10}}\\times {\\frac {3}{7}}+{\\frac {1}{10}}\\times {\\frac {4}{7}}={\\frac {31}{70}}}\n       \nNow, \n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        \u2223\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n      \n    \n    {\\displaystyle P(dot\\ sent\\mid dot\\ received)}\n   can be calculated:     \n\n  \n    \n      \n        P\n        (\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        \u2223\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        )\n        =\n        P\n        (\n        d\n        o\n        t\n         \n        r\n        e\n        c\n        e\n        i\n        v\n        e\n        d\n        \u2223\n        d\n        o\n        t\n         \n        s\n        e\n        n\n        t\n        )\n        \n          \n            \n              P\n              (\n              d\n              o\n              t\n               \n              s\n              e\n              n\n              t\n              )\n            \n            \n              P\n              (\n              d\n              o\n              t\n               \n              r\n              e\n              c\n              e\n              i\n              v\n              e\n              d\n              )\n            \n          \n        \n        =\n        \n          \n            9\n            10\n          \n        \n        \u00d7\n        \n          \n            \n              3\n              7\n            \n            \n              31\n              70\n            \n          \n        \n        =\n        \n          \n            27\n            31\n          \n        \n      \n    \n    {\\displaystyle P(dot\\ sent\\mid dot\\ received)=P(dot\\ received\\mid dot\\ sent){\\frac {P(dot\\ sent)}{P(dot\\ received)}}={\\frac {9}{10}}\\times {\\frac {\\frac {3}{7}}{\\frac {31}{70}}}={\\frac {27}{31}}}\n  \n\n\n== Statistical independence ==\n\nEvents A and B are defined to be statistically independent if the probability of the intersection of A and B is equal to the product of the probabilities of A and B: \n\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n        .\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A)P(B).}\n  If P(B) is not zero, then this is equivalent to the statement that\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        .\n      \n    \n    {\\displaystyle P(A\\mid B)=P(A).}\n  Similarly, if P(A) is not zero, then\n\n  \n    \n      \n        P\n        (\n        B\n        \u2223\n        A\n        )\n        =\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B\\mid A)=P(B)}\n  is also equivalent. Although the derived forms may seem more intuitive, they are not the preferred definition as the conditional probabilities may be undefined, and the preferred definition is symmetrical in A and B. Independence does not refer to a disjoint event.It should also be noted that given the independent event pair [A B] and an event C, the pair is defined to be conditionally independent if the product holds true:\n  \n    \n      \n        P\n        (\n        A\n        B\n        \u2223\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n        P\n        (\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(AB\\mid C)=P(A\\mid C)P(B\\mid C)}\n  \nThis theorem could be useful in applications where multiple independent events are being observed. \nIndependent events vs. mutually exclusive events\nThe concepts of mutually independent events and mutually exclusive events are separate and distinct. The following table contrasts results for the two cases (provided that the probability of the conditioning event is not zero).\n\nIn fact, mutually exclusive events cannot be statistically independent (unless both of them are impossible), since knowing that one occurs gives information about the other (in particular, that the latter will certainly not occur).\n\n\n== Common fallacies ==\nThese fallacies should not be confused with Robert K. Shope's 1978 \"conditional fallacy\", which deals with counterfactual examples that beg the question.\n\n\n=== Assuming conditional probability is of similar size to its inverse ===\n\nIn general, it cannot be assumed that P(A|B) \u2248 P(B|A). This can be an insidious error, even for those who are highly conversant with statistics. The relationship between P(A|B) and P(B|A) is given by Bayes' theorem:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                B\n                \u2223\n                A\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      A\n                      \u2223\n                      B\n                      )\n                      P\n                      (\n                      B\n                      )\n                    \n                    \n                      P\n                      (\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                \u21d4\n                \n                  \n                    \n                      P\n                      (\n                      B\n                      \u2223\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      A\n                      \u2223\n                      B\n                      )\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      B\n                      )\n                    \n                    \n                      P\n                      (\n                      A\n                      )\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(B\\mid A)&={\\frac {P(A\\mid B)P(B)}{P(A)}}\\\\\\Leftrightarrow {\\frac {P(B\\mid A)}{P(A\\mid B)}}&={\\frac {P(B)}{P(A)}}\\end{aligned}}}\n  That is, P(A|B) \u2248 P(B|A) only if P(B)/P(A) \u2248 1, or equivalently, P(A) \u2248 P(B).\n\n\n=== Assuming marginal and conditional probabilities are of similar size ===\nIn general, it cannot be assumed that P(A) \u2248 P(A|B). These probabilities are linked through the law of total probability:\n\n  \n    \n      \n        P\n        (\n        A\n        )\n        =\n        \n          \u2211\n          \n            n\n          \n        \n        P\n        (\n        A\n        \u2229\n        \n          B\n          \n            n\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            n\n          \n        \n        P\n        (\n        A\n        \u2223\n        \n          B\n          \n            n\n          \n        \n        )\n        P\n        (\n        \n          B\n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle P(A)=\\sum _{n}P(A\\cap B_{n})=\\sum _{n}P(A\\mid B_{n})P(B_{n}).}\n  where the events \n  \n    \n      \n        (\n        \n          B\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (B_{n})}\n   form a countable partition of \n  \n    \n      \n        \u03a9\n      \n    \n    {\\displaystyle \\Omega }\n  .\nThis fallacy may arise through selection bias. For example, in the context of a medical claim, let SC be the event that a sequela (chronic disease) S occurs as a consequence of circumstance (acute condition) C. Let H be the event that an individual seeks medical help. Suppose that in most cases, C does not cause S (so that P(SC) is low). Suppose also that medical attention is only sought if S has occurred due to C. From experience of patients, a doctor may therefore erroneously conclude that P(SC) is high. The actual probability observed by the doctor is P(SC|H).\n\n\n=== Over- or under-weighting priors ===\nNot taking prior probability into account partially or completely is called base rate neglect. The reverse, insufficient adjustment from the prior probability is conservatism.\n\n\n== Formal derivation ==\nFormally, P(A | B) is defined as the probability of A according to a new probability function on the sample space, such that outcomes not in B have probability 0 and that it is consistent with all original probability measures.Let \u03a9 be a discrete sample space with elementary events {\u03c9}, and let P be the probability measure with respect to the \u03c3-algebra of \u03a9. Suppose we are told that the event B \u2286 \u03a9 has occurred. A new probability distribution (denoted by the conditional notation) is to be assigned on {\u03c9} to reflect this. All events that are not in B will have null probability in the new distribution. For events in B, two conditions must be met: the probability of B is one and the relative magnitudes of the probabilities must be preserved. The former is required by the axioms of probability, and the latter stems from the fact that the new probability measure has to be the analog of P in which the probability of B is one - and every event that is not in B, therefore, has a null probability. Hence, for some scale factor \u03b1, the new distribution must satisfy:\n\n  \n    \n      \n        \u03c9\n        \u2208\n        B\n        :\n        P\n        (\n        \u03c9\n        \u2223\n        B\n        )\n        =\n        \u03b1\n        P\n        (\n        \u03c9\n        )\n      \n    \n    {\\displaystyle \\omega \\in B:P(\\omega \\mid B)=\\alpha P(\\omega )}\n  \n\n  \n    \n      \n        \u03c9\n        \u2209\n        B\n        :\n        P\n        (\n        \u03c9\n        \u2223\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\omega \\notin B:P(\\omega \\mid B)=0}\n  \n\n  \n    \n      \n        \n          \u2211\n          \n            \u03c9\n            \u2208\n            \u03a9\n          \n        \n        \n          P\n          (\n          \u03c9\n          \u2223\n          B\n          )\n        \n        =\n        1.\n      \n    \n    {\\displaystyle \\sum _{\\omega \\in \\Omega }{P(\\omega \\mid B)}=1.}\n  Substituting 1 and 2 into 3 to select \u03b1:\n\n  \n    \n      \n        \n          \n            \n              \n                1\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    \u03c9\n                    \u2208\n                    \u03a9\n                  \n                \n                \n                  P\n                  (\n                  \u03c9\n                  \u2223\n                  B\n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    \u03c9\n                    \u2208\n                    B\n                  \n                \n                \n                  P\n                  (\n                  \u03c9\n                  \u2223\n                  B\n                  )\n                \n                +\n                \n                  \n                    \n                      \n                        \u2211\n                        \n                          \u03c9\n                          \u2209\n                          B\n                        \n                      \n                      P\n                      (\n                      \u03c9\n                      \u2223\n                      B\n                      )\n                    \n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \u03b1\n                \n                  \u2211\n                  \n                    \u03c9\n                    \u2208\n                    B\n                  \n                \n                \n                  P\n                  (\n                  \u03c9\n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \u03b1\n                \u22c5\n                P\n                (\n                B\n                )\n              \n            \n            \n              \n                \u21d2\n                \u03b1\n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      P\n                      (\n                      B\n                      )\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}1&=\\sum _{\\omega \\in \\Omega }{P(\\omega \\mid B)}\\\\&=\\sum _{\\omega \\in B}{P(\\omega \\mid B)}+{\\cancelto {0}{\\sum _{\\omega \\notin B}P(\\omega \\mid B)}}\\\\&=\\alpha \\sum _{\\omega \\in B}{P(\\omega )}\\\\[5pt]&=\\alpha \\cdot P(B)\\\\[5pt]\\Rightarrow \\alpha &={\\frac {1}{P(B)}}\\end{aligned}}}\n  So the new probability distribution is\n\n  \n    \n      \n        \u03c9\n        \u2208\n        B\n        :\n        P\n        (\n        \u03c9\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              \u03c9\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\omega \\in B:P(\\omega \\mid B)={\\frac {P(\\omega )}{P(B)}}}\n  \n\n  \n    \n      \n        \u03c9\n        \u2209\n        B\n        :\n        P\n        (\n        \u03c9\n        \u2223\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\omega \\notin B:P(\\omega \\mid B)=0}\n  Now for a general event A,\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                A\n                \u2223\n                B\n                )\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    \u03c9\n                    \u2208\n                    A\n                    \u2229\n                    B\n                  \n                \n                \n                  P\n                  (\n                  \u03c9\n                  \u2223\n                  B\n                  )\n                \n                +\n                \n                  \n                    \n                      \n                        \u2211\n                        \n                          \u03c9\n                          \u2208\n                          A\n                          \u2229\n                          \n                            B\n                            \n                              c\n                            \n                          \n                        \n                      \n                      P\n                      (\n                      \u03c9\n                      \u2223\n                      B\n                      )\n                    \n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    \u03c9\n                    \u2208\n                    A\n                    \u2229\n                    B\n                  \n                \n                \n                  \n                    \n                      P\n                      (\n                      \u03c9\n                      )\n                    \n                    \n                      P\n                      (\n                      B\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      A\n                      \u2229\n                      B\n                      )\n                    \n                    \n                      P\n                      (\n                      B\n                      )\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(A\\mid B)&=\\sum _{\\omega \\in A\\cap B}{P(\\omega \\mid B)}+{\\cancelto {0}{\\sum _{\\omega \\in A\\cap B^{c}}P(\\omega \\mid B)}}\\\\&=\\sum _{\\omega \\in A\\cap B}{\\frac {P(\\omega )}{P(B)}}\\\\[5pt]&={\\frac {P(A\\cap B)}{P(B)}}\\end{aligned}}}\n  \n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nWeisstein, Eric W. \"Conditional Probability\". MathWorld.\nVisual explanation of conditional probability", "Statistical classification": "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis.\n\n\n== Relation to other problems ==\nClassification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value.  Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.\nA common subclass of classification is probabilistic classification.  Algorithms of this nature use statistical inference to find the best class for a given instance.  Unlike other algorithms, which simply output a \"best\" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers:\n\nIt can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier).\nCorrespondingly, it can abstain when its confidence of choosing any particular output is too low.\nBecause of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.\n\n\n== Frequentist procedures ==\nEarly work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.\n\n\n== Bayesian procedures ==\nUnlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.Some Bayesian procedures involve the calculation of  group membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation.\n\n\n== Binary and multiclass classification ==\nClassification can be thought of as two separate problems \u2013 binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.\n\n\n== Feature vectors ==\n\nMost algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance.  Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent).  Features may variously be binary (e.g. \"on\" or \"off\"); categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type); ordinal (e.g. \"large\", \"medium\" or \"small\"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10).\n\n\n== Linear classifiers ==\n\nA large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product.  The predicted category is the one with the highest score.  This type of score function is known as a linear predictor function and has the following general form:\n\nwhere Xi is the feature vector for instance i, \u03b2k is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k.  In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k.\nAlgorithms with this basic setup are known as linear classifiers.  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.\nExamples of such algorithms include\n\nLogistic regression \u2013 Statistical model for a binary dependent variable\nMultinomial logistic regression \u2013 Regression for more than two discrete outcomes\nProbit regression \u2013 Statistical regression where the dependent variable can take only two valuesPages displaying short descriptions of redirect targets\nThe perceptron algorithm\nSupport vector machine \u2013 Set of methods for supervised statistical learning\nLinear discriminant analysis \u2013 Method used in statistics, pattern recognition, and other fields\n\n\n== Algorithms ==\nSince no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. The most commonly used include:\nArtificial neural networks \u2013 Computational model used in machine learning, based on connected, hierarchical functionsPages displaying short descriptions of redirect targets\nBoosting (meta-algorithm) \u2013 Method in machine learningPages displaying short descriptions of redirect targets\nDecision tree learning \u2013 Machine learning algorithm\nRandom forest \u2013 Binary search tree based ensemble machine learning method\nGenetic programming \u2013 Technique whereby computer programs are encoded as a set of genes\nGene expression programming \u2013 Evolutionary algorithm\nMulti expression programming\nLinear genetic programming \u2013 type of genetic programming algorithmPages displaying wikidata descriptions as a fallback\nKernel estimation \u2013 Window functionPages displaying short descriptions of redirect targets\nk-nearest neighbor \u2013 Non-parametric classification methodPages displaying short descriptions of redirect targets\nLearning vector quantization\nLinear classifier \u2013 Statistical classification in machine learning\nFisher's linear discriminant \u2013 Method used in statistics, pattern recognition, and other fieldsPages displaying short descriptions of redirect targets\nLogistic regression \u2013 Statistical model for a binary dependent variable\nNaive Bayes classifier \u2013 Probabilistic classification algorithm\nPerceptron \u2013 Algorithm for supervised learning of binary classifiers\nQuadratic classifier \u2013 used in machine learning to separate measurements of two or more classes of objectsPages displaying wikidata descriptions as a fallback\nSupport vector machine \u2013 Set of methods for supervised statistical learning\nLeast squares support vector machine\n\n\n== Evaluation ==\nClassifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.\nThe measures precision and recall are popular metrics used to evaluate the quality of a classification system. More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms.\nAs a performance metric, the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes.\n\nFurther, it will not penalize an algorithm for simply rearranging the classes.\n\n\n== Application domains ==\n\nClassification has many applications. In some of these it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.\n\nBiological classification \u2013 The science of identifying, describing, defining and naming groups of biological organisms\nBiometric \u2013 Metrics related to human characteristicsPages displaying short descriptions of redirect targets identification\nComputer vision \u2013 Computerized information extraction from images\nMedical image analysis and medical imaging \u2013 Technique and process of creating visual representations of the interior of a body\nOptical character recognition \u2013 Computer recognition of visual text\nVideo tracking \u2013 finding the location in each frame of a video sequencePages displaying wikidata descriptions as a fallback\nCredit scoring \u2013 Numerical expression representing a person's creditworthinessPages displaying short descriptions of redirect targets\nDocument classification \u2013 Process of categorizing documents\nDrug discovery and development \u2013 Process of bringing a new pharmaceutical drug to the market\nToxicogenomics \u2013 branch of toxicology and genomicsPages displaying wikidata descriptions as a fallback\nQuantitative structure-activity relationship \u2013 Quantitative prediction of the biological, ecotoxicological or pharmaceutical activity of a moleculePages displaying short descriptions of redirect targets\nGeostatistics \u2013 Branch of statistics focusing on spatial data sets\nHandwriting recognition \u2013 Ability of a computer to receive and interpret intelligible handwritten input\nInternet search engines \u2013 Software system that is designed to search for information on the World Wide WebPages displaying short descriptions of redirect targets\nMicro-array classification\nPattern recognition \u2013 Automated recognition of patterns and regularities in data\nRecommender system \u2013 Information filtering system to predict users' preferences\nSpeech recognition \u2013 Automatic conversion of spoken language into text\nStatistical natural language processing \u2013 Field of linguistics and computer sciencePages displaying short descriptions of redirect targets\n\n\n== See also ==\n\n\n== References ==", "Standard deviation": "In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.\nStandard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \u03c3 (sigma), for the population standard deviation, or the Latin letter s, for the sample standard deviation.\nThe standard deviation of a random variable, sample, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data.\nThe standard deviation of a population or sample and the standard error of a statistic (e.g., of the sample mean) are quite different, but related. The sample mean's standard error is the standard deviation of the set of means that would be found by drawing an infinite number of repeated samples from the population and computing a mean for each sample.  The mean's standard error turns out to equal the population standard deviation divided by the square root of the sample size, and is estimated by using the sample standard deviation divided by the square root of the sample size. For example, a poll's standard error (what is reported as the margin of error of the poll), is the expected standard deviation of the estimated mean if the same poll were to be conducted multiple times. Thus, the standard error estimates the standard deviation of an estimate, which itself measures how much the estimate depends on the particular sample that was taken from the population. \nIn science, it is common to report both the standard deviation of the data (as a summary statistic) and the standard error of the estimate (as a measure of potential error in the findings). By convention, only effects more than two standard errors away from a null expectation are considered  \"statistically significant\", a safeguard against spurious conclusion that is really due to random sampling error.  \nWhen only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data, or to a modified quantity that is an unbiased estimate of the population standard deviation (the standard deviation of the entire population).\n\n\n== Basic examples ==\n\n\n=== Population standard deviation of grades of eight students ===\nSuppose that the entire population of interest is eight students in a particular class. For a finite set of numbers, the population standard deviation is found by taking the square root of the average of the squared deviations of the values subtracted from their average value. The marks of a class of eight students (that is, a statistical population) are the following eight values:\n\n  \n    \n      \n        2\n        ,\n         \n        4\n        ,\n         \n        4\n        ,\n         \n        4\n        ,\n         \n        5\n        ,\n         \n        5\n        ,\n         \n        7\n        ,\n         \n        9.\n      \n    \n    {\\displaystyle 2,\\ 4,\\ 4,\\ 4,\\ 5,\\ 5,\\ 7,\\ 9.}\n  These eight data points have the mean (average) of 5:\n\n  \n    \n      \n        \u03bc\n        =\n        \n          \n            \n              2\n              +\n              4\n              +\n              4\n              +\n              4\n              +\n              5\n              +\n              5\n              +\n              7\n              +\n              9\n            \n            8\n          \n        \n        =\n        \n          \n            40\n            8\n          \n        \n        =\n        5.\n      \n    \n    {\\displaystyle \\mu ={\\frac {2+4+4+4+5+5+7+9}{8}}={\\frac {40}{8}}=5.}\n  First, calculate the deviations of each data point from the mean, and square the result of each:\n\n  \n    \n      \n        \n          \n            \n              \n                (\n                2\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                (\n                \u2212\n                3\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                9\n              \n              \n              \n                (\n                5\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                \n                  0\n                  \n                    2\n                  \n                \n                =\n                0\n              \n            \n            \n              \n                (\n                4\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                (\n                \u2212\n                1\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                1\n              \n              \n              \n                (\n                5\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                \n                  0\n                  \n                    2\n                  \n                \n                =\n                0\n              \n            \n            \n              \n                (\n                4\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                (\n                \u2212\n                1\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                1\n              \n              \n              \n                (\n                7\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                \n                  2\n                  \n                    2\n                  \n                \n                =\n                4\n              \n            \n            \n              \n                (\n                4\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                (\n                \u2212\n                1\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                1\n              \n              \n              \n                (\n                9\n                \u2212\n                5\n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                \n                  4\n                  \n                    2\n                  \n                \n                =\n                16.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lll}(2-5)^{2}=(-3)^{2}=9&&(5-5)^{2}=0^{2}=0\\\\(4-5)^{2}=(-1)^{2}=1&&(5-5)^{2}=0^{2}=0\\\\(4-5)^{2}=(-1)^{2}=1&&(7-5)^{2}=2^{2}=4\\\\(4-5)^{2}=(-1)^{2}=1&&(9-5)^{2}=4^{2}=16.\\\\\\end{array}}}\n  The variance is the mean of these values:\n\n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n        =\n        \n          \n            \n              9\n              +\n              1\n              +\n              1\n              +\n              1\n              +\n              0\n              +\n              0\n              +\n              4\n              +\n              16\n            \n            8\n          \n        \n        =\n        \n          \n            32\n            8\n          \n        \n        =\n        4.\n      \n    \n    {\\displaystyle \\sigma ^{2}={\\frac {9+1+1+1+0+0+4+16}{8}}={\\frac {32}{8}}=4.}\n  and the population standard deviation is equal to the square root of the variance:\n\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            4\n          \n        \n        =\n        2.\n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {4}}=2.}\n  This  formula is valid only if the eight values with which we began form the complete population. If the values instead were a random sample drawn from some large parent population (for example, they were 8 students randomly and independently chosen from a class of 2 million), then one divides by 7 (which is n \u2212 1) instead of 8 (which is n) in the denominator of the last formula, and the result is \n  \n    \n      \n        s\n        =\n        \n          \n            32\n            \n              /\n            \n            7\n          \n        \n        \u2248\n        2.1.\n      \n    \n    {\\textstyle s={\\sqrt {32/7}}\\approx 2.1.}\n   In that case, the result of the original formula would be called the sample standard deviation and denoted by s instead of \n  \n    \n      \n        \u03c3\n        .\n      \n    \n    {\\displaystyle \\sigma .}\n   Dividing by n \u2212 1 rather than by n gives an unbiased estimate of the variance of the larger parent population. This is known as Bessel's correction. Roughly, the reason for it is that the formula for the sample variance relies on computing differences of  observations from the sample mean, and the sample mean itself was constructed to be as close as possible to the observations, so just dividing by n would underestimate the variability.\n\n\n=== Standard deviation of average height for adult men ===\nIf the population of interest is approximately normally distributed, the standard deviation provides information on the proportion of observations above or below certain values. For example, the average height for adult men in the United States is about 70 inches, with a standard deviation of around 3 inches. This means that most men (about 68%, assuming a normal distribution) have a height within 3 inches of the mean (67\u201373 inches) \u2013 one standard deviation \u2013 and almost all men (about 95%) have a height within 6 inches of the mean (64\u201376 inches) \u2013 two standard deviations. If the standard deviation were zero, then all men would be exactly 70 inches tall. If the standard deviation were 20 inches, then men would have much more variable heights, with a typical range of about 50\u201390 inches. Three standard deviations account for 99.73% of the sample population being studied, assuming the distribution is normal or bell-shaped (see the 68\u201395\u201399.7 rule, or the empirical rule, for more information).\n\n\n== Definition of population values ==\nLet   \u03bc be the expected value (the average) of random variable X with density f(x):\n\nThe standard deviation \u03c3 of X is  defined as \n\nwhich can be shown to equal \n  \n    \n      \n        \n          \n            E\n            \u2061\n            \n              [\n              \n                X\n                \n                  2\n                \n              \n              ]\n            \n            \u2212\n            (\n            E\n            \u2061\n            [\n            X\n            ]\n            \n              )\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\textstyle {\\sqrt {\\operatorname {E} \\left[X^{2}\\right]-(\\operatorname {E} [X])^{2}}}.}\n  \nUsing words, the standard deviation is the square root of the variance of X.\nThe standard deviation of a probability distribution is the same as that of a random variable having that distribution. \nNot all random variables have a standard deviation. If the distribution has fat tails going out to infinity, the standard deviation might not exist, because the integral might not converge. The normal distribution has tails going out to infinity, but its mean and standard deviation do exist, because the tails diminish quickly enough. The Pareto distribution with parameter \n  \n    \n      \n        \u03b1\n        \u2208\n        (\n        1\n        ,\n        2\n        ]\n      \n    \n    {\\displaystyle \\alpha \\in (1,2]}\n   has a mean, but not a standard deviation (loosely speaking, the standard deviation is infinite). The Cauchy distribution has neither a mean nor a standard deviation.\n\n\n=== Discrete random variable ===\nIn the case where X takes random values from a finite data set x1, x2, \u2026, xN, with each value having the same probability, the standard deviation is\n\nor, by using summation notation,\n\nIf, instead of having equal probabilities, the values have different probabilities, let x1 have probability p1, x2 have probability p2, \u2026, xN have probability pN.  In this case, the standard deviation will be\n\n\n=== Continuous random variable ===\nThe standard deviation of a continuous real-valued random variable X with probability density function p(x) is\n\nand where the integrals are definite integrals taken for x ranging over the set of possible values of the random variable X.\nIn the case of a parametric family of distributions, the standard deviation can be expressed in terms of the parameters. For example, in the case of the log-normal distribution with parameters \u03bc and \u03c32, the standard deviation is\n\n\n== Estimation ==\n\nOne can find the standard deviation of an entire population in cases (such as standardized testing) where every member of a population is sampled. In cases where that cannot be done, the standard deviation \u03c3 is estimated by examining a random sample taken from the population and computing a statistic of the sample, which is used as an estimate of the population standard deviation. Such a statistic is called an estimator, and the estimator (or the value of the estimator, namely the estimate) is called a sample standard deviation, and is denoted by s (possibly with modifiers).\nUnlike in the case of estimating the population mean, for which the sample mean is a simple estimator with many desirable properties (unbiased, efficient, maximum likelihood), there is no single estimator for the standard deviation with all these properties, and unbiased estimation of standard deviation is a very technically involved problem. Most often, the standard deviation is estimated using the corrected sample standard deviation (using N \u2212 1), defined below, and this is often referred to as the \"sample standard deviation\", without qualifiers. However, other estimators are better in other respects: the uncorrected estimator (using N) yields lower mean squared error, while using N \u2212 1.5 (for the normal distribution) almost completely eliminates bias.\n\n\n=== Uncorrected sample standard deviation ===\nThe formula for the population standard deviation (of a finite population) can be applied to the sample, using the size of the sample as the size of the population (though the actual population size from which the sample is drawn may be much larger). This estimator, denoted by sN, is known as the uncorrected sample standard deviation, or sometimes the standard deviation of the sample (considered as the entire population), and is defined as follows:\nwhere \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n        \n          x\n          \n            2\n          \n        \n        ,\n        \n        \u2026\n        ,\n        \n        \n          x\n          \n            N\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1},\\,x_{2},\\,\\ldots ,\\,x_{N}\\}}\n   are the observed values of the sample items, and \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   is the mean value of these observations, while the denominator N stands for the size of the sample: this is the square root of the sample variance, which is the average of the squared deviations about the sample mean.\nThis is a consistent estimator (it converges in probability to the population value as the number of samples goes to infinity), and is the maximum-likelihood estimate when the population is normally distributed. However, this is a biased estimator, as the estimates are generally too low. The bias decreases as sample size grows, dropping off as 1/N, and thus is most significant for small or moderate sample sizes; for \n  \n    \n      \n        N\n        >\n        75\n      \n    \n    {\\displaystyle N>75}\n   the bias is below 1%. Thus for very large sample sizes, the uncorrected sample standard deviation is generally acceptable. This estimator also has a uniformly smaller mean squared error than the corrected sample standard deviation.\n\n\n=== Corrected sample standard deviation ===\nIf the biased sample variance (the second central moment of the sample, which is a downward-biased estimate of the population variance) is used to compute an estimate of the population's standard deviation, the result is\n\n  \n    \n      \n        \n          s\n          \n            N\n          \n        \n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s_{N}={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}.}\n  Here taking the square root introduces further downward bias, by Jensen's inequality, due to the square root's being a concave function. The bias in the variance is easily corrected, but the bias from the square root is more difficult to correct, and depends on the distribution in question.\nAn unbiased estimator for the variance is given by applying Bessel's correction, using N \u2212 1 instead of N to yield the unbiased sample variance, denoted s2:\n\n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            \n              N\n              \u2212\n              1\n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            (\n            \n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle s^{2}={\\frac {1}{N-1}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}.}\n  This estimator is unbiased if the variance exists and the sample values are drawn independently with replacement. N \u2212 1 corresponds to the number of degrees of freedom in the vector of deviations from the mean, \n  \n    \n      \n        \n          (\n          \n            x\n            \n              1\n            \n          \n          \u2212\n          \n            \n              \n                x\n                \u00af\n              \n            \n          \n          ,\n          \n          \u2026\n          ,\n          \n          \n            x\n            \n              n\n            \n          \n          \u2212\n          \n            \n              \n                x\n                \u00af\n              \n            \n          \n          )\n          .\n        \n      \n    \n    {\\displaystyle \\textstyle (x_{1}-{\\bar {x}},\\;\\dots ,\\;x_{n}-{\\bar {x}}).}\n  \nTaking square roots reintroduces bias (because the square root is a nonlinear function which does not commute with the expectation, i.e. often \n  \n    \n      \n        E\n        [\n        \n          \n            X\n          \n        \n        ]\n        \u2260\n        \n          \n            E\n            [\n            X\n            ]\n          \n        \n      \n    \n    {\\textstyle E[{\\sqrt {X}}]\\neq {\\sqrt {E[X]}}}\n  ), yielding the corrected sample standard deviation, denoted by s:\n\n  \n    \n      \n        s\n        =\n        \n          \n            \n              \n                1\n                \n                  N\n                  \u2212\n                  1\n                \n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s={\\sqrt {{\\frac {1}{N-1}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}.}\n  As explained above, while s2 is an unbiased estimator for the population variance, s is still a biased estimator for the population standard deviation, though markedly less biased than the uncorrected sample standard deviation. This estimator is commonly used and generally known simply as the \"sample standard deviation\". The bias may still be large for small samples (N less than 10). As sample size increases, the amount of bias decreases. We obtain more information and the difference between \n  \n    \n      \n        \n          \n            1\n            N\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{N}}}\n   and \n  \n    \n      \n        \n          \n            1\n            \n              N\n              \u2212\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{N-1}}}\n   becomes smaller.\n\n\n=== Unbiased sample standard deviation ===\nFor unbiased estimation of standard deviation, there is no formula that works across all distributions, unlike for mean and variance. Instead, s is used as a basis, and is scaled by a correction factor to produce an unbiased estimate. For the normal distribution, an unbiased estimator is given by s/c4, where the correction factor (which depends on N) is given in terms of the Gamma function, and equals:\n\n  \n    \n      \n        \n          c\n          \n            4\n          \n        \n        (\n        N\n        )\n        \n        =\n        \n        \n          \n            \n              2\n              \n                N\n                \u2212\n                1\n              \n            \n          \n        \n        \n        \n        \n        \n          \n            \n              \u0393\n              \n                (\n                \n                  \n                    N\n                    2\n                  \n                \n                )\n              \n            \n            \n              \u0393\n              \n                (\n                \n                  \n                    \n                      N\n                      \u2212\n                      1\n                    \n                    2\n                  \n                \n                )\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle c_{4}(N)\\,=\\,{\\sqrt {\\frac {2}{N-1}}}\\,\\,\\,{\\frac {\\Gamma \\left({\\frac {N}{2}}\\right)}{\\Gamma \\left({\\frac {N-1}{2}}\\right)}}.}\n  This arises because the sampling distribution of the sample standard deviation follows a (scaled) chi distribution, and the correction factor is the mean of the chi distribution.\nAn approximation can be given by replacing N \u2212 1 with N \u2212 1.5, yielding:\n\n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                1\n                \n                  N\n                  \u2212\n                  1.5\n                \n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}={\\sqrt {{\\frac {1}{N-1.5}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}}},}\n  The error in this approximation decays quadratically (as 1/N2), and it is suited for all but the smallest samples or highest precision: for N = 3 the bias is equal to 1.3%, and for N = 9 the bias is already less than 0.1%.\nA more accurate approximation is to replace \n  \n    \n      \n        N\n        \u2212\n        1.5\n      \n    \n    {\\displaystyle N-1.5}\n   above with \n  \n    \n      \n        N\n        \u2212\n        1.5\n        +\n        1\n        \n          /\n        \n        (\n        8\n        (\n        N\n        \u2212\n        1\n        )\n        )\n      \n    \n    {\\displaystyle N-1.5+1/(8(N-1))}\n  .For other distributions, the correct formula depends on the distribution, but a rule of thumb is to use the further refinement of the approximation:\n\n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                1\n                \n                  N\n                  \u2212\n                  1.5\n                  \u2212\n                  \n                    \n                      1\n                      4\n                    \n                  \n                  \n                    \u03b3\n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}={\\sqrt {{\\frac {1}{N-1.5-{\\frac {1}{4}}\\gamma _{2}}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}}},}\n  where \u03b32 denotes the population excess kurtosis. The excess kurtosis may be either known beforehand for certain distributions, or estimated from the data.\n\n\n=== Confidence interval of a sampled standard deviation ===\n\nThe standard deviation we obtain by sampling a distribution is itself not absolutely accurate, both for mathematical reasons (explained here by the confidence interval) and for practical reasons of measurement (measurement error). The mathematical effect can be described by the confidence interval or CI. \nTo show how a larger sample will make the confidence interval narrower, consider the following examples: \nA small population of N = 2 has only 1 degree of freedom for estimating the standard deviation.  The result is that a 95% CI of the SD runs from 0.45 \u00d7 SD to 31.9 \u00d7 SD;  the factors here are as follows:\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            \n              q\n              \n                \n                  \u03b1\n                  2\n                \n              \n            \n            <\n            k\n            \n              \n                \n                  s\n                  \n                    2\n                  \n                \n                \n                  \u03c3\n                  \n                    2\n                  \n                \n              \n            \n            <\n            \n              q\n              \n                1\n                \u2212\n                \n                  \n                    \u03b1\n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        1\n        \u2212\n        \u03b1\n        ,\n      \n    \n    {\\displaystyle \\Pr \\left(q_{\\frac {\\alpha }{2}}<k{\\frac {s^{2}}{\\sigma ^{2}}}<q_{1-{\\frac {\\alpha }{2}}}\\right)=1-\\alpha ,}\n  where \n  \n    \n      \n        \n          q\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle q_{p}}\n   is the p-th quantile of the chi-square distribution with k degrees of freedom, and \n  \n    \n      \n        1\n        \u2212\n        \u03b1\n      \n    \n    {\\displaystyle 1-\\alpha }\n   is the confidence level.  This is equivalent to the following:\n\n  \n    \n      \n        Pr\n        \n          (\n          \n            k\n            \n              \n                \n                  s\n                  \n                    2\n                  \n                \n                \n                  q\n                  \n                    1\n                    \u2212\n                    \n                      \n                        \u03b1\n                        2\n                      \n                    \n                  \n                \n              \n            \n            <\n            \n              \u03c3\n              \n                2\n              \n            \n            <\n            k\n            \n              \n                \n                  s\n                  \n                    2\n                  \n                \n                \n                  q\n                  \n                    \n                      \u03b1\n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        =\n        1\n        \u2212\n        \u03b1\n        .\n      \n    \n    {\\displaystyle \\Pr \\left(k{\\frac {s^{2}}{q_{1-{\\frac {\\alpha }{2}}}}}<\\sigma ^{2}<k{\\frac {s^{2}}{q_{\\frac {\\alpha }{2}}}}\\right)=1-\\alpha .}\n  With k = 1, \n  \n    \n      \n        \n          q\n          \n            0.025\n          \n        \n        =\n        0.000982\n      \n    \n    {\\displaystyle q_{0.025}=0.000982}\n   and \n  \n    \n      \n        \n          q\n          \n            0.975\n          \n        \n        =\n        5.024\n      \n    \n    {\\displaystyle q_{0.975}=5.024}\n  .  The reciprocals of the square roots of these two numbers give us the factors 0.45 and 31.9 given above.\nA larger population of N = 10 has 9 degrees of freedom for estimating the standard deviation.  The same computations as above give us in this case a 95% CI running from 0.69 \u00d7 SD to 1.83 \u00d7 SD. So even with a sample population of 10, the actual SD can still be almost a factor 2 higher than the sampled SD. For a sample population N=100, this is down to 0.88 \u00d7 SD to 1.16 \u00d7 SD. To be more certain that the sampled SD is close to the actual SD we need to sample a large number of points.\nThese same formulae can be used to obtain confidence intervals on the variance of residuals from a least squares fit under standard normal theory, where k is now the number of degrees of freedom for error.\n\n\n=== Bounds on standard deviation ===\nFor a set of  N > 4 data spanning a range of values R, an upper bound on the standard deviation s is given by s = 0.6R. \nAn estimate of the standard deviation for N > 100 data taken to be approximately normal follows from the heuristic that 95% of the area under the normal curve lies roughly two standard deviations to either side of the mean, so that, with 95% probability the total range of values R represents four standard deviations so that s \u2248 R/4. This so-called range rule is useful in sample size estimation, as the range of possible values is easier to estimate than the standard deviation. Other divisors K(N) of the range such that s \u2248 R/K(N) are available for other values of N and for non-normal distributions.\n\n\n== Identities and mathematical properties ==\nThe standard deviation is invariant under changes in location, and scales directly with the scale of the random variable. Thus, for a constant c and random variables X and Y:\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                (\n                c\n                )\n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \u03c3\n                (\n                X\n                +\n                c\n                )\n              \n              \n                \n                =\n                \u03c3\n                (\n                X\n                )\n                ,\n              \n            \n            \n              \n                \u03c3\n                (\n                c\n                X\n                )\n              \n              \n                \n                =\n                \n                  |\n                \n                c\n                \n                  |\n                \n                \u03c3\n                (\n                X\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sigma (c)&=0\\\\\\sigma (X+c)&=\\sigma (X),\\\\\\sigma (cX)&=|c|\\sigma (X).\\end{aligned}}}\n  The standard deviation of the sum of two random variables can be related to their individual standard deviations and the covariance between them:\n\n  \n    \n      \n        \u03c3\n        (\n        X\n        +\n        Y\n        )\n        =\n        \n          \n            var\n            \u2061\n            (\n            X\n            )\n            +\n            var\n            \u2061\n            (\n            Y\n            )\n            +\n            2\n            \n            cov\n            \u2061\n            (\n            X\n            ,\n            Y\n            )\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle \\sigma (X+Y)={\\sqrt {\\operatorname {var} (X)+\\operatorname {var} (Y)+2\\,\\operatorname {cov} (X,Y)}}.\\,}\n  where \n  \n    \n      \n        \n          var\n          \n          =\n          \n          \n            \u03c3\n            \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle \\operatorname {var} \\,=\\,\\sigma ^{2}}\n   and \n  \n    \n      \n        \n          cov\n        \n      \n    \n    {\\displaystyle \\textstyle \\operatorname {cov} }\n   stand for variance and covariance, respectively.\nThe calculation of the sum of squared deviations can be related to moments calculated directly from the data.  In the following formula, the letter E is interpreted to mean expected value, i.e., mean.\n\n  \n    \n      \n        \u03c3\n        (\n        X\n        )\n        =\n        \n          \n            E\n            \u2061\n            \n              [\n              \n                (\n                X\n                \u2212\n                E\n                \u2061\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              ]\n            \n          \n        \n        =\n        \n          \n            E\n            \u2061\n            \n              [\n              \n                X\n                \n                  2\n                \n              \n              ]\n            \n            \u2212\n            (\n            E\n            \u2061\n            [\n            X\n            ]\n            \n              )\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma (X)={\\sqrt {\\operatorname {E} \\left[(X-\\operatorname {E} [X])^{2}\\right]}}={\\sqrt {\\operatorname {E} \\left[X^{2}\\right]-(\\operatorname {E} [X])^{2}}}.}\n  The sample standard deviation can be computed as:\n\n  \n    \n      \n        s\n        (\n        X\n        )\n        =\n        \n          \n            \n              N\n              \n                N\n                \u2212\n                1\n              \n            \n          \n        \n        \n          \n            E\n            \u2061\n            \n              [\n              \n                (\n                X\n                \u2212\n                E\n                \u2061\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              ]\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s(X)={\\sqrt {\\frac {N}{N-1}}}{\\sqrt {\\operatorname {E} \\left[(X-\\operatorname {E} [X])^{2}\\right]}}.}\n  For a finite population with equal probabilities at all points, we have\n\n  \n    \n      \n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              (\n              \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                \n                  x\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n              \n              )\n            \n            \u2212\n            \n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              (\n              \n                \n                  \n                    1\n                    N\n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                \n                  x\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n              \n              )\n            \n            \u2212\n            \n              \n                (\n                \n                  \n                    \n                      1\n                      N\n                    \n                  \n                  \n                    \u2211\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      N\n                    \n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}={\\sqrt {{\\frac {1}{N}}\\left(\\sum _{i=1}^{N}x_{i}^{2}\\right)-{\\bar {x}}^{2}}}={\\sqrt {\\left({\\frac {1}{N}}\\sum _{i=1}^{N}x_{i}^{2}\\right)-\\left({\\frac {1}{N}}\\sum _{i=1}^{N}x_{i}\\right)^{2}}},}\n  which means that the standard deviation is equal to the square root of the difference between the average of the squares of the values and the square of the average value.\nSee computational formula for the variance for proof, and for an analogous result for the sample standard deviation.\n\n\n== Interpretation and application ==\n\nA large standard deviation indicates that the data points can spread far from the mean and a small standard deviation indicates that they are clustered closely around the mean.\nFor example, each of the three populations {0, 0, 14, 14}, {0, 6, 8, 14} and {6, 6, 8, 8} has a mean of 7. Their standard deviations are 7, 5, and 1, respectively. The third population has a much smaller standard deviation than the other two because its values are all close to 7.  These standard deviations have the same units as the data points themselves. If, for instance, the data set {0, 6, 8, 14} represents the ages of a population of four siblings in years, the standard deviation is 5 years. As another example, the population {1000, 1006, 1008, 1014} may represent the distances traveled by four athletes, measured in meters. It has a mean of 1007 meters, and a standard deviation of 5 meters.\nStandard deviation may serve as a measure of uncertainty.  In physical science, for example, the reported standard deviation of a group of repeated measurements gives the precision of those measurements.  When deciding whether measurements agree with a theoretical prediction, the standard deviation of those measurements is of crucial importance: if the mean of the measurements is too far away from the prediction (with the distance measured in standard deviations), then the theory being tested probably needs to be revised. This makes sense since they fall outside the range of values that could reasonably be expected to occur, if the prediction were correct and the standard deviation appropriately quantified.  See prediction interval.\nWhile the standard deviation does measure how far typical values tend to be from the mean, other measures are available. An example is the mean absolute deviation, which might be considered a more direct measure of average distance, compared to the root mean square distance inherent in the standard deviation.\n\n\n=== Application examples ===\nThe practical value of understanding the standard deviation of a set of values is in appreciating how much variation there is from the average (mean).\n\n\n==== Experiment, industrial and hypothesis testing ====\nStandard deviation is often used to compare real-world data against a model to test the model.\nFor example, in industrial applications the weight of products coming off a production line may need to comply with a legally required value. By weighing some fraction of the products an average weight can be found, which will always be slightly different from the long-term average. By using standard deviations, a minimum and maximum value can be calculated that the averaged weight will be within some very high percentage of the time (99.9% or more). If it falls outside the range then the production process may need to be corrected. Statistical tests such as these are particularly important when the testing is relatively expensive. For example, if the product needs to be opened and drained and weighed, or if the product was otherwise used up by the test.\nIn experimental science, a theoretical model of reality is used. Particle physics conventionally uses a standard of \"5 sigma\" for the declaration of a discovery. A five-sigma level translates to one chance in 3.5 million that a random fluctuation would yield the result. This level of certainty was required in order to assert that a particle consistent with the Higgs boson had been discovered in two independent experiments at CERN, also leading to the declaration of the first observation of gravitational waves.\n\n\n==== Weather ====\nAs a simple example, consider the average daily maximum temperatures for two cities, one inland and one on the coast. It is helpful to understand that the range of daily maximum temperatures for cities near the coast is smaller than for cities inland. Thus, while these two cities may each have the same average maximum temperature, the standard deviation of the daily maximum temperature for the coastal city will be less than that of the inland city as, on any particular day, the actual maximum temperature is more likely to be farther from the average maximum temperature for the inland city than for the coastal one.\n\n\n==== Finance ====\nIn finance, standard deviation is often used as a measure of the risk associated with price-fluctuations of a given asset (stocks, bonds, property, etc.), or the risk of a portfolio of assets (actively managed mutual funds, index mutual funds, or ETFs). Risk is an important factor in determining how to efficiently manage a portfolio of investments because it determines the variation in returns on the asset and/or portfolio and gives investors a mathematical basis for investment decisions (known as mean-variance optimization). The fundamental concept of risk is that as it increases, the expected return on an investment should increase as well, an increase known as the risk premium. In other words, investors should expect a higher return on an investment when that investment carries a higher level of risk or uncertainty. When evaluating investments, investors should estimate both the expected return and the uncertainty of future returns. Standard deviation provides a quantified estimate of the uncertainty of future returns.\nFor example, assume an investor had to choose between two stocks. Stock A over the past 20 years had an average return of 10 percent, with a standard deviation of 20 percentage points (pp) and Stock B, over the same period, had average returns of 12 percent but a higher standard deviation of 30 pp. On the basis of risk and return, an investor may decide that Stock A is the safer choice, because Stock B's additional two percentage points of return is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return). Stock B is likely to fall short of the initial investment (but also to exceed the initial investment) more often than Stock A under the same circumstances, and is estimated to return only two percent more on average. In this example, Stock A is expected to earn about 10 percent, plus or minus 20 pp (a range of 30 percent to \u221210 percent), about two-thirds of the future year returns. When considering more extreme possible returns or outcomes in future, an investor should expect results of as much as 10 percent plus or minus 60 pp, or a range from 70 percent to \u221250 percent, which includes outcomes for three standard deviations from the average return (about 99.7 percent of probable returns).\nCalculating the average (or arithmetic mean) of the return of a security over a given period will generate the expected return of the asset. For each period, subtracting the expected return from the actual return results in the difference from the mean.  Squaring the difference in each period and taking the average gives the overall variance of the return of the asset.  The larger the variance, the greater risk the security carries.  Finding the square root of this variance will give the standard deviation of the investment tool in question.\nPopulation standard deviation is used to set the width of Bollinger Bands, a technical analysis tool. For example, the upper Bollinger Band is given as \n  \n    \n      \n        \n          \n            \n              \n                x\n                \u00af\n              \n            \n          \n          +\n          n\n          \n            \u03c3\n            \n              x\n            \n          \n          .\n        \n      \n    \n    {\\displaystyle \\textstyle {\\bar {x}}+n\\sigma _{x}.}\n   The most commonly used value for n is 2; there is about a five percent chance of going outside, assuming a normal distribution of returns.\nFinancial time series are known to be non-stationary series, whereas the statistical calculations above, such as standard deviation, apply only to stationary series. To apply the above statistical tools to non-stationary series, the series first must be transformed to a stationary series, enabling use of statistical tools that now have a valid basis from which to work.\n\n\n=== Geometric interpretation ===\nTo gain some geometric insights and clarification, we will start with a population of three values, x1, x2, x3. This defines a point P = (x1, x2, x3) in R3. Consider the line L = {(r, r, r) : r \u2208 R}. This is the \"main diagonal\" going through the origin. If our three given values were all equal, then the standard deviation would be zero and P would lie on L. So it is not unreasonable to assume that the standard deviation is related to the distance of P to L. That is indeed the case. To move orthogonally from L to the point P, one begins at the point:\n\n  \n    \n      \n        M\n        =\n        \n          (\n          \n            \n              \n                \n                  x\n                  \u00af\n                \n              \n            \n            ,\n            \n              \n                \n                  x\n                  \u00af\n                \n              \n            \n            ,\n            \n              \n                \n                  x\n                  \u00af\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle M=\\left({\\bar {x}},{\\bar {x}},{\\bar {x}}\\right)}\n  whose coordinates are the mean of the values we started out with. \n\nA little algebra shows that the distance between P and M (which is the same as the orthogonal distance between P and the line L) \n  \n    \n      \n        \n          \n            \n              \u2211\n              \n                i\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\textstyle {\\sqrt {\\sum _{i}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}}\n   is equal to the standard deviation of the vector (x1, x2, x3), multiplied by the square root of the number of dimensions of the vector (3 in this case).\n\n\n=== Chebyshev's inequality ===\n\nAn observation is rarely more than a few standard deviations away from the mean. Chebyshev's inequality ensures that, for all distributions for which the standard deviation is defined, the amount of data within a number of standard deviations of the mean is at least as much as given in the following table.\n\n\n=== Rules for normally distributed data ===\n\nThe central limit theorem states that the distribution of an average of many independent, identically distributed random variables tends toward the famous bell-shaped normal distribution with a probability density function of\n\n  \n    \n      \n        f\n        \n          (\n          \n            x\n            ,\n            \u03bc\n            ,\n            \n              \u03c3\n              \n                2\n              \n            \n          \n          )\n        \n        =\n        \n          \n            1\n            \n              \u03c3\n              \n                \n                  2\n                  \u03c0\n                \n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      \u2212\n                      \u03bc\n                    \n                    \u03c3\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f\\left(x,\\mu ,\\sigma ^{2}\\right)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  where \u03bc is the expected value of the random variables, \u03c3 equals their distribution's standard deviation divided by n1/2, and n is the number of random variables.  The standard deviation therefore is simply a scaling variable that adjusts how broad the curve will be, though it also appears in the normalizing constant.\nIf a data distribution is approximately normal, then the proportion of data values within z standard deviations of the mean is defined by:\n\n  \n    \n      \n        \n          Proportion\n        \n        =\n        erf\n        \u2061\n        \n          (\n          \n            \n              z\n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\text{Proportion}}=\\operatorname {erf} \\left({\\frac {z}{\\sqrt {2}}}\\right)}\n  where \n  \n    \n      \n        \n          erf\n        \n      \n    \n    {\\displaystyle \\textstyle \\operatorname {erf} }\n   is the error function. The proportion that is less than or equal to a number, x, is given by the cumulative distribution function:\n\n  \n    \n      \n        \n          Proportion\n        \n        \u2264\n        x\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          [\n          \n            1\n            +\n            erf\n            \u2061\n            \n              (\n              \n                \n                  \n                    x\n                    \u2212\n                    \u03bc\n                  \n                  \n                    \u03c3\n                    \n                      \n                        2\n                      \n                    \n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        \n          \n            1\n            2\n          \n        \n        \n          [\n          \n            1\n            +\n            erf\n            \u2061\n            \n              (\n              \n                \n                  z\n                  \n                    2\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\text{Proportion}}\\leq x={\\frac {1}{2}}\\left[1+\\operatorname {erf} \\left({\\frac {x-\\mu }{\\sigma {\\sqrt {2}}}}\\right)\\right]={\\frac {1}{2}}\\left[1+\\operatorname {erf} \\left({\\frac {z}{\\sqrt {2}}}\\right)\\right]}\n  .If a data distribution is approximately normal then about 68 percent of the data values are within one standard deviation of the mean (mathematically, \u03bc \u00b1 \u03c3, where \u03bc is the arithmetic mean), about 95 percent are within two standard deviations (\u03bc \u00b1 2\u03c3), and about 99.7 percent lie within three standard deviations (\u03bc \u00b1 3\u03c3). This is known as the 68\u201395\u201399.7 rule, or the empirical rule.\nFor various values of z, the percentage of values expected to lie in and outside the symmetric interval, CI = (\u2212z\u03c3, z\u03c3), are as follows:\n\n\n== Relationship between standard deviation and mean ==\nThe mean and the standard deviation of a set of data are descriptive statistics usually reported together. In a certain sense, the standard deviation is a \"natural\" measure of statistical dispersion if the center of the data is measured about the mean. This is because the standard deviation from the mean is smaller than from any other point. The precise statement is the following: suppose x1, ..., xn are real numbers and define the function:\n\n  \n    \n      \n        \u03c3\n        (\n        r\n        )\n        =\n        \n          \n            \n              \n                1\n                \n                  N\n                  \u2212\n                  1\n                \n              \n            \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              \n                (\n                \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  r\n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma (r)={\\sqrt {{\\frac {1}{N-1}}\\sum _{i=1}^{N}\\left(x_{i}-r\\right)^{2}}}.}\n  Using calculus or by completing the square, it is possible to show that \u03c3(r) has a unique minimum at the mean:\n\n  \n    \n      \n        r\n        =\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle r={\\bar {x}}.\\,}\n  Variability can also be measured by the coefficient of variation, which is the ratio of the standard deviation to the mean.  It is a dimensionless number.\n\n\n=== Standard deviation of the mean ===\n\nOften, we want some information about the precision of the mean we obtained. We can obtain this by determining the standard deviation of the sampled mean. Assuming statistical independence of the values in the sample, the standard deviation of the mean is related to the standard deviation of the distribution by:\n\n  \n    \n      \n        \n          \u03c3\n          \n            mean\n          \n        \n        =\n        \n          \n            1\n            \n              N\n            \n          \n        \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma _{\\text{mean}}={\\frac {1}{\\sqrt {N}}}\\sigma }\n  where N is the number of observations in the sample used to estimate the mean. This can easily be proven with (see basic properties of the variance):\n\n  \n    \n      \n        \n          \n            \n              \n                var\n                \u2061\n                (\n                X\n                )\n              \n              \n                \n                \u2261\n                \n                  \u03c3\n                  \n                    X\n                  \n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                var\n                \u2061\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                +\n                \n                  X\n                  \n                    2\n                  \n                \n                )\n              \n              \n                \n                \u2261\n                var\n                \u2061\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                )\n                +\n                var\n                \u2061\n                (\n                \n                  X\n                  \n                    2\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {var} (X)&\\equiv \\sigma _{X}^{2}\\\\\\operatorname {var} (X_{1}+X_{2})&\\equiv \\operatorname {var} (X_{1})+\\operatorname {var} (X_{2})\\\\\\end{aligned}}}\n  (Statistical independence is assumed.)\n\n  \n    \n      \n        var\n        \u2061\n        (\n        c\n        \n          X\n          \n            1\n          \n        \n        )\n        \u2261\n        \n          c\n          \n            2\n          \n        \n        \n        var\n        \u2061\n        (\n        \n          X\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {var} (cX_{1})\\equiv c^{2}\\,\\operatorname {var} (X_{1})}\n  hence\n\n  \n    \n      \n        \n          \n            \n              \n                var\n                \u2061\n                (\n                \n                  mean\n                \n                )\n              \n              \n                \n                =\n                var\n                \u2061\n                \n                  (\n                  \n                    \n                      \n                        1\n                        N\n                      \n                    \n                    \n                      \u2211\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                =\n                \n                  \n                    1\n                    \n                      N\n                      \n                        2\n                      \n                    \n                  \n                \n                var\n                \u2061\n                \n                  (\n                  \n                    \n                      \u2211\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      N\n                      \n                        2\n                      \n                    \n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                var\n                \u2061\n                (\n                \n                  X\n                  \n                    i\n                  \n                \n                )\n                =\n                \n                  \n                    N\n                    \n                      N\n                      \n                        2\n                      \n                    \n                  \n                \n                var\n                \u2061\n                (\n                X\n                )\n                =\n                \n                  \n                    1\n                    N\n                  \n                \n                var\n                \u2061\n                (\n                X\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {var} ({\\text{mean}})&=\\operatorname {var} \\left({\\frac {1}{N}}\\sum _{i=1}^{N}X_{i}\\right)={\\frac {1}{N^{2}}}\\operatorname {var} \\left(\\sum _{i=1}^{N}X_{i}\\right)\\\\&={\\frac {1}{N^{2}}}\\sum _{i=1}^{N}\\operatorname {var} (X_{i})={\\frac {N}{N^{2}}}\\operatorname {var} (X)={\\frac {1}{N}}\\operatorname {var} (X).\\end{aligned}}}\n  Resulting in:\n\n  \n    \n      \n        \n          \u03c3\n          \n            mean\n          \n        \n        =\n        \n          \n            \u03c3\n            \n              N\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{\\text{mean}}={\\frac {\\sigma }{\\sqrt {N}}}.}\n  In order to estimate the standard deviation of the mean \n  \n    \n      \n        \n          \u03c3\n          \n            mean\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{\\text{mean}}}\n   it is necessary to know the standard deviation of the entire population \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   beforehand. However, in most applications this parameter is unknown. For example, if a series of 10 measurements of a previously unknown quantity is performed in a laboratory, it is possible to calculate the resulting sample mean and sample standard deviation, but it is impossible to calculate the standard deviation of the mean. However, one can estimate the standard deviation of the entire population from the sample, and thus obtain an estimate for the standard error of the mean.\n\n\n== Rapid calculation methods ==\n\nThe following two formulas can represent a running (repeatedly updated) standard deviation. A set of two power sums s1 and s2 are computed over a set of N values of x, denoted as x1, ..., xN:\n\n  \n    \n      \n        \n          s\n          \n            j\n          \n        \n        =\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            x\n            \n              k\n            \n            \n              j\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s_{j}=\\sum _{k=1}^{N}{x_{k}^{j}}.}\n  Given the results of these running summations, the values N, s1, s2 can be used at any time to compute the current value of the running standard deviation:\n\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            \n              N\n              \n                s\n                \n                  2\n                \n              \n              \u2212\n              \n                s\n                \n                  1\n                \n                \n                  2\n                \n              \n            \n            N\n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\frac {\\sqrt {Ns_{2}-s_{1}^{2}}}{N}}}\n  Where N, as mentioned above, is the size of the set of values (or can also be regarded as s0).\nSimilarly for sample standard deviation,\n\n  \n    \n      \n        s\n        =\n        \n          \n            \n              \n                N\n                \n                  s\n                  \n                    2\n                  \n                \n                \u2212\n                \n                  s\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n              \n              \n                N\n                (\n                N\n                \u2212\n                1\n                )\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s={\\sqrt {\\frac {Ns_{2}-s_{1}^{2}}{N(N-1)}}}.}\n  In a computer implementation, as the two sj sums become large, we need to consider round-off error, arithmetic overflow, and arithmetic underflow. The method below calculates the running sums method with reduced rounding errors. This is a \"one pass\" algorithm for calculating variance of n samples without the need to store prior data during the calculation. Applying this method to a time series will result in successive values of standard deviation corresponding to n data points as n grows larger with each new sample, rather than a constant-width sliding window calculation.\nFor k = 1, ..., n:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  A\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                \n                  A\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  \n                    \n                      \n                        x\n                        \n                          k\n                        \n                      \n                      \u2212\n                      \n                        A\n                        \n                          k\n                          \u2212\n                          1\n                        \n                      \n                    \n                    k\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}A_{0}&=0\\\\A_{k}&=A_{k-1}+{\\frac {x_{k}-A_{k-1}}{k}}\\end{aligned}}}\n  where A is the mean value.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  Q\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \n                  Q\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                \n                  Q\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  \n                    \n                      k\n                      \u2212\n                      1\n                    \n                    k\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        x\n                        \n                          k\n                        \n                      \n                      \u2212\n                      \n                        A\n                        \n                          k\n                          \u2212\n                          1\n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n                =\n                \n                  Q\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  (\n                  \n                    \n                      x\n                      \n                        k\n                      \n                    \n                    \u2212\n                    \n                      A\n                      \n                        k\n                        \u2212\n                        1\n                      \n                    \n                  \n                  )\n                \n                \n                  (\n                  \n                    \n                      x\n                      \n                        k\n                      \n                    \n                    \u2212\n                    \n                      A\n                      \n                        k\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}Q_{0}&=0\\\\Q_{k}&=Q_{k-1}+{\\frac {k-1}{k}}\\left(x_{k}-A_{k-1}\\right)^{2}=Q_{k-1}+\\left(x_{k}-A_{k-1}\\right)\\left(x_{k}-A_{k}\\right)\\end{aligned}}}\n  Note: \n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle Q_{1}=0}\n   since \n  \n    \n      \n        k\n        \u2212\n        1\n        =\n        0\n      \n    \n    {\\displaystyle k-1=0}\n   or \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        =\n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}=A_{1}}\n  \nSample variance:\n\n  \n    \n      \n        \n          s\n          \n            n\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              Q\n              \n                n\n              \n            \n            \n              n\n              \u2212\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle s_{n}^{2}={\\frac {Q_{n}}{n-1}}}\n  Population variance:\n\n  \n    \n      \n        \n          \u03c3\n          \n            n\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              Q\n              \n                n\n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{n}^{2}={\\frac {Q_{n}}{n}}}\n  \n\n\n=== Weighted calculation ===\nWhen the values xi are weighted with unequal weights wi, the power sums s0, s1, s2 are each computed as:\n\n  \n    \n      \n        \n          s\n          \n            j\n          \n        \n        =\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          w\n          \n            k\n          \n        \n        \n          x\n          \n            k\n          \n          \n            j\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle s_{j}=\\sum _{k=1}^{N}w_{k}x_{k}^{j}.\\,}\n  And the standard deviation equations remain unchanged. s0 is now the sum of the weights and not the number of samples N.\nThe incremental method with reduced rounding errors can also be applied, with some additional complexity.\nA running sum of weights must be computed for each k from 1 to n:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  W\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \n                  W\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                \n                  W\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  w\n                  \n                    k\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}W_{0}&=0\\\\W_{k}&=W_{k-1}+w_{k}\\end{aligned}}}\n  and places where 1/n is used above must be replaced by wi/Wn:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  A\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                \n                  A\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  \n                    \n                      w\n                      \n                        k\n                      \n                    \n                    \n                      W\n                      \n                        k\n                      \n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      x\n                      \n                        k\n                      \n                    \n                    \u2212\n                    \n                      A\n                      \n                        k\n                        \u2212\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                \n                  Q\n                  \n                    0\n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \n                  Q\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                \n                  Q\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  \n                    \n                      \n                        w\n                        \n                          k\n                        \n                      \n                      \n                        W\n                        \n                          k\n                          \u2212\n                          1\n                        \n                      \n                    \n                    \n                      W\n                      \n                        k\n                      \n                    \n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        x\n                        \n                          k\n                        \n                      \n                      \u2212\n                      \n                        A\n                        \n                          k\n                          \u2212\n                          1\n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n                =\n                \n                  Q\n                  \n                    k\n                    \u2212\n                    1\n                  \n                \n                +\n                \n                  w\n                  \n                    k\n                  \n                \n                \n                  (\n                  \n                    \n                      x\n                      \n                        k\n                      \n                    \n                    \u2212\n                    \n                      A\n                      \n                        k\n                        \u2212\n                        1\n                      \n                    \n                  \n                  )\n                \n                \n                  (\n                  \n                    \n                      x\n                      \n                        k\n                      \n                    \n                    \u2212\n                    \n                      A\n                      \n                        k\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}A_{0}&=0\\\\A_{k}&=A_{k-1}+{\\frac {w_{k}}{W_{k}}}\\left(x_{k}-A_{k-1}\\right)\\\\Q_{0}&=0\\\\Q_{k}&=Q_{k-1}+{\\frac {w_{k}W_{k-1}}{W_{k}}}\\left(x_{k}-A_{k-1}\\right)^{2}=Q_{k-1}+w_{k}\\left(x_{k}-A_{k-1}\\right)\\left(x_{k}-A_{k}\\right)\\end{aligned}}}\n  In the final division,\n\n  \n    \n      \n        \n          \u03c3\n          \n            n\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              Q\n              \n                n\n              \n            \n            \n              W\n              \n                n\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\sigma _{n}^{2}={\\frac {Q_{n}}{W_{n}}}\\,}\n  and\n\n  \n    \n      \n        \n          s\n          \n            n\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              Q\n              \n                n\n              \n            \n            \n              \n                W\n                \n                  n\n                \n              \n              \u2212\n              1\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle s_{n}^{2}={\\frac {Q_{n}}{W_{n}-1}},}\n  or \n\n  \n    \n      \n        \n          s\n          \n            n\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              n\n              \u2032\n            \n            \n              \n                n\n                \u2032\n              \n              \u2212\n              1\n            \n          \n        \n        \n          \u03c3\n          \n            n\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle s_{n}^{2}={\\frac {n'}{n'-1}}\\sigma _{n}^{2},}\n  where n is the total number of elements, and n' is the number of elements with non-zero weights.\nThe above formulas become equal to the simpler formulas given above if weights are taken as equal to one.\n\n\n== History ==\nThe term standard deviation was first used in writing by Karl Pearson in 1894, following his use of it in lectures. This was as a replacement for earlier alternative names for the same idea: for example, Gauss used mean error.\n\n\n== Standard deviation index ==\nThe standard deviation index (SDI) is used in external quality assessments, particularly for medical laboratories. It is calculated as:\n  \n    \n      \n        S\n        D\n        I\n        =\n        \n          \n            \n              L\n              a\n              b\n              o\n              r\n              a\n              t\n              o\n              r\n              y\n               \n              m\n              e\n              a\n              n\n              \u2212\n              C\n              o\n              n\n              s\n              e\n              n\n              s\n              u\n              s\n               \n              g\n              r\n              o\n              u\n              p\n               \n              m\n              e\n              a\n              n\n            \n            \n              C\n              o\n              n\n              s\n              e\n              n\n              s\n              u\n              s\n               \n              g\n              r\n              o\n              u\n              p\n               \n              s\n              t\n              a\n              n\n              d\n              a\n              r\n              d\n               \n              d\n              e\n              v\n              i\n              a\n              t\n              i\n              o\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle SDI={\\frac {Laboratory\\ mean-Consensus\\ group\\ mean}{Consensus\\ group\\ standard\\ deviation}}}\n  \n\n\n== Higher dimensions ==\n\nIn two dimensions, the standard deviation can be illustrated with the standard deviation ellipse (see Multivariate normal distribution \u00a7 Geometric interpretation).\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\n\"Quadratic deviation\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\n\"Standard Deviation Calculator\"", "AdaBoost": "AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire in 1995, who won the 2003 G\u00f6del Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. Usually, AdaBoost is presented for binary classification, although it can be generalized to multiple classes or bounded intervals on the real line.AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.\nAlthough AdaBoost is typically used to combine weak base learners (such as decision stumps), it has been shown that it can also effectively combine strong base learners (such as deep decision trees), producing an even more accurate model.Every learning algorithm tends to suit some problem types better than others, and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset. AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.\n\n\n== Training ==\nAdaBoost refers to a particular method of training a boosted classifier. A boosted classifier is a classifier of the form\n\n  \n    \n      \n        \n          F\n          \n            T\n          \n        \n        (\n        x\n        )\n        =\n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            T\n          \n        \n        \n          f\n          \n            t\n          \n        \n        (\n        x\n        )\n        \n        \n      \n    \n    {\\displaystyle F_{T}(x)=\\sum _{t=1}^{T}f_{t}(x)\\,\\!}\n  where each \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{t}}\n   is a weak learner that takes an object \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   as input and returns a value indicating the class of the object. For example, in the two-class problem, the sign of the weak learner's output identifies the predicted object class and the absolute value gives the confidence in that classification. Similarly, the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  -th classifier is positive if the sample is in a positive class and negative otherwise.\nEach weak learner produces an output hypothesis \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   which fixes a prediction \n  \n    \n      \n        h\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle h(x_{i})}\n   for each sample in the training set. At each iteration \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  , a weak learner is selected and assigned a coefficient \n  \n    \n      \n        \n          \u03b1\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{t}}\n   such that the total training error \n  \n    \n      \n        \n          E\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle E_{t}}\n   of the resulting \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  -stage boosted classifier is minimized.\n\n  \n    \n      \n        \n          E\n          \n            t\n          \n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        E\n        [\n        \n          F\n          \n            t\n            \u2212\n            1\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        +\n        \n          \u03b1\n          \n            t\n          \n        \n        h\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle E_{t}=\\sum _{i}E[F_{t-1}(x_{i})+\\alpha _{t}h(x_{i})]}\n  Here \n  \n    \n      \n        \n          F\n          \n            t\n            \u2212\n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle F_{t-1}(x)}\n   is the boosted classifier that has been built up to the previous stage of training and \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          \u03b1\n          \n            t\n          \n        \n        h\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{t}(x)=\\alpha _{t}h(x)}\n   is the weak learner that is being considered for addition to the final classifier.\n\n\n=== Weighting ===\nAt each iteration of the training process, a weight \n  \n    \n      \n        \n          w\n          \n            i\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle w_{i,t}}\n   is assigned to each sample in the training set equal to the current error \n  \n    \n      \n        E\n        (\n        \n          F\n          \n            t\n            \u2212\n            1\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle E(F_{t-1}(x_{i}))}\n   on that sample. These weights can be used in the training of the weak learner. For instance, decision trees can be grown which favor the splitting of sets of samples with large weights.\n\n\n== Derivation ==\nThis derivation follows Rojas (2009):Suppose we have a data set \n  \n    \n      \n        {\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        (\n        \n          x\n          \n            N\n          \n        \n        ,\n        \n          y\n          \n            N\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle \\{(x_{1},y_{1}),\\ldots ,(x_{N},y_{N})\\}}\n   where each item \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   has an associated class \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \u2208\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle y_{i}\\in \\{-1,1\\}}\n  , and a set of weak classifiers \n  \n    \n      \n        {\n        \n          k\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          k\n          \n            L\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{k_{1},\\ldots ,k_{L}\\}}\n   each of which outputs a classification \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        \u2208\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle k_{j}(x_{i})\\in \\{-1,1\\}}\n   for each item. After the \n  \n    \n      \n        (\n        m\n        \u2212\n        1\n        )\n      \n    \n    {\\displaystyle (m-1)}\n  -th iteration our boosted classifier is a linear combination of the weak classifiers of the form:\n\n  \n    \n      \n        \n          C\n          \n            (\n            m\n            \u2212\n            1\n            )\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        \n          \u03b1\n          \n            1\n          \n        \n        \n          k\n          \n            1\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        +\n        \u22ef\n        +\n        \n          \u03b1\n          \n            m\n            \u2212\n            1\n          \n        \n        \n          k\n          \n            m\n            \u2212\n            1\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle C_{(m-1)}(x_{i})=\\alpha _{1}k_{1}(x_{i})+\\cdots +\\alpha _{m-1}k_{m-1}(x_{i})}\n  ,where the class will be the sign of \n  \n    \n      \n        \n          C\n          \n            (\n            m\n            \u2212\n            1\n            )\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle C_{(m-1)}(x_{i})}\n  . At the \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  -th iteration we want to extend this to a better boosted classifier by adding another weak classifier \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n  , with another weight \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n  :\n\n  \n    \n      \n        \n          C\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        \n          C\n          \n            (\n            m\n            \u2212\n            1\n            )\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        +\n        \n          \u03b1\n          \n            m\n          \n        \n        \n          k\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle C_{m}(x_{i})=C_{(m-1)}(x_{i})+\\alpha _{m}k_{m}(x_{i})}\n  So it remains to determine which weak classifier is the best choice for \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n  , and what its weight \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n   should be. We define the total error \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   of \n  \n    \n      \n        \n          C\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle C_{m}}\n   as the sum of its exponential loss on each data point, given as follows:\n\n  \n    \n      \n        E\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              C\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              C\n              \n                (\n                m\n                \u2212\n                1\n                )\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              \u03b1\n              \n                m\n              \n            \n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle E=\\sum _{i=1}^{N}e^{-y_{i}C_{m}(x_{i})}=\\sum _{i=1}^{N}e^{-y_{i}C_{(m-1)}(x_{i})}e^{-y_{i}\\alpha _{m}k_{m}(x_{i})}}\n  Letting \n  \n    \n      \n        \n          w\n          \n            i\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle w_{i}^{(1)}=1}\n   and \n  \n    \n      \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        =\n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              C\n              \n                m\n                \u2212\n                1\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle w_{i}^{(m)}=e^{-y_{i}C_{m-1}(x_{i})}}\n   for \n  \n    \n      \n        m\n        >\n        1\n      \n    \n    {\\displaystyle m>1}\n  , we have:\n\n  \n    \n      \n        E\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              \u03b1\n              \n                m\n              \n            \n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle E=\\sum _{i=1}^{N}w_{i}^{(m)}e^{-y_{i}\\alpha _{m}k_{m}(x_{i})}}\n  We can split this summation between those data points that are correctly classified by \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n   (so \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \n          k\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        1\n      \n    \n    {\\displaystyle y_{i}k_{m}(x_{i})=1}\n  ) and those that are misclassified (so \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \n          k\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        \u2212\n        1\n      \n    \n    {\\displaystyle y_{i}k_{m}(x_{i})=-1}\n  ):\n\n  \n    \n      \n        E\n        =\n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            =\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          e\n          \n            \u2212\n            \n              \u03b1\n              \n                m\n              \n            \n          \n        \n        +\n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          e\n          \n            \n              \u03b1\n              \n                m\n              \n            \n          \n        \n      \n    \n    {\\displaystyle E=\\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}e^{-\\alpha _{m}}+\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}e^{\\alpha _{m}}}\n  \n\n  \n    \n      \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          e\n          \n            \u2212\n            \n              \u03b1\n              \n                m\n              \n            \n          \n        \n        +\n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        (\n        \n          e\n          \n            \n              \u03b1\n              \n                m\n              \n            \n          \n        \n        \u2212\n        \n          e\n          \n            \u2212\n            \n              \u03b1\n              \n                m\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle =\\sum _{i=1}^{N}w_{i}^{(m)}e^{-\\alpha _{m}}+\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}(e^{\\alpha _{m}}-e^{-\\alpha _{m}})}\n  Since the only part of the right-hand side of this equation that depends on \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n   is \n  \n    \n      \n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n      \n    \n    {\\displaystyle \\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}}\n  , we see that the \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n   that minimizes \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   is the one that minimizes \n  \n    \n      \n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n      \n    \n    {\\displaystyle \\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}}\n   [assuming that \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle \\alpha _{m}>0}\n  ], i.e. the weak classifier with the lowest weighted error (with weights \n  \n    \n      \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        =\n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              C\n              \n                m\n                \u2212\n                1\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle w_{i}^{(m)}=e^{-y_{i}C_{m-1}(x_{i})}}\n  ).\nTo determine the desired weight \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n   that minimizes \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   with the \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n   that we just determined, we differentiate:\n\n  \n    \n      \n        \n          \n            \n              d\n              E\n            \n            \n              d\n              \n                \u03b1\n                \n                  m\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              d\n              (\n              \n                \u2211\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  =\n                  \n                    k\n                    \n                      m\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                \n              \n              \n                w\n                \n                  i\n                \n                \n                  (\n                  m\n                  )\n                \n              \n              \n                e\n                \n                  \u2212\n                  \n                    \u03b1\n                    \n                      m\n                    \n                  \n                \n              \n              +\n              \n                \u2211\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  \u2260\n                  \n                    k\n                    \n                      m\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                \n              \n              \n                w\n                \n                  i\n                \n                \n                  (\n                  m\n                  )\n                \n              \n              \n                e\n                \n                  \n                    \u03b1\n                    \n                      m\n                    \n                  \n                \n              \n              )\n            \n            \n              d\n              \n                \u03b1\n                \n                  m\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {dE}{d\\alpha _{m}}}={\\frac {d(\\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}e^{-\\alpha _{m}}+\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}e^{\\alpha _{m}})}{d\\alpha _{m}}}}\n  Setting this to zero and solving for \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n   yields:\n\n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              \n                \n                  \u2211\n                  \n                    \n                      y\n                      \n                        i\n                      \n                    \n                    =\n                    \n                      k\n                      \n                        m\n                      \n                    \n                    (\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    )\n                  \n                \n                \n                  w\n                  \n                    i\n                  \n                  \n                    (\n                    m\n                    )\n                  \n                \n              \n              \n                \n                  \u2211\n                  \n                    \n                      y\n                      \n                        i\n                      \n                    \n                    \u2260\n                    \n                      k\n                      \n                        m\n                      \n                    \n                    (\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    )\n                  \n                \n                \n                  w\n                  \n                    i\n                  \n                  \n                    (\n                    m\n                    )\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\alpha _{m}={\\frac {1}{2}}\\ln \\left({\\frac {\\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}}{\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}}}\\right)}\n  \nWe calculate the weighted error rate of the weak classifier to be \n  \n    \n      \n        \n          \u03f5\n          \n            m\n          \n        \n        =\n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          /\n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{m}=\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}/\\sum _{i=1}^{N}w_{i}^{(m)}}\n  , so it follows that:\n\n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              \n                1\n                \u2212\n                \n                  \u03f5\n                  \n                    m\n                  \n                \n              \n              \n                \u03f5\n                \n                  m\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\alpha _{m}={\\frac {1}{2}}\\ln \\left({\\frac {1-\\epsilon _{m}}{\\epsilon _{m}}}\\right)}\n  which is the negative logit function multiplied by 0.5. Due to the convexity of \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   as a function of \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n  , this new expression for \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{m}}\n   gives the global minimum of the loss function.\nNote: This derivation only applies when \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        \u2208\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle k_{m}(x_{i})\\in \\{-1,1\\}}\n  , though it can be a good starting guess in other cases, such as when the weak learner is biased (\n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n        (\n        x\n        )\n        \u2208\n        {\n        a\n        ,\n        b\n        }\n        ,\n        a\n        \u2260\n        \u2212\n        b\n      \n    \n    {\\displaystyle k_{m}(x)\\in \\{a,b\\},a\\neq -b}\n  ), has multiple leaves (\n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n        (\n        x\n        )\n        \u2208\n        {\n        a\n        ,\n        b\n        ,\n        \u2026\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle k_{m}(x)\\in \\{a,b,\\dots ,n\\}}\n  ) or is some other function \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n        (\n        x\n        )\n        \u2208\n        \n          R\n        \n      \n    \n    {\\displaystyle k_{m}(x)\\in \\mathbb {R} }\n  .\nThus we have derived the AdaBoost algorithm: At each iteration, choose the classifier \n  \n    \n      \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle k_{m}}\n  , which minimizes the total weighted error \n  \n    \n      \n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n      \n    \n    {\\displaystyle \\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}}\n  , use this to calculate the error rate \n  \n    \n      \n        \n          \u03f5\n          \n            m\n          \n        \n        =\n        \n          \u2211\n          \n            \n              y\n              \n                i\n              \n            \n            \u2260\n            \n              k\n              \n                m\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n        \n          /\n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          w\n          \n            i\n          \n          \n            (\n            m\n            )\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{m}=\\sum _{y_{i}\\neq k_{m}(x_{i})}w_{i}^{(m)}/\\sum _{i=1}^{N}w_{i}^{(m)}}\n  , use this to calculate the weight \n  \n    \n      \n        \n          \u03b1\n          \n            m\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              \n                1\n                \u2212\n                \n                  \u03f5\n                  \n                    m\n                  \n                \n              \n              \n                \u03f5\n                \n                  m\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\alpha _{m}={\\frac {1}{2}}\\ln \\left({\\frac {1-\\epsilon _{m}}{\\epsilon _{m}}}\\right)}\n  , and finally use this to improve the boosted classifier \n  \n    \n      \n        \n          C\n          \n            m\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle C_{m-1}}\n   to \n  \n    \n      \n        \n          C\n          \n            m\n          \n        \n        =\n        \n          C\n          \n            (\n            m\n            \u2212\n            1\n            )\n          \n        \n        +\n        \n          \u03b1\n          \n            m\n          \n        \n        \n          k\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle C_{m}=C_{(m-1)}+\\alpha _{m}k_{m}}\n  .\n\n\n== Statistical understanding of boosting ==\nBoosting is a form of linear regression in which the features of each sample \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   are the outputs of some weak learner \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   applied to \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  .\nWhile regression tries to fit \n  \n    \n      \n        F\n        (\n        x\n        )\n      \n    \n    {\\displaystyle F(x)}\n   to \n  \n    \n      \n        y\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y(x)}\n   as precisely as possible without loss of generalization, typically using least square error \n  \n    \n      \n        E\n        (\n        f\n        )\n        =\n        (\n        y\n        (\n        x\n        )\n        \u2212\n        f\n        (\n        x\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E(f)=(y(x)-f(x))^{2}}\n  , whereas the AdaBoost error function \n  \n    \n      \n        E\n        (\n        f\n        )\n        =\n        \n          e\n          \n            \u2212\n            y\n            (\n            x\n            )\n            f\n            (\n            x\n            )\n          \n        \n      \n    \n    {\\displaystyle E(f)=e^{-y(x)f(x)}}\n   takes into account the fact that only the sign of the final result is used, thus \n  \n    \n      \n        \n          |\n        \n        F\n        (\n        x\n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle |F(x)|}\n   can be far larger than 1 without increasing error. However, the exponential increase in the error for sample \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   as \n  \n    \n      \n        \u2212\n        y\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle -y(x_{i})f(x_{i})}\n   increases. resulting in excessive weights being assigned to outliers.\nOne feature of the choice of exponential error function is that the error of the final additive model is the product of the error of each stage, that is, \n  \n    \n      \n        \n          e\n          \n            \n              \u2211\n              \n                i\n              \n            \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n        =\n        \n          \u220f\n          \n            i\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle e^{\\sum _{i}-y_{i}f(x_{i})}=\\prod _{i}e^{-y_{i}f(x_{i})}}\n  . Thus it can be seen that the weight update in the AdaBoost algorithm is equivalent to recalculating the error on \n  \n    \n      \n        \n          F\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle F_{t}(x)}\n   after each stage.\nThere is a lot of flexibility allowed in the choice of loss function. As long as the loss function is monotonic and continuously differentiable, the classifier is always driven toward purer solutions. Zhang (2004) provides a loss function based on least squares, a modified Huber loss function:\n\n  \n    \n      \n        \u03d5\n        (\n        y\n        ,\n        f\n        (\n        x\n        )\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \u2212\n                  4\n                  y\n                  f\n                  (\n                  x\n                  )\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  y\n                  f\n                  (\n                  x\n                  )\n                  <\n                  \u2212\n                  1\n                  ,\n                \n              \n              \n                \n                  (\n                  y\n                  f\n                  (\n                  x\n                  )\n                  \u2212\n                  1\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  \u2212\n                  1\n                  \u2264\n                  y\n                  f\n                  (\n                  x\n                  )\n                  \u2264\n                  1.\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  y\n                  f\n                  (\n                  x\n                  )\n                  >\n                  1\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\phi (y,f(x))={\\begin{cases}-4yf(x)&{\\mbox{if }}yf(x)<-1,\\\\(yf(x)-1)^{2}&{\\mbox{if }}-1\\leq yf(x)\\leq 1.\\\\0&{\\mbox{if }}yf(x)>1\\end{cases}}}\n  This function is more well-behaved than LogitBoost for \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   close to 1 or -1, does not penalise \u2018overconfident\u2019 predictions (\n  \n    \n      \n        y\n        f\n        (\n        x\n        )\n        >\n        1\n      \n    \n    {\\displaystyle yf(x)>1}\n  ), unlike unmodified least squares, and only penalises samples misclassified with confidence greater than 1 linearly, as opposed to quadratically or exponentially, and is thus less susceptible to the effects of outliers.\n\n\n== Boosting as gradient descent ==\n\nBoosting can be seen as minimization of a convex loss function over a convex set of functions. Specifically, the loss being minimized by AdaBoost is the exponential loss\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \u03d5\n        (\n        i\n        ,\n        y\n        ,\n        f\n        )\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\sum _{i}\\phi (i,y,f)=\\sum _{i}e^{-y_{i}f(x_{i})}}\n  ,\nwhereas LogitBoost performs logistic regression, minimizing\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \u03d5\n        (\n        i\n        ,\n        y\n        ,\n        f\n        )\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            1\n            +\n            \n              e\n              \n                \u2212\n                \n                  y\n                  \n                    i\n                  \n                \n                f\n                (\n                \n                  x\n                  \n                    i\n                  \n                \n                )\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\sum _{i}\\phi (i,y,f)=\\sum _{i}\\ln \\left(1+e^{-y_{i}f(x_{i})}\\right)}\n  .\nIn the gradient descent analogy, the output of the classifier for each training point is considered a point \n  \n    \n      \n        \n          (\n          \n            \n              F\n              \n                t\n              \n            \n            (\n            \n              x\n              \n                1\n              \n            \n            )\n            ,\n            \u2026\n            ,\n            \n              F\n              \n                t\n              \n            \n            (\n            \n              x\n              \n                n\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(F_{t}(x_{1}),\\dots ,F_{t}(x_{n})\\right)}\n   in n-dimensional space, where each axis corresponds to a training sample, each weak learner \n  \n    \n      \n        h\n        (\n        x\n        )\n      \n    \n    {\\displaystyle h(x)}\n   corresponds to a vector of fixed orientation and length, and the goal is to reach the target point \n  \n    \n      \n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (y_{1},\\dots ,y_{n})}\n   (or any region where the value of loss function \n  \n    \n      \n        \n          E\n          \n            T\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle E_{T}(x_{1},\\dots ,x_{n})}\n   is less than the value at that point), in the fewest steps. Thus AdaBoost algorithms perform either Cauchy (find \n  \n    \n      \n        h\n        (\n        x\n        )\n      \n    \n    {\\displaystyle h(x)}\n   with the steepest gradient, choose \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   to minimize test error) or Newton (choose some target point, find \n  \n    \n      \n        \u03b1\n        h\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\alpha h(x)}\n   that brings \n  \n    \n      \n        \n          F\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle F_{t}}\n   closest to that point) optimization of training error.\n\n\n== Example algorithm (Discrete AdaBoost) ==\nWith:\n\nSamples \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        \u2026\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1}\\dots x_{n}}\n  \nDesired outputs \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        \u2026\n        \n          y\n          \n            n\n          \n        \n        ,\n        y\n        \u2208\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle y_{1}\\dots y_{n},y\\in \\{-1,1\\}}\n  \nInitial weights \n  \n    \n      \n        \n          w\n          \n            1\n            ,\n            1\n          \n        \n        \u2026\n        \n          w\n          \n            n\n            ,\n            1\n          \n        \n      \n    \n    {\\displaystyle w_{1,1}\\dots w_{n,1}}\n   set to \n  \n    \n      \n        \n          \n            1\n            n\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{n}}}\n  \nError function \n  \n    \n      \n        E\n        (\n        f\n        (\n        x\n        )\n        ,\n        y\n        ,\n        i\n        )\n        =\n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle E(f(x),y,i)=e^{-y_{i}f(x_{i})}}\n  \nWeak learners \n  \n    \n      \n        h\n        :\n        x\n        \u2192\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle h\\colon x\\rightarrow \\{-1,1\\}}\n  For \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   in \n  \n    \n      \n        1\n        \u2026\n        T\n      \n    \n    {\\displaystyle 1\\dots T}\n  :\n\nChoose \n  \n    \n      \n        \n          h\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle h_{t}(x)}\n  :\nFind weak learner \n  \n    \n      \n        \n          h\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle h_{t}(x)}\n   that minimizes \n  \n    \n      \n        \n          \u03f5\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{t}}\n  , the weighted sum error for misclassified points \n  \n    \n      \n        \n          \u03f5\n          \n            t\n          \n        \n        =\n        \n          \u2211\n          \n            \n              \n                \n                  \n                    h\n                    \n                      t\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  \u2260\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n                \n                  i\n                  =\n                  1\n                \n              \n            \n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle \\epsilon _{t}=\\sum _{\\stackrel {i=1}{h_{t}(x_{i})\\neq y_{i}}}^{n}w_{i,t}}\n  \nChoose \n  \n    \n      \n        \n          \u03b1\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              \n                1\n                \u2212\n                \n                  \u03f5\n                  \n                    t\n                  \n                \n              \n              \n                \u03f5\n                \n                  t\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\alpha _{t}={\\frac {1}{2}}\\ln \\left({\\frac {1-\\epsilon _{t}}{\\epsilon _{t}}}\\right)}\n  \nAdd to ensemble:\n\n  \n    \n      \n        \n          F\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          F\n          \n            t\n            \u2212\n            1\n          \n        \n        (\n        x\n        )\n        +\n        \n          \u03b1\n          \n            t\n          \n        \n        \n          h\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle F_{t}(x)=F_{t-1}(x)+\\alpha _{t}h_{t}(x)}\n  \nUpdate weights:\n\n  \n    \n      \n        \n          w\n          \n            i\n            ,\n            t\n            +\n            1\n          \n        \n        =\n        \n          w\n          \n            i\n            ,\n            t\n          \n        \n        \n          e\n          \n            \u2212\n            \n              y\n              \n                i\n              \n            \n            \n              \u03b1\n              \n                t\n              \n            \n            \n              h\n              \n                t\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle w_{i,t+1}=w_{i,t}e^{-y_{i}\\alpha _{t}h_{t}(x_{i})}}\n   for \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   in \n  \n    \n      \n        1\n        \u2026\n        n\n      \n    \n    {\\displaystyle 1\\dots n}\n  \nRenormalize \n  \n    \n      \n        \n          w\n          \n            i\n            ,\n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle w_{i,t+1}}\n   such that \n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \n          w\n          \n            i\n            ,\n            t\n            +\n            1\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{i}w_{i,t+1}=1}\n  \n(Note: It can be shown that \n  \n    \n      \n        \n          \n            \n              \n                \u2211\n                \n                  \n                    h\n                    \n                      t\n                      +\n                      1\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  =\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                w\n                \n                  i\n                  ,\n                  t\n                  +\n                  1\n                \n              \n            \n            \n              \n                \u2211\n                \n                  \n                    h\n                    \n                      t\n                      +\n                      1\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  \u2260\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                w\n                \n                  i\n                  ,\n                  t\n                  +\n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  \n                    h\n                    \n                      t\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  =\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                w\n                \n                  i\n                  ,\n                  t\n                \n              \n            \n            \n              \n                \u2211\n                \n                  \n                    h\n                    \n                      t\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  \u2260\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                w\n                \n                  i\n                  ,\n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\sum _{h_{t+1}(x_{i})=y_{i}}w_{i,t+1}}{\\sum _{h_{t+1}(x_{i})\\neq y_{i}}w_{i,t+1}}}={\\frac {\\sum _{h_{t}(x_{i})=y_{i}}w_{i,t}}{\\sum _{h_{t}(x_{i})\\neq y_{i}}w_{i,t}}}}\n   at every step, which can simplify the calculation of the new weights.)\n\n\n== Variants ==\n\n\n=== Real AdaBoost ===\nThe output of decision trees is a class probability estimate \n  \n    \n      \n        p\n        (\n        x\n        )\n        =\n        P\n        (\n        y\n        =\n        1\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(x)=P(y=1|x)}\n  , the probability that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is in the positive class. Friedman, Hastie and Tibshirani derive an analytical minimizer for \n  \n    \n      \n        \n          e\n          \n            \u2212\n            y\n            \n              (\n              \n                \n                  F\n                  \n                    t\n                    \u2212\n                    1\n                  \n                \n                (\n                x\n                )\n                +\n                \n                  f\n                  \n                    t\n                  \n                \n                (\n                p\n                (\n                x\n                )\n                )\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle e^{-y\\left(F_{t-1}(x)+f_{t}(p(x))\\right)}}\n   for some fixed \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n   (typically chosen using weighted least squares error):\n\n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              x\n              \n                1\n                \u2212\n                x\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{t}(x)={\\frac {1}{2}}\\ln \\left({\\frac {x}{1-x}}\\right)}\n  .Thus, rather than multiplying the output of the entire tree by some fixed value, each leaf node is changed to output half the logit transform of its previous value.\n\n\n=== LogitBoost ===\n\nLogitBoost represents an application of established logistic regression techniques to the AdaBoost method. Rather than minimizing error with respect to y, weak learners are chosen to minimize the (weighted least-squares) error of \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{t}(x)}\n   with respect to\n\n  \n    \n      \n        \n          z\n          \n            t\n          \n        \n        =\n        \n          \n            \n              \n                y\n                \n                  \u2217\n                \n              \n              \u2212\n              \n                p\n                \n                  t\n                \n              \n              (\n              x\n              )\n            \n            \n              2\n              \n                p\n                \n                  t\n                \n              \n              (\n              x\n              )\n              (\n              1\n              \u2212\n              \n                p\n                \n                  t\n                \n              \n              (\n              x\n              )\n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle z_{t}={\\frac {y^{*}-p_{t}(x)}{2p_{t}(x)(1-p_{t}(x))}},}\n  where\n\n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              e\n              \n                \n                  F\n                  \n                    t\n                    \u2212\n                    1\n                  \n                \n                (\n                x\n                )\n              \n            \n            \n              \n                e\n                \n                  \n                    F\n                    \n                      t\n                      \u2212\n                      1\n                    \n                  \n                  (\n                  x\n                  )\n                \n              \n              +\n              \n                e\n                \n                  \u2212\n                  \n                    F\n                    \n                      t\n                      \u2212\n                      1\n                    \n                  \n                  (\n                  x\n                  )\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle p_{t}(x)={\\frac {e^{F_{t-1}(x)}}{e^{F_{t-1}(x)}+e^{-F_{t-1}(x)}}},}\n  \n\n  \n    \n      \n        \n          w\n          \n            t\n          \n        \n        =\n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        )\n        (\n        1\n        \u2212\n        \n          p\n          \n            t\n          \n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle w_{t}=p_{t}(x)(1-p_{t}(x))}\n  \n\n  \n    \n      \n        \n          y\n          \n            \u2217\n          \n        \n        =\n        \n          \n            \n              y\n              +\n              1\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle y^{*}={\\frac {y+1}{2}}.}\n  That is \n  \n    \n      \n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle z_{t}}\n   is the Newton\u2013Raphson approximation of the minimizer of the log-likelihood error at stage \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  , and the weak learner \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{t}}\n   is chosen as the learner that best approximates \n  \n    \n      \n        \n          z\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle z_{t}}\n   by weighted least squares.\nAs p approaches either 1 or 0, the value of \n  \n    \n      \n        \n          p\n          \n            t\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        (\n        1\n        \u2212\n        \n          p\n          \n            t\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle p_{t}(x_{i})(1-p_{t}(x_{i}))}\n   becomes very small and the z term, which is large for misclassified samples, can become numerically unstable, due to machine precision rounding errors. This can be overcome by enforcing some limit on the absolute value of z and the minimum value of w\n\n\n=== Gentle AdaBoost ===\nWhile previous boosting algorithms choose \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{t}}\n   greedily, minimizing the overall test error as much as possible at each step, GentleBoost features a bounded step size. \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle f_{t}}\n   is chosen to minimize \n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \n          w\n          \n            t\n            ,\n            i\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          f\n          \n            t\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sum _{i}w_{t,i}(y_{i}-f_{t}(x_{i}))^{2}}\n  , and no further coefficient is applied. Thus, in the case where a weak learner exhibits perfect classification performance, GentleBoost chooses \n  \n    \n      \n        \n          f\n          \n            t\n          \n        \n        (\n        x\n        )\n        =\n        \n          \u03b1\n          \n            t\n          \n        \n        \n          h\n          \n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{t}(x)=\\alpha _{t}h_{t}(x)}\n   exactly equal to \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  , while steepest descent algorithms try to set \n  \n    \n      \n        \n          \u03b1\n          \n            t\n          \n        \n        =\n        \u221e\n      \n    \n    {\\displaystyle \\alpha _{t}=\\infty }\n  . Empirical observations about the good performance of GentleBoost appear to back up Schapire and Singer's remark that allowing excessively large values of \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   can lead to poor generalization performance.\n\n\n=== Early Termination ===\nA technique for speeding up processing of boosted classifiers, early termination refers to only testing each potential object with as many layers of the final classifier necessary to meet some confidence threshold, speeding up computation for cases where the class of the object can easily be determined. One such scheme is the object detection framework introduced by Viola and Jones: in an application with significantly more negative samples than positive, a cascade of separate boost classifiers is trained, the output of each stage biased such that some acceptably small fraction of positive samples is mislabeled as negative, and all samples marked as negative after each stage are discarded. If 50% of negative samples are filtered out by each stage, only a very small number of objects would pass through the entire classifier, reducing computation effort. This method has since been generalized, with a formula provided for choosing optimal thresholds at each stage to achieve some desired false positive and false negative rate.In the field of statistics, where AdaBoost is more commonly applied to problems of moderate dimensionality, early stopping is used as a strategy to reduce overfitting. A validation set of samples is separated from the training set, performance of the classifier on the samples used for training is compared to performance on the validation samples, and training is terminated if performance on the validation sample is seen to decrease even as performance on the training set continues to improve.\n\n\n=== Totally corrective algorithms ===\nFor steepest descent versions of AdaBoost, where \n  \n    \n      \n        \n          \u03b1\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{t}}\n   is chosen at each layer t to minimize test error, the next layer added is said to be maximally independent of layer t: it is unlikely to choose a weak learner t+1 that is similar to learner t. However, there remains the possibility that t+1 produces similar information to some other earlier layer. Totally corrective algorithms, such as LPBoost, optimize the value of every coefficient after each step, such that new layers added are always maximally independent of every previous layer. This can be accomplished by backfitting, linear programming or some other method.\n\n\n=== Pruning ===\nPruning is the process of removing poorly performing weak classifiers to improve memory and execution-time cost of the boosted classifier. The simplest methods, which can be particularly effective in conjunction with totally corrective training, are weight- or margin-trimming: when the coefficient, or the contribution to the total test error, of some weak classifier falls below a certain threshold, that classifier is dropped. Margineantu & Dietterich suggested an alternative criterion for trimming: weak classifiers should be selected such that the diversity of the ensemble is maximized. If two weak learners produce very similar outputs, efficiency can be improved by removing one of them and increasing the coefficient of the remaining weak learner.\n\n\n== See also ==\nBootstrap aggregating\nCoBoosting\nBrownBoost\nGradient boosting\nMultiplicative weight update method \u00a7 AdaBoost algorithm\n\n\n== References ==\n\n\n== Further reading ==", "Statistical hypothesis testing": "A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.\nHypothesis testing allows us to make probabilistic statements about population parameters.\n\n\n== History ==\n\n\n=== Early use ===\nWhile hypothesis testing was popularized early in the 20th century, early forms were used in the 1700s. The first use is credited to John Arbuthnot (1710), followed by Pierre-Simon Laplace (1770s), in analyzing the human sex ratio at birth; see \u00a7 Human sex ratio.\n\n\n=== Modern origins and early controversy ===\nModern significance testing is largely the product of Karl Pearson (p-value, Pearson's chi-squared test), William Sealy Gosset (Student's t-distribution), and Ronald Fisher (\"null hypothesis\", analysis of variance, \"significance test\"), while hypothesis testing was developed by Jerzy Neyman and Egon Pearson (son of Karl). Ronald Fisher began his life in statistics as a Bayesian (Zabell 1992), but Fisher soon grew disenchanted with the subjectivity involved (namely use of the principle of indifference when determining prior probabilities), and sought to provide a more \"objective\" approach to inductive inference.Fisher emphasized rigorous experimental design and methods to extract a result from few samples assuming Gaussian distributions. Neyman (who teamed with the younger Pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions. Modern hypothesis testing is an inconsistent hybrid of the Fisher vs Neyman/Pearson formulation, methods and terminology developed in the early 20th century.\nFisher popularized the \"significance test\". He required a null-hypothesis (corresponding to a population frequency distribution) and a sample. His (now familiar) calculations determined whether to reject the null-hypothesis or not. Significance testing did not utilize an alternative hypothesis so there was no concept of a Type II error.\nThe p-value was devised as an informal, but objective, index meant to help a researcher determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis. Hypothesis testing (and Type I/II errors) was devised by Neyman and Pearson as a more objective alternative to Fisher's p-value, also meant to determine researcher behaviour, but without requiring any inductive inference by the researcher.Neyman & Pearson considered a different problem to Fisher (which they called \"hypothesis testing\"). They initially considered two simple hypotheses (both with frequency distributions). They calculated two probabilities and typically selected the hypothesis associated with the higher probability (the hypothesis more likely to have generated the sample). Their method always selected a hypothesis. It also allowed the calculation of both types of error probabilities.\nFisher and Neyman/Pearson clashed bitterly. Neyman/Pearson considered their formulation to be an improved generalization of significance testing (the defining paper was abstract; Mathematicians have generalized and refined the theory for decades). Fisher thought that it was not applicable to scientific research because often, during the course of the experiment, it is discovered that the initial assumptions about the null hypothesis are questionable due to unexpected sources of error. He believed that the use of rigid reject/accept decisions based on models formulated before data is collected was incompatible with this common scenario faced by scientists and attempts to apply this method to scientific research would lead to mass confusion.The dispute between Fisher and Neyman\u2013Pearson was waged on philosophical grounds, characterized by a philosopher as a dispute over the proper role of models in statistical inference.Events intervened: Neyman accepted a position in the University of California, Berkeley in 1938, breaking his partnership with Pearson and separating disputants (who had occupied the same building) by much of the planetary diameter. World War II provided an intermission in the debate. The dispute between Fisher and Neyman terminated (unresolved after 27 years) with Fisher's death in 1962. Neyman wrote a well-regarded eulogy. Some of Neyman's later publications reported p-values and significance levels.The modern version of hypothesis testing is a hybrid of the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by Fisher) beginning in the 1940s (but signal detection, for example, still uses the Neyman/Pearson formulation). Great conceptual differences and many caveats in addition to those mentioned above were ignored. Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the subject taught today in introductory statistics has more similarities with Fisher's method than theirs.Sometime around 1940, authors of statistical text books began combining the two approaches by using the p-value in place of the test statistic (or data) to test against the Neyman\u2013Pearson \"significance level\".\n\n\n=== Early choices of null hypothesis ===\nPaul Meehl has argued that the epistemological importance of the choice of null hypothesis has gone largely unacknowledged. When the null hypothesis is predicted by theory, a more precise experiment will be a more severe test of the underlying theory. When the null hypothesis defaults to \"no difference\" or \"no effect\", a more precise experiment is a less severe test of the theory that motivated performing the experiment. An examination of the origins of the latter practice may therefore be useful:\n1778: Pierre Laplace compares the birthrates of boys and girls in multiple European cities. He states: \"it is natural to conclude that these possibilities are very nearly in the same ratio\". Thus Laplace's null hypothesis that the birthrates of boys and girls should be equal given \"conventional wisdom\".1900: Karl Pearson develops the chi squared test to determine \"whether a given form of frequency curve will effectively describe the samples drawn from a given population.\" Thus the null hypothesis is that a population is described by some distribution predicted by theory. He uses as an example the numbers of five and sixes in the Weldon dice throw data.1904: Karl Pearson develops the concept of \"contingency\" in order to determine whether outcomes are independent of a given categorical factor. Here the null hypothesis is by default that two things are unrelated (e.g. scar formation and death rates from smallpox). The null hypothesis in this case is no longer predicted by theory or conventional wisdom, but is instead the principle of indifference that led Fisher and others to dismiss the use of \"inverse probabilities\".\n\n\n=== Philosophy ===\nHypothesis testing and philosophy intersect. Inferential statistics, which includes hypothesis testing, is applied probability. Both probability and its application are intertwined with philosophy. Philosopher David Hume wrote, \"All knowledge degenerates into probability.\" Competing practical definitions of probability reflect philosophical differences. The most common application of hypothesis testing is in the scientific interpretation of experimental data, which is naturally studied by the philosophy of science.\nFisher and Neyman opposed the subjectivity of probability. Their views contributed to the objective definitions. The core of their historical disagreement was philosophical.\nMany of the philosophical criticisms of hypothesis testing are discussed by statisticians in other contexts, particularly correlation does not imply causation and the design of experiments.\nHypothesis testing is of continuing interest to philosophers.\n\n\n=== Education ===\n\nStatistics is increasingly being taught in schools with hypothesis testing being one of the elements taught. Many conclusions reported in the popular press (political opinion polls to medical studies) are based on statistics. Some writers have stated that statistical analysis of this kind allows for thinking clearly about problems involving mass data, as well as the effective reporting of trends and inferences from said data, but caution that writers for a broad public should have a solid understanding of the field in order to use the terms and concepts correctly. An introductory college statistics class places much emphasis on hypothesis testing \u2013 perhaps half of the course. Such fields as literature and divinity now include findings based on statistical analysis (see the Bible Analyzer). An introductory statistics class teaches hypothesis testing as a cookbook process. Hypothesis testing is also taught at the postgraduate level. Statisticians learn how to create good statistical test procedures (like z, Student's t, F and chi-squared). Statistical hypothesis testing is considered a mature area within statistics, but a limited amount of development continues.\nAn academic study states that the cookbook method of teaching introductory statistics leaves no time for history, philosophy or controversy. Hypothesis testing has been taught as received unified method. Surveys showed that graduates of the class were filled with philosophical misconceptions (on all aspects of statistical inference) that persisted among instructors. While the problem was addressed more than a decade ago, and calls for educational reform continue, students still graduate from statistics classes holding fundamental misconceptions about hypothesis testing. Ideas for improving the teaching of hypothesis testing include encouraging students to search for statistical errors in published papers, teaching the history of statistics and emphasizing the controversy in a generally dry subject.\n\n\n== The testing process ==\nIn the statistics literature, statistical hypothesis testing plays a fundamental role. There are two mathematically equivalent processes that can be used.The usual line of reasoning is as follows:\n\nThere is an initial research hypothesis of which the truth is unknown.\nThe first step is to state the relevant null and alternative hypotheses. This is important, as mis-stating the hypotheses will muddy the rest of the process.\nThe second step is to consider the statistical assumptions being made about the sample in doing the test; for example, assumptions about the statistical independence or about the form of the distributions of the observations. This is equally important as invalid assumptions will mean that the results of the test are invalid.\nDecide which test is appropriate, and state the relevant test statistic T.\nDerive the distribution of the test statistic under the null hypothesis from the assumptions. In standard cases this will be a well-known result. For example, the test statistic might follow a Student's t distribution with known degrees of freedom, or a normal distribution with known mean and variance. If the distribution of the test statistic is completely fixed by the null hypothesis we call the hypothesis simple, otherwise it is called composite.\nSelect a significance level (\u03b1), a probability threshold below which the null hypothesis will be rejected. Common values are 5% and 1%.\nThe distribution of the test statistic under the null hypothesis partitions the possible values of T into those for which the null hypothesis is rejected\u2014the so-called critical region\u2014and those for which it is not. The probability of  T occurring in the critical region under the null hypothesis is \u03b1. In the case of a composite null hypothesis, the maximum of that  probability is \u03b1.\nCompute from the observations the observed value tobs of the test statistic T.\nDecide to either reject the null hypothesis in favor of the alternative or not reject it. The decision rule is to reject the null hypothesis H0 if the observed value tobs is in the critical region, and not to reject the null hypothesis otherwise.A common alternative formulation of this process goes as follows:\n\nCompute from the observations the observed value tobs of the test statistic T.\nCalculate the p-value. This is the probability, under the null hypothesis, of sampling a test statistic at least as extreme as that which was observed (the maximal probability of that event, if the hypothesis is composite).\nReject the null hypothesis, in favor of the alternative hypothesis, if and only if the p-value is less than (or equal to) the significance level (the selected probability) threshold (\u03b1), for example 0.05 or 0.01.The former process was advantageous in the past when only tables of test statistics at common probability thresholds were available. It allowed a decision to be made without the calculation of a probability. It was adequate for classwork and for operational use, but it was deficient for reporting results. The latter process relied on extensive tables or on computational support not always available. The explicit calculation of a probability is useful for reporting. The calculations are now trivially performed with appropriate software.\nThe difference in the two processes applied to the Radioactive suitcase example (below):\n\n\"The Geiger-counter reading is 10. The limit is 9. Check the suitcase.\"\n\"The Geiger-counter reading is high; 97% of safe suitcases have lower readings. The limit is 95%. Check the suitcase.\"The former report is adequate, the latter gives a more detailed explanation of the data and the reason why the suitcase is being checked.\nNot rejecting the null hypothesis does not mean the null hypothesis is \"accepted\" (see the Interpretation section).\nThe processes described here are perfectly adequate for computation. They seriously neglect the design of experiments considerations.It is particularly critical that appropriate sample sizes be estimated before conducting the experiment.\nThe phrase \"test of significance\" was coined by statistician Ronald Fisher.\n\n\n=== Interpretation ===\nThe p-value is the probability that a given result (or a more significant result) would occur under the null hypothesis. At a significance level of 0.05, a fair coin would be expected to (incorrectly) reject the null hypothesis (that it is fair) in about 1 out of every 20 tests. The p-value does not provide the probability that either the null hypothesis or its opposite is correct (a common source of confusion).If the p-value is less than the chosen significance threshold (equivalently, if the observed test statistic is in the critical region), then we say the null hypothesis is rejected at the chosen level of significance. If the p-value is not less than the chosen significance threshold (equivalently, if the observed test statistic is outside the critical region), then the null hypothesis is not rejected.\nIn the Lady tasting tea example (below), Fisher required the Lady to properly categorize all of the cups of tea to justify the conclusion that the result was unlikely to result from chance. His test revealed that if the lady was effectively guessing at random (the null hypothesis), there was a 1.4% chance that the observed results (perfectly ordered tea) would occur.\nRejecting the hypothesis that a large paw print originated from a bear does not immediately prove the existence of Bigfoot. Hypothesis testing emphasizes the rejection, which is based on a probability, rather than the acceptance.\n\"The probability of rejecting the null hypothesis is a function of five factors: whether the test is one- or two-tailed, the level of significance, the standard deviation, the amount of deviation from the null hypothesis, and the number of observations.\"\n\n\n=== Use and importance ===\nStatistics are helpful in analyzing most collections of data. This is equally true of hypothesis testing which can justify conclusions even when no scientific theory exists. In the Lady tasting tea example, it was \"obvious\" that no difference existed between (milk poured into tea) and (tea poured into milk). The data contradicted the \"obvious\".\nReal world applications of hypothesis testing include:\nTesting whether more men than women suffer from nightmares\nEstablishing authorship of documents\nEvaluating the effect of the full moon on behavior\nDetermining the range at which a bat can detect an insect by echo\nDeciding whether hospital carpeting results in more infections\nSelecting the best means to stop smoking\nChecking whether bumper stickers reflect car owner behavior\nTesting the claims of handwriting analystsStatistical hypothesis testing plays an important role in the whole of statistics and in statistical inference. For example, Lehmann (1992) in a review of the fundamental paper by Neyman and Pearson (1933) says: \"Nevertheless, despite their shortcomings, the new paradigm formulated in the 1933 paper, and the many developments carried out within its framework continue to play a central role in both the theory and practice of statistics and can be expected to do so in the foreseeable future\".\nSignificance testing has been the favored statistical tool in some experimental social sciences (over 90% of articles in the Journal of Applied Psychology during the early 1990s). Other fields have favored the estimation of parameters (e.g. effect size). Significance testing is used as a substitute for the traditional comparison of predicted value and experimental result at the core of the scientific method. When theory is only capable of predicting the sign of a relationship, a directional (one-sided) hypothesis test can be configured so that only a statistically significant result supports theory. This form of theory appraisal is the most heavily criticized application of hypothesis testing.\n\n\n=== Cautions ===\n\"If the government required statistical procedures to carry warning labels like those on drugs, most inference methods would have long labels indeed.\" This caution applies to hypothesis tests and alternatives to them.\nThe successful hypothesis test is associated with a probability and a type-I error rate. The conclusion might be wrong.\nThe conclusion of the test is only as solid as the sample upon which it is based. The design of the experiment is critical. A number of unexpected effects have been observed including:\n\nThe clever Hans effect. A horse appeared to be capable of doing simple arithmetic.\nThe Hawthorne effect. Industrial workers were more productive in better illumination, and most productive in worse.\nThe placebo effect. Pills with no medically active ingredients were remarkably effective.A statistical analysis of misleading data produces misleading conclusions. The issue of data quality can be more subtle. In forecasting for example, there is no agreement on a measure of forecast accuracy. In the absence of a consensus measurement, no decision based on measurements will be without controversy.\nPublication bias: Statistically nonsignificant results may be less likely to be published, which can bias the literature.\nMultiple testing: When multiple true null hypothesis tests are conducted at once without adjustment, the overall probability of Type I error is higher than the nominal alpha level.\nThose making critical decisions based on the results of a hypothesis test are prudent to look at the details rather than the conclusion alone. In the physical sciences most results are fully accepted only when independently confirmed. The general advice concerning statistics is, \"Figures never lie, but liars figure\" (anonymous).\n\n\n== Definition of terms ==\nThe following definitions are mainly based on the exposition in the book by Lehmann and Romano:\nStatistical hypothesis: A statement about the parameters describing a population (not a sample).\nTest statistic: A value calculated from a sample without any unknown parameters, often to summarize the sample for comparison purposes.\nSimple hypothesis: Any hypothesis which specifies the population distribution completely.\nComposite hypothesis: Any hypothesis which does not specify the population distribution completely.\nNull hypothesis (H0)\nPositive data: Data that enable the investigator to reject a null hypothesis.\nAlternative hypothesis (H1)\nRegion of rejection / Critical region: The set of values of the test statistic for which the null hypothesis is rejected.\nCritical value\nPower of a test (1 \u2212 \u03b2)\nSize: For simple hypotheses, this is the test's probability of incorrectly rejecting the null hypothesis. The false positive rate. For composite hypotheses this is the supremum of the probability of rejecting the null hypothesis over all cases covered by the null hypothesis. The complement of the false positive rate is termed specificity in biostatistics. (\"This is a specific test. Because the result is positive, we can confidently say that the patient has the condition.\") See sensitivity and specificity and Type I and type II errors for exhaustive definitions.\nSignificance level of a test (\u03b1)\np-value\nStatistical significance test: A predecessor to the statistical hypothesis test (see the Origins section). An experimental result was said to be statistically significant if a sample was sufficiently inconsistent with the (null) hypothesis. This was variously considered common sense, a pragmatic heuristic for identifying meaningful experimental results, a convention establishing a threshold of statistical evidence or a method for drawing conclusions from data. The statistical hypothesis test added mathematical rigor and philosophical consistency to the concept by making the alternative hypothesis explicit. The term is loosely used for the modern version which is now part of statistical hypothesis testing.\nConservative test: A test is conservative if, when constructed for a given nominal significance level, the true probability of incorrectly rejecting the null hypothesis is never greater than the nominal level.\nExact testA statistical hypothesis test compares a test statistic (z or t for examples) to a threshold. The test statistic (the formula found in the table below) is based on optimality. For a fixed level of Type I error rate, use of these statistics minimizes Type II error rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality:\n\nMost powerful test: For a given size or significance level, the test with the greatest power (probability of rejection) for a given value of the parameter(s) being tested, contained in the alternative hypothesis.\nUniformly most powerful test (UMP)\n\n\n== Common test statistics ==\n\n\n== Examples ==\n\n\n=== Human sex ratio ===\n\nThe earliest use of statistical hypothesis testing is generally credited to the question of whether male and female births are equally likely (null hypothesis), which was addressed in the 1700s by John Arbuthnot (1710), and later by Pierre-Simon Laplace (1770s).Arbuthnot examined birth records in London for each of the 82 years from 1629 to 1710, and applied the sign test, a simple non-parametric test. In every year, the number of males born in London exceeded the number of females. Considering more male or more female births as equally likely, the probability of the observed outcome is 0.582, or about 1 in 4,836,000,000,000,000,000,000,000; in modern terms, this is the p-value. Arbuthnot concluded that this is too small to be due to chance and must instead be due to divine providence: \"From whence it follows, that it is Art, not Chance, that governs.\" In modern terms, he rejected the null hypothesis of equally likely male and female births at the p = 1/282 significance level.\nLaplace considered the statistics of almost half a million births. The statistics showed an excess of boys compared to girls. He concluded by calculation of a p-value that the excess was a real, but unexplained, effect.\n\n\n=== Lady tasting tea ===\n\nIn a famous example of hypothesis testing, known as the Lady tasting tea, Dr. Muriel Bristol, a colleague of Fisher, claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. One could then ask what the probability was for her getting the number she got correct, but just by chance. The null hypothesis was that the Lady had no such ability. The test statistic was a simple count of the number of successes in selecting the 4 cups. The critical region was the single case of 4 successes of 4 possible based on a conventional probability criterion (< 5%). A pattern of 4 successes corresponds to 1 out of 70 possible combinations (p\u2248 1.4%). Fisher asserted that no alternative hypothesis was (ever) required. The lady correctly identified every cup, which would be considered a statistically significant result.\n\n\n=== Courtroom trial ===\nA statistical test procedure is comparable to a criminal trial; a defendant is considered not guilty as long as his or her guilt is not proven. The prosecutor tries to prove the guilt of the defendant. Only when there is enough evidence for the prosecution is the defendant convicted.\nIn the start of the procedure, there are two hypotheses \n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  : \"the defendant is not guilty\", and \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  : \"the defendant is guilty\". The first one, \n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  , is called the null hypothesis. The second one, \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  , is called the alternative hypothesis. It is the alternative hypothesis that one hopes to support.\nThe hypothesis of innocence is rejected only when an error is very unlikely, because one doesn't want to convict an innocent defendant. Such an error is called error of the first kind (i.e., the conviction of an innocent person), and the occurrence of this error is controlled to be rare. As a consequence of this asymmetric behaviour, an error of the second kind (acquitting a person who committed the crime), is more common.\n\nA criminal trial can be regarded as either or both of two decision processes: guilty vs not guilty or evidence vs a threshold (\"beyond a reasonable doubt\"). In one view, the defendant is judged; in the other view the performance of the prosecution (which bears the burden of proof) is judged. A hypothesis test can be regarded as either a judgment of a hypothesis or as a judgment of evidence.\n\n\n=== Philosopher's beans ===\nThe following example was produced by a philosopher describing scientific methods generations before hypothesis testing was\nformalized and popularized.\n\nFew beans of this handful are white.\nMost beans in this bag are white.\nTherefore: Probably, these beans were taken from another bag.\nThis is an hypothetical inference.\n\nThe beans in the bag are the population. The handful are the sample. The null hypothesis is that the sample originated from the population. The criterion for rejecting the null-hypothesis is the \"obvious\" difference in appearance (an informal difference in the mean). The interesting result is that consideration of a real population and a real sample produced an imaginary bag. The philosopher was considering logic rather than probability. To be a real statistical hypothesis test, this example requires the formalities of a probability calculation and a comparison of that probability to a standard.\nA simple generalization of the example considers a mixed bag of beans and a handful that contain either very few or very many white beans. The generalization considers both extremes. It requires more calculations and more comparisons to arrive at a formal answer, but the core philosophy is unchanged; If the composition of the handful is greatly different from that of the bag, then the sample probably originated from another bag. The original example is termed a one-sided or a one-tailed test while the generalization is termed a two-sided or two-tailed test.\nThe statement also relies on the inference that the sampling was random.  If someone had been picking through the bag to find white beans, then it would explain why the handful had so many white beans, and also explain why the number of white beans in the bag was depleted (although the bag is probably intended to be assumed much larger than one's hand).\n\n\n=== Clairvoyant card game ===\nA person (the subject) is tested for clairvoyance. They are shown the back face of a randomly chosen playing card 25 times and asked which of the four suits it belongs to. The number of hits, or correct answers, is called X.\nAs we try to find evidence of their clairvoyance, for the time being the null hypothesis is that the person is not clairvoyant. The alternative is: the person is (more or less) clairvoyant.\nIf the null hypothesis is valid, the only thing the test person can do is guess. For every card, the probability (relative frequency) of any single suit appearing is 1/4. If the alternative is valid, the test subject will predict the suit correctly with probability greater than 1/4. We will call the probability of guessing correctly p. The hypotheses, then, are:\n\nnull hypothesis \n  \n    \n      \n        \n          :\n        \n        \n        \n          H\n          \n            0\n          \n        \n        :\n        p\n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{:}}\\qquad H_{0}:p={\\tfrac {1}{4}}}\n       (just guessing)and\n\nalternative hypothesis \n  \n    \n      \n        \n          :\n        \n        \n          H\n          \n            1\n          \n        \n        :\n        p\n        >\n        \n          \n            \n              1\n              4\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{:}}H_{1}:p>{\\tfrac {1}{4}}}\n      (true clairvoyant).When the test subject correctly predicts all 25 cards, we will consider them clairvoyant, and reject the null hypothesis. Thus also with 24 or 23 hits. With only 5 or 6 hits, on the other hand, there is no cause to consider them so. But what about 12 hits, or 17 hits? What is the critical number, c, of hits, at which point we consider the subject to be clairvoyant? How do we determine the critical value c? With the choice c=25 (i.e. we only accept clairvoyance when all cards are predicted correctly) we're more critical than with c=10. In the first case almost no test subjects will be recognized to be clairvoyant, in the second case, a certain number will pass the test. In practice, one decides how critical one will be. That is, one decides how often one accepts an error of the first kind \u2013 a false positive, or Type I error. With c = 25 the probability of such an error is:\n\n  \n    \n      \n        P\n        (\n        \n          reject \n        \n        \n          H\n          \n            0\n          \n        \n        \u2223\n        \n          H\n          \n            0\n          \n        \n        \n           is valid\n        \n        )\n        =\n        P\n        (\n        X\n        =\n        25\n        \u2223\n        p\n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n        )\n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                  4\n                \n              \n            \n            )\n          \n          \n            25\n          \n        \n        \u2248\n        \n          10\n          \n            \u2212\n            15\n          \n        \n        ,\n      \n    \n    {\\displaystyle P({\\text{reject }}H_{0}\\mid H_{0}{\\text{ is valid}})=P(X=25\\mid p={\\tfrac {1}{4}})=\\left({\\tfrac {1}{4}}\\right)^{25}\\approx 10^{-15},}\n  and hence, very small. The probability of a false positive is the probability of randomly guessing correctly all 25 times.\nBeing less critical, with c=10, gives:\n\n  \n    \n      \n        P\n        (\n        \n          reject \n        \n        \n          H\n          \n            0\n          \n        \n        \u2223\n        \n          H\n          \n            0\n          \n        \n        \n           is valid\n        \n        )\n        =\n        P\n        (\n        X\n        \u2265\n        10\n        \u2223\n        p\n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n        )\n        =\n        \n          \u2211\n          \n            k\n            =\n            10\n          \n          \n            25\n          \n        \n        P\n        (\n        X\n        =\n        k\n        \u2223\n        p\n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n        )\n        =\n        \n          \u2211\n          \n            k\n            =\n            10\n          \n          \n            25\n          \n        \n        \n          \n            \n              (\n            \n            \n              25\n              k\n            \n            \n              )\n            \n          \n        \n        (\n        1\n        \u2212\n        \n          \n            \n              1\n              4\n            \n          \n        \n        \n          )\n          \n            25\n            \u2212\n            k\n          \n        \n        (\n        \n          \n            \n              1\n              4\n            \n          \n        \n        \n          )\n          \n            k\n          \n        \n        \u2248\n        0\n        \n          .\n        \n        0713.\n      \n    \n    {\\displaystyle P({\\text{reject }}H_{0}\\mid H_{0}{\\text{ is valid}})=P(X\\geq 10\\mid p={\\tfrac {1}{4}})=\\sum _{k=10}^{25}P(X=k\\mid p={\\tfrac {1}{4}})=\\sum _{k=10}^{25}{\\binom {25}{k}}(1-{\\tfrac {1}{4}})^{25-k}({\\tfrac {1}{4}})^{k}\\approx 0{.}0713.}\n  Thus, c = 10 yields a much greater probability of false positive.\nBefore the test is actually performed, the maximum acceptable probability of a Type I error (\u03b1) is determined. Typically, values in the range of 1% to 5% are selected. (If the maximum acceptable error rate is zero, an infinite number of correct guesses is required.) Depending on this Type 1 error rate, the critical value c is calculated. For example, if we select an error rate of 1%, c is calculated thus:\n\n  \n    \n      \n        P\n        (\n        \n          reject \n        \n        \n          H\n          \n            0\n          \n        \n        \u2223\n        \n          H\n          \n            0\n          \n        \n        \n           is valid\n        \n        )\n        =\n        P\n        (\n        X\n        \u2265\n        c\n        \u2223\n        p\n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n        )\n        \u2264\n        0\n        \n          .\n        \n        01.\n      \n    \n    {\\displaystyle P({\\text{reject }}H_{0}\\mid H_{0}{\\text{ is valid}})=P(X\\geq c\\mid p={\\tfrac {1}{4}})\\leq 0{.}01.}\n  From all the numbers c, with this property, we choose the smallest, in order to minimize the probability of a Type II error, a false negative. For the above example, we select: \n  \n    \n      \n        c\n        =\n        13\n      \n    \n    {\\displaystyle c=13}\n  .\n\n\n=== Radioactive suitcase ===\nAs an example, consider determining whether a suitcase contains some radioactive material. Placed under a Geiger counter, it produces 10 counts per minute. The null hypothesis is that no radioactive material is in the suitcase and that all measured counts are due to ambient radioactivity typical of the surrounding air and harmless objects. We can then calculate how likely it is that we would observe 10 counts per minute if the null hypothesis were true. If the null hypothesis predicts (say) on average 9 counts per minute, then according to the Poisson distribution typical for radioactive decay there is about 41% chance of recording 10 or more counts. Thus we can say that the suitcase is compatible with the null hypothesis (this does not guarantee that there is no radioactive material, just that we don't have enough evidence to suggest there is). On the other hand, if the null hypothesis predicts 3 counts per minute (for which the Poisson distribution predicts only 0.1% chance of recording 10 or more counts) then the suitcase is not compatible with the null hypothesis, and there are likely other factors responsible to produce the measurements.\nThe test does not directly assert the presence of radioactive material. A successful test asserts that the claim of no radioactive material present is unlikely given the reading (and therefore ...). The double negative (disproving the null hypothesis) of the method is confusing, but using a counter-example to disprove is standard mathematical practice. The attraction of the method is its practicality. We know (from experience) the expected range of counts with only ambient radioactivity present, so we can say that a measurement is unusually large. Statistics just formalizes the intuitive by using numbers instead of adjectives. We probably do not know the characteristics of the radioactive suitcases; We just assume\nthat they produce larger readings.\nTo slightly formalize intuition: radioactivity is suspected if the Geiger-count with the suitcase is among or exceeds the greatest (5% or 1%) of the Geiger-counts made with ambient radiation alone. This makes no assumptions about the distribution of counts. Many ambient radiation observations are required to obtain good probability estimates for rare events.\nThe test described here is more fully the null-hypothesis statistical significance test. The null hypothesis represents what we would believe by default, before seeing any evidence. Statistical significance is a possible finding of the test, declared when the observed sample is unlikely to have occurred by chance if the null hypothesis were true. The name of the test describes its formulation and its possible outcome. One characteristic of the test is its crisp decision: to reject or not reject the null hypothesis. A calculated value is compared to a threshold, which is determined from the tolerable risk of error.\n\n\n== Variations and sub-classes ==\nStatistical hypothesis testing is a key technique of both frequentist inference and Bayesian inference, although the two types of inference have notable differences. Statistical hypothesis tests define a procedure that controls (fixes) the probability of incorrectly deciding that a default position (null hypothesis) is incorrect. The procedure is based on how likely it would be for a set of observations to occur if the null hypothesis were true. This probability of making an incorrect decision is not the probability that the null hypothesis is true, nor whether any specific alternative hypothesis is true. This contrasts with other possible techniques of decision theory in which the null and alternative hypothesis are treated on a more equal basis.\nOne na\u00efve Bayesian approach to hypothesis testing is to base decisions on the posterior probability, but this fails when comparing point and continuous hypotheses. Other approaches to decision making, such as Bayesian decision theory, attempt to balance the consequences of incorrect decisions across all possibilities, rather than concentrating on a single null hypothesis. A number of other approaches to reaching a decision based on data are available via decision theory and optimal decisions, some of which have desirable properties. Hypothesis testing, though, is a dominant approach to data analysis in many fields of science. Extensions to the theory of hypothesis testing include the study of the power of tests, i.e. the probability of correctly rejecting the null hypothesis given that it is false. Such considerations can be used for the purpose of sample size determination prior to the collection of data.\n\n\n== Neyman\u2013Pearson hypothesis testing ==\nAn example of Neyman\u2013Pearson hypothesis testing (or null hypothesis statistical significance testing) can be made by a change to the radioactive suitcase example. If the \"suitcase\" is actually a shielded container for the transportation of radioactive material, then a test might be used to select among three hypotheses: no radioactive source present, one present, two (all) present. The test could be required for safety, with actions required in each case. The Neyman\u2013Pearson lemma of hypothesis testing says that a good criterion for the selection of hypotheses is the ratio of their probabilities (a likelihood ratio). A simple method of solution is to select the hypothesis with the highest probability for the Geiger counts observed. The typical result matches intuition: few counts imply no source, many counts imply two sources and intermediate counts imply one source. Notice also that usually there are problems for proving a negative. Null hypotheses should be at least falsifiable.\nNeyman\u2013Pearson theory can accommodate both prior probabilities and the costs of actions resulting from decisions. The former allows each test to consider the results of earlier tests (unlike Fisher's significance tests). The latter allows the consideration of economic issues (for example) as well as probabilities. A likelihood ratio remains a good criterion for selecting among hypotheses.\nThe two forms of hypothesis testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman\u2013Pearson test is more like multiple choice. In the view of Tukey the former produces a conclusion on the basis of only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman\u2013Pearson). The major Neyman\u2013Pearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the optimality of the (Student's) t-test, \"there can be no better test for the hypothesis under consideration\" (p 321). Neyman\u2013Pearson theory was proving the optimality of Fisherian methods from its inception.\nFisher's significance testing has proven a popular flexible statistical tool in application with little mathematical growth potential. Neyman\u2013Pearson hypothesis testing is claimed as a pillar of mathematical statistics, creating a new paradigm for the field. It also stimulated new applications in statistical process control, detection theory, decision theory and game theory. Both formulations have been successful, but the successes have been of a different character.\nThe dispute over formulations is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study Neyman\u2013Pearson theory in graduate school. Mathematicians are proud of uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible or complementary. The dispute has become more complex since Bayesian inference has achieved respectability.\nThe terminology is inconsistent. Hypothesis testing can mean any mixture of two formulations that both changed with time. Any discussion of significance testing vs hypothesis testing is doubly vulnerable to confusion.\nFisher thought that hypothesis testing was a useful strategy for performing industrial quality control, however, he strongly disagreed that hypothesis testing could be useful for scientists.\nHypothesis testing provides a means of finding test statistics used in significance testing. The concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination. The two methods remain philosophically distinct. They usually (but not always) produce the same mathematical answer. The preferred answer is context dependent. While the existing merger of Fisher and Neyman\u2013Pearson theories has been heavily criticized, modifying the merger to achieve Bayesian goals has been considered.\n\n\n== Criticism ==\n\nCriticism of statistical hypothesis testing fills volumes. Much of the criticism can be summarized by the following issues:\n\nThe interpretation of a p-value is dependent upon stopping rule and definition of multiple comparison. The former often changes during the course of a study and the latter is unavoidably ambiguous. (i.e. \"p values depend on both the (data) observed and on the other possible (data) that might have been observed but weren't\").\nConfusion resulting (in part) from combining the methods of Fisher and Neyman\u2013Pearson which are conceptually distinct.\nEmphasis on statistical significance to the exclusion of estimation and confirmation by repeated experiments.\nRigidly requiring statistical significance as a criterion for publication, resulting in publication bias. Most of the criticism is indirect. Rather than being wrong, statistical hypothesis testing is misunderstood, overused and misused.\nWhen used to detect whether a difference exists between groups, a paradox arises. As improvements are made to experimental design (e.g. increased precision of measurement and sample size), the test becomes more lenient. Unless one accepts the absurd assumption that all sources of noise in the data cancel out completely, the chance of finding statistical significance in either direction approaches 100%. However, this absurd assumption that the mean difference between two groups cannot be zero implies that the data cannot be independent and identically distributed (i.i.d.) because the expected difference between any two subgroups of i.i.d. random variates is zero; therefore, the i.i.d. assumption is also absurd.\nLayers of philosophical concerns. The probability of statistical significance is a function of decisions made by experimenters/analysts. If the decisions are based on convention they are termed arbitrary or mindless while those not so based may be termed subjective. To minimize type II errors, large samples are recommended. In psychology practically all null hypotheses are claimed to be false for sufficiently large samples so \"...it is usually nonsensical to perform an experiment with the sole aim of rejecting the null hypothesis.\" \"Statistically significant findings are often misleading\" in psychology. Statistical significance does not imply practical significance, and correlation does not imply causation. Casting doubt on the null hypothesis is thus far from directly supporting the research hypothesis.\n\"[I]t does not tell us what we want to know\". Lists of dozens of complaints are available.Critics and supporters are largely in factual agreement regarding the characteristics of null hypothesis significance testing (NHST): While it can provide critical information, it is inadequate as the sole tool for statistical analysis. Successfully rejecting the null hypothesis may offer no support for the research hypothesis. The continuing controversy concerns the selection of the best statistical practices for the near-term future given the existing practices. However, adequate research design can minimize this issue. Critics would prefer to ban NHST completely, forcing a complete departure from those practices, while supporters suggest a less absolute change.Controversy over significance testing, and its effects on publication bias in particular, has produced several results. The American Psychological Association has strengthened its statistical reporting requirements after review, medical journal publishers have recognized the obligation to publish some results that are not statistically significant to combat publication bias and a journal (Journal of Articles in Support of the Null Hypothesis) has been created to publish such results exclusively. Textbooks have added some cautions and increased coverage of the tools necessary to estimate the size of the sample required to produce significant results. Major organizations have not abandoned use of significance tests although some have discussed doing so.\n\n\n== Alternatives ==\n\nA unifying position of critics is that statistics should not lead to an accept-reject conclusion or decision, but to an estimated value with an interval estimate; this data-analysis philosophy is broadly referred to as estimation statistics. Estimation statistics can be accomplished with either frequentist [1] or Bayesian methods.One strong critic of significance testing suggested a list of reporting alternatives: effect sizes for importance, prediction intervals for confidence, replications and extensions for replicability, meta-analyses for generality. None of these suggested alternatives produces a conclusion/decision. Lehmann said that hypothesis testing theory can be presented in terms of conclusions/decisions, probabilities, or confidence intervals. \"The distinction between the ... approaches is largely one of reporting and interpretation.\"On one \"alternative\" there is no disagreement: Fisher himself said, \"In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.\" Cohen, an influential critic of significance testing, concurred, \"... don't look for a magic alternative to NHST [null hypothesis significance testing] ... It doesn't exist.\" \"... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication.\" The \"alternative\" to significance testing is repeated testing. The easiest way to decrease statistical uncertainty is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology. An indirect approach to replication is meta-analysis.\nBayesian inference is one proposed alternative to significance testing. (Nickerson cited 10 sources suggesting it, including Rozeboom (1960)). For example, Bayesian parameter estimation can provide rich information about the data from which researchers can draw inferences, while using uncertain priors that exert only minimal influence on the results when enough data is available. Psychologist John K. Kruschke has suggested Bayesian estimation as an alternative for the t-test and has also contrasted Bayesian estimation for assessing null values with Bayesian model comparison for hypothesis testing. Two competing models/hypotheses can be compared using Bayes factors. Bayesian methods could be criticized for requiring information that is seldom available in the cases where significance testing is most heavily used. Neither the prior probabilities nor the probability distribution of the test statistic under the alternative hypothesis are often available in the social sciences.Advocates of a Bayesian approach sometimes claim that the goal of a researcher is most often to objectively assess the probability that a hypothesis is true based on the data they have collected.  Neither Fisher's significance testing, nor Neyman\u2013Pearson hypothesis testing can provide this information, and do not claim to. The probability a hypothesis is true can only be derived from use of Bayes' Theorem, which was unsatisfactory to both the Fisher and Neyman\u2013Pearson camps due to the explicit use of subjectivity in the form of the prior probability. Fisher's strategy is to sidestep this with the p-value (an objective index based on the data alone) followed by inductive inference, while Neyman\u2013Pearson devised their approach of inductive behaviour.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nLehmann E.L. (1992) \"Introduction to Neyman and Pearson (1933) On the Problem of the Most Efficient Tests of Statistical Hypotheses\". In: Breakthroughs in Statistics, Volume 1, (Eds Kotz, S., Johnson, N.L.), Springer-Verlag. ISBN 0-387-94037-5 (followed by reprinting of the paper)\nNeyman, J.; Pearson, E.S. (1933). \"On the Problem of the Most Efficient Tests of Statistical Hypotheses\". Philosophical Transactions of the Royal Society A. 231 (694\u2013706): 289\u2013337. Bibcode:1933RSPTA.231..289N. doi:10.1098/rsta.1933.0009.\n\n\n== External links ==\n\n\"Statistical hypotheses, verification of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nWilson Gonz\u00e1lez, Georgina; Kay Sankaran (September 10, 1997). \"Hypothesis Testing\". Environmental Sampling & Monitoring Primer. Virginia Tech.\nBayesian critique of classical hypothesis testing\nCritique of classical hypothesis testing highlighting long-standing qualms of statisticians\nDallal GE (2007) The Little Handbook of Statistical Practice (A good tutorial)\nReferences for arguments for and against hypothesis testing\nStatistical Tests Overview: How to choose the correct statistical test\n[2] Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery; Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana\n\n\n=== Online calculators ===\nMBAStats confidence interval and hypothesis test calculators\nSome p-value and hypothesis test calculators.", "Arithmetic mean": "In mathematics and statistics, the arithmetic mean (  arr-ith-MET-ik), arithmetic average, or just the mean or average (when the context is clear), is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results from an experiment, an observational study, or a survey. The term \"arithmetic mean\" is preferred in some mathematics and statistics contexts because it helps distinguish it from other types of means, such as geometric and harmonic.\nIn addition to mathematics and statistics, the arithmetic mean is frequently used in economics, anthropology, history, and almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.\nWhile the arithmetic mean is often used to report central tendencies, it is not a robust statistic: it is greatly influenced by outliers (values much larger or smaller than most others). For skewed distributions, such as the distribution of income for which a few people's incomes are substantially higher than most people's, the arithmetic mean may not coincide with one's notion of \"middle\". In that case, robust statistics, such as the median, may provide a better description of central tendency.\n\n\n== Definition ==\nGiven a data set \n  \n    \n      \n        X\n        =\n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle X=\\{x_{1},\\ldots ,x_{n}\\}}\n  , the arithmetic mean (also mean or average), denoted \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   (read \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   bar), is the mean of the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   values \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n  .The arithmetic mean is a data set's most commonly used and readily understood measure of central tendency. In statistics, the term average refers to any measurement of central tendency. The arithmetic mean of a set of observed data is equal to the sum of the numerical values of each observation, divided by the total number of observations. Symbolically, for a data set consisting of the values \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\dots ,x_{n}}\n  , the arithmetic mean is defined by the formula:\n\n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          (\n          \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              \n                x\n                \n                  i\n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \n                x\n                \n                  2\n                \n              \n              +\n              \u22ef\n              +\n              \n                x\n                \n                  n\n                \n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}={\\frac {1}{n}}\\left(\\sum _{i=1}^{n}{x_{i}}\\right)={\\frac {x_{1}+x_{2}+\\dots +x_{n}}{n}}}\n  (For an explanation of the summation operator, see summation.)\nFor example, if the monthly salaries of \n  \n    \n      \n        10\n      \n    \n    {\\displaystyle 10}\n   employees are \n  \n    \n      \n        {\n        2500\n        ,\n        2700\n        ,\n        2400\n        ,\n        2300\n        ,\n        2550\n        ,\n        2650\n        ,\n        2750\n        ,\n        2450\n        ,\n        2600\n        ,\n        2400\n        }\n      \n    \n    {\\displaystyle \\{2500,2700,2400,2300,2550,2650,2750,2450,2600,2400\\}}\n  , then the arithmetic mean is:\n\n  \n    \n      \n        \n          \n            \n              2500\n              +\n              2700\n              +\n              2400\n              +\n              2300\n              +\n              2550\n              +\n              2650\n              +\n              2750\n              +\n              2450\n              +\n              2600\n              +\n              2400\n            \n            10\n          \n        \n        =\n        2530\n      \n    \n    {\\displaystyle {\\frac {2500+2700+2400+2300+2550+2650+2750+2450+2600+2400}{10}}=2530}\n  If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean and denoted by the Greek letter \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  . If the data set is a statistical sample (a subset of the population), it is called the sample mean (which for a data set \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is denoted as \n  \n    \n      \n        \n          \n            X\n            \u00af\n          \n        \n      \n    \n    {\\displaystyle {\\overline {X}}}\n  ).\nThe arithmetic mean can be similarly defined for vectors in multiple dimensions, not only scalar values; this is often referred to as a centroid. More generally, because the arithmetic mean is a convex combination (meaning its coefficients sum to \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  ), it can be defined on a convex space, not only a vector space.\n\n\n== Motivating properties ==\nThe arithmetic mean has several properties that make it interesting, especially as a measure of central tendency. These include:\n\nIf numbers \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\dotsc ,x_{n}}\n   have mean \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n  , then \n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        )\n        +\n        \u22ef\n        +\n        (\n        \n          x\n          \n            n\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle (x_{1}-{\\bar {x}})+\\dotsb +(x_{n}-{\\bar {x}})=0}\n  . Since \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i}-{\\bar {x}}}\n   is the distance from a given number to the mean, one way to interpret this property is by saying that the numbers to the left of the mean are balanced by the numbers to the right. The mean is the only number for which the residuals (deviations from the estimate) sum to zero.  This can also be interpretted as saying that the mean is translationally invariant in the sense that for any real number \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , \n  \n    \n      \n        \n          \n            \n              x\n              +\n              a\n            \n            \u00af\n          \n        \n        =\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        +\n        a\n      \n    \n    {\\displaystyle {\\overline {x+a}}={\\bar {x}}+a}\n  .\nIf it is required to use a single number as a \"typical\" value for a set of known numbers \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\dotsc ,x_{n}}\n  , then the arithmetic mean of the numbers does this best since it minimizes the sum of squared deviations from the typical value: the sum of \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (x_{i}-{\\bar {x}})^{2}}\n  . The sample mean is also the best single predictor because it has the lowest root mean squared error. If the arithmetic mean of a population of numbers is desired, then the estimate of it that is unbiased is the arithmetic mean of a sample drawn from the population.The arithmetic mean is independent of scale of the units of measurement, in the sense that \n  \n    \n      \n        \n          avg\n        \n        (\n        c\n        \n          a\n          \n            1\n          \n        \n        ,\n        \u22ef\n        ,\n        c\n        \n          a\n          \n            n\n          \n        \n        )\n        =\n        c\n        \u22c5\n        \n          avg\n        \n        (\n        \n          a\n          \n            1\n          \n        \n        ,\n        \u22ef\n        ,\n        \n          a\n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\text{avg}}(ca_{1},\\cdots ,ca_{n})=c\\cdot {\\text{avg}}(a_{1},\\cdots ,a_{n}).}\n    So, for example, calculating a mean of liters and then converting to gallons is the same as converting to gallons first and then calculating the mean.  This is also called first order homogeneity.\n\n\n=== Additional properties ===\nThe arithmetic mean of a sample is always between the largest and smallest values in that sample.\nThe arithmetic mean of any amount of equal-sized number groups together is the arithmetic mean of the arithmetic means of each group.\n\n\n== Contrast with median ==\n\nThe arithmetic mean may be contrasted with the median. The median is defined such that no more than half the values are larger, and no more than half are smaller than it. If elements in the data increase arithmetically when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample \n  \n    \n      \n        {\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n        }\n      \n    \n    {\\displaystyle \\{1,2,3,4\\}}\n  . The mean is \n  \n    \n      \n        2.5\n      \n    \n    {\\displaystyle 2.5}\n  , as is the median. However, when we consider a sample that cannot be arranged to increase arithmetically, such as \n  \n    \n      \n        {\n        1\n        ,\n        2\n        ,\n        4\n        ,\n        8\n        ,\n        16\n        }\n      \n    \n    {\\displaystyle \\{1,2,4,8,16\\}}\n  , the median and arithmetic average can differ significantly. In this case, the arithmetic average is \n  \n    \n      \n        6.2\n      \n    \n    {\\displaystyle 6.2}\n  , while the median is \n  \n    \n      \n        4\n      \n    \n    {\\displaystyle 4}\n  . The average value can vary considerably from most values in the sample and can be larger or smaller than most.\nThere are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.\n\n\n== Generalizations ==\n\n\n=== Weighted average ===\n\nA weighted average, or weighted mean, is an average in which some data points count more heavily than others in that they are given more weight in the calculation. For example, the arithmetic mean of \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n   and \n  \n    \n      \n        5\n      \n    \n    {\\displaystyle 5}\n   is \n  \n    \n      \n        \n          \n            \n              3\n              +\n              5\n            \n            2\n          \n        \n        =\n        4\n      \n    \n    {\\displaystyle {\\frac {3+5}{2}}=4}\n  , or equivalently \n  \n    \n      \n        3\n        \n          \n            1\n            2\n          \n        \n        +\n        5\n        \n          \n            1\n            2\n          \n        \n        =\n        4\n      \n    \n    {\\displaystyle 3{\\frac {1}{2}}+5{\\frac {1}{2}}=4}\n  . In contrast, a weighted mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as \n  \n    \n      \n        3\n        \n          \n            2\n            3\n          \n        \n        +\n        5\n        \n          \n            1\n            3\n          \n        \n        =\n        \n          \n            11\n            3\n          \n        \n      \n    \n    {\\displaystyle 3{\\frac {2}{3}}+5{\\frac {1}{3}}={\\frac {11}{3}}}\n  . Here the weights, which necessarily sum to one, are \n  \n    \n      \n        \n          \n            2\n            3\n          \n        \n      \n    \n    {\\displaystyle {\\frac {2}{3}}}\n   and \n  \n    \n      \n        \n          \n            1\n            3\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{3}}}\n  , the former being twice the latter. The arithmetic mean (sometimes called the \"unweighted average\" or \"equally weighted average\") can be interpreted as a special case of a weighted average in which all weights are equal to the same number (\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{2}}}\n   in the above example and \n  \n    \n      \n        \n          \n            1\n            n\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{n}}}\n   in a situation with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   numbers being averaged).\n\n\n=== Continuous probability distributions ===\n\nIf a numerical property, and any sample of data from it, can take on any value from a continuous range instead of, for example, just integers, then the probability of a number falling into some range of possible values can be described by integrating a continuous probability distribution across this range, even when the naive probability for a sample number taking one certain value from infinitely many is zero. In this context, the analog of a weighted average, in which there are infinitely many possibilities for the precise value of the variable in each range, is called the mean of the probability distribution. The most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the median mentioned above and the mode (the three Ms), are equal. This equality does not hold for other probability distributions, as illustrated for the log-normal distribution here.\n\n\n=== Angles ===\n\nParticular care is needed when using cyclic data, such as phases or angles. Taking the arithmetic mean of 1\u00b0 and 359\u00b0 yields a result of 180\u00b0.\nThis is incorrect for two reasons:\n\nFirstly, angle measurements are only defined up to an additive constant of 360\u00b0 (\n  \n    \n      \n        2\n        \u03c0\n      \n    \n    {\\displaystyle 2\\pi }\n   or \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  , if measuring in radians). Thus, these could easily be called 1\u00b0 and -1\u00b0, or 361\u00b0 and 719\u00b0, since each one of them produces a different average.\nSecondly, in this situation, 0\u00b0 (or 360\u00b0) is geometrically a better average value: there is lower dispersion about it (the points are both 1\u00b0 from it and 179\u00b0 from 180\u00b0, the putative average).In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (that is, define the mean as the central point: the point about which one has the lowest dispersion) and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1\u00b0 and 359\u00b0 is 2\u00b0, not 358\u00b0).\n\n\n== Symbols and encoding ==\nThe arithmetic mean is often denoted by a bar (vinculum or macron), as in \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n  .Some software (text processors, web browsers) may not display the \"x\u0304\" symbol correctly. For example, the HTML symbol \"x\u0304\" combines two codes \u2014 the base letter \"x\" plus a code for the line above (\u0304 or \u00af).In some document formats (such as PDF), the symbol may be replaced by a \"\u00a2\" (cent) symbol when copied to a text processor such as Microsoft Word.\n\n\n== See also ==\n\nFr\u00e9chet mean\nGeneralized mean\nGeometric mean\nHarmonic mean\nInequality of arithmetic and geometric means\nSample mean and covariance\nStandard deviation\nStandard error of the mean\nSummary statistics\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nHuff, Darrell (1993). How to Lie with Statistics. W. W. Norton. ISBN 978-0-393-31072-6.\n\n\n== External links ==\nCalculations and comparisons between arithmetic mean and geometric mean of two numbers\nCalculate the arithmetic mean of a series of numbers on fxSolver", "Correlation and dependence": "In statistics, correlation  or dependence  is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of variables are linearly related.  \nFamiliar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve.\nCorrelations are useful because they can indicate a predictive relationship that can be exploited in practice.  For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).\nFormally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence.  In informal parlance, correlation is synonymous with dependence. However, when used in a technical sense, correlation refers to any of several specific types of mathematical operations between the tested variables and their respective expected values. Essentially, correlation is the measure of how two or more variables are related to one another.  There are several correlation coefficients, often denoted \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   or \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , measuring the degree of correlation.  The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other).  Other correlation coefficients \u2013 such as Spearman's rank correlation \u2013 have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables.\n\n\n== Pearson's product-moment coefficient ==\n\nThe most familiar measure of dependence between two quantities is the Pearson product-moment correlation coefficient (PPMCC), or \"Pearson's correlation coefficient\", commonly called simply \"the correlation coefficient\". It is obtained by taking the ratio of the covariance of the two variables in question of our numerical dataset, normalized to the square root of their variances.   Mathematically, one simply divides the covariance of the two variables by the product of their standard deviations. Karl Pearson developed the coefficient from a similar but slightly different idea by Francis Galton.A Pearson product-moment correlation coefficient attempts to establish a line of best fit through a dataset of two variables by essentially laying out the expected values and the resulting Pearson's correlation coefficient indicates how far away the actual dataset is from the expected values.  Depending on the sign of our Pearson's correlation coefficient, we can end up with either a negative or positive correlation if there is any sort of relationship between the variables of our data set.\nThe population correlation coefficient \n  \n    \n      \n        \n          \u03c1\n          \n            X\n            ,\n            Y\n          \n        \n      \n    \n    {\\displaystyle \\rho _{X,Y}}\n   between two random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   with expected values \n  \n    \n      \n        \n          \u03bc\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\mu _{X}}\n   and \n  \n    \n      \n        \n          \u03bc\n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle \\mu _{Y}}\n   and standard deviations \n  \n    \n      \n        \n          \u03c3\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{X}}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{Y}}\n   is defined as:\n\nwhere \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle \\operatorname {E} }\n   is the expected value operator, \n  \n    \n      \n        cov\n      \n    \n    {\\displaystyle \\operatorname {cov} }\n   means covariance, and \n  \n    \n      \n        corr\n      \n    \n    {\\displaystyle \\operatorname {corr} }\n   is a widely used alternative notation for the correlation coefficient. The Pearson correlation is defined only if both standard deviations are finite and positive. An alternative formula purely in terms of moments is:\n\n\n=== Correlation and independence ===\nIt is a corollary of the Cauchy\u2013Schwarz inequality that the absolute value of the Pearson correlation coefficient is not bigger than 1. Therefore, the value of a correlation coefficient ranges between \u22121 and +1. The correlation coefficient is +1 in the case of a perfect direct (increasing) linear relationship (correlation), \u22121 in the case of a perfect inverse (decreasing) linear relationship (anti-correlation), and some value in the open interval \n  \n    \n      \n        (\n        \u2212\n        1\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (-1,1)}\n   in all other cases, indicating the degree of linear dependence between the variables. As it approaches zero there is less of a relationship (closer to uncorrelated). The closer the coefficient is to either \u22121 or 1, the stronger the correlation between the variables.\nIf the variables are independent, Pearson's correlation coefficient is 0, but the converse is not true because the correlation coefficient detects only linear dependencies between two variables.\n\nFor example, suppose the random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is symmetrically distributed about zero, and \n  \n    \n      \n        Y\n        =\n        \n          X\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle Y=X^{2}}\n  . Then \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is completely determined by \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , so that \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are perfectly dependent, but their correlation is zero; they are uncorrelated. However, in the special case when \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are jointly normal, uncorrelatedness is equivalent to independence.\nEven though uncorrelated data does not necessarily imply independence, one can check if random variables are independent if their mutual information is 0.\n\n\n=== Sample correlation coefficient ===\nGiven a series of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   measurements of the pair \n  \n    \n      \n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          Y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (X_{i},Y_{i})}\n   indexed by \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ldots ,n}\n  , the sample correlation coefficient can be used to estimate the population Pearson correlation \n  \n    \n      \n        \n          \u03c1\n          \n            X\n            ,\n            Y\n          \n        \n      \n    \n    {\\displaystyle \\rho _{X,Y}}\n   between \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .  The sample correlation coefficient is defined as\n\n  \n    \n      \n        \n          r\n          \n            x\n            y\n          \n        \n        \n        \n          \n            =\n            \n              \n              \n                d\n                e\n                f\n              \n            \n          \n        \n        \n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    y\n                    \u00af\n                  \n                \n              \n              )\n            \n            \n              (\n              n\n              \u2212\n              1\n              )\n              \n                s\n                \n                  x\n                \n              \n              \n                s\n                \n                  y\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    y\n                    \u00af\n                  \n                \n              \n              )\n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                y\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    y\n                    \u00af\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle r_{xy}\\quad {\\overset {\\underset {\\mathrm {def} }{}}{=}}\\quad {\\frac {\\sum \\limits _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{(n-1)s_{x}s_{y}}}={\\frac {\\sum \\limits _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sqrt {\\sum \\limits _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}\\sum \\limits _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}},}\n  where \n  \n    \n      \n        \n          \n            x\n            \u00af\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}}\n   and \n  \n    \n      \n        \n          \n            y\n            \u00af\n          \n        \n      \n    \n    {\\displaystyle {\\overline {y}}}\n   are the sample means of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , and \n  \n    \n      \n        \n          s\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle s_{x}}\n   and \n  \n    \n      \n        \n          s\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle s_{y}}\n   are the corrected sample standard deviations of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\nEquivalent expressions for \n  \n    \n      \n        \n          r\n          \n            x\n            y\n          \n        \n      \n    \n    {\\displaystyle r_{xy}}\n   are\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  r\n                  \n                    x\n                    y\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \u2211\n                      \n                        x\n                        \n                          i\n                        \n                      \n                      \n                        y\n                        \n                          i\n                        \n                      \n                      \u2212\n                      n\n                      \n                        \n                          \n                            x\n                            \u00af\n                          \n                        \n                      \n                      \n                        \n                          \n                            y\n                            \u00af\n                          \n                        \n                      \n                    \n                    \n                      n\n                      \n                        s\n                        \n                          x\n                        \n                        \u2032\n                      \n                      \n                        s\n                        \n                          y\n                        \n                        \u2032\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      n\n                      \u2211\n                      \n                        x\n                        \n                          i\n                        \n                      \n                      \n                        y\n                        \n                          i\n                        \n                      \n                      \u2212\n                      \u2211\n                      \n                        x\n                        \n                          i\n                        \n                      \n                      \u2211\n                      \n                        y\n                        \n                          i\n                        \n                      \n                    \n                    \n                      \n                        \n                          n\n                          \u2211\n                          \n                            x\n                            \n                              i\n                            \n                            \n                              2\n                            \n                          \n                          \u2212\n                          (\n                          \u2211\n                          \n                            x\n                            \n                              i\n                            \n                          \n                          \n                            )\n                            \n                              2\n                            \n                          \n                        \n                      \n                       \n                      \n                        \n                          n\n                          \u2211\n                          \n                            y\n                            \n                              i\n                            \n                            \n                              2\n                            \n                          \n                          \u2212\n                          (\n                          \u2211\n                          \n                            y\n                            \n                              i\n                            \n                          \n                          \n                            )\n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}r_{xy}&={\\frac {\\sum x_{i}y_{i}-n{\\bar {x}}{\\bar {y}}}{ns'_{x}s'_{y}}}\\\\[5pt]&={\\frac {n\\sum x_{i}y_{i}-\\sum x_{i}\\sum y_{i}}{{\\sqrt {n\\sum x_{i}^{2}-(\\sum x_{i})^{2}}}~{\\sqrt {n\\sum y_{i}^{2}-(\\sum y_{i})^{2}}}}}.\\end{aligned}}}\n  where \n  \n    \n      \n        \n          s\n          \n            x\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle s'_{x}}\n   and \n  \n    \n      \n        \n          s\n          \n            y\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle s'_{y}}\n   are the uncorrected sample standard deviations of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\nIf \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   are results of measurements that contain measurement error, the realistic limits on the correlation coefficient are not \u22121 to +1 but a smaller range. For the case of a linear model with a single independent variable, the coefficient of determination (R squared) is the square of \n  \n    \n      \n        \n          r\n          \n            x\n            y\n          \n        \n      \n    \n    {\\displaystyle r_{xy}}\n  , Pearson's product-moment coefficient.\n\n\n== Example ==\nConsider the joint probability distribution of X and Y given in the table below.\n\nFor this joint distribution, the marginal distributions are:\n\n  \n    \n      \n        \n          P\n        \n        (\n        X\n        =\n        x\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  \n                  \n                    for \n                  \n                  x\n                  =\n                  0\n                \n              \n              \n                \n                  \n                    \n                      2\n                      3\n                    \n                  \n                \n                \n                  \n                  \n                    for \n                  \n                  x\n                  =\n                  1\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {P} (X=x)={\\begin{cases}{\\frac {1}{3}}&\\quad {\\text{for }}x=0\\\\{\\frac {2}{3}}&\\quad {\\text{for }}x=1\\end{cases}}}\n  \n  \n    \n      \n        \n          P\n        \n        (\n        Y\n        =\n        y\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  \n                  \n                    for \n                  \n                  y\n                  =\n                  \u2212\n                  1\n                \n              \n              \n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  \n                  \n                    for \n                  \n                  y\n                  =\n                  0\n                \n              \n              \n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  \n                  \n                    for \n                  \n                  y\n                  =\n                  1\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {P} (Y=y)={\\begin{cases}{\\frac {1}{3}}&\\quad {\\text{for }}y=-1\\\\{\\frac {1}{3}}&\\quad {\\text{for }}y=0\\\\{\\frac {1}{3}}&\\quad {\\text{for }}y=1\\end{cases}}}\n  This yields the following expectations and variances:\n\n  \n    \n      \n        \n          \u03bc\n          \n            X\n          \n        \n        =\n        \n          \n            2\n            3\n          \n        \n      \n    \n    {\\displaystyle \\mu _{X}={\\frac {2}{3}}}\n  \n\n  \n    \n      \n        \n          \u03bc\n          \n            Y\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mu _{Y}=0}\n  \n\n  \n    \n      \n        \n          \u03c3\n          \n            X\n          \n          \n            2\n          \n        \n        =\n        \n          \n            2\n            9\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{X}^{2}={\\frac {2}{9}}}\n  \n\n  \n    \n      \n        \n          \u03c3\n          \n            Y\n          \n          \n            2\n          \n        \n        =\n        \n          \n            2\n            3\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{Y}^{2}={\\frac {2}{3}}}\n  Therefore:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03c1\n                  \n                    X\n                    ,\n                    Y\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      \n                        \u03c3\n                        \n                          X\n                        \n                      \n                      \n                        \u03c3\n                        \n                          Y\n                        \n                      \n                    \n                  \n                \n                \n                  E\n                \n                [\n                (\n                X\n                \u2212\n                \n                  \u03bc\n                  \n                    X\n                  \n                \n                )\n                (\n                Y\n                \u2212\n                \n                  \u03bc\n                  \n                    Y\n                  \n                \n                )\n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      \n                        \u03c3\n                        \n                          X\n                        \n                      \n                      \n                        \u03c3\n                        \n                          Y\n                        \n                      \n                    \n                  \n                \n                \n                  \u2211\n                  \n                    x\n                    ,\n                    y\n                  \n                \n                \n                  (\n                  x\n                  \u2212\n                  \n                    \u03bc\n                    \n                      X\n                    \n                  \n                  )\n                  (\n                  y\n                  \u2212\n                  \n                    \u03bc\n                    \n                      Y\n                    \n                  \n                  )\n                  \n                    P\n                  \n                  (\n                  X\n                  =\n                  x\n                  ,\n                  Y\n                  =\n                  y\n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  (\n                  \n                    1\n                    \u2212\n                    \n                      \n                        2\n                        3\n                      \n                    \n                  \n                  )\n                \n                (\n                \u2212\n                1\n                \u2212\n                0\n                )\n                \n                  \n                    1\n                    3\n                  \n                \n                +\n                \n                  (\n                  \n                    0\n                    \u2212\n                    \n                      \n                        2\n                        3\n                      \n                    \n                  \n                  )\n                \n                (\n                0\n                \u2212\n                0\n                )\n                \n                  \n                    1\n                    3\n                  \n                \n                +\n                \n                  (\n                  \n                    1\n                    \u2212\n                    \n                      \n                        2\n                        3\n                      \n                    \n                  \n                  )\n                \n                (\n                1\n                \u2212\n                0\n                )\n                \n                  \n                    1\n                    3\n                  \n                \n                =\n                0.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho _{X,Y}&={\\frac {1}{\\sigma _{X}\\sigma _{Y}}}\\mathrm {E} [(X-\\mu _{X})(Y-\\mu _{Y})]\\\\[5pt]&={\\frac {1}{\\sigma _{X}\\sigma _{Y}}}\\sum _{x,y}{(x-\\mu _{X})(y-\\mu _{Y})\\mathrm {P} (X=x,Y=y)}\\\\[5pt]&=\\left(1-{\\frac {2}{3}}\\right)(-1-0){\\frac {1}{3}}+\\left(0-{\\frac {2}{3}}\\right)(0-0){\\frac {1}{3}}+\\left(1-{\\frac {2}{3}}\\right)(1-0){\\frac {1}{3}}=0.\\end{aligned}}}\n  \n\n\n== Rank correlation coefficients ==\n\nRank correlation coefficients, such as Spearman's rank correlation coefficient and Kendall's rank correlation coefficient (\u03c4) measure the extent to which, as one variable increases, the other variable tends to increase, without requiring that increase to be represented by a linear relationship. If, as the one variable increases, the other decreases, the rank correlation coefficients will be negative. It is common to regard these rank correlation coefficients as alternatives to Pearson's coefficient, used either to reduce the amount of calculation or to make the coefficient less sensitive to non-normality in distributions. However, this view has little mathematical basis, as rank correlation coefficients measure a different type of relationship than the Pearson product-moment correlation coefficient, and are best seen as measures of a different type of association, rather than as an alternative measure of the population correlation coefficient.To illustrate the nature of rank correlation, and its difference from linear correlation, consider the following four pairs of numbers \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  :\n\n(0, 1), (10, 100), (101, 500), (102, 2000).As we go from each pair to the next pair \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   increases, and so does \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . This relationship is perfect, in the sense that an increase in \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is always accompanied by an increase in \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . This means that we have a perfect rank correlation, and both Spearman's and Kendall's correlation coefficients are 1, whereas in this example Pearson product-moment correlation coefficient is 0.7544, indicating that the points are far from lying on a straight line. In the same way if \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   always decreases when \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   increases, the rank correlation coefficients will be \u22121, while the Pearson product-moment correlation coefficient may or may not be close to \u22121, depending on how close the points are to a straight line.  Although in the extreme cases of perfect rank correlation the two coefficients are both equal (being both +1 or both \u22121), this is not generally the case, and so values of the two coefficients cannot meaningfully be compared. For example, for the three pairs (1, 1) (2, 3) (3, 2) Spearman's coefficient is 1/2, while Kendall's coefficient is 1/3.\n\n\n== Other measures of dependence among random variables ==\n\nThe information given by a correlation coefficient is not enough to define the dependence structure between random variables. The correlation coefficient completely defines the dependence structure only in very particular cases, for example when the distribution is a multivariate normal distribution. (See diagram above.) In the case of elliptical distributions it characterizes the (hyper-)ellipses of equal density; however, it does not completely characterize the dependence structure (for example, a multivariate t-distribution's degrees of freedom determine the level of tail dependence).\nDistance correlation was introduced to address the deficiency of Pearson's correlation that it can be zero for dependent random variables; zero distance correlation implies independence.\nThe Randomized Dependence Coefficient is a computationally efficient, copula-based measure of dependence between multivariate random variables. RDC is invariant with respect to non-linear scalings of random variables, is capable of discovering a wide range of functional association patterns and takes value zero at independence.\nFor two binary variables, the odds ratio measures their dependence, and takes range non-negative numbers, possibly infinity: \n  \n    \n      \n        [\n        0\n        ,\n        +\n        \u221e\n        ]\n      \n    \n    {\\displaystyle [0,+\\infty ]}\n  . Related statistics such as Yule's Y and Yule's Q normalize this to the correlation-like range \n  \n    \n      \n        [\n        \u2212\n        1\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [-1,1]}\n  . The odds ratio is generalized by the logistic model to model cases where the dependent variables are discrete and there may be one or more independent variables.\nThe correlation ratio, entropy-based mutual information, total correlation, dual total correlation and polychoric correlation are all also capable of detecting more general dependencies, as is consideration of the copula between them, while the coefficient of determination generalizes the correlation coefficient to multiple regression.\n\n\n== Sensitivity to the data distribution ==\n\nThe degree of dependence between variables X and Y does not depend on the scale on which the variables are expressed.   That is, if we are analyzing the relationship between X and Y, most correlation measures are unaffected by transforming X to a + bX and Y to c + dY, where a, b, c, and d are constants (b and d being positive).  This is true of some correlation statistics as well as their population analogues. Some correlation statistics, such as the rank correlation coefficient, are also invariant to monotone transformations of the marginal distributions of X and/or Y.\n\nMost correlation measures are sensitive to the manner in which X and Y are sampled.  Dependencies tend to be stronger if viewed over a wider range of values. Thus, if we consider the correlation coefficient between the heights of fathers and their sons over all adult males, and compare it to the same correlation coefficient calculated when the fathers are selected to be between 165 cm and 170 cm in height, the correlation will be weaker in the latter case. Several techniques have been developed that attempt to correct for range restriction in one or both variables, and are commonly used in meta-analysis; the most common are Thorndike's case II and case III equations.Various correlation measures in use may be undefined for certain joint distributions of X and Y.  For example, the Pearson correlation coefficient is defined in terms of moments, and hence will be undefined if the moments are undefined.  Measures of dependence based on quantiles are always defined.  Sample-based statistics intended to estimate population measures of dependence may or may not have desirable statistical properties such as being unbiased, or asymptotically consistent, based on the spatial structure of the population from which the data were sampled.\nSensitivity to the data distribution can be used to an advantage. For example, scaled correlation is designed to use the sensitivity to the range in order to pick out correlations between fast components of time series. By reducing the range of values in a controlled manner, the correlations on long time scale are filtered out and only the correlations on short time scales are revealed.\n\n\n== Correlation matrices ==\nThe correlation matrix of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   random variables \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\ldots ,X_{n}}\n   is the \n  \n    \n      \n        n\n        \u00d7\n        n\n      \n    \n    {\\displaystyle n\\times n}\n   matrix \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   whose \n  \n    \n      \n        (\n        i\n        ,\n        j\n        )\n      \n    \n    {\\displaystyle (i,j)}\n   entry is\n\n  \n    \n      \n        \n          c\n          \n            i\n            j\n          \n        \n        :=\n        corr\n        \u2061\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          X\n          \n            j\n          \n        \n        )\n        =\n        \n          \n            \n              cov\n              \u2061\n              (\n              \n                X\n                \n                  i\n                \n              \n              ,\n              \n                X\n                \n                  j\n                \n              \n              )\n            \n            \n              \n                \u03c3\n                \n                  \n                    X\n                    \n                      i\n                    \n                  \n                \n              \n              \n                \u03c3\n                \n                  \n                    X\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n        ,\n        \n        \n          if\n        \n         \n        \n          \u03c3\n          \n            \n              X\n              \n                i\n              \n            \n          \n        \n        \n          \u03c3\n          \n            \n              X\n              \n                j\n              \n            \n          \n        \n        >\n        0.\n      \n    \n    {\\displaystyle c_{ij}:=\\operatorname {corr} (X_{i},X_{j})={\\frac {\\operatorname {cov} (X_{i},X_{j})}{\\sigma _{X_{i}}\\sigma _{X_{j}}}},\\quad {\\text{if}}\\ \\sigma _{X_{i}}\\sigma _{X_{j}}>0.}\n  Thus the diagonal entries are all identically one.  If the measures of correlation used are product-moment coefficients, the correlation matrix is the same as the covariance matrix of the standardized random variables \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        \n          /\n        \n        \u03c3\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle X_{i}/\\sigma (X_{i})}\n   for \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  .  This applies both to the matrix of population correlations (in which case \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is the population standard deviation), and to the matrix of sample correlations (in which case \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   denotes the sample standard deviation). Consequently, each is necessarily a positive-semidefinite matrix. Moreover, the correlation matrix is strictly positive definite if no variable can have all its values exactly generated as a linear function of the values of the others.\nThe correlation matrix is symmetric because the correlation between \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n   and \n  \n    \n      \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{j}}\n   is the same as the correlation between \n  \n    \n      \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{j}}\n   and \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  .\nA correlation matrix appears, for example, in one formula for the coefficient of multiple determination, a measure of goodness of fit in multiple regression.\nIn statistical modelling, correlation matrices representing the relationships between variables are categorized into different correlation structures, which are distinguished by factors such as the number of parameters required to estimate them. For example, in an exchangeable correlation matrix, all pairs of variables are modeled as having the same correlation, so all non-diagonal elements of the matrix are equal to each other. On the other hand, an autoregressive matrix is often used when variables represent a time series, since correlations are likely to be greater when measurements are closer in time. Other examples include independent, unstructured, M-dependent, and Toeplitz.\nIn exploratory data analysis, the iconography of correlations consists in replacing a correlation matrix by a diagram where the \u201cremarkable\u201d correlations are represented by a solid line (positive correlation), or a dotted line (negative correlation).\n\n\n=== Nearest valid correlation matrix ===\nIn some applications (e.g., building data models from only partially observed data) one wants to find the \"nearest\" correlation matrix to an \"approximate\" correlation matrix (e.g., a matrix which typically lacks semi-definite positiveness due to the way it has been computed).\nIn 2002, Higham formalized the notion of nearness using the Frobenius norm and provided a method for computing the nearest correlation matrix using the Dykstra's projection algorithm, of which an implementation is available as an online Web API.This sparked interest in the subject, with new theoretical (e.g., computing the nearest correlation matrix with factor structure) and numerical (e.g. usage the Newton's method for computing the nearest correlation matrix) results obtained in the subsequent years.\n\n\n== Uncorrelatedness and independence of stochastic processes ==\nSimilarly for two stochastic processes \n  \n    \n      \n        \n          \n            {\n            \n              X\n              \n                t\n              \n            \n            }\n          \n          \n            t\n            \u2208\n            \n              \n                T\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\left\\{X_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n   and \n  \n    \n      \n        \n          \n            {\n            \n              Y\n              \n                t\n              \n            \n            }\n          \n          \n            t\n            \u2208\n            \n              \n                T\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\left\\{Y_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n  : If they are independent, then they are uncorrelated.:\u200ap. 151\u200a The opposite of this statement might not be true. Even if two variables are uncorrelated, they might not be independent to each other.\n\n\n== Common misconceptions ==\n\n\n=== Correlation and causality ===\n \nThe conventional dictum that \"correlation does not imply causation\" means that correlation cannot be used by itself to infer a causal relationship between the variables. This dictum should not be taken to mean that correlations cannot indicate the potential existence of causal relations. However, the causes underlying the correlation, if any, may be indirect and unknown, and high correlations also overlap with identity relations (tautologies), where no causal process exists. Consequently, a correlation between two variables is not a sufficient condition to establish a causal relationship (in either direction).\nA correlation between age and height in children is fairly causally transparent, but a correlation between mood and health in people is less so. Does improved mood lead to improved health, or does good health lead to good mood, or both? Or does some other factor underlie both? In other words, a correlation can be taken as evidence for a possible causal relationship, but cannot indicate what the causal relationship, if any, might be.\n\n\n=== Simple linear correlations ===\n\nThe Pearson correlation coefficient indicates the strength of a linear relationship between two variables, but its value generally does not completely characterize their relationship.  In particular, if the conditional mean of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , denoted \n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid X)}\n  , is not linear in \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , the correlation coefficient will not fully determine the form of \n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid X)}\n  .\nThe adjacent image shows scatter plots of Anscombe's quartet, a set of four different pairs of variables created by Francis Anscombe. The four \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   variables have the same mean (7.5), variance (4.12), correlation (0.816) and regression line (y = 3 + 0.5x). However, as can be seen on the plots, the distribution of the variables is very different. The first one (top left) seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one (top right) is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear. In this case the Pearson correlation coefficient does not indicate that there is an exact functional relationship: only the extent to which that relationship can be approximated by a linear relationship. In the third case (bottom left), the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.816. Finally, the fourth example (bottom right) shows another example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear.\nThese examples indicate that the correlation coefficient, as a summary statistic, cannot replace visual examination of the data. The examples are sometimes said to demonstrate that the Pearson correlation assumes that the data follow a normal distribution, but this is only partially correct. The Pearson correlation can be accurately calculated for any distribution that has a finite covariance matrix, which includes most distributions encountered in practice. However, the Pearson correlation coefficient (taken together with the sample mean and variance) is only a sufficient statistic if the data is drawn from a multivariate normal distribution. As a result, the Pearson correlation coefficient fully characterizes the relationship between variables if and only if the data are drawn from a multivariate normal distribution.\n\n\n== Bivariate normal distribution ==\nIf a pair \n  \n    \n      \n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle (X,Y)}\n   of random variables follows a bivariate normal distribution, the conditional mean \n  \n    \n      \n        E\n        \u2061\n        (\n        X\n        \u2223\n        Y\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (X\\mid Y)}\n   is a linear function of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , and the conditional mean \n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid X)}\n   is a linear function of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . The correlation coefficient \n  \n    \n      \n        \n          \u03c1\n          \n            X\n            ,\n            Y\n          \n        \n      \n    \n    {\\displaystyle \\rho _{X,Y}}\n   between \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , along with the marginal means and variances of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , determines this linear relationship:\n\n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        X\n        )\n        =\n        E\n        \u2061\n        (\n        Y\n        )\n        +\n        \n          \u03c1\n          \n            X\n            ,\n            Y\n          \n        \n        \u22c5\n        \n          \u03c3\n          \n            Y\n          \n        \n        \n          \n            \n              X\n              \u2212\n              E\n              \u2061\n              (\n              X\n              )\n            \n            \n              \u03c3\n              \n                X\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid X)=\\operatorname {E} (Y)+\\rho _{X,Y}\\cdot \\sigma _{Y}{\\frac {X-\\operatorname {E} (X)}{\\sigma _{X}}},}\n  where \n  \n    \n      \n        E\n        \u2061\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (X)}\n   and \n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (Y)}\n   are the expected values of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , respectively, and \n  \n    \n      \n        \n          \u03c3\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{X}}\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{Y}}\n   are the standard deviations of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , respectively. \n\nThe empirical correlation \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   is an estimate of the correlation coefficient \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  . A distribution estimate for \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   is given bywhere \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   is the Gaussian hypergeometric function and \n  \n    \n      \n        \u03bd\n        =\n        N\n        \u2212\n        1\n        >\n        1\n      \n    \n    {\\displaystyle \\nu =N-1>1}\n   . This density is both a Bayesian posterior density and an exact optimal  confidence distribution density.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nCohen, J.; Cohen P.; West, S.G. & Aiken, L.S. (2002). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.). Psychology Press. ISBN 978-0-8058-2223-6.\n\"Correlation (in statistics)\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nOestreicher, J. & D. R. (February 26, 2015). Plague of Equals: A science thriller of international disease, politics and drug discovery. California: Omega Cat Press. p. 408. ISBN 978-0963175540.\n\n\n== External links ==\n\nMathWorld page on the (cross-)correlation coefficient/s of a sample\nCompute significance between two correlations, for the comparison of two correlation values.\n\"A MATLAB Toolbox for computing Weighted Correlation Coefficients\". Archived from the original on 24 April 2021.\nProof that the Sample Bivariate Correlation has limits plus or minus 1\nInteractive Flash simulation on the correlation of two normally distributed variables by Juha Puranen.\nCorrelation analysis. Biomedical Statistics\nR-Psychologist Correlation visualization of correlation between two numeric variables", "Confusion matrix": "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one; in unsupervised learning it is usually called a matching matrix. \nEach row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa \u2013 both variants are found in the literature. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another).\nIt is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table).\n\n\n== Example ==\nGiven a sample of 12 individuals, 8 that have been diagnosed with cancer and 4 that are cancer-free, where individuals with cancer belong to class 1 (positive) and non-cancer individuals belong to class 0 (negative), we can display that data as follows:\n\nAssume that we have a classifier that distinguishes between individuals with and without cancer in some way, we can take the 12 individuals and run them through the classifier. The classifier then makes 9 accurate predictions and misses 3: 2 individuals with cancer wrongly predicted as being cancer-free (sample 1 and 2), and 1 person without cancer that is wrongly predicted to have cancer (sample 9).\n\nNotice, that if we compare the actual classification set to the predicted classification set, there are 4 different outcomes that could result in any particular column. One, if the actual classification is positive and the predicted classification is positive (1,1), this is called a true positive result because the positive sample was correctly identified by the classifier. Two, if the actual classification is positive and the predicted classification is negative (1,0), this is called a false negative result because the positive sample is incorrectly identified by the classifier as being negative. Third, if the actual classification is negative and the predicted classification is positive (0,1), this is called a false positive result because the negative sample is incorrectly identified by the classifier as being positive. Fourth, if the actual classification is negative and the predicted classification is negative (0,0), this is called a true negative result because the negative sample gets correctly identified by the classifier.\nWe can then perform the comparison between actual and predicted classifications and add this information to the table, making correct results appear in green so they are more easily identifiable.\n\nThe template for any binary confusion matrix uses the four kinds of results discussed above (true positives, false negatives, false positives, and true negatives) along with the positive and negative classifications. The four outcomes can be formulated in a 2\u00d72 confusion matrix, as follows:\n\nThe color convention of the three data tables above were picked to match this confusion matrix, in order to easily differentiate the data.\nNow, we can simply total up each type of result, substitute into the template, and create a confusion matrix that will concisely summarize the results of testing the classifier:\n\nIn this confusion matrix, of the 8 samples with cancer, the system judged that 2 were cancer-free, and of the 4 samples without cancer, it predicted that 1 did have cancer. All correct predictions are located in the diagonal of the table (highlighted in green), so it is easy to visually inspect the table for prediction errors, as values outside the diagonal will represent them. By summing up the 2 rows of the confusion matrix, one can also deduce the total number of positive (P) and negative (N) samples in the original dataset, i.e. \n  \n    \n      \n        P\n        =\n        T\n        P\n        +\n        F\n        N\n      \n    \n    {\\displaystyle P=TP+FN}\n   and \n  \n    \n      \n        N\n        =\n        F\n        P\n        +\n        T\n        N\n      \n    \n    {\\displaystyle N=FP+TN}\n  .\n\n\n== Table of confusion ==\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix) is a table with two rows and two columns that reports the number of true positives, false negatives, false positives, and true negatives. This allows more detailed analysis than simply observing the proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced; that is, when the numbers of observations in different classes vary greatly. \nFor example, if there were 95 cancer samples and only 5 non-cancer samples in the data, a particular classifier might classify all the observations as having cancer. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate (sensitivity) for the cancer class but a 0% recognition rate for the non-cancer class. F1 score is even more unreliable in such cases, and here would yield over 97.4%, whereas informedness removes such bias and yields 0 as the probability of an informed decision for any form of guessing (here always guessing cancer).\nAccording to Davide Chicco and Giuseppe Jurman, the most informative metric to evaluate a confusion matrix is the Matthews correlation coefficient (MCC).Other metrics can be included in a confusion matrix, each of them having their significance and use.\n\n\n== Confusion matrices with more than two categories ==\nConfusion matrix is not limited to binary classification and can be used in multi-class classifiers as well. The confusion matrices discussed above have only two conditions: positive and negative. For example, the table below summarizes communication of a whistled language between two speakers, zero values omitted for clarity.\n\n\n== See also ==\nPositive and negative predictive values\n\n\n== References ==", "Sample (statistics)": "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population in question. Sampling has lower costs and faster data collection than measuring the entire population and can provide insights in cases where it is infeasible to measure an entire population. \nEach observation measures one or more properties (such as weight, location, colour or mass) of independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. Acceptance sampling is used to determine if a production lot of material meets the governing specifications.\n\n\n== Population definition ==\nSuccessful statistical practice is based on focused problem definition. In sampling, this includes defining the \"population\" from which our sample is drawn. A population can be defined as including all people or items with the characteristics one wishes to understand. Because there is very rarely enough time or money to gather information from everyone or everything in a population, the goal becomes finding a representative sample (or subset) of that population.\nSometimes what defines a population is obvious. For example, a manufacturer needs to decide whether a batch of material from production is of high enough quality to be released to the customer or should be scrapped or reworked due to poor quality. In this case, the batch is the population.\nAlthough the population of interest often consists of physical objects, sometimes it is necessary to sample over time, space, or some combination of these dimensions. For instance, an investigation of supermarket staffing could examine checkout line length at various times, or a study on endangered penguins might aim to understand their usage of various hunting grounds over time. For the time dimension, the focus may be on periods or discrete occasions.\nIn other cases, the examined 'population' may be even less tangible. For example, Joseph Jagger studied the behaviour of roulette wheels at a casino in Monte Carlo, and used this to identify a biased wheel. In this case, the 'population' Jagger wanted to investigate was the overall behaviour of the wheel (i.e. the probability distribution of its results over infinitely many trials), while his 'sample' was formed from observed results from that wheel. Similar considerations arise when taking repeated measurements of some physical characteristic such as the electrical conductivity of copper.\nThis situation often arises when seeking knowledge about the cause system of which the observed population is an outcome. In such cases, sampling theory may treat the observed population as a sample from a larger 'superpopulation'. For example, a researcher might study the success rate of a new 'quit smoking' program on a test group of 100 patients, in order to predict the effects of the program if it were made available nationwide. Here the superpopulation is \"everybody in the country, given access to this treatment\" \u2013 a group that does not yet exist since the program isn't yet available to all.\nThe population from which the sample is drawn may not be the same as the population from which information is desired. Often there is a large but not complete overlap between these two groups due to frame issues etc. (see below). Sometimes they may be entirely separate \u2013 for instance, one might study rats in order to get a better understanding of human health, or one might study records from people born in 2008 in order to make predictions about people born in 2009.\nTime spent in making the sampled population and population of concern precise is often well spent because it raises many issues, ambiguities, and questions that would otherwise have been overlooked at this stage.\n\n\n== Sampling frame ==\n\nIn the most straightforward case, such as the sampling of a batch of material from production (acceptance sampling by lots), it would be most desirable to identify and measure every single item in the population and to include any one of them in our sample. However, in the more general case this is not usually possible or practical. There is no way to identify all rats in the set of all rats. Where voting is not compulsory, there is no way to identify which people will vote at a forthcoming election (in advance of the election). These imprecise populations are not amenable to sampling in any of the ways below and to which we could apply statistical theory.\nAs a remedy, we seek a sampling frame which has the property that we can identify every single element and include any in our sample. The most straightforward type of frame is a list of elements of the population (preferably the entire population) with appropriate contact information. For example, in an opinion poll, possible sampling frames include an electoral register and a telephone directory.\nA probability sample is a sample in which every unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits makes it possible to produce unbiased estimates of population totals, by weighting sampled units according to their probability of selection.\n\nExample: We want to estimate the total income of adults living in a given street. We visit each household in that street, identify all adults living there, and randomly select one adult from each household. (For example, we can allocate each person a random number, generated from a uniform distribution between 0 and 1, and select the person with the highest number in each household). We then interview the selected person and find their income.\nPeople living on their own are certain to be selected, so we simply add their income to our estimate of the total. But a person living in a household of two adults has only a one-in-two chance of selection. To reflect this, when we come to such a household, we would count the selected person's income twice towards the total. (The person who is selected from that household can be loosely viewed as also representing the person who isn't selected.)\n\nIn the above example, not everybody has the same probability of selection; what makes it a probability sample is the fact that each person's probability is known. When every element in the population does have the same probability of selection, this is known as an 'equal probability of selection' (EPS) design. Such designs are also referred to as 'self-weighting' because all sampled units are given the same weight.\nProbability sampling includes: Simple Random Sampling, Systematic Sampling, Stratified Sampling, Probability Proportional to Size Sampling, and Cluster or Multistage Sampling. These various ways of probability sampling have two things in common:\n\nEvery element has a known nonzero probability of being sampled and\ninvolves random selection at some point.\n\n\n=== Nonprobability sampling ===\n\nNonprobability sampling is any sampling method where some elements of the population have no chance of selection (these are sometimes referred to as 'out of coverage'/'undercovered'), or where the probability of selection can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population.\n\nExample: We visit every household in a given street, and interview the first person to answer the door. In any household with more than one occupant, this is a nonprobability sample, because some people are more likely to answer the door (e.g. an unemployed person who spends most of their time at home is more likely to answer than an employed housemate who might be at work when the interviewer calls) and it's not practical to calculate these probabilities.\n\nNonprobability sampling methods include convenience sampling, quota sampling, and purposive sampling. In addition, nonresponse effects may turn any probability design into a nonprobability design if the characteristics of nonresponse are not well understood, since nonresponse effectively modifies each element's probability of being sampled.\n\n\n== Sampling methods ==\nWithin any of the types of frames identified above, a variety of sampling methods can be employed individually or in combination. Factors commonly influencing the choice between these designs include:\n\nNature and quality of the frame\nAvailability of auxiliary information about units on the frame\nAccuracy requirements, and the need to measure accuracy\nWhether detailed analysis of the sample is expected\nCost/operational concerns\n\n\n=== Simple random sampling ===\n\nIn a simple random sample (SRS) of a given size, all subsets of a sampling frame have an equal probability of being selected. Each element of the frame thus has an equal probability of selection: the frame is not subdivided or partitioned. Furthermore, any given pair of elements has the same chance of selection as any other such pair (and similarly for triples, and so on). This minimizes bias and simplifies analysis of results. In particular, the variance between individual results within the sample is a good indicator of variance in the overall population, which makes it relatively easy to estimate the accuracy of results.\nSimple random sampling can be vulnerable to sampling error because the randomness of the selection may result in a sample that doesn't reflect the makeup of the population. For instance, a simple random sample of ten people from a given country will on average produce five men and five women, but any given trial is likely to over represent one sex and underrepresent the other. Systematic and stratified techniques attempt to overcome this problem by \"using information about the population\" to choose a more \"representative\" sample.\nAlso, simple random sampling can be cumbersome and tedious when sampling from a large target population. In some cases, investigators are interested in research questions specific to subgroups of the population. For example, researchers might be interested in examining whether cognitive ability as a predictor of job performance is equally applicable across racial groups. Simple random sampling cannot accommodate the needs of researchers in this situation, because it does not provide subsamples of the population, and other sampling strategies, such as stratified sampling, can be used instead.\n\n\n=== Systematic sampling ===\n\nSystematic sampling (also known as interval sampling) relies on arranging the study population according to some ordering scheme and then selecting elements at regular intervals through that ordered list. Systematic sampling involves a random start and then proceeds with the selection of every kth element from then onwards. In this case, k=(population size/sample size). It is important that the starting point is not automatically the first in the list, but is instead randomly chosen from within the first to the kth element in the list. A simple example would be to select every 10th name from the telephone directory (an 'every 10th' sample, also referred to as 'sampling with a skip of 10').\nAs long as the starting point is randomized, systematic sampling is a type of probability sampling. It is easy to implement and the stratification induced can make it efficient, if the variable by which the list is ordered is correlated with the variable of interest. 'Every 10th' sampling is especially useful for efficient sampling from databases.\nFor example, suppose we wish to sample people from a long street that starts in a poor area (house No. 1) and ends in an expensive district (house No. 1000). A simple random selection of addresses from this street could easily end up with too many from the high end and too few from the low end (or vice versa), leading to an unrepresentative sample. Selecting (e.g.) every 10th street number along the street ensures that the sample is spread evenly along the length of the street, representing all of these districts. (Note that if we always start at house #1 and end at #991, the sample is slightly biased towards the low end; by randomly selecting the start between #1 and #10, this bias is eliminated.)\nHowever, systematic sampling is especially vulnerable to periodicities in the list. If periodicity is present and the period is a multiple or factor of the interval used, the sample is especially likely to be unrepresentative of the overall population, making the scheme less accurate than simple random sampling.\nFor example, consider a street where the odd-numbered houses are all on the north (expensive) side of the road, and the even-numbered houses are all on the south (cheap) side. Under the sampling scheme given above, it is impossible to get a representative sample; either the houses sampled will all be from the odd-numbered, expensive side, or they will all be from the even-numbered, cheap side, unless the researcher has previous knowledge of this bias and avoids it by a using a skip which ensures jumping between the two sides (any odd-numbered skip).\nAnother drawback of systematic sampling is that even in scenarios where it is more accurate than SRS, its theoretical properties make it difficult to quantify that accuracy. (In the two examples of systematic sampling that are given above, much of the potential sampling error is due to variation between neighbouring houses \u2013 but because this method never selects two neighbouring houses, the sample will not give us any information on that variation.)\nAs described above, systematic sampling is an EPS method, because all elements have the same probability of selection (in the example given, one in ten). It is not 'simple random sampling' because different subsets of the same size have different selection probabilities \u2013 e.g. the set {4,14,24,...,994} has a one-in-ten probability of selection, but the set {4,13,24,34,...} has zero probability of selection.\nSystematic sampling can also be adapted to a non-EPS approach; for an example, see discussion of PPS samples below.\n\n\n=== Stratified sampling ===\n\nWhen the population embraces a number of distinct categories, the frame can be organized by these categories into separate \"strata.\" Each stratum is then sampled as an independent sub-population, out of which individual elements can be randomly selected. The ratio of the size of this random selection (or sample) to the size of the population is called a sampling fraction.  There are several potential benefits to stratified sampling.First, dividing the population into distinct, independent strata can enable researchers to draw inferences about specific subgroups that may be lost in a more generalized random sample.\nSecond, utilizing a stratified sampling method can lead to more efficient statistical estimates (provided that strata are selected based upon relevance to the criterion in question, instead of availability of the samples). Even if a stratified sampling approach does not lead to increased statistical efficiency, such a tactic will not result in less efficiency than would simple random sampling, provided that each stratum is proportional to the group's size in the population.\nThird, it is sometimes the case that data are more readily available for individual, pre-existing strata within a population than for the overall population; in such cases, using a stratified sampling approach may be more convenient than aggregating data across groups (though this may potentially be at odds with the previously noted importance of utilizing criterion-relevant strata).\nFinally, since each stratum is treated as an independent population, different sampling approaches can be applied to different strata, potentially enabling researchers to use the approach best suited (or most cost-effective) for each identified subgroup within the population.\nThere are, however, some potential drawbacks to using stratified sampling. First, identifying strata and implementing such an approach can increase the cost and complexity of sample selection, as well as leading to increased complexity of population estimates. Second, when examining multiple criteria, stratifying variables may be related to some, but not to others, further complicating the design, and potentially reducing the utility of the strata. Finally, in some cases (such as designs with a large number of strata, or those with a specified minimum sample size per group), stratified sampling can potentially require a larger sample than would other methods (although in most cases, the required sample size would be no larger than would be required for simple random sampling).\n\nA stratified sampling approach is most effective when three conditions are met\nVariability within strata are minimized\nVariability between strata are maximized\nThe variables upon which the population is stratified are strongly correlated with the desired dependent variable.Advantages over other sampling methodsFocuses on important subpopulations and ignores irrelevant ones.\nAllows use of different sampling techniques for different subpopulations.\nImproves the accuracy/efficiency of estimation.\nPermits greater balancing of statistical power of tests of differences between strata by sampling equal numbers from strata varying widely in size.DisadvantagesRequires selection of relevant stratification variables which can be difficult.\nIs not useful when there are no homogeneous subgroups.\nCan be expensive to implement.PoststratificationStratification is sometimes introduced after the sampling phase in a process called \"poststratification\". This approach is typically implemented due to a lack of prior knowledge of an appropriate stratifying variable or when the experimenter lacks the necessary information to create a stratifying variable during the sampling phase. Although the method is susceptible to the pitfalls of post hoc approaches, it can provide several benefits in the right situation. Implementation usually follows a simple random sample. In addition to allowing for stratification on an ancillary variable, poststratification can be used to implement weighting, which can improve the precision of a sample's estimates.\nOversamplingChoice-based sampling is one of the stratified sampling strategies. In choice-based sampling, the data are stratified on the target and a sample is taken from each stratum so that the rare target class will be more represented in the sample. The model is then built on this biased sample. The effects of the input variables on the target are often estimated with more precision with the choice-based sample even when a smaller overall sample size is taken, compared to a random sample. The results usually must be adjusted to correct for the oversampling.\n\n\n=== Probability-proportional-to-size sampling ===\n\nIn some cases the sample designer has access to an \"auxiliary variable\" or \"size measure\", believed to be correlated to the variable of interest, for each element in the population. These data can be used to improve accuracy in sample design. One option is to use the auxiliary variable as a basis for stratification, as discussed above.\nAnother option is probability proportional to size ('PPS') sampling, in which the selection probability for each element is set to be proportional to its size measure, up to a maximum of 1. In a simple PPS design, these selection probabilities can then be used as the basis for Poisson sampling. However, this has the drawback of variable sample size, and different portions of the population may still be over- or under-represented due to chance variation in selections.\nSystematic sampling theory can be used to create a probability proportionate to size sample. This is done by treating each count within the size variable as a single sampling unit. Samples are then identified by selecting at even intervals among these counts within the size variable. This method is sometimes called PPS-sequential or monetary unit sampling in the case of audits or forensic sampling.\n\nExample: Suppose we have six schools with populations of 150, 180, 200, 220, 260, and 490 students respectively (total 1500 students), and we want to use student population as the basis for a PPS sample of size three. To do this, we could allocate the first school numbers 1 to 150, the second school 151 to 330 (= 150 + 180), the third school 331 to 530, and so on to the last school (1011 to 1500). We then generate a random start between 1 and 500 (equal to 1500/3) and count through the school populations by multiples of 500. If our random start was 137, we would select the schools which have been allocated numbers 137, 637, and 1137, i.e. the first, fourth, and sixth schools.\n\nThe PPS approach can improve accuracy for a given sample size by concentrating sample on large elements that have the greatest impact on population estimates. PPS sampling is commonly used for surveys of businesses, where element size varies greatly and auxiliary information is often available \u2013 for instance, a survey attempting to measure the number of guest-nights spent in hotels might use each hotel's number of rooms as an auxiliary variable. In some cases, an older measurement of the variable of interest can be used as an auxiliary variable when attempting to produce more current estimates.\n\n\n=== Cluster sampling ===\n\nSometimes it is more cost-effective to select respondents in groups ('clusters'). Sampling is often clustered by geography, or by time periods. (Nearly all samples are in some sense 'clustered' in time \u2013 although this is rarely taken into account in the analysis.) For instance, if surveying households within a city, we might choose to select 100 city blocks and then interview every household within the selected blocks.\nClustering can reduce travel and administrative costs. In the example above, an interviewer can make a single trip to visit several households in one block, rather than having to drive to a different block for each household.\nIt also means that one does not need a sampling frame listing all elements in the target population. Instead, clusters can be chosen from a cluster-level frame, with an element-level frame created only for the selected clusters. In the example above, the sample only requires a block-level city map for initial selections, and then a household-level map of the 100 selected blocks, rather than a household-level map of the whole city.\nCluster sampling (also known as clustered sampling) generally increases the variability of sample estimates above that of simple random sampling, depending on how the clusters differ between one another as compared to the within-cluster variation. For this reason, cluster sampling requires a larger sample than SRS to achieve the same level of accuracy \u2013 but cost savings from clustering might still make this a cheaper option.\nCluster sampling is commonly implemented as multistage sampling. This is a complex form of cluster sampling in which two or more levels of units are embedded one in the other. The first stage consists of constructing the clusters that will be used to sample from. In the second stage, a sample of primary units is randomly selected from each cluster (rather than using all units contained in all selected clusters). In following stages, in each of those selected clusters, additional samples of units are selected, and so on. All ultimate units (individuals, for instance) selected at the last step of this procedure are then surveyed. This technique, thus, is essentially the process of taking random subsamples of preceding random samples.\nMultistage sampling can substantially reduce sampling costs, where the complete population list would need to be constructed (before other sampling methods could be applied). By eliminating the work involved in describing clusters that are not selected, multistage sampling can reduce the large costs associated with traditional cluster sampling. However, each sample may not be a full representative of the whole population.\n\n\n=== Quota sampling ===\n\nIn quota sampling, the population is first segmented into mutually exclusive sub-groups, just as in stratified sampling. Then judgement is used to select the subjects or units from each segment based on a specified proportion. For example, an interviewer may be told to sample 200 females and 300 males between the age of 45 and 60.\nIt is this second step which makes the technique one of non-probability sampling. In quota sampling the selection of the sample is non-random. For example, interviewers might be tempted to interview those who look most helpful. The problem is that these samples may be biased because not everyone gets a chance of selection. This random element is its greatest weakness and quota versus probability has been a matter of controversy for several years.\n\n\n=== Minimax sampling ===\nIn imbalanced datasets, where the sampling ratio does not follow the population statistics, one can resample the dataset in a conservative manner called minimax sampling. The minimax sampling has its origin in Anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be minimax ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the sampling ratio of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the best.\n\n\n=== Accidental sampling ===\nAccidental sampling (sometimes known as grab, convenience or opportunity sampling) is a type of nonprobability sampling which involves the sample being drawn from that part of the population which is close to hand. That is, a population is selected because it is readily available and convenient. It may be through meeting the person or including a person in the sample when one meets them or chosen by finding them through technological means such as the internet or through phone. The researcher using such a sample cannot scientifically make generalizations about the total population from this sample because it would not be representative enough. For example, if the interviewer were to conduct such a survey at a shopping center early in the morning on a given day, the people that they could interview would be limited to those given there at that given time, which would not represent the views of other members of society in such an area, if the survey were to be conducted at different times of day and several times per week. This type of sampling is most useful for pilot testing. Several important considerations for researchers using convenience samples include:\n\nAre there controls within the research design or experiment which can serve to lessen the impact of a non-random convenience sample, thereby ensuring the results will be more representative of the population?\nIs there good reason to believe that a particular convenience sample would or should respond or behave differently than a random sample from the same population?\nIs the question being asked by the research one that can adequately be answered using a convenience sample?In social science research, snowball sampling is a similar technique, where existing study subjects are used to recruit more subjects into the sample. Some variants of snowball sampling, such as respondent driven sampling, allow calculation of selection probabilities and are probability sampling methods under certain conditions.\n\n\n=== Voluntary Sampling ===\n\nThe voluntary sampling method is a type of non-probability sampling. Volunteers choose to complete a survey.\nVolunteers may be invited through advertisements in social media. The target population for advertisements can be selected by characteristics like location, age, sex, income, occupation, education, or interests using tools provided by the social medium. The advertisement may include a message about the research and link to a survey. After following the link and completing the survey, the volunteer submits the data to be included in the sample population. This method can reach a global population but is limited by the campaign budget. Volunteers outside the invited population may also be included in the sample.\nIt is difficult to make generalizations from this sample because it may not represent the total population. Often, volunteers have a strong interest in the main topic of the survey.\n\n\n=== Line-intercept sampling ===\nLine-intercept sampling is a method of sampling elements in a region whereby an element is sampled if a chosen line segment, called a \"transect\", intersects the element.\n\n\n=== Panel sampling ===\nPanel sampling is the method of first selecting a group of participants through a random sampling method and then asking that group for (potentially the same) information several times over a period of time. Therefore, each participant is interviewed at two or more time points; each period of data collection is called a \"wave\". The method was developed by sociologist Paul Lazarsfeld in 1938 as a means of studying political campaigns. This longitudinal sampling-method allows estimates of changes in the population, for example with regard to chronic illness to job stress to weekly food expenditures. Panel sampling can also be used to inform researchers about within-person health changes due to age or to help explain changes in continuous dependent variables such as spousal interaction. There have been several proposed methods of analyzing panel data, including MANOVA, growth curves, and structural equation modeling with lagged effects.\n\n\n=== Snowball sampling ===\nSnowball sampling involves finding a small group of initial respondents and using them to recruit more respondents. It is particularly useful in cases where the population is hidden or difficult to enumerate.\n\n\n=== Theoretical sampling ===\nTheoretical sampling occurs when samples are selected on the basis of the results of the data collected so far with a goal of developing a deeper understanding of the area or develop theories. Extreme or very specific cases might be selected in order to maximize the likelihood a phenomenon will actually be observable.\n\n\n== Replacement of selected units ==\nSampling schemes may be without replacement ('WOR' \u2013 no element can be selected more than once in the same sample) or with replacement ('WR' \u2013 an element may appear multiple times in the one sample). For example, if we catch fish, measure them, and immediately return them to the water before continuing with the sample, this is a WR design, because we might end up catching and measuring the same fish more than once. However, if we do not return the fish to the water or tag and release each fish after catching it, this becomes a WOR design.\n\n\n== Sample size determination ==\n\nFormulas, tables, and power function charts are well known approaches to determine sample size.\nSteps for using sample size tables:\n\nPostulate the effect size of interest, \u03b1, and \u03b2.\nCheck sample size tableSelect the table corresponding to the selected \u03b1\nLocate the row corresponding to the desired power\nLocate the column corresponding to the estimated effect size.\nThe intersection of the column and row is the minimum sample size required.\n\n\n== Sampling and data collection ==\nGood data collection involves:\n\nFollowing the defined sampling process\nKeeping the data in time order\nNoting comments and other contextual events\nRecording non-responses\n\n\n== Applications of sampling ==\nSampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. It is not necessary to look at all of them to determine the topics that are discussed during the day, nor is it necessary to look at all the tweets to determine the sentiment on each of the topics. A theoretical formulation for sampling Twitter data has been developed.In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.\n\n\n== Errors in sample surveys ==\n\nSurvey results are typically subject to some error. Total errors can be classified into sampling errors and non-sampling errors. The term \"error\" here includes systematic biases as well as random errors.\n\n\n=== Sampling errors and biases ===\nSampling errors and biases are induced by the sample design. They include:\n\nSelection bias: When the true selection probabilities differ from those assumed in calculating the results.\nRandom sampling error: Random variation in the results due to the elements in the sample being selected at random.\n\n\n=== Non-sampling error ===\nNon-sampling errors are other errors which can impact final survey estimates, caused by problems in data collection, processing, or sample design. Such errors may include:\n\nOver-coverage: inclusion of data from outside of the population\nUnder-coverage: sampling frame does not include elements in the population.\nMeasurement error: e.g. when respondents misunderstand a question, or find it difficult to answer\nProcessing error: mistakes in data coding\nNon-response or Participation bias: failure to obtain complete data from all selected individualsAfter sampling, a review should be held of the exact process followed in sampling, rather than that intended, in order to study any effects that any divergences might have on subsequent analysis.\nA particular problem involves non-response. Two major types of non-response exist:\nunit nonresponse (lack of completion of any part of the survey)\nitem non-response (submission or participation in survey but failing to complete one or more components/questions of the survey)In survey sampling, many of the individuals identified as part of the sample may be unwilling to participate, not have the time to participate (opportunity cost), or survey administrators may not have been able to contact them. In this case, there is a risk of differences between respondents and nonrespondents, leading to biased estimates of population parameters. This is often addressed by improving survey design, offering incentives, and conducting follow-up studies which make a repeated attempt to contact the unresponsive and to characterize their similarities and differences with the rest of the frame. The effects can also be mitigated by weighting the data (when population benchmarks are available) or by imputing data based on answers to other questions. Nonresponse is particularly a problem in internet sampling. Reasons for this problem may include improperly designed surveys, over-surveying (or survey fatigue),\nand the fact that potential participants may have multiple e-mail addresses, which they don't use anymore or don't check regularly.\n\n\n== Survey weights ==\nIn many situations the sample fraction may be varied by stratum and data will have to be weighted to correctly represent the population. Thus for example, a simple random sample of individuals in the United Kingdom might not include some in remote Scottish islands who would be inordinately expensive to sample. A cheaper method would be to use a stratified sample with urban and rural strata. The rural sample could be under-represented in the sample, but weighted up appropriately in the analysis to compensate.\nMore generally, data should usually be weighted if the sample design does not give each individual an equal chance of being selected. For instance, when households have equal selection probabilities but one person is interviewed from within each household, this gives people from large households a smaller chance of being interviewed. This can be accounted for using survey weights. Similarly, households with more than one telephone line have a greater chance of being selected in a random digit dialing sample, and weights can adjust for this.\nWeights can also serve other purposes, such as helping to correct for non-response.\n\n\n== Methods of producing random samples ==\nRandom number table\nMathematical algorithms for pseudo-random number generators\nPhysical randomization devices such as coins, playing cards or sophisticated devices such as ERNIE\n\n\n== History ==\nRandom sampling by using lots is an old idea, mentioned several times in the Bible. In 1786 Pierre Simon Laplace estimated the population of France by using a sample, along with ratio estimator. He also computed probabilistic estimates of the error. These were not expressed as modern confidence intervals but as the sample size that would be needed to achieve a particular upper bound on the sampling error with probability 1000/1001. His estimates used Bayes' theorem with a uniform prior probability and assumed that his sample was random. Alexander Ivanovich Chuprov introduced sample surveys to Imperial Russia in the 1870s.In the US the 1936 Literary Digest prediction of a Republican win in the presidential election went badly awry, due to severe bias [1]. More than two million people responded to the study with their names obtained through magazine subscription lists and telephone directories. It was not appreciated that these lists were heavily biased towards Republicans and the resulting sample, though very large, was deeply flawed.\n\n\n== See also ==\n\n\n== Notes ==\nThe textbook by Groves et alia provides an overview of survey methodology, including recent literature on questionnaire development (informed by cognitive psychology) :\n\nRobert Groves, et alia. Survey methodology (2010 2nd ed. [2004]) ISBN 0-471-48348-6.The other books focus on the statistical theory of survey sampling and require some knowledge of basic statistics, as discussed in the following  textbooks:\n\nDavid S. Moore and George P. McCabe (February 2005). \"Introduction to the practice of statistics\" (5th edition). W.H. Freeman & Company. ISBN 0-7167-6282-X.\nFreedman, David; Pisani, Robert; Purves, Roger (2007). Statistics (4th ed.). New York: Norton. ISBN 978-0-393-92972-0.The elementary book by Scheaffer et alia uses quadratic equations from high-school algebra:\n\nScheaffer, Richard L., William Mendenhal and R. Lyman Ott. Elementary survey sampling, Fifth Edition. Belmont: Duxbury Press, 1996.More mathematical statistics is required for Lohr, for S\u00e4rndal et alia, and for Cochran (classic):\n\nCochran, William G. (1977). Sampling techniques (Third ed.). Wiley. ISBN 978-0-471-16240-7.\nLohr, Sharon L. (1999). Sampling: Design and analysis. Duxbury. ISBN 978-0-534-35361-2.\nS\u00e4rndal, Carl-Erik; Swensson, Bengt; Wretman, Jan (1992). Model assisted survey sampling. Springer-Verlag. ISBN 978-0-387-40620-6.The historically important books by Deming and Kish remain valuable for insights for social scientists (particularly about the U.S. census and the Institute for Social Research at the University of Michigan):\n\nDeming, W. Edwards (1966). Some Theory of Sampling. Dover Publications. ISBN 978-0-486-64684-8. OCLC 166526.\nKish, Leslie (1995) Survey Sampling, Wiley, ISBN 0-471-10949-5\n\n\n== References ==\n\n\n== Further reading ==\nSingh, G N, Jaiswal, A. K., and Pandey A. K. (2021), Improved Imputation Methods for Missing Data in Two-Occasion Successive Sampling, Communications in Statistics: Theory and Methods. DOI:10.1080/03610926.2021.1944211\nChambers, R L, and Skinner, C J (editors) (2003), Analysis of Survey Data, Wiley, ISBN 0-471-89987-9\nDeming, W. Edwards (1975) On probability as a basis for action, The American Statistician, 29(4), pp. 146\u2013152.\nGy, P (2012) Sampling of Heterogeneous and Dynamic Material Systems: Theories of Heterogeneity, Sampling and Homogenizing, Elsevier Science, ISBN 978-0444556066\nKorn, E.L., and Graubard, B.I. (1999) Analysis of Health Surveys, Wiley, ISBN 0-471-13773-1\nLucas, Samuel R. (2012). doi:10.1007%2Fs11135-012-9775-3 \"Beyond the Existence Proof: Ontological Conditions, Epistemological Implications, and In-Depth Interview Research.\"], Quality & Quantity, doi:10.1007/s11135-012-9775-3.\nStuart, Alan (1962) Basic Ideas of Scientific Sampling, Hafner Publishing Company, New York\nSmith, T. M. F. (1984). \"Present Position and Potential Developments: Some Personal Views: Sample surveys\". Journal of the Royal Statistical Society, Series A. 147 (The 150th Anniversary of the Royal Statistical Society, number 2): 208\u2013221. doi:10.2307/2981677. JSTOR 2981677.\nSmith, T. M. F. (1993). \"Populations and Selection: Limitations of Statistics (Presidential address)\". Journal of the Royal Statistical Society, Series A. 156 (2): 144\u2013166. doi:10.2307/2982726. JSTOR 2982726. (Portrait of T. M. F. Smith on page 144)\nSmith, T. M. F. (2001). \"Centenary: Sample surveys\". Biometrika. 88 (1): 167\u2013243. doi:10.1093/biomet/88.1.167.\nSmith, T. M. F. (2001). \"Biometrika centenary: Sample surveys\".  In D. M. Titterington and D. R. Cox (ed.). Biometrika: One Hundred Years. Oxford University Press. pp. 165\u2013194. ISBN 978-0-19-850993-6.\nWhittle, P. (May 1954). \"Optimum preventative sampling\". Journal of the Operations Research Society of America. 2 (2): 197\u2013203. doi:10.1287/opre.2.2.197. JSTOR 166605.\n\n\n== Standards ==\n\n\n=== ISO ===\nISO 2859 series\nISO 3951 series\n\n\n=== ASTM ===\nASTM E105 Standard Practice for Probability Sampling Of Materials\nASTM E122 Standard Practice for Calculating Sample Size to Estimate, With a Specified Tolerable Error, the Average for Characteristic of a Lot or Process\nASTM E141 Standard Practice for Acceptance of Evidence Based on the Results of Probability Sampling\nASTM E1402 Standard Terminology Relating to Sampling\nASTM E1994 Standard Practice for Use of Process Oriented AOQL and LTPD Sampling Plans\nASTM E2234 Standard Practice for Sampling a Stream of Product by Attributes Indexed by AQL\n\n\n=== ANSI, ASQ ===\nANSI/ASQ Z1.4\n\n\n=== U.S. federal and military standards ===\nMIL-STD-105\nMIL-STD-1916\n\n\n== External links ==\n Media related to Sampling (statistics) at Wikimedia Commons", "F1 score": "In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification. \nThe F1 score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric. The more generic \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   score applies additional weights, valuing one of precision or recall more than the other.\nThe highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero.\n\n\n== Etymology ==\nThe name F-measure is believed to be named after a different F function in Van Rijsbergen's book, when introduced to the Fourth Message Understanding Conference (MUC-4, 1992).\n\n\n== Definition ==\nThe traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall:\n\n  \n    \n      \n        \n          F\n          \n            1\n          \n        \n        =\n        \n          \n            2\n            \n              \n                \n                  r\n                  e\n                  c\n                  a\n                  l\n                  l\n                \n                \n                  \u2212\n                  1\n                \n              \n              +\n              \n                \n                  p\n                  r\n                  e\n                  c\n                  i\n                  s\n                  i\n                  o\n                  n\n                \n                \n                  \u2212\n                  1\n                \n              \n            \n          \n        \n        =\n        2\n        \n          \n            \n              \n                p\n                r\n                e\n                c\n                i\n                s\n                i\n                o\n                n\n              \n              \u22c5\n              \n                r\n                e\n                c\n                a\n                l\n                l\n              \n            \n            \n              \n                p\n                r\n                e\n                c\n                i\n                s\n                i\n                o\n                n\n              \n              +\n              \n                r\n                e\n                c\n                a\n                l\n                l\n              \n            \n          \n        \n        =\n        \n          \n            \n              2\n              \n                t\n                p\n              \n            \n            \n              2\n              \n                t\n                p\n              \n              +\n              \n                f\n                p\n              \n              +\n              \n                f\n                n\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{1}={\\frac {2}{\\mathrm {recall} ^{-1}+\\mathrm {precision} ^{-1}}}=2{\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {2\\mathrm {tp} }{2\\mathrm {tp} +\\mathrm {fp} +\\mathrm {fn} }}}\n  .\n\n\n=== F\u03b2 score ===\nA more general F score, \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n  , that uses a positive real factor \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  , where \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   is chosen such that recall is considered \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   times as important as precision, is:\n\n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n        =\n        (\n        1\n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        )\n        \u22c5\n        \n          \n            \n              \n                p\n                r\n                e\n                c\n                i\n                s\n                i\n                o\n                n\n              \n              \u22c5\n              \n                r\n                e\n                c\n                a\n                l\n                l\n              \n            \n            \n              (\n              \n                \u03b2\n                \n                  2\n                \n              \n              \u22c5\n              \n                p\n                r\n                e\n                c\n                i\n                s\n                i\n                o\n                n\n              \n              )\n              +\n              \n                r\n                e\n                c\n                a\n                l\n                l\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }=(1+\\beta ^{2})\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{(\\beta ^{2}\\cdot \\mathrm {precision} )+\\mathrm {recall} }}}\n  .In terms of Type I and type II errors this becomes:\n\n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n        =\n        \n          \n            \n              (\n              1\n              +\n              \n                \u03b2\n                \n                  2\n                \n              \n              )\n              \u22c5\n              \n                t\n                r\n                u\n                e\n                 \n                p\n                o\n                s\n                i\n                t\n                i\n                v\n                e\n              \n            \n            \n              (\n              1\n              +\n              \n                \u03b2\n                \n                  2\n                \n              \n              )\n              \u22c5\n              \n                t\n                r\n                u\n                e\n                 \n                p\n                o\n                s\n                i\n                t\n                i\n                v\n                e\n              \n              +\n              \n                \u03b2\n                \n                  2\n                \n              \n              \u22c5\n              \n                f\n                a\n                l\n                s\n                e\n                 \n                n\n                e\n                g\n                a\n                t\n                i\n                v\n                e\n              \n              +\n              \n                f\n                a\n                l\n                s\n                e\n                 \n                p\n                o\n                s\n                i\n                t\n                i\n                v\n                e\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle F_{\\beta }={\\frac {(1+\\beta ^{2})\\cdot \\mathrm {true\\ positive} }{(1+\\beta ^{2})\\cdot \\mathrm {true\\ positive} +\\beta ^{2}\\cdot \\mathrm {false\\ negative} +\\mathrm {false\\ positive} }}\\,}\n  .Two commonly used values for \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   are 2, which weighs recall higher than precision, and 0.5, which weighs recall lower than precision.\nThe F-measure was derived so that \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   \"measures the effectiveness of retrieval with respect to a user who attaches \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   times as much importance to recall as precision\". It is based on Van Rijsbergen's effectiveness measure\n\n  \n    \n      \n        E\n        =\n        1\n        \u2212\n        \n          \n            (\n            \n              \n                \n                  \u03b1\n                  p\n                \n              \n              +\n              \n                \n                  \n                    1\n                    \u2212\n                    \u03b1\n                  \n                  r\n                \n              \n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle E=1-\\left({\\frac {\\alpha }{p}}+{\\frac {1-\\alpha }{r}}\\right)^{-1}}\n  .Their relationship is \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n        =\n        1\n        \u2212\n        E\n      \n    \n    {\\displaystyle F_{\\beta }=1-E}\n   where \n  \n    \n      \n        \u03b1\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                \u03b2\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}}\n  .\n\n\n== Diagnostic testing ==\nThis is related to the field of binary classification where recall is often termed \"sensitivity\". \n\n\n== Dependence of the F-score on class imbalance ==\nPrecision-recall curve, and thus the \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   score, explicitly depends on the ratio \n\n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   of positive to negative test cases.\nThis means that comparison of the\nF-score across different problems with differing class ratios is\nproblematic. One way to address this issue (see e.g., Siblini et al,\n2020\n) is to use a standard class ratio \n  \n    \n      \n        \n          r\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle r_{0}}\n   when making such comparisons.\n\n\n== Applications ==\nThe F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance. Earlier works focused primarily on the F1 score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall and so \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   is seen in wide application.\nThe F-score is also used in machine learning. However, the F-measures do not take true negatives into account, hence measures such as the Matthews correlation coefficient, Informedness or  Cohen's kappa may be preferred to assess the performance of a binary classifier.The F-score has been widely used in the natural language processing literature, such as in the evaluation of named entity recognition and word segmentation.\n\n\n== Properties ==\nThe F1 score is the Dice coefficient of the set of retrieved items and the set of relevant items.\n\n\n== Criticism ==\nDavid Hand and others criticize the widespread use of the F1 score since it gives equal importance to precision and recall. In practice, different types of mis-classifications incur different costs. In other words, the relative importance of precision and recall is an aspect of the problem.According to Davide Chicco and Giuseppe Jurman, the F1 score is less truthful and informative than the Matthews correlation coefficient (MCC) in binary evaluation classification.David Powers has pointed out that F1 ignores the True Negatives and thus is misleading for unbalanced classes, while kappa and correlation measures are symmetric and assess both directions of predictability - the classifier predicting the true class and the true class predicting the classifier prediction, proposing separate multiclass measures Informedness and Markedness for the two directions, noting that their geometric mean is correlation.Another source of critique of F1, is its lack of symmetry. It means it may change its value when dataset labeling is changed - the \"positive\" samples are named \"negative\" and vice versa.\nThis criticism is met by the P4 metric definition, which is sometimes indicated as a symmetrical extension of F1.\n\n\n== Difference from Fowlkes\u2013Mallows index ==\nWhile the F-measure is the harmonic mean of recall and precision, the Fowlkes\u2013Mallows index is their geometric mean.\n\n\n== Extension to multi-class classification ==\nThe F-score is also used for evaluating classification problems with more than two classes (Multiclass classification). In this setup, the final score is obtained by micro-averaging (biased by class frequency) or macro-averaging (taking all classes as equally important). For macro-averaging, two different formulas have been used by applicants: the F-score of (arithmetic) class-wise precision and recall means or the arithmetic mean of class-wise F-scores, where the latter exhibits more desirable properties.\n\n\n== See also ==\nBLEU\nConfusion matrix\nHypothesis tests for accuracy\nMETEOR\nNIST (metric)\nReceiver operating characteristic\nROUGE (metric)\nUncertainty coefficient, aka Proficiency\nWord error rate\nLEPOR\n\n\n== References ==", "Stepwise regression": "In statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Usually, this takes the form of a forward, backward, or combined sequence of F-tests or t-tests. \nThe frequent practice of fitting the final selected model followed by reporting estimates and confidence intervals without adjusting them to take the model building process into account has led to calls to stop using stepwise model building altogether or to at least make sure model uncertainty is correctly reflected.\nAlternatives include other model selection techniques, such as adjusted R2, Akaike information criterion, Bayesian information criterion, Mallows's Cp, PRESS, or false discovery rate.\n\n\n== Main approaches ==\nThe main approaches for stepwise regression are:\n\nForward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.\nBackward elimination, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables can be deleted without a statistically significant loss of fit.\nBidirectional elimination, a combination of the above, testing at each step for variables to be included or excluded.\n\n\n== Alternatives ==\n\nA widely used algorithm was first proposed by Efroymson (1960). This is an automatic procedure for statistical model selection in cases where there is a large number of potential explanatory variables, and no underlying theory on which to base the model selection. The procedure is used primarily in regression analysis, though the basic approach is applicable in many forms of model selection. This is a variation on forward selection. At each stage in the process, after a new variable is added, a test is made to check if some variables can be deleted without appreciably increasing the residual sum of squares (RSS). The procedure terminates when the measure is (locally) maximized, or when the available improvement falls below some critical value.\nOne of the main issues with stepwise regression is that it searches a large space of possible models.  Hence it is prone to overfitting the data.  In other words, stepwise regression will often fit much better in sample than it does on new out-of-sample data. Extreme cases have been noted where models have achieved statistical significance working on random numbers. This problem can be mitigated if the criterion for adding (or deleting) a variable is stiff enough. The key line in the sand is at what can be thought of as the Bonferroni point: namely how significant the best spurious variable should be based on chance alone. On a t-statistic scale, this occurs at about \n  \n    \n      \n        \n          \n            2\n            log\n            \u2061\n            p\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {2\\log p}}}\n  , where p is the number of predictors. Unfortunately, this means that many variables which actually carry signal will not be included. This fence turns out to be the right trade-off between over-fitting and missing signal. If we look at the risk of different cutoffs, then using this bound will be within a \n  \n    \n      \n        2\n        log\n        \u2061\n        p\n      \n    \n    {\\displaystyle 2\\log p}\n   factor of the best possible risk. Any other cutoff will end up having a larger such risk inflation.\n\n\n== Model accuracy ==\n\nA way to test for errors in models created by step-wise regression, is to not rely on the model's F-statistic, significance, or multiple R, but instead assess the model against a set of data that was not used to create the model. This is often done by building a model based on a sample of the dataset available (e.g., 70%) \u2013 the \u201ctraining set\u201d \u2013 and use the remainder of the dataset (e.g., 30%) as a validation set to assess the accuracy of the model. Accuracy is then often measured as the actual standard error (SE), MAPE (Mean absolute percentage error), or mean error between the predicted value and the actual value in the hold-out sample. This method is particularly valuable when data are collected in different settings (e.g., different times, social vs. solitary situations) or when models are assumed to be generalizable.\n\n\n== Criticism ==\nStepwise regression procedures are used in data mining, but are controversial. Several points of criticism have been made.\n\nThe tests themselves are biased, since they are based on the same data. Wilkinson and Dallal (1981) computed percentage points of the multiple correlation coefficient by simulation and showed that a final regression obtained by forward selection, said by the F-procedure to be significant at 0.1%, was in fact only significant at 5%.\nWhen estimating the degrees of freedom, the number of the candidate independent variables from the best fit selected may be smaller than the total number of final model variables, causing the fit to appear better than it is when adjusting the r2 value for the number of degrees of freedom. It is important to consider how many degrees of freedom have been used in the entire model, not just count the number of independent variables in the resulting fit.\nModels that are created may be over-simplifications of the real models of the data.Such criticisms, based upon limitations of the relationship between a model and procedure and data set used to fit it, are usually addressed by verifying the model on an independent data set, as in the PRESS procedure.\nCritics regard the procedure as a paradigmatic example of data dredging, intense computation often being an inadequate substitute for subject area expertise. Additionally, the results of stepwise regression are often used incorrectly without adjusting them for the occurrence of model selection. Especially the practice of fitting the final selected model as if no model selection had taken place and reporting of estimates and confidence intervals as if least-squares theory were valid for them, has been described as a scandal. Widespread incorrect usage and the availability of alternatives such as ensemble learning, leaving all variables in the model, or using expert judgement to identify relevant variables have led to calls to totally avoid stepwise model selection.\n\n\n== See also ==\nFreedman's paradox\nLogistic regression\nLeast-angle regression\nOccam's razor\nRegression validation\nLasso (statistics)\n\n\n== References ==", "False positives and false negatives": "A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.In statistical hypothesis testing, the analogous concepts are known as type I and type II errors, where a positive result corresponds to rejecting the null hypothesis, and a negative result corresponds to not rejecting the null hypothesis. The terms are often used interchangeably, but there are differences in detail and interpretation due to the differences between medical testing and statistical hypothesis testing.\n\n\n== False positive error ==\nA false positive error, or false positive, is a result that indicates a given condition exists when it does not. For example, a pregnancy test which indicates a woman is pregnant when she is not, or the conviction of an innocent person.\nA false positive error is a type I error where the test is checking a single condition, and wrongly gives an affirmative (positive) decision. However it is important to distinguish between the type 1 error rate and the probability of a positive result being false. The latter is known as the false positive risk (see Ambiguity in the definition of false positive rate, below).\n\n\n== False negative error ==\nA false negative error, or false negative, is a test result which wrongly indicates that a condition does not hold. For example, when a pregnancy test indicates a woman is not pregnant, but she is, or when a person guilty of a crime is acquitted, these are false negatives. The condition \"the woman is pregnant\", or \"the person is guilty\" holds, but the test (the pregnancy test or the trial in a court of law) fails to realize this condition, and wrongly decides that the person is not pregnant or not guilty.\nA false negative error is a type II error occurring in a test where a single condition is checked for, and the result of the test is erroneous, that the condition is absent.\n\n\n== Related terms ==\n\n\n=== False positive and false negative rates ===\n\nThe false positive rate (FPR) is the proportion of all negatives that still yield positive test outcomes, i.e., the conditional probability of a positive test result given an event that was not present.\nThe false positive rate is equal to the significance level. The specificity of the test is equal to 1 minus the false positive rate.\nIn statistical hypothesis testing, this fraction is given the Greek letter \u03b1, and 1 \u2212 \u03b1 is defined as the specificity of the test. Increasing the specificity of the test lowers the probability of type I errors, but may raise the probability of type II errors (false negatives that reject the alternative hypothesis when it is true).Complementarily, the false negative rate (FNR) is the proportion of positives which yield negative test outcomes with the test, i.e., the conditional probability of a negative test result given that the condition being looked for is present.\nIn statistical hypothesis testing, this fraction is given the letter \u03b2. The \"power\" (or the \"sensitivity\") of the test is equal to 1 \u2212 \u03b2.\n\n\n=== Ambiguity in the definition of false positive rate ===\nThe term false discovery rate (FDR) was used by Colquhoun (2014) to mean the probability that a \"significant\" result was a false positive. Later Colquhoun (2017) used the term false positive risk (FPR) for the same quantity, to avoid confusion with the term FDR as used by people who work on multiple comparisons. Corrections for multiple comparisons aim only to correct the type I error rate, so the result is a (corrected) p-value. Thus they are susceptible to the same misinterpretation as any other p-value. The false positive risk is always higher, often much higher, than the p-value.Confusion of these two ideas, the error of the transposed conditional, has caused much mischief. Because of the ambiguity of notation in this field, it is essential to look at the definition in every paper. The hazards of reliance on p-values was emphasized in Colquhoun (2017) by pointing out that even an observation of p = 0.001 was not necessarily strong evidence against the null hypothesis. Despite the fact that the likelihood ratio in favor of the alternative hypothesis over the null is close to 100, if the hypothesis was implausible, with a prior probability of a real effect being 0.1, even the observation of p = 0.001 would have a false positive rate of 8 percent. It wouldn't even reach the 5 percent level. As a consequence, it has been recommended that every p-value should be accompanied by the prior probability of there being a real effect that it would be necessary to assume in order to achieve a false positive risk of 5%. For example, if we observe p = 0.05 in a single experiment, we would have to be 87% certain that there as a real effect before the experiment was done to achieve a false positive risk of 5%.\n\n\n=== Receiver operating characteristic ===\nThe article \"Receiver operating characteristic\" discusses parameters in statistical signal processing based on ratios of errors of various types.\n\n\n== See also ==\nFalse positive rate\nPositive and negative predictive values\nWhy Most Published Research Findings Are False\n\n\n== Notes ==\n\n\n== References ==", "Linear separability": "In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane.\nThe problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas.  In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.\n\n\n== Mathematical definition ==\nLet \n  \n    \n      \n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle X_{0}}\n   and \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n   be two sets of points in an n-dimensional Euclidean space. Then \n  \n    \n      \n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle X_{0}}\n   and \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n   are linearly separable if there exist n + 1 real numbers \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n        ,\n        .\n        .\n        ,\n        \n          w\n          \n            n\n          \n        \n        ,\n        k\n      \n    \n    {\\displaystyle w_{1},w_{2},..,w_{n},k}\n  , such that every point \n  \n    \n      \n        x\n        \u2208\n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x\\in X_{0}}\n   satisfies \n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n          \n        \n        \n          x\n          \n            i\n          \n        \n        >\n        k\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}w_{i}x_{i}>k}\n   and every point \n  \n    \n      \n        x\n        \u2208\n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x\\in X_{1}}\n   satisfies \n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n          \n        \n        \n          x\n          \n            i\n          \n        \n        <\n        k\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}w_{i}x_{i}<k}\n  , where \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   is the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th component of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .\nEquivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap).In simple 2D, it can also be imagined that the set of points under a linear transformation collapses into a line, on which there exists a value, k, greater than which one set of points will fall into, and lesser than which the other set of points fall.\n\n\n== Examples ==\nThree non-collinear points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case):\n\nHowever, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need two straight lines and thus is not linearly separable:\n\nNotice that three points which are collinear and of the form \"+ \u22c5\u22c5\u22c5 \u2014 \u22c5\u22c5\u22c5 +\" are also not linearly separable.\n\n\n== Linear separability of Boolean functions in n variables ==\nA Boolean function in n variables can be thought of as an assignment of 0 or 1 to each vertex of a Boolean hypercube in n dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be linearly separable provided these two sets of points are linearly separable. The number of distinct Boolean functions is \n  \n    \n      \n        \n          2\n          \n            \n              2\n              \n                n\n              \n            \n          \n        \n      \n    \n    {\\displaystyle 2^{2^{n}}}\n  where n is the number of variables passed into the function.\n\n\n== Support vector machines ==\n\nClassifying data is a common task in machine learning.\nSuppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a new data point will be in. In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p \u2212 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier.\nMore formally, given some training data \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  , a set of n points of the form\n\n  \n    \n      \n        \n          \n            D\n          \n        \n        =\n        \n          \n            {\n            \n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              ,\n              \n                y\n                \n                  i\n                \n              \n              )\n              \u2223\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2208\n              \n                \n                  R\n                \n                \n                  p\n                \n              \n              ,\n              \n              \n                y\n                \n                  i\n                \n              \n              \u2208\n              {\n              \u2212\n              1\n              ,\n              1\n              }\n            \n            }\n          \n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}=\\left\\{(\\mathbf {x} _{i},y_{i})\\mid \\mathbf {x} _{i}\\in \\mathbb {R} ^{p},\\,y_{i}\\in \\{-1,1\\}\\right\\}_{i=1}^{n}}\n  where the yi is either 1 or \u22121, indicating the set to which the point \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   belongs. Each \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle y_{i}=1}\n   from those having \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \u2212\n        1\n      \n    \n    {\\displaystyle y_{i}=-1}\n  . Any hyperplane can be written as the set of points \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   satisfying\n\n  \n    \n      \n        \n          w\n        \n        \u22c5\n        \n          x\n        \n        \u2212\n        b\n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=0,}\n  where \n  \n    \n      \n        \u22c5\n      \n    \n    {\\displaystyle \\cdot }\n   denotes the dot product and \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n   the (not necessarily normalized) normal vector to the hyperplane. The parameter \n  \n    \n      \n        \n          \n            \n              b\n              \n                \u2016\n                \n                  w\n                \n                \u2016\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}\n   determines the offset of the hyperplane from the origin along the normal vector \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  .\nIf the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance.\n\n\n== See also ==\nHyperplane separation theorem\nKirchberger's theorem\nPerceptron\nVapnik\u2013Chervonenkis dimension\n\n\n== References ==", "Hierarchical clustering": "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\n\nAgglomerative: This is a \"bottom-up\" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: This is a \"top-down\" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\nThe standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{3})}\n   and requires \n  \n    \n      \n        \u03a9\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Omega (n^{2})}\n   memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{2})}\n  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n          \n        \n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{2}\\log n)}\n  , an improvement on the aforementioned bound of \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(n^{3})}\n  , at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable.\nExcept for the special case of single-linkage, none of the algorithms (except exhaustive search in \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          2\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(2^{n})}\n  ) can be guaranteed to find the optimum solution.\nDivisive clustering with an exhaustive search is \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          2\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(2^{n})}\n  , but it is common to use faster heuristics to choose splits, such as k-means.\nHierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances.\n\n\n== Cluster Linkage ==\nIn order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate distance d, such as the Euclidean distance, between single observations of the data set, and a linkage criterion, which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets. The choice of metric as well as linkage can have a major impact on the result of the clustering, where the lower level metric determines which objects are most similar, whereas the linkage criterion influences the shape of the clusters. For example, complete-linkage tends to produce more spherical clusters than single-linkage.\nThe linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.\nSome commonly used linkage criteria between two sets of observations A and B and a distance d are:\nSome of these can only be recomputed recursively (WPGMA, WPGMC), for many a recursive computation with Lance-Williams-equations is more efficient, while for other (Mini-Max, Hausdorff, Medoid) the distances have to be computed with the slower full formula. Other linkage criteria include:\n\nThe probability that candidate clusters spawn from the same distribution function (V-linkage).\nThe product of in-degree and out-degree on a k-nearest-neighbour graph (graph degree linkage).\nThe increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters.\n\n\n== Agglomerative clustering example ==\n\nFor example, suppose this data is to be clustered, and the Euclidean distance is the distance metric.\nThe hierarchical clustering dendrogram would be:\n\nCutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row (from the top) of the dendrogram will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number but larger clusters.\nThis method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.\nOptionally, one can also construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the single-linkage clustering page; it can easily be adapted to different types of linkage (see below).\nSuppose we have merged the two closest elements b and c, we now have the following clusters {a}, {b, c}, {d}, {e} and {f}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters.\nUsually the distance between two clusters \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n   and \n  \n    \n      \n        \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}}\n   is one of the following:\n\nThe maximum distance between elements of each cluster (also called complete-linkage clustering):\n  \n    \n      \n        max\n        {\n        \n        d\n        (\n        x\n        ,\n        y\n        )\n        :\n        x\n        \u2208\n        \n          \n            A\n          \n        \n        ,\n        \n        y\n        \u2208\n        \n          \n            B\n          \n        \n        \n        }\n        .\n      \n    \n    {\\displaystyle \\max\\{\\,d(x,y):x\\in {\\mathcal {A}},\\,y\\in {\\mathcal {B}}\\,\\}.}\n  The minimum distance between elements of each cluster (also called single-linkage clustering):\n  \n    \n      \n        min\n        {\n        \n        d\n        (\n        x\n        ,\n        y\n        )\n        :\n        x\n        \u2208\n        \n          \n            A\n          \n        \n        ,\n        \n        y\n        \u2208\n        \n          \n            B\n          \n        \n        \n        }\n        .\n      \n    \n    {\\displaystyle \\min\\{\\,d(x,y):x\\in {\\mathcal {A}},\\,y\\in {\\mathcal {B}}\\,\\}.}\n  The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in UPGMA):\n  \n    \n      \n        \n          \n            1\n            \n              \n                |\n              \n              \n                \n                  A\n                \n              \n              \n                |\n              \n              \u22c5\n              \n                |\n              \n              \n                \n                  B\n                \n              \n              \n                |\n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            \u2208\n            \n              \n                A\n              \n            \n          \n        \n        \n          \u2211\n          \n            y\n            \u2208\n            \n              \n                B\n              \n            \n          \n        \n        d\n        (\n        x\n        ,\n        y\n        )\n        .\n      \n    \n    {\\displaystyle {1 \\over {|{\\mathcal {A}}|\\cdot |{\\mathcal {B}}|}}\\sum _{x\\in {\\mathcal {A}}}\\sum _{y\\in {\\mathcal {B}}}d(x,y).}\n  The sum of all intra-cluster variance.\nThe increase in variance for the cluster being merged (Ward's method)\nThe probability that candidate clusters spawn from the same distribution function (V-linkage).In case of tied minimum distances, a pair is randomly chosen, thus being able to generate several structurally different dendrograms. Alternatively, all tied pairs may be joined at the same time, generating a unique dendrogram.One can always decide to stop clustering when there is a sufficiently small number of clusters (number criterion). Some linkages may also guarantee that agglomeration occurs at a greater distance between clusters than the previous agglomeration, and then one can stop clustering when the clusters are too far apart to be merged (distance criterion). However, this is not the case of, e.g., the centroid linkage where the so-called reversals (inversions, departures from ultrametricity) may occur.\n\n\n== Divisive clustering ==\nThe basic principle of divisive clustering was published as the DIANA (DIvisive ANAlysis Clustering) algorithm. Initially, all data is in the same cluster, and the largest cluster is split until every object is separate.\nBecause there exist \n  \n    \n      \n        O\n        (\n        \n          2\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle O(2^{n})}\n   ways of splitting each cluster, heuristics are needed. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder.\n\n\n== Software ==\n\n\n=== Open source implementations ===\n\nALGLIB implements several hierarchical clustering algorithms (single-link, complete-link, Ward) in C++ and C# with O(n\u00b2) memory and O(n\u00b3) run time.\nELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms.\nJulia has an implementation inside the Clustering.jl package.\nOctave, the GNU analog to MATLAB implements hierarchical clustering in function \"linkage\".\nOrange, a data mining software suite, includes hierarchical clustering with interactive dendrogram visualisation.\nR has built-in functions and packages that provide functions for hierarchical clustering.\nSciPy implements hierarchical clustering in Python, including the efficient SLINK algorithm.\nscikit-learn also implements hierarchical clustering in Python.\nWeka includes hierarchical cluster analysis.\n\n\n=== Commercial implementations ===\nMATLAB includes hierarchical cluster analysis.\nSAS includes hierarchical cluster analysis in PROC CLUSTER.\nMathematica includes a Hierarchical Clustering Package.\nNCSS includes hierarchical cluster analysis.\nSPSS includes hierarchical cluster analysis.\nQlucore Omics Explorer includes hierarchical cluster analysis.\nStata includes hierarchical cluster analysis.\nCrimeStat includes a nearest neighbor hierarchical cluster algorithm with a graphical output for a Geographic Information System.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nKaufman, L.; Rousseeuw, P.J. (1990). Finding Groups in Data: An Introduction to Cluster Analysis (1 ed.). New York: John Wiley. ISBN 0-471-87876-6.\nHastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2009). \"14.3.12 Hierarchical clustering\". The Elements of Statistical Learning (2nd ed.). New York: Springer. pp. 520\u20138. ISBN 978-0-387-84857-0. Archived from the original (PDF) on 2009-11-10. Retrieved 2009-10-20.", "Precision and recall": "In pattern recognition, information retrieval, object detection and classification (machine learning), precision and recall are performance metrics that apply to data retrieved from a collection, corpus or sample space.\nPrecision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\nConsider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements).\nWhen a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.\nAdopting a hypothesis-testing approach from statistics, in which, in this case, the null hypothesis is that a given item is irrelevant (i.e., not a dog), absence of type I and type II errors (i.e., perfect specificity and sensitivity of 100% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative).  \nMore generally, recall is simply the complement of the type II error rate (i.e., one minus the type II error rate). Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs. an irrelevant item.\nThe above cat and dog example contained 8 \u2212 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 \u2212 5 = 7 type II errors, for a type II error rate of 7/12.  Precision can be seen as a measure of quality, and recall as a measure of quantity. \nHigher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).\n\n\n== Introduction ==\nIn information retrieval, the instances are documents and the task is to return a set of relevant documents given a search term. Recall is the number of relevant documents retrieved by a search divided by the total number of existing relevant documents, while precision is the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.\nIn a classification task, the precision for a class is the number of true positives (i.e. the number of items correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class (i.e. the sum of true positives and false positives, which are items incorrectly labelled as belonging to the class). Recall in this context is defined as the number of true positives divided by the total number of elements that actually belong to the positive class (i.e. the sum of true positives and false negatives, which are items which were not labelled as belonging to the positive class but should have been).\nIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\nPrecision and recall are not particularly useful metrics when used in isolation. For instance, it is possible to have perfect recall by simply retrieving every single item. Likewise, it is possible to have near-perfect precision by selecting only a very small number of extremely likely items.\nIn a classification task, a precision score of 1.0 for a class C means that every item labelled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labelled correctly) whereas a recall of 1.0 means that every item from class C was labelled as belonging to class C (but says nothing about how many items from other classes were incorrectly also labelled as belonging to class C).\nOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff.  Consider a brain surgeon removing a cancerous tumor from a patient's brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain cells he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\nUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. precision at a recall level of 0.75) or both are combined into a single measure. Examples of measures that are a combination of precision and recall are the F-measure (the weighted harmonic mean of precision and recall), or the Matthews correlation coefficient, which is a geometric mean of the chance-corrected variants: the regression coefficients Informedness (DeltaP') and Markedness (DeltaP). Accuracy is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence). Inverse Precision and Inverse Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as ROC curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.  The first problem is 'solved' by using Accuracy and the second problem is 'solved' by discounting the chance component and renormalizing to Cohen's kappa, but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision, and their geometric mean Matthews correlation coefficient thus acts like a debiased F-measure.\n\n\n== Definition (information retrieval context) ==\nIn information retrieval contexts, precision and recall are defined in terms of a set of retrieved documents (e.g. the list of documents produced by a web search engine for a query) and a set of relevant documents (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. relevance.\n\n\n=== Precision ===\nIn the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query:\n\nFor example, for a text search on a set of documents, precision is the number of correct results divided by the number of all returned results.\nPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n.\nPrecision is used with recall, the percent of all relevant documents that is returned by the search. The two measures are sometimes used together in the F1 Score (or f-measure) to provide a single measurement for a system.\nNote that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and technology.\n\n\n=== Recall ===\nIn information retrieval, recall is the fraction of the relevant documents that are successfully retrieved.\n\nFor example, for a text search on a set of documents, recall is the number of correct results divided by the number of results that should have been returned.\nIn binary classification, recall is called sensitivity. It can be viewed as the probability that a relevant document is retrieved by the query.\n\n\n=== Connection ===\nPrecision and recall can be interpreted as (estimated) conditional probabilities:\nPrecision is given by \n  \n    \n      \n        P\n        (\n        C\n        =\n        P\n        \n          |\n        \n        \n          \n            \n              C\n              ^\n            \n          \n        \n        =\n        P\n        )\n      \n    \n    {\\displaystyle P(C=P|{\\hat {C}}=P)}\n   while recall is given by \n  \n    \n      \n        P\n        (\n        \n          \n            \n              C\n              ^\n            \n          \n        \n        =\n        P\n        \n          |\n        \n        C\n        =\n        P\n        )\n      \n    \n    {\\displaystyle P({\\hat {C}}=P|C=P)}\n  , where \n  \n    \n      \n        \n          \n            \n              C\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {C}}}\n   is the predicted class and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is the actual class. Both quantities are, therefore, connected by Bayes' theorem.\n\n\n== Definition (classification context) ==\nFor classification tasks, the terms true positives, true negatives, false positives, and false negatives (see Type I and type II errors for definitions) compare the results of the classifier under test with trusted external judgments.  The terms positive and negative refer to the classifier's prediction (sometimes known as the expectation), and the terms true and false refer to whether that prediction corresponds to the external judgment (sometimes known as the observation).\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2\u00d72 contingency table or confusion matrix, as follows:\n\nPrecision and recall are then defined as:\nRecall in this context is also referred to as the true positive rate or sensitivity, and precision is also referred to as positive predictive value (PPV); other related measures used in classification include true negative rate and accuracy. True negative rate is also called specificity.\n\n\n== Imbalanced data ==\n\nAccuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score. There are many metrics that don't suffer from this problem. For example, balanced accuracy (bACC) normalizes true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum by two:\n\nFor the previous example (95 negative and 5 positive samples), classifying all as negative gives 0.5 balanced accuracy score (the maximum bACC score is one), which is equivalent to the expected value of a random guess in a balanced data set. Balanced accuracy can serve as an overall performance metric for a model, whether or not the true labels are imbalanced in the data, assuming the cost of FN is the same as FP.\nAnother metric is the predicted positive condition rate (PPCR), which identifies the percentage of the total population that is flagged. For example, for a search engine that returns 30 results (retrieved documents) out of 1,000,000 documents, the PPCR is 0.003%.\n \nAccording to Saito and Rehmsmeier, precision-recall plots are more informative than ROC plots when evaluating binary classifiers on imbalanced data. In such scenarios, ROC plots may be visually deceptive with respect to conclusions about the reliability of classification performance.Different from the above approaches, if an imbalance scaling is applied directly by weighting the confusion matrix elements, the standard metrics definitions still apply even in the case of imbalanced datasets. The weighting procedure relates the confusion matrix elements to the support set of each considered class.\n\n\n== Probabilistic interpretation ==\nOne can also interpret precision and recall not as ratios but as estimations of probabilities:\nPrecision is the estimated probability that a document randomly selected from the pool of retrieved documents is relevant.\nRecall is the estimated probability that a document randomly selected from the pool of relevant documents is retrieved.Another interpretation is that precision is the average probability of relevant retrieval and recall is the average probability of complete retrieval averaged over multiple retrieval queries.\n\n\n== F-measure ==\n\nA measure that combines precision and recall is the harmonic mean of precision and recall, the traditional F-measure or balanced F-score:\n\nThis measure is approximately the average of the two when they are close, and is more generally the harmonic mean, which, for the case of two numbers, coincides with the square of the geometric mean divided by the arithmetic mean. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. This is also known as the \n  \n    \n      \n        \n          F\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle F_{1}}\n   measure, because recall and precision are evenly weighted.\nIt is a special case of the general \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   measure (for non-negative real values of \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  ):\n\nTwo other commonly used \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   measures are the \n  \n    \n      \n        \n          F\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle F_{2}}\n   measure, which weights recall higher than precision, and the \n  \n    \n      \n        \n          F\n          \n            0.5\n          \n        \n      \n    \n    {\\displaystyle F_{0.5}}\n   measure, which puts more emphasis on precision than recall.\nThe F-measure was derived by van Rijsbergen (1979) so that \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   \"measures the effectiveness of retrieval with respect to a user who attaches \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   times as much importance to recall as precision\".  It is based on van Rijsbergen's effectiveness measure \n  \n    \n      \n        \n          E\n          \n            \u03b1\n          \n        \n        =\n        1\n        \u2212\n        \n          \n            1\n            \n              \n                \n                  \u03b1\n                  P\n                \n              \n              +\n              \n                \n                  \n                    1\n                    \u2212\n                    \u03b1\n                  \n                  R\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\alpha }=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}}\n  , the second term being the weighted harmonic mean of precision and recall with weights \n  \n    \n      \n        (\n        \u03b1\n        ,\n        1\n        \u2212\n        \u03b1\n        )\n      \n    \n    {\\displaystyle (\\alpha ,1-\\alpha )}\n  .  Their relationship is \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n        =\n        1\n        \u2212\n        \n          E\n          \n            \u03b1\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }=1-E_{\\alpha }}\n   where \n  \n    \n      \n        \u03b1\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                \u03b2\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}}\n  .\n\n\n== Limitations as goals ==\nThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the ROC curve (AUC).\n\n\n== See also ==\nUncertainty coefficient, also called proficiency\nSensitivity and specificity\nConfusion matrix\n\n\n== References ==\n\n\n== External links ==\nInformation Retrieval \u2013 C. J. van Rijsbergen 1979\nComputing Precision and Recall for a Multi-class Classification Problem", "Machine learning": "Machine learning (ML) is a field devoted to understanding and building methods that let machines \"learn\" \u2013 that is, methods that leverage data to improve computer performance on some set of tasks.Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.In its application across business problems, machine learning is also referred to as predictive analytics.\n\n\n== Overview ==\nLearning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can sometimes be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". Other times, they can be more nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\n\n\n== History and relationships to other fields ==\n\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.By the early 1960s an experimental \"learning machine\" with punched tape memory, called CyberTron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\n\n\n=== Artificial intelligence ===\n\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.:\u200a488\u200aHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.:\u200a25\u200aMachine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\n\n\n=== Data mining ===\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).\n\n\n=== Generalization ===\nThe difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n\n\n=== Statistics ===\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\n\n\n=== Physics ===\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\n\n\n== Theory ==\n\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalization error.\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\n\n== Approaches ==\n\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize. Although each algorithm has advantages and limitations, no single algorithm works for all problems.\n\n\n=== Supervised learning ===\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\n\n=== Unsupervised learning ===\n\nUnsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function. Though unsupervised learning encompasses other domains involving summarizing and explaining data features. Unsupervised learning algorithms streamlined the process of survey and graph large indel based haplotypes of a gene of interest from pan-genome.\n  \nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n\n\n=== Semi-supervised learning ===\n\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\n\n=== Reinforcement learning ===\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\n\n=== Dimensionality reduction ===\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). This results in a smaller dimension of data (2D instead of 3D), while keeping all original variables in the model without changing the data.\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\n\n\n=== Other types ===\nOther approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.As of 2022, deep learning is the dominant approach for much ongoing work in the field of machine learning.\n\n\n==== Self-learning ====\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s perform action a\nreceive consequence situation s'\ncompute emotion of being in consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.\n\n\n==== Feature learning ====\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\n\n==== Sparse dictionary learning ====\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\n\n==== Anomaly detection ====\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\n\n==== Robot learning ====\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\n\n\n==== Association rules ====\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\n\n=== Models ===\nPerforming machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.\n\n\n==== Artificial neural networks ====\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\n\n==== Decision trees ====\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\n\n==== Support-vector machines ====\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n\n==== Regression analysis ====\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n\n\n==== Bayesian networks ====\n\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n\n==== Gaussian processes ====\n\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\n\n\n==== Genetic algorithms ====\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\n\n=== Training models ===\nTypically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Bias models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams.\n\n\n==== Federated learning ====\n\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\n\n== Applications ==\nThere are many applications for machine learning, including:\n\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.\n\n\n== Limitations ==\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\n\n\n=== Bias ===\n\nMachine learning approaches in particular can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"There's nothing artificial about AI...It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\n\n=== Explainability ===\n\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\n\n=== Overfitting ===\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.\n\n\n=== Other limitations and vulnerabilities ===\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. Some systems are so brittle that changing a single adversarial pixel predictably induces misclassification. Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models which are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\n\n\n== Model assessments ==\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).\n\n\n== Ethics ==\n\nMachine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\nAI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on the objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increase profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\n\n=== Neuromorphic/Physical Neural Networks ===\nA physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.\n\n\n=== Embedded Machine Learning ===\nEmbedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more.\n\n\n== Software ==\nSoftware suites containing a variety of machine learning algorithms include the following:\n\n\n=== Free and open-source software ===\n\n\n=== Proprietary software with free and open-source editions ===\nKNIME\nRapidMiner\n\n\n=== Proprietary software ===\n\n\n== Journals ==\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\n\n== Conferences ==\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\n\n== See also ==\nAutomated machine learning \u2013 Process of automating the application of machine learning\nBig data \u2013 Information assets characterized by high volume, velocity, and variety\nDifferentiable programming \u2013 Programming paradigm\nList of important publications in machine learning\nList of datasets for machine-learning research\n\n\n== References ==\n\n\n== Sources ==\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\n\n\n== Further reading ==\n\n\n== External links ==\n\n Quotations related to Machine learning at Wikiquote\nInternational Machine Learning Society\nmloss is an academic database of open-source machine learning software.", "Clustering high-dimensional data": "Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional spaces of data are often encountered in areas such as medicine, where DNA microarray technology can produce many measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.\n\n\n== Problems ==\nFour problems need to be overcome for clustering in high-dimensional data:\nMultiple dimensions are hard to think in, impossible to visualize, and, due to the exponential growth of the number of possible values with each dimension, complete enumeration of all subspaces becomes intractable with increasing dimensionality. This problem is known as the curse of dimensionality.\nThe concept of distance becomes less precise as the number of dimensions grows, since the distance between any two points in a given dataset converges. The discrimination of the nearest and farthest point in particular becomes meaningless:\n  \n    \n      \n        \n          lim\n          \n            d\n            \u2192\n            \u221e\n          \n        \n        \n          \n            \n              d\n              i\n              s\n              \n                t\n                \n                  max\n                \n              \n              \u2212\n              d\n              i\n              s\n              \n                t\n                \n                  min\n                \n              \n            \n            \n              d\n              i\n              s\n              \n                t\n                \n                  min\n                \n              \n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\lim _{d\\to \\infty }{\\frac {dist_{\\max }-dist_{\\min }}{dist_{\\min }}}=0}\n  A cluster is intended to group objects that are related, based on observations of their attribute's values. However, given a large number of attributes some of the attributes will usually not be meaningful for a given cluster. For example, in newborn screening a cluster of samples might identify newborns that share similar blood values, which might lead to insights about the relevance of certain blood values for a disease. But for different diseases, different blood values might form a cluster, and other values might be uncorrelated. This is known as the local feature relevance problem: different clusters might be found in different subspaces, so a global filtering of attributes is not sufficient.\nGiven a large number of attributes, it is likely that some attributes are correlated. Hence, clusters might exist in arbitrarily oriented affine subspaces.Recent research indicates that the discrimination problems only occur when there is a high number of irrelevant dimensions, and that shared-nearest-neighbor approaches can improve results.\n\n\n== Approaches ==\nApproaches towards clustering in axis-parallel or arbitrarily oriented affine subspaces differ in how they interpret the overall goal, which is finding clusters in data with high dimensionality. An overall different approach is to find clusters based on pattern in the data matrix, often referred to as biclustering, which is a technique frequently utilized in bioinformatics.\n\n\n=== Subspace clustering ===\n\nThe adjacent image shows a mere two-dimensional space where a number of clusters can be identified. In the one-dimensional subspaces, the clusters \n  \n    \n      \n        \n          c\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle c_{a}}\n   (in subspace \n  \n    \n      \n        {\n        x\n        }\n      \n    \n    {\\displaystyle \\{x\\}}\n  ) and \n  \n    \n      \n        \n          c\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle c_{b}}\n  , \n  \n    \n      \n        \n          c\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle c_{c}}\n  , \n  \n    \n      \n        \n          c\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle c_{d}}\n   (in subspace \n  \n    \n      \n        {\n        y\n        }\n      \n    \n    {\\displaystyle \\{y\\}}\n  ) can be found. \n  \n    \n      \n        \n          c\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle c_{c}}\n   cannot be considered a cluster in a two-dimensional (sub-)space, since it is too sparsely distributed in the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   axis. In two dimensions, the two clusters \n  \n    \n      \n        \n          c\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle c_{ab}}\n   and \n  \n    \n      \n        \n          c\n          \n            a\n            d\n          \n        \n      \n    \n    {\\displaystyle c_{ad}}\n   can be identified.\nThe problem of subspace clustering is given by the fact that there are \n  \n    \n      \n        \n          2\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle 2^{d}}\n   different subspaces of a space with \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   dimensions. If the subspaces are not axis-parallel, an infinite number of subspaces is possible. Hence, subspace clustering algorithms utilize some kind of heuristic to remain computationally feasible, at the risk of producing inferior results. For example, the downward-closure property (cf. association rules) can be used to build higher-dimensional subspaces only by combining lower-dimensional ones, as any subspace T containing a cluster, will result in a full space S also to contain that cluster (i.e. S \u2286 T), an approach taken by most of the traditional algorithms such as CLIQUE, SUBCLU. It is also possible to define a subspace using different degrees of relevance for each dimension, an approach taken by iMWK-Means, EBK-Modes and CBK-Modes.\n\n\n=== Projected clustering ===\nProjected clustering seeks to assign each point to a unique cluster, but clusters may exist in different subspaces. The general approach is to use a special distance function together with a regular clustering algorithm.\nFor example, the PreDeCon algorithm checks which attributes seem to support a clustering for each point, and adjusts the distance function such that dimensions with low variance are amplified in the distance function. In the figure above, the cluster \n  \n    \n      \n        \n          c\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle c_{c}}\n   might be found using DBSCAN with a distance function that places less emphasis on the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  -axis and thus exaggerates the low difference in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  -axis sufficiently enough to group the points into a cluster.\nPROCLUS uses a similar approach with a k-medoid clustering. Initial medoids are guessed, and for each medoid the subspace spanned by attributes with low variance is determined. Points are assigned to the medoid closest, considering only the subspace of that medoid in determining the distance. The algorithm then proceeds as the regular PAM algorithm.\nIf the distance function weights attributes differently, but never with 0 (and hence never drops irrelevant attributes), the algorithm is called a \"soft\"-projected clustering algorithm.\n\n\n=== Projection-based clustering ===\nProjection-based clustering is based on a nonlinear projection of high-dimensional data into a two-dimensional space. Typical projection-methods like t-distributed stochastic neighbor embedding (t-SNE), or neighbor retrieval visualizer (NerV)  are used to project data explicitly into two dimensions disregarding the subspaces of higher dimension than two and preserving only relevant neighborhoods in high-dimensional data. In the next step, the Delaunay graph between the projected points is calculated, and each vertex between two projected points is weighted with the high-dimensional distance between the corresponding high-dimensional data points. Thereafter the shortest path between every pair of points is computed using the Dijkstra algorithm. The shortest paths are then used in the clustering process, which involves two choices depending on the structure type in the high-dimensional data. This Boolean choice can be decided by looking at the topographic map of high-dimensional structures. In a benchmarking of 34 comparable clustering methods, projection-based clustering was the only algorithm that always was able to find the high-dimensional distance or density-based structure of the dataset. Projection-based clustering is accessible in the open-source R package \"ProjectionBasedClustering\" on CRAN.\n\n\n=== Hybrid approaches ===\nNot all algorithms try to either find a unique cluster assignment for each point or all clusters in all subspaces; many settle for a result in between, where a number of possibly overlapping, but not necessarily exhaustive set of clusters are found. An example is FIRES, which is from its basic approach a subspace clustering algorithm, but uses a heuristic too aggressive to credibly produce all subspace clusters. Another hybrid approach is to include a human-into-the-algorithmic-loop: Human domain expertise can help to reduce an exponential search space through heuristic selection of samples. This can be beneficial in the health domain where, e.g., medical doctors are confronted with high-dimensional descriptions of patient conditions and measurements on the success of certain therapies. An important  question in such data is to compare and correlate patient conditions and therapy results along with combinations of dimensions. The number of dimensions is often very large, consequently one needs to map them to a smaller number of relevant dimensions to be more amenable for expert analysis. This is because irrelevant, redundant, and conflicting dimensions can negatively affect effectiveness and efficiency of the whole analytic process.\n\n\n=== Correlation clustering ===\nAnother type of subspaces is considered in Correlation clustering (Data Mining).\n\n\n== Software ==\nELKI includes various subspace and correlation clustering algorithms\nFCPS includes over fifty clustering algorithms\n\n\n== References ==", "Mixture model": "In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with \"mixture distributions\" relate to deriving the properties of the overall population from those of the sub-populations, \"mixture models\" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.\nMixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.\n\n\n== Structure ==\n\n\n=== General mixture model ===\nA typical finite-dimensional mixture model is a hierarchical model consisting of the following components:\n\nN random variables that are observed, each distributed according to a mixture of K components, with the components belonging to the same parametric family of distributions (e.g., all normal, all Zipfian, etc.) but with different parameters\nN random latent variables specifying the identity of the mixture component of each observation, each distributed according to a K-dimensional categorical distribution\nA set of K mixture weights, which are probabilities that sum to 1.\nA set of K parameters, each specifying the parameter of the corresponding mixture component.  In many cases, each \"parameter\" is actually a set of parameters.  For example, if the mixture components are Gaussian distributions, there will be a mean and variance for each component. If the mixture components are categorical distributions (e.g., when each observation is a token from a finite alphabet of size V), there will be a vector of V probabilities summing to 1.In addition, in a Bayesian setting, the mixture weights and parameters will themselves be random variables, and prior distributions will be placed over the variables.  In such a case, the weights are typically viewed as a K-dimensional random vector drawn from a Dirichlet distribution (the conjugate prior of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.\nMathematically, a basic parametric mixture model can be described as follows:\n\n  \n    \n      \n        \n          \n            \n              \n                K\n              \n              \n                =\n              \n              \n                \n                  number of mixture components\n                \n              \n            \n            \n              \n                N\n              \n              \n                =\n              \n              \n                \n                  number of observations\n                \n              \n            \n            \n              \n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  parameter of distribution of observation associated with component \n                \n                i\n              \n            \n            \n              \n                \n                  \u03d5\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  mixture weight, i.e., prior probability of a particular component \n                \n                i\n              \n            \n            \n              \n                \n                  \u03d5\n                \n              \n              \n                =\n              \n              \n                K\n                \n                  -dimensional vector composed of all the individual \n                \n                \n                  \u03d5\n                  \n                    1\n                    \u2026\n                    K\n                  \n                \n                \n                  ; must sum to 1\n                \n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  component of observation \n                \n                i\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  observation \n                \n                i\n              \n            \n            \n              \n                F\n                (\n                x\n                \n                  |\n                \n                \u03b8\n                )\n              \n              \n                =\n              \n              \n                \n                  probability distribution of an observation, parametrized on \n                \n                \u03b8\n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                \n                  |\n                \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                F\n                (\n                \n                  \u03b8\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}K&=&{\\text{number of mixture components}}\\\\N&=&{\\text{number of observations}}\\\\\\theta _{i=1\\dots K}&=&{\\text{parameter of distribution of observation associated with component }}i\\\\\\phi _{i=1\\dots K}&=&{\\text{mixture weight, i.e., prior probability of a particular component }}i\\\\{\\boldsymbol {\\phi }}&=&K{\\text{-dimensional vector composed of all the individual }}\\phi _{1\\dots K}{\\text{; must sum to 1}}\\\\z_{i=1\\dots N}&=&{\\text{component of observation }}i\\\\x_{i=1\\dots N}&=&{\\text{observation }}i\\\\F(x|\\theta )&=&{\\text{probability distribution of an observation, parametrized on }}\\theta \\\\z_{i=1\\dots N}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}|z_{i=1\\dots N}&\\sim &F(\\theta _{z_{i}})\\end{array}}}\n  In a Bayesian setting, all parameters are associated with random variables, as follows:\n\n  \n    \n      \n        \n          \n            \n              \n                K\n                ,\n                N\n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03d5\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03d5\n                \n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                ,\n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                ,\n                F\n                (\n                x\n                \n                  |\n                \n                \u03b8\n                )\n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \u03b1\n              \n              \n                =\n              \n              \n                \n                  shared hyperparameter for component parameters\n                \n              \n            \n            \n              \n                \u03b2\n              \n              \n                =\n              \n              \n                \n                  shared hyperparameter for mixture weights\n                \n              \n            \n            \n              \n                H\n                (\n                \u03b8\n                \n                  |\n                \n                \u03b1\n                )\n              \n              \n                =\n              \n              \n                \n                  prior probability distribution of component parameters, parametrized on \n                \n                \u03b1\n              \n            \n            \n              \n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                \u223c\n              \n              \n                H\n                (\n                \u03b8\n                \n                  |\n                \n                \u03b1\n                )\n              \n            \n            \n              \n                \n                  \u03d5\n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    S\n                    y\n                    m\n                    m\n                    e\n                    t\n                    r\n                    i\n                    c\n                    -\n                    D\n                    i\n                    r\n                    i\n                    c\n                    h\n                    l\n                    e\n                    t\n                  \n                  \n                    K\n                  \n                \n                \u2061\n                (\n                \u03b2\n                )\n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                \n                  |\n                \n                \n                  \u03d5\n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                \n                  |\n                \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                ,\n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                \u223c\n              \n              \n                F\n                (\n                \n                  \u03b8\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}K,N&=&{\\text{as above}}\\\\\\theta _{i=1\\dots K},\\phi _{i=1\\dots K},{\\boldsymbol {\\phi }}&=&{\\text{as above}}\\\\z_{i=1\\dots N},x_{i=1\\dots N},F(x|\\theta )&=&{\\text{as above}}\\\\\\alpha &=&{\\text{shared hyperparameter for component parameters}}\\\\\\beta &=&{\\text{shared hyperparameter for mixture weights}}\\\\H(\\theta |\\alpha )&=&{\\text{prior probability distribution of component parameters, parametrized on }}\\alpha \\\\\\theta _{i=1\\dots K}&\\sim &H(\\theta |\\alpha )\\\\{\\boldsymbol {\\phi }}&\\sim &\\operatorname {Symmetric-Dirichlet} _{K}(\\beta )\\\\z_{i=1\\dots N}|{\\boldsymbol {\\phi }}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}|z_{i=1\\dots N},\\theta _{i=1\\dots K}&\\sim &F(\\theta _{z_{i}})\\end{array}}}\n  This characterization uses F and H to describe arbitrary distributions over observations and parameters, respectively.  Typically H will be the conjugate prior of F.  The two most common choices of F are Gaussian aka \"normal\" (for real-valued observations) and categorical (for discrete observations).  Other common possibilities for the distribution of the mixture components are:\n\nBinomial distribution, for the number of \"positive occurrences\" (e.g., successes, yes votes, etc.) given a fixed number of total occurrences\nMultinomial distribution, similar to the binomial distribution, but for counts of multi-way occurrences (e.g., yes/no/maybe in a survey)\nNegative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\nPoisson distribution, for the number of occurrences of an event in a given period of time, for an event that is characterized by a fixed rate of occurrence\nExponential distribution, for the time before the next event occurs, for an event that is characterized by a fixed rate of occurrence\nLog-normal distribution, for positive real numbers that are assumed to grow exponentially, such as incomes or prices\nMultivariate normal distribution (aka multivariate Gaussian distribution), for vectors of correlated outcomes that are individually Gaussian-distributed\nMultivariate Student's t-distribution, for vectors of heavy-tailed correlated outcomes\nA vector of Bernoulli-distributed values, corresponding, e.g., to a black-and-white image, with each value representing a pixel; see the handwriting-recognition example below\n\n\n=== Specific examples ===\n\n\n==== Gaussian mixture model ====\n\nA typical non-Bayesian Gaussian mixture model looks like this:\n\n  \n    \n      \n        \n          \n            \n              \n                K\n                ,\n                N\n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  \u03d5\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03d5\n                \n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                ,\n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                {\n                \n                  \u03bc\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03c3\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                  \n                    2\n                  \n                \n                }\n              \n            \n            \n              \n                \n                  \u03bc\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  mean of component \n                \n                i\n              \n            \n            \n              \n                \n                  \u03c3\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                  \n                    2\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  variance of component \n                \n                i\n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    N\n                  \n                \n                (\n                \n                  \u03bc\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                ,\n                \n                  \u03c3\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}K,N&=&{\\text{as above}}\\\\\\phi _{i=1\\dots K},{\\boldsymbol {\\phi }}&=&{\\text{as above}}\\\\z_{i=1\\dots N},x_{i=1\\dots N}&=&{\\text{as above}}\\\\\\theta _{i=1\\dots K}&=&\\{\\mu _{i=1\\dots K},\\sigma _{i=1\\dots K}^{2}\\}\\\\\\mu _{i=1\\dots K}&=&{\\text{mean of component }}i\\\\\\sigma _{i=1\\dots K}^{2}&=&{\\text{variance of component }}i\\\\z_{i=1\\dots N}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}&\\sim &{\\mathcal {N}}(\\mu _{z_{i}},\\sigma _{z_{i}}^{2})\\end{array}}}\n  \n\nA Bayesian version of a Gaussian mixture model is as follows:\n\n  \n    \n      \n        \n          \n            \n              \n                K\n                ,\n                N\n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  \u03d5\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03d5\n                \n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n                ,\n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  as above\n                \n              \n            \n            \n              \n                \n                  \u03b8\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                {\n                \n                  \u03bc\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n                ,\n                \n                  \u03c3\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                  \n                    2\n                  \n                \n                }\n              \n            \n            \n              \n                \n                  \u03bc\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  mean of component \n                \n                i\n              \n            \n            \n              \n                \n                  \u03c3\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                  \n                    2\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  variance of component \n                \n                i\n              \n            \n            \n              \n                \n                  \u03bc\n                  \n                    0\n                  \n                \n                ,\n                \u03bb\n                ,\n                \u03bd\n                ,\n                \n                  \u03c3\n                  \n                    0\n                  \n                  \n                    2\n                  \n                \n              \n              \n                =\n              \n              \n                \n                  shared hyperparameters\n                \n              \n            \n            \n              \n                \n                  \u03bc\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    N\n                  \n                \n                (\n                \n                  \u03bc\n                  \n                    0\n                  \n                \n                ,\n                \u03bb\n                \n                  \u03c3\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  \u03c3\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  I\n                  n\n                  v\n                  e\n                  r\n                  s\n                  e\n                  -\n                  G\n                  a\n                  m\n                  m\n                  a\n                \n                \u2061\n                (\n                \u03bd\n                ,\n                \n                  \u03c3\n                  \n                    0\n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  \u03d5\n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    S\n                    y\n                    m\n                    m\n                    e\n                    t\n                    r\n                    i\n                    c\n                    -\n                    D\n                    i\n                    r\n                    i\n                    c\n                    h\n                    l\n                    e\n                    t\n                  \n                  \n                    K\n                  \n                \n                \u2061\n                (\n                \u03b2\n                )\n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    N\n                  \n                \n                (\n                \n                  \u03bc\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                ,\n                \n                  \u03c3\n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}K,N&=&{\\text{as above}}\\\\\\phi _{i=1\\dots K},{\\boldsymbol {\\phi }}&=&{\\text{as above}}\\\\z_{i=1\\dots N},x_{i=1\\dots N}&=&{\\text{as above}}\\\\\\theta _{i=1\\dots K}&=&\\{\\mu _{i=1\\dots K},\\sigma _{i=1\\dots K}^{2}\\}\\\\\\mu _{i=1\\dots K}&=&{\\text{mean of component }}i\\\\\\sigma _{i=1\\dots K}^{2}&=&{\\text{variance of component }}i\\\\\\mu _{0},\\lambda ,\\nu ,\\sigma _{0}^{2}&=&{\\text{shared hyperparameters}}\\\\\\mu _{i=1\\dots K}&\\sim &{\\mathcal {N}}(\\mu _{0},\\lambda \\sigma _{i}^{2})\\\\\\sigma _{i=1\\dots K}^{2}&\\sim &\\operatorname {Inverse-Gamma} (\\nu ,\\sigma _{0}^{2})\\\\{\\boldsymbol {\\phi }}&\\sim &\\operatorname {Symmetric-Dirichlet} _{K}(\\beta )\\\\z_{i=1\\dots N}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}&\\sim &{\\mathcal {N}}(\\mu _{z_{i}},\\sigma _{z_{i}}^{2})\\end{array}}}\n  \n  \n    \n      \n    \n    {\\displaystyle }\n  \n\n\n==== Multivariate Gaussian mixture model ====\nA Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions.  In a multivariate distribution (i.e. one modelling a vector \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n   with N random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by\n\n  \n    \n      \n        p\n        (\n        \n          \u03b8\n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            K\n          \n        \n        \n          \u03d5\n          \n            i\n          \n        \n        \n          \n            N\n          \n        \n        (\n        \n          \n            \u03bc\n            \n              i\n            \n          \n          ,\n          \n            \u03a3\n            \n              i\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\boldsymbol {\\theta }})=\\sum _{i=1}^{K}\\phi _{i}{\\mathcal {N}}({\\boldsymbol {\\mu _{i},\\Sigma _{i}}})}\n  where the ith vector component is characterized by normal distributions with weights \n  \n    \n      \n        \n          \u03d5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\phi _{i}}\n  , means \n  \n    \n      \n        \n          \n            \u03bc\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu _{i}}}}\n   and covariance matrices \n  \n    \n      \n        \n          \n            \u03a3\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\Sigma _{i}}}}\n  .  To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution \n  \n    \n      \n        p\n        (\n        \n          x\n          \n            |\n          \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle p({\\boldsymbol {x|\\theta }})}\n   of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n   conditioned on the parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   to be estimated.  With this formulation, the posterior distribution \n  \n    \n      \n        p\n        (\n        \n          \u03b8\n          \n            |\n          \n          x\n        \n        )\n      \n    \n    {\\displaystyle p({\\boldsymbol {\\theta |x}})}\n   is also a Gaussian mixture model of the form \n\n  \n    \n      \n        p\n        (\n        \n          \u03b8\n          \n            |\n          \n          x\n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            K\n          \n        \n        \n          \n            \n              \n                \u03d5\n                \n                  i\n                \n              \n              ~\n            \n          \n        \n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              \n                \n                  \u03bc\n                  \n                    i\n                  \n                \n                ~\n              \n            \n          \n          ,\n          \n            \n              \n                \n                  \u03a3\n                  \n                    i\n                  \n                \n                ~\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\boldsymbol {\\theta |x}})=\\sum _{i=1}^{K}{\\tilde {\\phi _{i}}}{\\mathcal {N}}({\\boldsymbol {{\\tilde {\\mu _{i}}},{\\tilde {\\Sigma _{i}}}}})}\n  with new parameters \n  \n    \n      \n        \n          \n            \n              \n                \u03d5\n                \n                  i\n                \n              \n              ~\n            \n          \n        \n        ,\n        \n          \n            \n              \n                \u03bc\n                \n                  i\n                \n              \n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {\\phi _{i}}},{\\boldsymbol {\\tilde {\\mu _{i}}}}}\n   and \n  \n    \n      \n        \n          \n            \n              \n                \u03a3\n                \n                  i\n                \n              \n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tilde {\\Sigma _{i}}}}}\n   that are updated using the EM algorithm.\n Although EM-based parameter updates are well-established, providing the initial estimates for these parameters is currently an area of active research.  Note that this formulation yields a closed-form solution to the complete posterior distribution.  Estimations of the random variable \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   may be obtained via one of several estimators, such as the mean or maximum of the posterior distribution.\nSuch distributions are useful for assuming patch-wise shapes of images and clusters, for example.  In the case of image representation, each Gaussian may be tilted, expanded, and warped according to the covariance matrices \n  \n    \n      \n        \n          \n            \u03a3\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\Sigma _{i}}}}\n  .  One Gaussian distribution of the set is fit to each patch (usually of size 8x8 pixels) in the image.  Notably, any distribution of points around a cluster (see k-means) may be accurately given enough Gaussian components, but scarcely over K=20 components are needed to accurately model a given image distribution or cluster of data.\n\n\n==== Categorical mixture model ====\n\nA typical non-Bayesian mixture model with categorical observations looks like this:\n\n  \n    \n      \n        K\n        ,\n        N\n        :\n      \n    \n    {\\displaystyle K,N:}\n   as above\n\n  \n    \n      \n        \n          \u03d5\n          \n            i\n            =\n            1\n            \u2026\n            K\n          \n        \n        ,\n        \n          \u03d5\n        \n        :\n      \n    \n    {\\displaystyle \\phi _{i=1\\dots K},{\\boldsymbol {\\phi }}:}\n   as above\n\n  \n    \n      \n        \n          z\n          \n            i\n            =\n            1\n            \u2026\n            N\n          \n        \n        ,\n        \n          x\n          \n            i\n            =\n            1\n            \u2026\n            N\n          \n        \n        :\n      \n    \n    {\\displaystyle z_{i=1\\dots N},x_{i=1\\dots N}:}\n   as above\n\n  \n    \n      \n        V\n        :\n      \n    \n    {\\displaystyle V:}\n   dimension of categorical observations, e.g., size of word vocabulary\n\n  \n    \n      \n        \n          \u03b8\n          \n            i\n            =\n            1\n            \u2026\n            K\n            ,\n            j\n            =\n            1\n            \u2026\n            V\n          \n        \n        :\n      \n    \n    {\\displaystyle \\theta _{i=1\\dots K,j=1\\dots V}:}\n   probability for component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   of observing item \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n\n  \n    \n      \n        \n          \n            \u03b8\n          \n          \n            i\n            =\n            1\n            \u2026\n            K\n          \n        \n        :\n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}_{i=1\\dots K}:}\n   vector of dimension \n  \n    \n      \n        V\n        ,\n      \n    \n    {\\displaystyle V,}\n   composed of \n  \n    \n      \n        \n          \u03b8\n          \n            i\n            ,\n            1\n            \u2026\n            V\n          \n        \n        ;\n      \n    \n    {\\displaystyle \\theta _{i,1\\dots V};}\n   must sum to 1The random variables:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  Categorical\n                \n                (\n                \n                  \n                    \u03b8\n                  \n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}z_{i=1\\dots N}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}&\\sim &{\\text{Categorical}}({\\boldsymbol {\\theta }}_{z_{i}})\\end{array}}}\n  \n\nA typical Bayesian mixture model with categorical observations looks like this:\n\n  \n    \n      \n        K\n        ,\n        N\n        :\n      \n    \n    {\\displaystyle K,N:}\n   as above\n\n  \n    \n      \n        \n          \u03d5\n          \n            i\n            =\n            1\n            \u2026\n            K\n          \n        \n        ,\n        \n          \u03d5\n        \n        :\n      \n    \n    {\\displaystyle \\phi _{i=1\\dots K},{\\boldsymbol {\\phi }}:}\n   as above\n\n  \n    \n      \n        \n          z\n          \n            i\n            =\n            1\n            \u2026\n            N\n          \n        \n        ,\n        \n          x\n          \n            i\n            =\n            1\n            \u2026\n            N\n          \n        \n        :\n      \n    \n    {\\displaystyle z_{i=1\\dots N},x_{i=1\\dots N}:}\n   as above\n\n  \n    \n      \n        V\n        :\n      \n    \n    {\\displaystyle V:}\n   dimension of categorical observations, e.g., size of word vocabulary\n\n  \n    \n      \n        \n          \u03b8\n          \n            i\n            =\n            1\n            \u2026\n            K\n            ,\n            j\n            =\n            1\n            \u2026\n            V\n          \n        \n        :\n      \n    \n    {\\displaystyle \\theta _{i=1\\dots K,j=1\\dots V}:}\n   probability for component \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   of observing item \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n\n  \n    \n      \n        \n          \n            \u03b8\n          \n          \n            i\n            =\n            1\n            \u2026\n            K\n          \n        \n        :\n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}_{i=1\\dots K}:}\n   vector of dimension \n  \n    \n      \n        V\n        ,\n      \n    \n    {\\displaystyle V,}\n   composed of \n  \n    \n      \n        \n          \u03b8\n          \n            i\n            ,\n            1\n            \u2026\n            V\n          \n        \n        ;\n      \n    \n    {\\displaystyle \\theta _{i,1\\dots V};}\n   must sum to 1\n\n  \n    \n      \n        \u03b1\n        :\n      \n    \n    {\\displaystyle \\alpha :}\n   shared concentration hyperparameter of \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   for each component\n\n  \n    \n      \n        \u03b2\n        :\n      \n    \n    {\\displaystyle \\beta :}\n   concentration hyperparameter of \n  \n    \n      \n        \n          \u03d5\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\phi }}}\n  The random variables:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03d5\n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    S\n                    y\n                    m\n                    m\n                    e\n                    t\n                    r\n                    i\n                    c\n                    -\n                    D\n                    i\n                    r\n                    i\n                    c\n                    h\n                    l\n                    e\n                    t\n                  \n                  \n                    K\n                  \n                \n                \u2061\n                (\n                \u03b2\n                )\n              \n            \n            \n              \n                \n                  \n                    \u03b8\n                  \n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    K\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  \n                    Symmetric-Dirichlet\n                  \n                  \n                    V\n                  \n                \n                (\n                \u03b1\n                )\n              \n            \n            \n              \n                \n                  z\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                Categorical\n                \u2061\n                (\n                \n                  \u03d5\n                \n                )\n              \n            \n            \n              \n                \n                  x\n                  \n                    i\n                    =\n                    1\n                    \u2026\n                    N\n                  \n                \n              \n              \n                \u223c\n              \n              \n                \n                  Categorical\n                \n                (\n                \n                  \n                    \u03b8\n                  \n                  \n                    \n                      z\n                      \n                        i\n                      \n                    \n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}{\\boldsymbol {\\phi }}&\\sim &\\operatorname {Symmetric-Dirichlet} _{K}(\\beta )\\\\{\\boldsymbol {\\theta }}_{i=1\\dots K}&\\sim &{\\text{Symmetric-Dirichlet}}_{V}(\\alpha )\\\\z_{i=1\\dots N}&\\sim &\\operatorname {Categorical} ({\\boldsymbol {\\phi }})\\\\x_{i=1\\dots N}&\\sim &{\\text{Categorical}}({\\boldsymbol {\\theta }}_{z_{i}})\\end{array}}}\n  \n\n\n== Examples ==\n\n\n=== A financial model ===\n\nFinancial returns often behave differently in normal situations and during crisis times. A mixture model for return data seems reasonable. Sometimes the model used is a jump-diffusion model, or as a mixture of two normal distributions. See Financial economics \u00a7 Challenges and criticism and Financial risk management \u00a7 Banking for further context.\n\n\n=== House prices ===\nAssume that we observe the prices of N different houses.  Different types of houses in different neighborhoods will have vastly different prices, but the price of a particular type of house in a particular neighborhood (e.g., three-bedroom house in moderately upscale neighborhood) will tend to cluster fairly closely around the mean.  One possible model of such prices would be to assume that the prices are accurately described by a mixture model with K different components, each distributed as a normal distribution with unknown mean and variance, with each component specifying a particular combination of house type/neighborhood.  Fitting this model to observed prices, e.g., using the expectation-maximization algorithm, would tend to cluster the prices according to house type/neighborhood and reveal the spread of prices in each type/neighborhood. (Note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow exponentially, a log-normal distribution might actually be a better model than a normal distribution.)\n\n\n=== Topics in a document ===\nAssume that a document is composed of N different words from a total vocabulary of size V, where each word corresponds to one of K possible topics.  The distribution of such words could be modelled as a mixture of K different V-dimensional categorical distributions.  A model of this sort is commonly termed a topic model.  Note that expectation maximization applied to such a model will typically fail to produce realistic results, due (among other things) to the excessive number of parameters.  Some sorts of additional assumptions are typically necessary to get good results.  Typically two sorts of additional components are added to the model:\n\nA prior distribution is placed over the parameters describing the topic distributions, using a Dirichlet distribution with a concentration parameter that is set significantly below 1, so as to encourage sparse distributions (where only a small number of words have significantly non-zero probabilities).\nSome sort of additional constraint is placed over the topic identities of words, to take advantage of natural clustering.For example, a Markov chain could be placed on the topic identities (i.e., the latent variables specifying the mixture component of each observation), corresponding to the fact that nearby words belong to similar topics. (This results in a hidden Markov model, specifically one where a prior distribution is placed over state transitions that favors transitions that stay in the same state.)\nAnother possibility is the latent Dirichlet allocation model, which divides up the words into D different documents and assumes that in each document only a small number of topics occur with any frequency.\n\n\n=== Handwriting recognition ===\nThe following example is based on an example in Christopher M. Bishop, Pattern Recognition and Machine Learning.Imagine that we are given an N\u00d7N black-and-white image that is known to be a scan of a hand-written digit between 0 and 9, but we don't know which digit is written.  We can create a mixture model with \n  \n    \n      \n        K\n        =\n        10\n      \n    \n    {\\displaystyle K=10}\n   different components, where each component is a vector of size \n  \n    \n      \n        \n          N\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle N^{2}}\n   of Bernoulli distributions (one per pixel).  Such a model can be trained with the expectation-maximization algorithm on an unlabeled set of hand-written digits, and will effectively cluster the images according to the digit being written.  The same model could then be used to recognize the digit of another image simply by holding the parameters constant, computing the probability of the new image for each possible digit (a trivial calculation), and returning the digit that generated the highest probability.\n\n\n=== Assessing projectile accuracy (a.k.a. circular error probable, CEP) ===\nMixture models apply in the problem of directing multiple projectiles at a target (as in air, land, or sea defense applications), where the physical and/or statistical characteristics of the projectiles differ within the multiple projectiles. An example might be shots from multiple munitions types or shots from multiple locations directed at one target. The combination of projectile types may be characterized as a Gaussian mixture model. Further, a well-known measure of accuracy for a group of projectiles is the circular error probable (CEP), which is the number R such that, on average, half of the group of projectiles falls within the circle of radius R about the target point. The mixture model can be used to determine (or estimate) the value R. The mixture model properly captures the different types of projectiles.\n\n\n=== Direct and indirect applications ===\nThe financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.\nIn an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two normal distributions with different means may result in a density with two modes, which is not modeled by standard parametric distributions. Another example is given by the possibility of mixture distributions to model fatter tails than the basic Gaussian ones, so as to be a candidate for modeling more extreme events. When combined with dynamical consistency, this approach has been applied to financial derivatives valuation in presence of the volatility smile in the context of local volatility models. This defines our application.\n\n\n=== Predictive Maintenance ===\nThe mixture model-based clustering is also predominantly used in identifying the state of the machine in predictive maintenance. Density plots are used to analyze the density of high dimensional features. If multi-model densities are observed, then it is assumed that a finite set of densities are formed by a finite set of normal mixtures. A multivariate Gaussian mixture model is used to cluster the feature data into k number of groups where k represents each state of the machine. The machine state can be a normal state, power off state, or faulty state. Each formed cluster can be diagnosed using techniques such as spectral analysis. In the recent years, this has also been widely used in other areas such as early fault detection.\n\n\n=== Fuzzy image segmentation ===\n\nIn image processing and computer vision, traditional image segmentation models often assign to one pixel only one exclusive pattern. In fuzzy or soft segmentation, any pattern can have certain \"ownership\" over any single pixel. If the patterns are Gaussian, fuzzy segmentation naturally results in Gaussian mixtures. Combined with other analytic or geometric tools (e.g., phase transitions over diffusive boundaries), such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods.\n\n\n=== Point set registration ===\nProbabilistic mixture models such as Gaussian mixture models (GMM) are used to resolve point set registration problems in image processing and computer vision fields. For pair-wise point set registration, one point set is regarded as the centroids of mixture models, and the other point set is regarded as data points (observations). State-of-the-art methods are e.g. coherent point drift (CPD) \nand Student's t-distribution mixture models (TMM). \nThe result of recent research demonstrate the superiority of hybrid mixture models \n(e.g. combining Student's t-Distritubtion and Watson distribution/Bingham distribution to model spatial positions and axes orientations separately) compare to CPD and TMM, in terms of inherent robustness, accuracy and discriminative capacity.\n\n\n== Identifiability ==\nIdentifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.\n\n\n=== Example ===\nLet J be the class of all binomial distributions with n = 2. Then a mixture of two members of J would have\n\n  \n    \n      \n        \n          p\n          \n            0\n          \n        \n        =\n        \u03c0\n        (\n        1\n        \u2212\n        \n          \u03b8\n          \n            1\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        (\n        1\n        \u2212\n        \u03c0\n        )\n        (\n        1\n        \u2212\n        \n          \u03b8\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle p_{0}=\\pi (1-\\theta _{1})^{2}+(1-\\pi )(1-\\theta _{2})^{2}}\n  \n\n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        =\n        2\n        \u03c0\n        \n          \u03b8\n          \n            1\n          \n        \n        (\n        1\n        \u2212\n        \n          \u03b8\n          \n            1\n          \n        \n        )\n        +\n        2\n        (\n        1\n        \u2212\n        \u03c0\n        )\n        \n          \u03b8\n          \n            2\n          \n        \n        (\n        1\n        \u2212\n        \n          \u03b8\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle p_{1}=2\\pi \\theta _{1}(1-\\theta _{1})+2(1-\\pi )\\theta _{2}(1-\\theta _{2})}\n  and p2 = 1 \u2212 p0 \u2212 p1. Clearly, given p0 and p1, it is not possible to determine the above mixture model uniquely, as there are three parameters (\u03c0, \u03b81, \u03b82) to be determined.\n\n\n=== Definition ===\nConsider a mixture of parametric distributions of the same class. Let\n\n  \n    \n      \n        J\n        =\n        {\n        f\n        (\n        \u22c5\n        ;\n        \u03b8\n        )\n        :\n        \u03b8\n        \u2208\n        \u03a9\n        }\n      \n    \n    {\\displaystyle J=\\{f(\\cdot ;\\theta ):\\theta \\in \\Omega \\}}\n  be the class of all component distributions. Then the convex hull K of J defines the class of all finite mixture of distributions in J:\n\n  \n    \n      \n        K\n        =\n        \n          {\n          \n            p\n            (\n            \u22c5\n            )\n            :\n            p\n            (\n            \u22c5\n            )\n            =\n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              a\n              \n                i\n              \n            \n            \n              f\n              \n                i\n              \n            \n            (\n            \u22c5\n            ;\n            \n              \u03b8\n              \n                i\n              \n            \n            )\n            ,\n            \n              a\n              \n                i\n              \n            \n            >\n            0\n            ,\n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              a\n              \n                i\n              \n            \n            =\n            1\n            ,\n            \n              f\n              \n                i\n              \n            \n            (\n            \u22c5\n            ;\n            \n              \u03b8\n              \n                i\n              \n            \n            )\n            \u2208\n            J\n             \n            \u2200\n            i\n            ,\n            n\n          \n          }\n        \n      \n    \n    {\\displaystyle K=\\left\\{p(\\cdot ):p(\\cdot )=\\sum _{i=1}^{n}a_{i}f_{i}(\\cdot ;\\theta _{i}),a_{i}>0,\\sum _{i=1}^{n}a_{i}=1,f_{i}(\\cdot ;\\theta _{i})\\in J\\ \\forall i,n\\right\\}}\n  K is said to be identifiable if all its members are unique, that is, given two members p and p\u2032 in K, being mixtures of k distributions and k\u2032 distributions respectively in J, we have p = p\u2032 if and only if, first of all, k = k\u2032 and secondly we can reorder the summations such that ai = ai\u2032 and \u0192i = \u0192i\u2032 for all i.\n\n\n== Parameter estimation and system identification ==\nParametric mixture models are often used when we know the distribution Y and we can sample from X, but we would like to determine the ai and \u03b8i values.  Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.\nIt is common to think of probability mixture modeling as a missing data problem.  One way to understand this is to assume that the data points under consideration have \"membership\" in one of the distributions we are using to model the data.  When we start, this membership is unknown, or missing.  The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.\nA variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum a posteriori estimation (MAP).  Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values.  Some notable departures are the graphical methods as outlined in Tarter and Lock and more recently minimum message length (MML) techniques such as Figueiredo and Jain and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).\n\n\n=== Expectation maximization (EM) ===\nExpectation maximization (EM) is seemingly the most popular technique used to determine the parameters of a mixture with an a priori given number of components. This is a particular way of implementing maximum likelihood estimation for this problem. EM is of particular appeal for finite normal mixtures where closed-form expressions are possible such as in the following iterative algorithm by Dempster et al. (1977)\n\n  \n    \n      \n        \n          w\n          \n            s\n          \n          \n            (\n            j\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          h\n          \n            s\n          \n          \n            (\n            j\n            )\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle w_{s}^{(j+1)}={\\frac {1}{N}}\\sum _{t=1}^{N}h_{s}^{(j)}(t)}\n  \n\n  \n    \n      \n        \n          \u03bc\n          \n            s\n          \n          \n            (\n            j\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  t\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                h\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              (\n              t\n              )\n              \n                x\n                \n                  (\n                  t\n                  )\n                \n              \n            \n            \n              \n                \u2211\n                \n                  t\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                h\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              (\n              t\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\mu _{s}^{(j+1)}={\\frac {\\sum _{t=1}^{N}h_{s}^{(j)}(t)x^{(t)}}{\\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}\n  \n\n  \n    \n      \n        \n          \u03a3\n          \n            s\n          \n          \n            (\n            j\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  t\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                h\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              (\n              t\n              )\n              [\n              \n                x\n                \n                  (\n                  t\n                  )\n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  s\n                \n                \n                  (\n                  j\n                  +\n                  1\n                  )\n                \n              \n              ]\n              [\n              \n                x\n                \n                  (\n                  t\n                  )\n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  s\n                \n                \n                  (\n                  j\n                  +\n                  1\n                  )\n                \n              \n              \n                ]\n                \n                  \u22a4\n                \n              \n            \n            \n              \n                \u2211\n                \n                  t\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                h\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              (\n              t\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{s}^{(j+1)}={\\frac {\\sum _{t=1}^{N}h_{s}^{(j)}(t)[x^{(t)}-\\mu _{s}^{(j+1)}][x^{(t)}-\\mu _{s}^{(j+1)}]^{\\top }}{\\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}\n  with the posterior probabilities\n\n  \n    \n      \n        \n          h\n          \n            s\n          \n          \n            (\n            j\n            )\n          \n        \n        (\n        t\n        )\n        =\n        \n          \n            \n              \n                w\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              \n                p\n                \n                  s\n                \n              \n              (\n              \n                x\n                \n                  (\n                  t\n                  )\n                \n              \n              ;\n              \n                \u03bc\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              ,\n              \n                \u03a3\n                \n                  s\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              )\n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                w\n                \n                  i\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              \n                p\n                \n                  i\n                \n              \n              (\n              \n                x\n                \n                  (\n                  t\n                  )\n                \n              \n              ;\n              \n                \u03bc\n                \n                  i\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              ,\n              \n                \u03a3\n                \n                  i\n                \n                \n                  (\n                  j\n                  )\n                \n              \n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle h_{s}^{(j)}(t)={\\frac {w_{s}^{(j)}p_{s}(x^{(t)};\\mu _{s}^{(j)},\\Sigma _{s}^{(j)})}{\\sum _{i=1}^{n}w_{i}^{(j)}p_{i}(x^{(t)};\\mu _{i}^{(j)},\\Sigma _{i}^{(j)})}}.}\n  Thus on the basis of the current estimate for the parameters, the conditional probability for a given observation x(t) being generated from state s is determined for each t = 1, \u2026, N ; N being the sample size.  The parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample.\nDempster also showed that each successive EM iteration will not decrease the likelihood, a property not shared by other gradient based maximization techniques.  Moreover, EM naturally embeds within it constraints on the probability vector, and for sufficiently large sample sizes positive definiteness of the covariance iterates.  This is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values. Theoretically EM is a first-order algorithm and as such converges slowly to a fixed-point solution. Redner and Walker (1984) make this point arguing in favour of superlinear and second order Newton and quasi-Newton methods and reporting slow convergence in EM on the basis of their empirical tests.  They do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not.  The relative merits of EM and other algorithms vis-\u00e0-vis convergence have been discussed in other literature.Other common objections to the use of EM are that it has a propensity to spuriously identify local maxima, as well as displaying sensitivity to initial values. One may address these problems by evaluating EM at several initial points in the parameter space but this is computationally costly and other approaches, such as the annealing EM method of Udea and Nakano (1998) (in which the initial components are essentially forced to overlap, providing a less heterogeneous basis for initial guesses), may be preferable.\nFigueiredo and Jain note that convergence to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed when the number of model components exceeds the optimal/true one.  On this basis they suggest a unified approach to estimation and identification in which the initial n is chosen to greatly exceed the expected optimal value.  Their optimization routine is constructed via a minimum message length (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in n and consider estimation and identification jointly.\n\n\n==== The expectation step ====\nWith initial guesses for the parameters of our mixture model, \"partial membership\" of each data point in each constituent distribution is computed by calculating expectation values for the membership variables of each data point.  That is, for each data point xj and distribution Yi, the membership value yi, j is:\n\n  \n    \n      \n        \n          y\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          \n            \n              \n                a\n                \n                  i\n                \n              \n              \n                f\n                \n                  Y\n                \n              \n              (\n              \n                x\n                \n                  j\n                \n              \n              ;\n              \n                \u03b8\n                \n                  i\n                \n              \n              )\n            \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              \n                x\n                \n                  j\n                \n              \n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle y_{i,j}={\\frac {a_{i}f_{Y}(x_{j};\\theta _{i})}{f_{X}(x_{j})}}.}\n  \n\n\n==== The maximization step ====\nWith expectation values in hand for group membership, plug-in estimates are recomputed for the distribution parameters.\nThe mixing coefficients ai are the means of the membership values over the N data points.\n\n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          y\n          \n            i\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{i}={\\frac {1}{N}}\\sum _{j=1}^{N}y_{i,j}}\n  The component model parameters \u03b8i are also calculated by expectation maximization using data points xj that have been weighted using the membership values.  For example, if \u03b8 is a mean \u03bc\n\n  \n    \n      \n        \n          \u03bc\n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  j\n                \n              \n              \n                y\n                \n                  i\n                  ,\n                  j\n                \n              \n              \n                x\n                \n                  j\n                \n              \n            \n            \n              \n                \u2211\n                \n                  j\n                \n              \n              \n                y\n                \n                  i\n                  ,\n                  j\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mu _{i}={\\frac {\\sum _{j}y_{i,j}x_{j}}{\\sum _{j}y_{i,j}}}.}\n  With new estimates for ai and the \u03b8i's, the expectation step is repeated to recompute new membership values.  The entire procedure is repeated until model parameters converge.\n\n\n=== Markov chain Monte Carlo ===\nAs an alternative to the EM algorithm, the mixture model parameters can be deduced using posterior sampling as indicated by Bayes' theorem.  This is still regarded as an incomplete data problem whereby membership of data points is the missing data.  A two-step iterative procedure known as Gibbs sampling can be used.\nThe previous example of a mixture of two Gaussian distributions can demonstrate how the method works.  As before, initial guesses of the parameters for the mixture model are made.  Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a Bernoulli distribution (that is, it will be assigned to either the first or the second Gaussian).  The Bernoulli parameter \u03b8 is determined for each data point on the basis of one of the constituent distributions.  Draws from the distribution generate membership associations for each data point.  Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.\n\n\n=== Moment matching ===\nThe method of moment matching is one of the oldest techniques for determining the mixture parameters dating back to Karl Pearson's seminal work of 1894.\nIn this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value.  In many instances extraction of solutions to the moment equations may present non-trivial algebraic or computational problems.  Moreover, numerical analysis by Day has indicated that such methods may be inefficient compared to EM. Nonetheless, there has been renewed interest in this method, e.g., Craigmile and Titterington (1998) and Wang.McWilliam and Loh (2009) consider the characterisation of a hyper-cuboid normal mixture copula in large dimensional systems for which EM would be computationally prohibitive.  Here a pattern analysis routine is used to generate multivariate tail-dependencies consistent with a set of univariate and (in some sense) bivariate moments.  The performance of this method is then evaluated using equity log-return data with Kolmogorov\u2013Smirnov test statistics suggesting a good descriptive fit.\n\n\n=== Spectral method ===\nSome problems in mixture model estimation can be solved using spectral methods.\nIn particular it becomes useful if data points xi are points in high-dimensional real space, and the hidden distributions are known to be log-concave (such as Gaussian distribution or Exponential distribution).\nSpectral methods of learning mixture models are based on the use of Singular Value Decomposition of a matrix which contains data points.\nThe idea is to consider the top k singular vectors, where k is the number of distributions to be learned. The projection\nof each data point to a linear subspace spanned by those vectors groups points originating from the same distribution\nvery close together, while points from different distributions stay far apart.\nOne distinctive feature of the spectral method is that it allows us to prove that if\ndistributions satisfy certain separation condition (e.g., not too close), then the estimated mixture will be very close to the true one with high probability.\n\n\n=== Graphical Methods ===\nTarter and Lock describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intra-component variance.  In this way one may more readily identify components having differing means.  While this \u03bb-method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure.\n\n\n=== Other methods ===\nSome of them can even probably learn mixtures of heavy-tailed distributions including those with\ninfinite variance (see links to papers below).\nIn this setting, EM based methods would not work, since the Expectation step would diverge due to presence of\noutliers.\n\n\n=== A simulation ===\nTo simulate a sample of size N that is from a mixture of distributions Fi, i=1 to n, with probabilities pi (sum= pi = 1):\n\nGenerate N random numbers from a categorical distribution of size n and probabilities pi for i= 1= to n.  These tell you which of the Fi each of the N values will come from.  Denote by mi the quantity of random numbers assigned to the ith category.\nFor each i, generate mi random numbers from the Fi distribution.\n\n\n== Extensions ==\nIn a Bayesian setting, additional levels can be added to the graphical model defining the mixture model.  For example, in the common latent Dirichlet allocation topic model, the observations are sets of words drawn from D different documents and the K mixture components represent topics that are shared across documents.  Each document has a different set of mixture weights, which specify the topics prevalent in that document.  All sets of mixture weights share common hyperparameters.\nA very common extension is to connect the latent variables defining the mixture component identities into a Markov chain, instead of assuming that they are independent identically distributed random variables.  The resulting model is termed a hidden Markov model and is one of the most common sequential hierarchical models.  Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.\n\n\n== History ==\nMixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan, 2000) although common reference is made to the work of Karl Pearson (1894) as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations.  The motivation for this work was provided by the zoologist Walter Frank Raphael Weldon who had speculated in 1893 (in Tarter and Lock) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson's approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.\nWhile his work was successful in identifying two potentially distinct sub-populations and in demonstrating the flexibility of mixtures as a moment matching tool, the formulation required the solution of a 9th degree (nonic) polynomial which at the time posed a significant computational challenge.\nSubsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood (MLE) parameterisation techniques that research really took off. Since that time there has been a vast body of research on the subject spanning areas such as fisheries research, agriculture, botany, economics, medicine, genetics, psychology, palaeontology, electrophoresis, finance, geology and zoology.\n\n\n== See also ==\n\n\n=== Mixture ===\nMixture density\nMixture (probability)\nFlexible Mixture Model (FMM)\nSubspace Gaussian mixture model\n\n\n=== Hierarchical models ===\nGraphical model\nHierarchical Bayes model\n\n\n=== Outlier detection ===\nRANSAC\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Books on mixture models ===\nEveritt, B.S.; Hand, D.J. (1981). Finite mixture distributions. Chapman & Hall. ISBN 978-0-412-22420-1.\nLindsay, B. G. (1995). Mixture Models: Theory, Geometry, and Applications. NSF-CBMS Regional Conference Series in Probability and Statistics. Vol. 5. Hayward: Institute of Mathematical Statistics.\nMarin, J.M.; Mengersen, K.; Robert, C. P. (2011). \"Bayesian modelling and inference on mixtures of distributions\" (PDF).  In Dey, D.; Rao, C.R. (eds.). Essential Bayesian models. Handbook of statistics: Bayesian thinking - modeling and computation. Vol. 25. Elsevier. ISBN 9780444537324.\nMcLachlan, G.J.; Peel, D. (2000). Finite Mixture Models. Wiley. ISBN 978-0-471-00626-8.\nPress, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). \"Section 16.1. Gaussian Mixture Models and k-Means Clustering\". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8.\nTitterington, D.; Smith, A.; Makov, U. (1985). Statistical Analysis of Finite Mixture Distributions. Wiley. ISBN 978-0-471-90763-3.\n\n\n=== Application of Gaussian mixture models ===\nReynolds, D.A.; Rose, R.C. (January 1995). \"Robust text-independent speaker identification using Gaussian mixture speaker models\". IEEE Transactions on Speech and Audio Processing. 3 (1): 72\u201383. doi:10.1109/89.365379.\nPermuter, H.; Francos, J.; Jermyn, I.H. (2003). Gaussian mixture models of texture and colour for image database retrieval. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings (ICASSP '03). doi:10.1109/ICASSP.2003.1199538.\nPermuter, Haim; Francos, Joseph; Jermyn, Ian (2006). \"A study of Gaussian mixture models of color and texture features for image classification and segmentation\" (PDF). Pattern Recognition. 39 (4): 695\u2013706. Bibcode:2006PatRe..39..695P. doi:10.1016/j.patcog.2005.10.028. S2CID 8530776.\nLemke, Wolfgang (2005). Term Structure Modeling and Estimation in a State Space Framework. Springer Verlag. ISBN 978-3-540-28342-3.\nBrigo, Damiano; Mercurio, Fabio (2001). Displaced and Mixture Diffusions for Analytically-Tractable Smile Models. Mathematical Finance \u2013 Bachelier Congress 2000. Proceedings. Springer Verlag.\nBrigo, Damiano; Mercurio, Fabio (June 2002). \"Lognormal-mixture dynamics and calibration to market volatility smiles\". International Journal of Theoretical and Applied Finance. 5 (4): 427. CiteSeerX 10.1.1.210.4165. doi:10.1142/S0219024902001511.\nSpall, J. C.; Maryak, J. L. (1992). \"A feasible Bayesian estimator of quantiles for projectile accuracy from non-i.i.d. data\". Journal of the American Statistical Association. 87 (419): 676\u2013681. doi:10.1080/01621459.1992.10475269. JSTOR 2290205.\nAlexander, Carol (December 2004). \"Normal mixture diffusion with uncertain volatility: Modelling short- and long-term smile effects\" (PDF). Journal of Banking & Finance. 28 (12): 2957\u201380. doi:10.1016/j.jbankfin.2003.10.017.\nStylianou, Yannis; Pantazis, Yannis; Calderero, Felipe; Larroy, Pedro; Severin, Francois; Schimke, Sascha; Bonal, Rolando; Matta, Federico; Valsamakis, Athanasios (2005). GMM-Based Multimodal Biometric Verification (PDF).\nChen, J.; Adebomi, 0.E.; Olusayo, O.S.; Kulesza, W. (2010). The Evaluation of the Gaussian Mixture Probability Hypothesis Density approach for multi-target tracking. IEEE International Conference on Imaging Systems and Techniques, 2010. doi:10.1109/IST.2010.5548541.\n\n\n== External links ==\nNielsen, Frank (23 March 2012). \"K-MLE: A fast algorithm for learning statistical mixture models\". k-MLE: A fast algorithm for learning statistical mixture models. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 869\u2013872. arXiv:1203.5181. Bibcode:2012arXiv1203.5181N. doi:10.1109/ICASSP.2012.6288022. ISBN 978-1-4673-0046-9. S2CID 935615.\nThe SOCR demonstrations of EM and Mixture Modeling\nMixture modelling page (and the Snob program for Minimum Message Length (MML) applied to finite mixture models), maintained by D.L. Dowe.\nPyMix \u2013 Python Mixture Package, algorithms and data structures for a broad variety of mixture model based data mining applications in Python\nsklearn.mixture \u2013 A module from the scikit-learn Python library for learning Gaussian Mixture Models (and sampling from them), previously packaged with SciPy and now packaged as a SciKit\nGMM.m Matlab code for GMM Implementation\nGPUmix C++ implementation of Bayesian Mixture Models using EM and MCMC with 100x speed acceleration using GPGPU.\n[2] Matlab code for GMM Implementation using EM algorithm\n[3] jMEF: A Java open source library for learning and processing mixtures of exponential families (using duality with Bregman divergences). Includes a Matlab wrapper.\nVery Fast and clean C implementation of the Expectation Maximization (EM) algorithm for estimating Gaussian Mixture Models (GMMs).\nmclust is an R package for mixture modeling.\ndpgmm Pure Python Dirichlet process Gaussian mixture model implementation (variational).\nGaussian Mixture Models Blog post on Gaussian Mixture Models trained via Expectation Maximization, with an implementation in Python.", "Likelihood function": "The likelihood function (often simply called the likelihood) is the joint probability of the observed data viewed as a function of the parameters of a statistical model.In maximum likelihood estimation, the arg max (over \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  ) of the likelihood function serves as a point estimate for \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , while the Fisher information (often approximated by the likelihood's Hessian matrix) indicates the estimate's precision. Meanwhile in Bayesian statistics, parameter estimates are derived from the converse of the likelihood, the so-called posterior probability, which is calculated via Bayes' rule.\n\n\n== Definition ==\nThe likelihood function, parameterized by a (possibly multivariate) parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , is usually defined differently for discrete and continuous probability distributions (a more general definition is discussed below). Given a probability density or mass function\n\n  \n    \n      \n        x\n        \u21a6\n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        ,\n        \n      \n    \n    {\\displaystyle x\\mapsto f(x\\mid \\theta ),\\!}\n  where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is a realization of the random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , the likelihood function is\n\n  \n    \n      \n        \u03b8\n        \u21a6\n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        ,\n        \n      \n    \n    {\\displaystyle \\theta \\mapsto f(x\\mid \\theta ),\\!}\n  often written\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n        .\n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x).\\!}\n  In other words, when \n  \n    \n      \n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(x\\mid \\theta )}\n   is viewed as a function of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   with \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   fixed, it is a probability density function, and when viewed as a function of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   with \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   fixed, it is a likelihood function. The likelihood function does not specify the probability that \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is the truth, given the observed sample \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  . Such an interpretation is a common error, with potentially disastrous consequences (see prosecutor's fallacy).\n\n\n=== Discrete probability distribution ===\nLet \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   be a discrete random variable with probability mass function \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   depending on a parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . Then the function\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n        =\n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        x\n        )\n        =\n        \n          P\n          \n            \u03b8\n          \n        \n        (\n        X\n        =\n        x\n        )\n        ,\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x)=p_{\\theta }(x)=P_{\\theta }(X=x),}\n  considered as a function of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , is the likelihood function, given the outcome \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of the random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . Sometimes the probability of \"the value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   for the parameter value \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   \" is written as P(X = x | \u03b8) or P(X = x; \u03b8). The likelihood is the probability that a particular outcome \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is observed when the true value of the parameter is \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , equivalent to the probability mass on \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  ; it is not a probability density over the parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . The likelihood, \n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x)}\n  , should not be confused with \n  \n    \n      \n        P\n        (\n        \u03b8\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle P(\\theta \\mid x)}\n  , which is the posterior probability of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   given the data \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .\nGiven no event (no data), the likelihood is 1; any non-trivial event will have a lower likelihood.\n\n\n==== Example ====\n\nConsider a simple statistical model of a coin flip: a single parameter \n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{H}}}\n   that expresses the \"fairness\" of the coin. The parameter is the probability that a coin lands heads up (\"H\") when tossed. \n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{H}}}\n   can take on any value within the range 0.0 to 1.0. For a perfectly fair coin, \n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{\\text{H}}=0.5}\n  .\nImagine flipping a fair coin twice, and observing two heads in two tosses (\"HH\"). Assuming that each successive coin flip is i.i.d., then the probability of observing HH is\n\n  \n    \n      \n        P\n        (\n        \n          HH\n        \n        \u2223\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        )\n        =\n        \n          0.5\n          \n            2\n          \n        \n        =\n        0.25.\n      \n    \n    {\\displaystyle P({\\text{HH}}\\mid p_{\\text{H}}=0.5)=0.5^{2}=0.25.}\n  Equivalently, the likelihood at \n  \n    \n      \n        \u03b8\n        =\n        0.5\n      \n    \n    {\\displaystyle \\theta =0.5}\n   given that \"HH\" was observed is 0.25:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        \u2223\n        \n          HH\n        \n        )\n        =\n        0.25.\n      \n    \n    {\\displaystyle {\\mathcal {L}}(p_{\\text{H}}=0.5\\mid {\\text{HH}})=0.25.}\n  This is not the same as saying that \n  \n    \n      \n        P\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        \u2223\n        H\n        H\n        )\n        =\n        0.25\n      \n    \n    {\\displaystyle P(p_{\\text{H}}=0.5\\mid HH)=0.25}\n  , a conclusion which could only be reached via Bayes' theorem given knowledge about the marginal probabilities  \n  \n    \n      \n        P\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        )\n      \n    \n    {\\displaystyle P(p_{\\text{H}}=0.5)}\n   and \n  \n    \n      \n        P\n        (\n        H\n        H\n        )\n      \n    \n    {\\displaystyle P(HH)}\n  .\nNow suppose that the coin is not a fair coin, but instead that \n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.3\n      \n    \n    {\\displaystyle p_{\\text{H}}=0.3}\n  . Then the probability of two heads on two flips is\n\n  \n    \n      \n        P\n        (\n        \n          HH\n        \n        \u2223\n        \n          p\n          \n            H\n          \n        \n        =\n        0.3\n        )\n        =\n        \n          0.3\n          \n            2\n          \n        \n        =\n        0.09.\n      \n    \n    {\\displaystyle P({\\text{HH}}\\mid p_{\\text{H}}=0.3)=0.3^{2}=0.09.}\n  Hence\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.3\n        \u2223\n        \n          HH\n        \n        )\n        =\n        0.09.\n      \n    \n    {\\displaystyle {\\mathcal {L}}(p_{\\text{H}}=0.3\\mid {\\text{HH}})=0.09.}\n  More generally, for each value of \n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{\\text{H}}}\n  , we can calculate the corresponding likelihood. The result of such calculations is displayed in Figure 1. Note that the integral of \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n   over [0, 1] is 1/3; likelihoods need not integrate or sum to one over the parameter space.\n\n\n=== Continuous probability distribution ===\nLet \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   be a random variable following an absolutely continuous probability distribution with density function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   (a function of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  ) which depends on a parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . Then the function\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n        =\n        \n          f\n          \n            \u03b8\n          \n        \n        (\n        x\n        )\n        ,\n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x)=f_{\\theta }(x),\\,}\n  considered as a function of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , is the likelihood function (of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , given the outcome \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  ). Again, note that \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n   is not a probability density or mass function over \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , despite being a function of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   given the observation \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  .\n\n\n==== Relationship between the likelihood and probability density functions ====\nThe use of the probability density in specifying the likelihood function above is justified as follows. Given an observation \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n  , the likelihood for the interval \n  \n    \n      \n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n      \n    \n    {\\displaystyle [x_{j},x_{j}+h]}\n  , where \n  \n    \n      \n        h\n        >\n        0\n      \n    \n    {\\displaystyle h>0}\n   is a constant, is given by \n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        \u2208\n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])}\n  . Observe that\n\n  \n    \n      \n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        \u2208\n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n        )\n        =\n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            1\n            h\n          \n        \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        \u2208\n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n        )\n      \n    \n    {\\displaystyle \\operatorname {argmax} _{\\theta }{\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])=\\operatorname {argmax} _{\\theta }{\\frac {1}{h}}{\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])}\n  ,since \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is positive and constant. Because\n\n  \n    \n      \n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            1\n            h\n          \n        \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        \u2208\n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n        )\n        =\n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            1\n            h\n          \n        \n        Pr\n        (\n        \n          x\n          \n            j\n          \n        \n        \u2264\n        x\n        \u2264\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        \u2223\n        \u03b8\n        )\n        =\n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            1\n            h\n          \n        \n        \n          \u222b\n          \n            \n              x\n              \n                j\n              \n            \n          \n          \n            \n              x\n              \n                j\n              \n            \n            +\n            h\n          \n        \n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle \\operatorname {argmax} _{\\theta }{\\frac {1}{h}}{\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])=\\operatorname {argmax} _{\\theta }{\\frac {1}{h}}\\Pr(x_{j}\\leq x\\leq x_{j}+h\\mid \\theta )=\\operatorname {argmax} _{\\theta }{\\frac {1}{h}}\\int _{x_{j}}^{x_{j}+h}f(x\\mid \\theta )\\,dx,}\n  where \n  \n    \n      \n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(x\\mid \\theta )}\n   is the probability density function, it follows that\n\n  \n    \n      \n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        \u2208\n        [\n        \n          x\n          \n            j\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        +\n        h\n        ]\n        )\n        =\n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            1\n            h\n          \n        \n        \n          \u222b\n          \n            \n              x\n              \n                j\n              \n            \n          \n          \n            \n              x\n              \n                j\n              \n            \n            +\n            h\n          \n        \n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\operatorname {argmax} _{\\theta }{\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])=\\operatorname {argmax} _{\\theta }{\\frac {1}{h}}\\int _{x_{j}}^{x_{j}+h}f(x\\mid \\theta )\\,dx}\n  .The first fundamental theorem of calculus provides that\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                \n                  lim\n                  \n                    h\n                    \u2192\n                    \n                      0\n                      \n                        +\n                      \n                    \n                  \n                \n                \n                  \n                    1\n                    h\n                  \n                \n                \n                  \u222b\n                  \n                    \n                      x\n                      \n                        j\n                      \n                    \n                  \n                  \n                    \n                      x\n                      \n                        j\n                      \n                    \n                    +\n                    h\n                  \n                \n                f\n                (\n                x\n                \u2223\n                \u03b8\n                )\n                \n                d\n                x\n                =\n                f\n                (\n                \n                  x\n                  \n                    j\n                  \n                \n                \u2223\n                \u03b8\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\lim _{h\\to 0^{+}}{\\frac {1}{h}}\\int _{x_{j}}^{x_{j}+h}f(x\\mid \\theta )\\,dx=f(x_{j}\\mid \\theta ).\\end{aligned}}}\n  Then\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  argmax\n                  \n                    \u03b8\n                  \n                \n                \u2061\n                \n                  \n                    L\n                  \n                \n                (\n                \u03b8\n                \u2223\n                \n                  x\n                  \n                    j\n                  \n                \n                )\n                =\n                \n                  argmax\n                  \n                    \u03b8\n                  \n                \n                \u2061\n                \n                  [\n                  \n                    \n                      lim\n                      \n                        h\n                        \u2192\n                        \n                          0\n                          \n                            +\n                          \n                        \n                      \n                    \n                    \n                      \n                        L\n                      \n                    \n                    (\n                    \u03b8\n                    \u2223\n                    x\n                    \u2208\n                    [\n                    \n                      x\n                      \n                        j\n                      \n                    \n                    ,\n                    \n                      x\n                      \n                        j\n                      \n                    \n                    +\n                    h\n                    ]\n                    )\n                  \n                  ]\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  argmax\n                  \n                    \u03b8\n                  \n                \n                \u2061\n                \n                  [\n                  \n                    \n                      lim\n                      \n                        h\n                        \u2192\n                        \n                          0\n                          \n                            +\n                          \n                        \n                      \n                    \n                    \n                      \n                        1\n                        h\n                      \n                    \n                    \n                      \u222b\n                      \n                        \n                          x\n                          \n                            j\n                          \n                        \n                      \n                      \n                        \n                          x\n                          \n                            j\n                          \n                        \n                        +\n                        h\n                      \n                    \n                    f\n                    (\n                    x\n                    \u2223\n                    \u03b8\n                    )\n                    \n                    d\n                    x\n                  \n                  ]\n                \n                =\n                \n                  argmax\n                  \n                    \u03b8\n                  \n                \n                \u2061\n                f\n                (\n                \n                  x\n                  \n                    j\n                  \n                \n                \u2223\n                \u03b8\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&\\operatorname {argmax} _{\\theta }{\\mathcal {L}}(\\theta \\mid x_{j})=\\operatorname {argmax} _{\\theta }\\left[\\lim _{h\\to 0^{+}}{\\mathcal {L}}(\\theta \\mid x\\in [x_{j},x_{j}+h])\\right]\\\\[4pt]={}&\\operatorname {argmax} _{\\theta }\\left[\\lim _{h\\to 0^{+}}{\\frac {1}{h}}\\int _{x_{j}}^{x_{j}+h}f(x\\mid \\theta )\\,dx\\right]=\\operatorname {argmax} _{\\theta }f(x_{j}\\mid \\theta ).\\end{aligned}}}\n  Therefore,\n\n  \n    \n      \n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        \n          x\n          \n            j\n          \n        \n        )\n        =\n        \n          argmax\n          \n            \u03b8\n          \n        \n        \u2061\n        f\n        (\n        \n          x\n          \n            j\n          \n        \n        \u2223\n        \u03b8\n        )\n        ,\n        \n      \n    \n    {\\displaystyle \\operatorname {argmax} _{\\theta }{\\mathcal {L}}(\\theta \\mid x_{j})=\\operatorname {argmax} _{\\theta }f(x_{j}\\mid \\theta ),\\!}\n  and so maximizing the probability density at \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   amounts to maximizing the likelihood of the specific observation \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n  .\n\n\n=== In general ===\nIn measure-theoretic probability theory, the density function is defined as the Radon\u2013Nikodym derivative of the probability distribution relative to a common dominating measure. The likelihood function is this density interpreted as a function of the parameter, rather than the random variable. Thus, we can construct a likelihood function for any distribution, whether discrete, continuous, a mixture, or otherwise. (Likelihoods are comparable, e.g. for parameter estimation, only if they are Radon\u2013Nikodym derivatives with respect to the same dominating measure.)\nThe above discussion of the likelihood for discrete random variables uses the counting measure, under which the probability density at any outcome equals the probability of that outcome.\n\n\n=== Likelihoods for mixed continuous\u2013discrete distributions ===\nThe above can be extended in a simple way to allow consideration of distributions which contain both discrete and continuous components. Suppose that the distribution consists of a number of discrete probability masses \n  \n    \n      \n        \n          p\n          \n            k\n          \n        \n        \u03b8\n      \n    \n    {\\displaystyle p_{k}\\theta }\n   and a density \n  \n    \n      \n        f\n        (\n        x\n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(x\\mid \\theta )}\n  , where the sum of all the \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  's added to the integral of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is always one. Assuming that it is possible to distinguish an observation corresponding to one of the discrete probability masses from one which corresponds to the density component, the likelihood function for an observation from the continuous component can be dealt with in the manner shown above. For an observation from the discrete component, the likelihood function for an observation from the discrete component is simply\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n        =\n        \n          p\n          \n            k\n          \n        \n        (\n        \u03b8\n        )\n        ,\n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\theta \\mid x)=p_{k}(\\theta ),\\!}\n  where \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is the index of the discrete probability mass corresponding to observation \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , because maximizing the probability mass (or probability) at \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   amounts to maximizing the likelihood of the specific observation.\nThe fact that the likelihood function can be defined in a way that includes contributions that are not commensurate (the density and the probability mass) arises from the way in which the likelihood function is defined up to a constant of proportionality, where this \"constant\" can change with the observation \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , but not with the parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  .\n\n\n=== Regularity conditions ===\nIn the context of parameter estimation, the likelihood function is usually assumed to obey certain conditions, known as regularity conditions. These conditions are assumed in various proofs involving likelihood functions, and need to be verified in each particular application. For maximum likelihood estimation, the existence of a global maximum of the likelihood function is of the utmost importance. By the extreme value theorem, it suffices that the likelihood function is continuous on a compact parameter space for the maximum likelihood estimator to exist. While the continuity assumption is usually met, the compactness assumption about the parameter space is often not, as the bounds of the true parameter values are unknown. In that case, concavity of the likelihood function plays a key role.\nMore specifically, if the likelihood function is twice continuously differentiable on the k-dimensional parameter space \n  \n    \n      \n        \n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\Theta \\,}\n   assumed to be an open connected subset of \n  \n    \n      \n        \n        \n          \n            R\n          \n          \n            k\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\,\\mathbb {R} ^{k}\\;,}\n   there exists a unique maximum \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        \u2208\n        \u0398\n      \n    \n    {\\displaystyle {\\hat {\\theta }}\\in \\Theta }\n   if the matrix of second partials\n\n  \n    \n      \n        \n          H\n        \n        (\n        \u03b8\n        )\n        \u2261\n        \n          \n            [\n            \n              \n              \n                \n                  \n                    \n                      \u2202\n                      \n                        2\n                      \n                    \n                    L\n                  \n                  \n                    \n                    \u2202\n                    \n                      \u03b8\n                      \n                        i\n                      \n                    \n                    \n                    \u2202\n                    \n                      \u03b8\n                      \n                        j\n                      \n                    \n                    \n                  \n                \n              \n              \n            \n            ]\n          \n          \n            i\n            ,\n            j\n            =\n            1\n            ,\n            1\n          \n          \n            \n              n\n              \n                \n                  i\n                \n              \n            \n            ,\n            \n              n\n              \n                \n                  j\n                \n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {H} (\\theta )\\equiv \\left[\\,{\\frac {\\partial ^{2}L}{\\,\\partial \\theta _{i}\\,\\partial \\theta _{j}\\,}}\\,\\right]_{i,j=1,1}^{n_{\\mathrm {i} },n_{\\mathrm {j} }}\\;}\n   is negative definite for every \n  \n    \n      \n        \n        \u03b8\n        \u2208\n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\theta \\in \\Theta \\,}\n   at which the gradient \n  \n    \n      \n        \n        \u2207\n        L\n        \u2261\n        \n          \n            [\n            \n              \n              \n                \n                  \n                    \u2202\n                    L\n                  \n                  \n                    \n                    \u2202\n                    \n                      \u03b8\n                      \n                        i\n                      \n                    \n                    \n                  \n                \n              \n              \n            \n            ]\n          \n          \n            i\n            =\n            1\n          \n          \n            \n              n\n              \n                \n                  i\n                \n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\;\\nabla L\\equiv \\left[\\,{\\frac {\\partial L}{\\,\\partial \\theta _{i}\\,}}\\,\\right]_{i=1}^{n_{\\mathrm {i} }}\\;}\n   vanishes,and if\n\n  \n    \n      \n        \n          lim\n          \n            \u03b8\n            \u2192\n            \u2202\n            \u0398\n          \n        \n        L\n        (\n        \u03b8\n        )\n        =\n        0\n        \n        ,\n      \n    \n    {\\displaystyle \\lim _{\\theta \\to \\partial \\Theta }L(\\theta )=0\\;,}\n  i.e. the likelihood function approaches a constant on the boundary of the parameter space, \n  \n    \n      \n        \n        \u2202\n        \u0398\n        \n        ,\n      \n    \n    {\\displaystyle \\;\\partial \\Theta \\;,}\n   which may include the points at infinity if \n  \n    \n      \n        \n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\Theta \\,}\n   is unbounded. M\u00e4kel\u00e4inen et al. prove this result using Morse theory while informally appealing to a mountain pass property. Mascarenhas restates their proof using the mountain pass theorem.In the proofs of consistency and asymptotic normality of the maximum likelihood estimator, additional assumptions are made about the probability densities that form the basis of a particular likelihood function. These conditions were first established by Chanda. In particular, for almost all \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , and for all \n  \n    \n      \n        \n        \u03b8\n        \u2208\n        \u0398\n        \n        ,\n      \n    \n    {\\displaystyle \\,\\theta \\in \\Theta \\,,}\n  \n\n  \n    \n      \n        \n          \n            \n              \u2202\n              log\n              \u2061\n              f\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  r\n                \n              \n            \n          \n        \n        \n        ,\n        \n        \n          \n            \n              \n                \u2202\n                \n                  2\n                \n              \n              log\n              \u2061\n              f\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  r\n                \n              \n              \u2202\n              \n                \u03b8\n                \n                  s\n                \n              \n            \n          \n        \n        \n        ,\n        \n        \n          \n            \n              \n                \u2202\n                \n                  3\n                \n              \n              log\n              \u2061\n              f\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  r\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  s\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  t\n                \n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial \\log f}{\\partial \\theta _{r}}}\\,,\\quad {\\frac {\\partial ^{2}\\log f}{\\partial \\theta _{r}\\partial \\theta _{s}}}\\,,\\quad {\\frac {\\partial ^{3}\\log f}{\\partial \\theta _{r}\\,\\partial \\theta _{s}\\,\\partial \\theta _{t}}}\\,}\n  exist for all \n  \n    \n      \n        \n        r\n        ,\n        s\n        ,\n        t\n        =\n        1\n        ,\n        2\n        ,\n        \u2026\n        ,\n        k\n        \n      \n    \n    {\\displaystyle \\,r,s,t=1,2,\\ldots ,k\\,}\n   in order to ensure the existence of a Taylor expansion. Second, for almost all \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and for every \n  \n    \n      \n        \n        \u03b8\n        \u2208\n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\theta \\in \\Theta \\,}\n   it must be that\n\n  \n    \n      \n        \n          |\n          \n            \n              \n                \u2202\n                f\n              \n              \n                \u2202\n                \n                  \u03b8\n                  \n                    r\n                  \n                \n              \n            \n          \n          |\n        \n        <\n        \n          F\n          \n            r\n          \n        \n        (\n        x\n        )\n        \n        ,\n        \n        \n          |\n          \n            \n              \n                \n                  \u2202\n                  \n                    2\n                  \n                \n                f\n              \n              \n                \u2202\n                \n                  \u03b8\n                  \n                    r\n                  \n                \n                \n                \u2202\n                \n                  \u03b8\n                  \n                    s\n                  \n                \n              \n            \n          \n          |\n        \n        <\n        \n          F\n          \n            r\n            s\n          \n        \n        (\n        x\n        )\n        \n        ,\n        \n        \n          |\n          \n            \n              \n                \n                  \u2202\n                  \n                    3\n                  \n                \n                f\n              \n              \n                \u2202\n                \n                  \u03b8\n                  \n                    r\n                  \n                \n                \n                \u2202\n                \n                  \u03b8\n                  \n                    s\n                  \n                \n                \n                \u2202\n                \n                  \u03b8\n                  \n                    t\n                  \n                \n              \n            \n          \n          |\n        \n        <\n        \n          H\n          \n            r\n            s\n            t\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\left|{\\frac {\\partial f}{\\partial \\theta _{r}}}\\right|<F_{r}(x)\\,,\\quad \\left|{\\frac {\\partial ^{2}f}{\\partial \\theta _{r}\\,\\partial \\theta _{s}}}\\right|<F_{rs}(x)\\,,\\quad \\left|{\\frac {\\partial ^{3}f}{\\partial \\theta _{r}\\,\\partial \\theta _{s}\\,\\partial \\theta _{t}}}\\right|<H_{rst}(x)}\n  where \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   is such that \n  \n    \n      \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          H\n          \n            r\n            s\n            t\n          \n        \n        (\n        z\n        )\n        \n          d\n        \n        z\n        \u2264\n        M\n        <\n        \u221e\n        \n        .\n      \n    \n    {\\displaystyle \\,\\int _{-\\infty }^{\\infty }H_{rst}(z)\\mathrm {d} z\\leq M<\\infty \\;.}\n   This boundedness of the derivatives is needed to allow for differentiation under the integral sign. And lastly, it is assumed that the information matrix,\n\n  \n    \n      \n        \n          I\n        \n        (\n        \u03b8\n        )\n        =\n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \n            \n              \u2202\n              log\n              \u2061\n              f\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  r\n                \n              \n            \n          \n        \n         \n        \n          \n            \n              \u2202\n              log\n              \u2061\n              f\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  s\n                \n              \n            \n          \n        \n         \n        f\n         \n        \n          d\n        \n        z\n      \n    \n    {\\displaystyle \\mathbf {I} (\\theta )=\\int _{-\\infty }^{\\infty }{\\frac {\\partial \\log f}{\\partial \\theta _{r}}}\\ {\\frac {\\partial \\log f}{\\partial \\theta _{s}}}\\ f\\ \\mathrm {d} z}\n  is positive definite and \n  \n    \n      \n        \n        \n          |\n          \n            \n              I\n            \n            (\n            \u03b8\n            )\n          \n          |\n        \n        \n      \n    \n    {\\displaystyle \\,\\left|\\mathbf {I} (\\theta )\\right|\\,}\n   is finite. This ensures that the score has a finite variance.The above conditions are sufficient, but not necessary. That is, a model that does not meet these regularity conditions may or may not have a maximum likelihood estimator of the properties mentioned above. Further, in case of non-independently or non-identically distributed observations additional properties may need to be assumed.\nIn Bayesian statistics, almost identical regularity conditions are imposed on the likelihood function in order to proof asymptotic normality of the posterior probability, and therefore to justify a Laplace approximation of the posterior in large samples.\n\n\n== Likelihood ratio and relative likelihood ==\n\n\n=== Likelihood ratio ===\nA likelihood ratio is the ratio of any two specified likelihoods, frequently written as:\n\n  \n    \n      \n        \u039b\n        (\n        \n          \u03b8\n          \n            1\n          \n        \n        :\n        \n          \u03b8\n          \n            2\n          \n        \n        \u2223\n        x\n        )\n        =\n        \n          \n            \n              \n                \n                  L\n                \n              \n              (\n              \n                \u03b8\n                \n                  1\n                \n              \n              \u2223\n              x\n              )\n            \n            \n              \n                \n                  L\n                \n              \n              (\n              \n                \u03b8\n                \n                  2\n                \n              \n              \u2223\n              x\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Lambda (\\theta _{1}:\\theta _{2}\\mid x)={\\frac {{\\mathcal {L}}(\\theta _{1}\\mid x)}{{\\mathcal {L}}(\\theta _{2}\\mid x)}}}\n  The likelihood ratio is central to likelihoodist statistics: the law of likelihood states that degree to which data (considered as evidence) supports one parameter value versus another is measured by the likelihood ratio.\nIn frequentist inference, the likelihood ratio is the basis for a test statistic, the so-called likelihood-ratio test. By the Neyman\u2013Pearson lemma, this is the most powerful test for comparing two simple hypotheses at a given significance level. Numerous other tests can be viewed as likelihood-ratio tests or approximations thereof. The asymptotic distribution of the log-likelihood ratio, considered as a test statistic, is given by Wilks' theorem.\nThe likelihood ratio is also of central importance in Bayesian inference, where it is known as the Bayes factor, and is used in Bayes' rule. Stated in terms of odds, Bayes' rule states that the posterior odds of two alternatives, \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n   and \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n  , given an event \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , is the prior odds, times the likelihood ratio. As an equation:\n\n  \n    \n      \n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n        =\n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        )\n        \u22c5\n        \u039b\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n        .\n      \n    \n    {\\displaystyle O(A_{1}:A_{2}\\mid B)=O(A_{1}:A_{2})\\cdot \\Lambda (A_{1}:A_{2}\\mid B).}\n  The likelihood ratio is not directly used in AIC-based statistics. Instead, what is used is the relative likelihood of models (see below).\n\n\n=== Relative likelihood function ===\n\nSince the actual value of the likelihood function depends on the sample, it is often convenient to work with a standardized measure. Suppose that the maximum likelihood estimate for the parameter \u03b8 is \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n  .  Relative plausibilities of other \u03b8 values may be found by comparing the likelihoods of those other values with the likelihood of \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n  .  The relative likelihood of \u03b8 is defined to be\n\n  \n    \n      \n        R\n        (\n        \u03b8\n        )\n        =\n        \n          \n            \n              \n                \n                  L\n                \n              \n              (\n              \u03b8\n              \u2223\n              x\n              )\n            \n            \n              \n                \n                  L\n                \n              \n              (\n              \n                \n                  \n                    \u03b8\n                    ^\n                  \n                \n              \n              \u2223\n              x\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle R(\\theta )={\\frac {{\\mathcal {L}}(\\theta \\mid x)}{{\\mathcal {L}}({\\hat {\\theta }}\\mid x)}}.}\n  Thus, the relative likelihood is the likelihood ratio (discussed above) with the fixed denominator \n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}({\\hat {\\theta }})}\n  . This corresponds to standardizing the likelihood to have a maximum of 1.\n\n\n==== Likelihood region ====\nA likelihood region is the set of all values of \u03b8 whose relative likelihood is greater than or equal to a given threshold. In terms of percentages, a p% likelihood region for \u03b8 is defined to be\n\n  \n    \n      \n        \n          {\n          \n            \u03b8\n            :\n            R\n            (\n            \u03b8\n            )\n            \u2265\n            \n              \n                p\n                100\n              \n            \n          \n          }\n        \n        .\n      \n    \n    {\\displaystyle \\left\\{\\theta :R(\\theta )\\geq {\\frac {p}{100}}\\right\\}.}\n  If \u03b8 is a single real parameter, a p% likelihood region will usually comprise an interval of real values. If the region does comprise an interval, then it is called a likelihood interval.Likelihood intervals, and more generally likelihood regions, are used for interval estimation within likelihoodist statistics: they are similar to confidence intervals in frequentist statistics and credible intervals in Bayesian statistics. Likelihood intervals are interpreted directly in terms of relative likelihood, not in terms of coverage probability (frequentism) or posterior probability (Bayesianism).\nGiven a model, likelihood intervals can be compared to confidence intervals. If \u03b8 is a single real parameter, then under certain conditions, a 14.65% likelihood interval (about 1:7 likelihood) for \u03b8 will be the same as a 95% confidence interval (19/20 coverage probability). In a slightly different formulation suited to the use of log-likelihoods (see Wilks' theorem), the test statistic is twice the difference in log-likelihoods and the probability distribution of the test statistic is approximately a chi-squared distribution with degrees-of-freedom (df) equal to the difference in df's between the two models (therefore, the e\u22122 likelihood interval is the same as the 0.954 confidence interval; assuming difference in df's to be 1).\n\n\n== Likelihoods that eliminate nuisance parameters ==\nIn many cases, the likelihood is a function of more than one parameter but interest focuses on the estimation of only one, or at most a few of them, with the others being considered as nuisance parameters. Several alternative approaches have been developed to eliminate such nuisance parameters, so that a likelihood can be written as a function of only the parameter (or parameters) of interest: the main approaches are profile, conditional, and marginal likelihoods. These approaches are also useful when a high-dimensional likelihood surface needs to be reduced to one or two parameters of interest in order to allow a graph.\n\n\n=== Profile likelihood ===\nIt is possible to reduce the dimensions by concentrating the likelihood function for a subset of parameters by expressing the nuisance parameters as functions of the parameters of interest and replacing them in the likelihood function. In general, for a likelihood function depending on the parameter vector \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle \\mathbf {\\theta } }\n   that can be partitioned into \n  \n    \n      \n        \n          \u03b8\n        \n        =\n        \n          (\n          \n            \n              \n                \u03b8\n              \n              \n                1\n              \n            \n            :\n            \n              \n                \u03b8\n              \n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {\\theta } =\\left(\\mathbf {\\theta } _{1}:\\mathbf {\\theta } _{2}\\right)}\n  , and where a correspondence \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          (\n          \n            \n              \u03b8\n            \n            \n              1\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {\\theta }} _{2}=\\mathbf {\\hat {\\theta }} _{2}\\left(\\mathbf {\\theta } _{1}\\right)}\n   can be determined explicitly, concentration reduces computational burden of the original maximization problem.For instance, in a linear regression with normally distributed errors, \n  \n    \n      \n        \n          y\n        \n        =\n        \n          X\n        \n        \u03b2\n        +\n        u\n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {X} \\beta +u}\n  , the coefficient vector could be partitioned into \n  \n    \n      \n        \u03b2\n        =\n        \n          [\n          \n            \n              \u03b2\n              \n                1\n              \n            \n            :\n            \n              \u03b2\n              \n                2\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\beta =\\left[\\beta _{1}:\\beta _{2}\\right]}\n   (and consequently the design matrix \n  \n    \n      \n        \n          X\n        \n        =\n        \n          [\n          \n            \n              \n                X\n              \n              \n                1\n              \n            \n            :\n            \n              \n                X\n              \n              \n                2\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbf {X} =\\left[\\mathbf {X} _{1}:\\mathbf {X} _{2}\\right]}\n  ). Maximizing with respect to \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta _{2}}\n   yields an optimal value function \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n        (\n        \n          \u03b2\n          \n            1\n          \n        \n        )\n        =\n        \n          \n            (\n            \n              \n                \n                  X\n                \n                \n                  2\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                \n                  X\n                \n                \n                  2\n                \n              \n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            X\n          \n          \n            2\n          \n          \n            \n              T\n            \n          \n        \n        \n          (\n          \n            \n              y\n            \n            \u2212\n            \n              \n                X\n              \n              \n                1\n              \n            \n            \n              \u03b2\n              \n                1\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\beta _{2}(\\beta _{1})=\\left(\\mathbf {X} _{2}^{\\mathsf {T}}\\mathbf {X} _{2}\\right)^{-1}\\mathbf {X} _{2}^{\\mathsf {T}}\\left(\\mathbf {y} -\\mathbf {X} _{1}\\beta _{1}\\right)}\n  . Using this result, the maximum likelihood estimator for \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   can then be derived as\n\n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  X\n                \n                \n                  1\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                (\n                \n                  \n                    I\n                  \n                  \u2212\n                  \n                    \n                      P\n                    \n                    \n                      2\n                    \n                  \n                \n                )\n              \n              \n                \n                  X\n                \n                \n                  1\n                \n              \n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            X\n          \n          \n            1\n          \n          \n            \n              T\n            \n          \n        \n        \n          (\n          \n            \n              I\n            \n            \u2212\n            \n              \n                P\n              \n              \n                2\n              \n            \n          \n          )\n        \n        \n          y\n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}_{1}=\\left(\\mathbf {X} _{1}^{\\mathsf {T}}\\left(\\mathbf {I} -\\mathbf {P} _{2}\\right)\\mathbf {X} _{1}\\right)^{-1}\\mathbf {X} _{1}^{\\mathsf {T}}\\left(\\mathbf {I} -\\mathbf {P} _{2}\\right)\\mathbf {y} }\n  where \n  \n    \n      \n        \n          \n            P\n          \n          \n            2\n          \n        \n        =\n        \n          \n            X\n          \n          \n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  X\n                \n                \n                  2\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                \n                  X\n                \n                \n                  2\n                \n              \n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            X\n          \n          \n            2\n          \n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {P} _{2}=\\mathbf {X} _{2}\\left(\\mathbf {X} _{2}^{\\mathsf {T}}\\mathbf {X} _{2}\\right)^{-1}\\mathbf {X} _{2}^{\\mathsf {T}}}\n   is the projection matrix of \n  \n    \n      \n        \n          \n            X\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} _{2}}\n  . This result is known as the Frisch\u2013Waugh\u2013Lovell theorem.\nSince graphically the procedure of concentration is equivalent to slicing the likelihood surface along the ridge of values of the nuisance parameter \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta _{2}}\n   that maximizes the likelihood function, creating an isometric profile of the likelihood function for a given \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  , the result of this procedure is also known as profile likelihood. In addition to being graphed, the profile likelihood can also be used to compute confidence intervals that often have better small-sample properties than those based on asymptotic standard errors calculated from the full likelihood.\n\n\n=== Conditional likelihood ===\nSometimes it is possible to find a sufficient statistic for the nuisance parameters, and conditioning on this statistic results in a likelihood which does not depend on the nuisance parameters.One example occurs in 2\u00d72 tables, where conditioning on all four marginal totals leads to a conditional likelihood based on the non-central hypergeometric distribution. This form of conditioning is also the basis for Fisher's exact test.\n\n\n=== Marginal likelihood ===\n\nSometimes we can remove the nuisance parameters by considering a likelihood based on only part of the information in the data, for example by using the set of ranks rather than the numerical values. Another example occurs in linear mixed models, where considering a likelihood for the residuals only after fitting the fixed effects leads to residual maximum likelihood estimation of the variance components.\n\n\n=== Partial likelihood ===\nA partial likelihood is an adaption of the full likelihood such that only a part of the parameters (the parameters of interest) occur in it. It is a key component of the proportional hazards model: using a restriction on the hazard function, the likelihood does not contain the shape of the hazard over time.\n\n\n== Products of likelihoods ==\nThe likelihood, given two or more independent events, is the product of the likelihoods of each of the individual events:\n\n  \n    \n      \n        \u039b\n        (\n        A\n        \u2223\n        \n          X\n          \n            1\n          \n        \n        \u2227\n        \n          X\n          \n            2\n          \n        \n        )\n        =\n        \u039b\n        (\n        A\n        \u2223\n        \n          X\n          \n            1\n          \n        \n        )\n        \u22c5\n        \u039b\n        (\n        A\n        \u2223\n        \n          X\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Lambda (A\\mid X_{1}\\land X_{2})=\\Lambda (A\\mid X_{1})\\cdot \\Lambda (A\\mid X_{2})}\n  This follows from the definition of independence in probability: the probabilities of two independent events happening, given a model, is the product of the probabilities.\nThis is particularly important when the events are from independent and identically distributed random variables, such as independent observations or sampling with replacement. In such a situation, the likelihood function factors into a product of individual likelihood functions.\nThe empty product has value 1, which corresponds to the likelihood, given no event, being 1: before any data, the likelihood is always 1. This is similar to a uniform prior in Bayesian statistics, but in likelihoodist statistics this is not an improper prior because likelihoods are not integrated.\n\n\n== Log-likelihood ==\n\nLog-likelihood function is a logarithmic transformation of the likelihood function, often denoted by a lowercase l or \n  \n    \n      \n        \u2113\n      \n    \n    {\\displaystyle \\ell }\n  , to contrast with the uppercase L or \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n   for the likelihood. Because logarithms are strictly increasing functions, maximizing the likelihood is equivalent to maximizing the log-likelihood. But for practical purposes it is more convenient to work with the log-likelihood function in maximum likelihood estimation, in particular since most common probability distributions\u2014notably the exponential family\u2014are only logarithmically concave, and concavity of the objective function plays a key role in the maximization.\nGiven the independence of each event, the overall log-likelihood of intersection equals the sum of the log-likelihoods of the individual events. This is analogous to the fact that the overall log-probability is the sum of the log-probability of the individual events. In addition to the mathematical convenience from this, the adding process of log-likelihood has an intuitive interpretation, as often expressed as \"support\" from the data. When the parameters are estimated using the log-likelihood for the maximum likelihood estimation, each data point is used by being added to the total log-likelihood. As the data can be viewed as an evidence that support the estimated parameters, this process can be interpreted as \"support from independent evidence adds\", and the log-likelihood is the \"weight of evidence\". Interpreting negative log-probability as information content or surprisal, the support (log-likelihood) of a model, given an event, is the negative of the surprisal of the event, given the model: a model is supported by an event to the extent that the event is unsurprising, given the model.\nA logarithm of a likelihood ratio is equal to the difference of the log-likelihoods:\n\n  \n    \n      \n        log\n        \u2061\n        \n          \n            \n              L\n              (\n              A\n              )\n            \n            \n              L\n              (\n              B\n              )\n            \n          \n        \n        =\n        log\n        \u2061\n        L\n        (\n        A\n        )\n        \u2212\n        log\n        \u2061\n        L\n        (\n        B\n        )\n        =\n        \u2113\n        (\n        A\n        )\n        \u2212\n        \u2113\n        (\n        B\n        )\n        .\n      \n    \n    {\\displaystyle \\log {\\frac {L(A)}{L(B)}}=\\log L(A)-\\log L(B)=\\ell (A)-\\ell (B).}\n  Just as the likelihood, given no event, being 1, the log-likelihood, given no event, is 0, which corresponds to the value of the empty sum: without any data, there is no support for any models.\n\n\n=== Graph ===\nThe graph of the log-likelihood is called the support curve (in the univariate case)..\nIn the multivariate case, the concept generalizes into a support surface over the parameter space.\nIt has a relation to, but is distinct from, the support of a distribution.\nThe term was coined by A. W. F. Edwards in the context of statistical hypothesis testing, i.e. whether or not the data \"support\" one hypothesis (or parameter value) being tested more than any other.\nThe log-likelihood function being plotted is used in the computation of the score (the gradient of the log-likelihood) and Fisher information (the curvature of the log-likelihood). This, the graph has a direct interpretation in the context of maximum likelihood estimation and likelihood-ratio tests.\n\n\n=== Likelihood equations ===\nIf the log-likelihood function is smooth, its gradient with respect to the parameter, known as the score and written \n  \n    \n      \n        \n          s\n          \n            n\n          \n        \n        (\n        \u03b8\n        )\n        \u2261\n        \n          \u2207\n          \n            \u03b8\n          \n        \n        \n          \u2113\n          \n            n\n          \n        \n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle s_{n}(\\theta )\\equiv \\nabla _{\\theta }\\ell _{n}(\\theta )}\n  , exists and allows for the application of differential calculus. The basic way to maximize a differentiable function is to find the stationary points (the points where the derivative is zero); since the derivative of a sum is just the sum of the derivatives, but the derivative of a product requires the product rule, it is easier to compute the stationary points of the log-likelihood of independent events than for the likelihood of independent events.\nThe equations defined by the stationary point of the score function serve as estimating equations for the maximum likelihood estimator.\n\n  \n    \n      \n        \n          s\n          \n            n\n          \n        \n        (\n        \u03b8\n        )\n        =\n        \n          0\n        \n      \n    \n    {\\displaystyle s_{n}(\\theta )=\\mathbf {0} }\n  In that sense, the maximum likelihood estimator is implicitly defined by the value at \n  \n    \n      \n        \n          0\n        \n      \n    \n    {\\displaystyle \\mathbf {0} }\n   of the inverse function \n  \n    \n      \n        \n          s\n          \n            n\n          \n          \n            \u2212\n            1\n          \n        \n        :\n        \n          \n            E\n          \n          \n            d\n          \n        \n        \u2192\n        \u0398\n      \n    \n    {\\displaystyle s_{n}^{-1}:\\mathbb {E} ^{d}\\to \\Theta }\n  , where \n  \n    \n      \n        \n          \n            E\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} ^{d}}\n   is the d-dimensional Euclidean space, and \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   is the parameter space. Using the inverse function theorem, it can be shown that \n  \n    \n      \n        \n          s\n          \n            n\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle s_{n}^{-1}}\n   is well-defined in an open neighborhood about \n  \n    \n      \n        \n          0\n        \n      \n    \n    {\\displaystyle \\mathbf {0} }\n   with probability going to one, and \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        =\n        \n          s\n          \n            n\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        \n          0\n        \n        )\n      \n    \n    {\\displaystyle {\\hat {\\theta }}_{n}=s_{n}^{-1}(\\mathbf {0} )}\n   is a consistent estimate of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . As a consequence there exists a sequence \n  \n    \n      \n        \n          {\n          \n            \n              \n                \n                  \u03b8\n                  ^\n                \n              \n            \n            \n              n\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{{\\hat {\\theta }}_{n}\\right\\}}\n   such that \n  \n    \n      \n        \n          s\n          \n            n\n          \n        \n        (\n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        )\n        =\n        \n          0\n        \n      \n    \n    {\\displaystyle s_{n}({\\hat {\\theta }}_{n})=\\mathbf {0} }\n   asymptotically almost surely, and \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        \n          \n            \u2192\n            \n              p\n            \n          \n        \n        \n          \u03b8\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}_{n}{\\xrightarrow {\\text{p}}}\\theta _{0}}\n  . A similar result can be established using Rolle's theorem.The second derivative evaluated at \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n  , known as Fisher information, determines the curvature of the likelihood surface, and thus indicates the precision of the estimate.\n\n\n=== Exponential families ===\n\nThe log-likelihood is also particularly useful for exponential families of distributions, which include many of the common parametric probability distributions. The probability distribution function (and thus likelihood function) for exponential families contain products of factors involving exponentiation. The logarithm of such a function is a sum of products, again easier to differentiate than the original function.\nAn exponential family is one whose probability density function is of the form (for some functions, writing \n  \n    \n      \n        \u27e8\n        \u2212\n        ,\n        \u2212\n        \u27e9\n      \n    \n    {\\displaystyle \\langle -,-\\rangle }\n   for the inner product):\n\n  \n    \n      \n        p\n        (\n        x\n        \u2223\n        \n          \u03b8\n        \n        )\n        =\n        h\n        (\n        x\n        )\n        exp\n        \u2061\n        \n          \n            (\n          \n        \n        \u27e8\n        \n          \u03b7\n        \n        (\n        \n          \u03b8\n        \n        )\n        ,\n        \n          T\n        \n        (\n        x\n        )\n        \u27e9\n        \u2212\n        A\n        (\n        \n          \u03b8\n        \n        )\n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle p(x\\mid {\\boldsymbol {\\theta }})=h(x)\\exp {\\Big (}\\langle {\\boldsymbol {\\eta }}({\\boldsymbol {\\theta }}),\\mathbf {T} (x)\\rangle -A({\\boldsymbol {\\theta }}){\\Big )}.}\n  Each of these terms has an interpretation, but simply switching from probability to likelihood and taking logarithms yields the sum:\n\n  \n    \n      \n        \u2113\n        (\n        \n          \u03b8\n        \n        \u2223\n        x\n        )\n        =\n        \u27e8\n        \n          \u03b7\n        \n        (\n        \n          \u03b8\n        \n        )\n        ,\n        \n          T\n        \n        (\n        x\n        )\n        \u27e9\n        \u2212\n        A\n        (\n        \n          \u03b8\n        \n        )\n        +\n        log\n        \u2061\n        h\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle \\ell ({\\boldsymbol {\\theta }}\\mid x)=\\langle {\\boldsymbol {\\eta }}({\\boldsymbol {\\theta }}),\\mathbf {T} (x)\\rangle -A({\\boldsymbol {\\theta }})+\\log h(x).}\n  The \n  \n    \n      \n        \n          \u03b7\n        \n        (\n        \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\eta }}({\\boldsymbol {\\theta }})}\n   and \n  \n    \n      \n        h\n        (\n        x\n        )\n      \n    \n    {\\displaystyle h(x)}\n   each correspond to a change of coordinates, so in these coordinates, the log-likelihood of an exponential family is given by the simple formula:\n\n  \n    \n      \n        \u2113\n        (\n        \n          \u03b7\n        \n        \u2223\n        x\n        )\n        =\n        \u27e8\n        \n          \u03b7\n        \n        ,\n        \n          T\n        \n        (\n        x\n        )\n        \u27e9\n        \u2212\n        A\n        (\n        \n          \u03b7\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\ell ({\\boldsymbol {\\eta }}\\mid x)=\\langle {\\boldsymbol {\\eta }},\\mathbf {T} (x)\\rangle -A({\\boldsymbol {\\eta }}).}\n  In words, the log-likelihood of an exponential family is inner product of the natural parameter \n  \n    \n      \n        \n          \u03b7\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\eta }}}\n   and the sufficient statistic \n  \n    \n      \n        \n          T\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathbf {T} (x)}\n  , minus the normalization factor (log-partition function) \n  \n    \n      \n        A\n        (\n        \n          \u03b7\n        \n        )\n      \n    \n    {\\displaystyle A({\\boldsymbol {\\eta }})}\n  . Thus for example the maximum likelihood estimate can be computed by taking derivatives of the sufficient statistic T and the log-partition function A.\n\n\n==== Example: the gamma distribution ====\nThe gamma distribution is an exponential family with two parameters, \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   and \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  . The likelihood function is\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03b1\n        ,\n        \u03b2\n        \u2223\n        x\n        )\n        =\n        \n          \n            \n              \u03b2\n              \n                \u03b1\n              \n            \n            \n              \u0393\n              (\n              \u03b1\n              )\n            \n          \n        \n        \n          x\n          \n            \u03b1\n            \u2212\n            1\n          \n        \n        \n          e\n          \n            \u2212\n            \u03b2\n            x\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\alpha ,\\beta \\mid x)={\\frac {\\beta ^{\\alpha }}{\\Gamma (\\alpha )}}x^{\\alpha -1}e^{-\\beta x}.}\n  Finding the maximum likelihood estimate of \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   for a single observed value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   looks rather daunting. Its logarithm is much simpler to work with:\n\n  \n    \n      \n        log\n        \u2061\n        \n          \n            L\n          \n        \n        (\n        \u03b1\n        ,\n        \u03b2\n        \u2223\n        x\n        )\n        =\n        \u03b1\n        log\n        \u2061\n        \u03b2\n        \u2212\n        log\n        \u2061\n        \u0393\n        (\n        \u03b1\n        )\n        +\n        (\n        \u03b1\n        \u2212\n        1\n        )\n        log\n        \u2061\n        x\n        \u2212\n        \u03b2\n        x\n        .\n        \n      \n    \n    {\\displaystyle \\log {\\mathcal {L}}(\\alpha ,\\beta \\mid x)=\\alpha \\log \\beta -\\log \\Gamma (\\alpha )+(\\alpha -1)\\log x-\\beta x.\\,}\n  To maximize the log-likelihood, we first take the partial derivative with respect to \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  :\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              log\n              \u2061\n              \n                \n                  L\n                \n              \n              (\n              \u03b1\n              ,\n              \u03b2\n              \u2223\n              x\n              )\n            \n            \n              \u2202\n              \u03b2\n            \n          \n        \n        =\n        \n          \n            \u03b1\n            \u03b2\n          \n        \n        \u2212\n        x\n        .\n      \n    \n    {\\displaystyle {\\frac {\\partial \\log {\\mathcal {L}}(\\alpha ,\\beta \\mid x)}{\\partial \\beta }}={\\frac {\\alpha }{\\beta }}-x.}\n  If there are a number of independent observations \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n  , then the joint log-likelihood will be the sum of individual log-likelihoods, and the derivative of this sum will be a sum of derivatives of each individual log-likelihood:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    \n                      \u2202\n                      log\n                      \u2061\n                      \n                        \n                          L\n                        \n                      \n                      (\n                      \u03b1\n                      ,\n                      \u03b2\n                      \u2223\n                      \n                        x\n                        \n                          1\n                        \n                      \n                      ,\n                      \u2026\n                      ,\n                      \n                        x\n                        \n                          n\n                        \n                      \n                      )\n                    \n                    \n                      \u2202\n                      \u03b2\n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      \u2202\n                      log\n                      \u2061\n                      \n                        \n                          L\n                        \n                      \n                      (\n                      \u03b1\n                      ,\n                      \u03b2\n                      \u2223\n                      \n                        x\n                        \n                          1\n                        \n                      \n                      )\n                    \n                    \n                      \u2202\n                      \u03b2\n                    \n                  \n                \n                +\n                \u22ef\n                +\n                \n                  \n                    \n                      \u2202\n                      log\n                      \u2061\n                      \n                        \n                          L\n                        \n                      \n                      (\n                      \u03b1\n                      ,\n                      \u03b2\n                      \u2223\n                      \n                        x\n                        \n                          n\n                        \n                      \n                      )\n                    \n                    \n                      \u2202\n                      \u03b2\n                    \n                  \n                \n                =\n                \n                  \n                    \n                      n\n                      \u03b1\n                    \n                    \u03b2\n                  \n                \n                \u2212\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  x\n                  \n                    i\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\frac {\\partial \\log {\\mathcal {L}}(\\alpha ,\\beta \\mid x_{1},\\ldots ,x_{n})}{\\partial \\beta }}\\\\={}&{\\frac {\\partial \\log {\\mathcal {L}}(\\alpha ,\\beta \\mid x_{1})}{\\partial \\beta }}+\\cdots +{\\frac {\\partial \\log {\\mathcal {L}}(\\alpha ,\\beta \\mid x_{n})}{\\partial \\beta }}={\\frac {n\\alpha }{\\beta }}-\\sum _{i=1}^{n}x_{i}.\\end{aligned}}}\n  To complete the maximization procedure for the joint log-likelihood, the equation is set to zero and solved for \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  :\n\n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        =\n        \n          \n            \u03b1\n            \n              \n                x\n                \u00af\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\beta }}={\\frac {\\alpha }{\\bar {x}}}.}\n  Here \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\beta }}}\n   denotes the maximum-likelihood estimate, and \n  \n    \n      \n        \n          \n            \n              \n                x\n                \u00af\n              \n            \n          \n          =\n          \n            \n              1\n              n\n            \n          \n          \n            \u2211\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            x\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle {\\bar {x}}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}}\n   is the sample mean of the observations.\n\n\n== Background and interpretation ==\n\n\n=== Historical remarks ===\n\nThe term \"likelihood\" has been in use in English since at least late Middle English. Its formal use to refer to a specific function in mathematical statistics was proposed by Ronald Fisher, in two research papers published in 1921 and 1922. The 1921 paper introduced what is today called a \"likelihood interval\"; the 1922 paper introduced the term \"method of maximum likelihood\". Quoting Fisher:\n\n[I]n 1922, I proposed the term 'likelihood,' in view of the fact that, with respect to [the parameter], it is not a probability, and does not obey the laws of probability, while at the same time it bears to the problem of rational choice among the possible values of [the parameter] a relation similar to that which probability bears to the problem of predicting events in games of chance. . . . Whereas, however, in relation to psychological judgment, likelihood has some resemblance to probability, the two concepts are wholly distinct. . . .\"\nThe concept of likelihood should not be confused with probability as mentioned by Sir Ronald Fisher\n\nI stress this because in spite of the emphasis that I have always laid upon the difference between probability and likelihood there is still a tendency to treat likelihood as though it were a sort of probability. The first result is thus that there are two different measures of rational belief appropriate to different cases. Knowing the population we can express our incomplete knowledge of, or expectation of, the sample in terms of probability; knowing the sample we can express our incomplete knowledge of the population in terms of likelihood.\nFisher's invention of statistical likelihood was in reaction against an earlier form of reasoning called inverse probability. His use of the term \"likelihood\" fixed the meaning of the term within mathematical statistics.\nA. W. F. Edwards (1972) established the axiomatic basis for use of the log-likelihood ratio as a measure of relative support for one hypothesis against another. The support function is then the natural logarithm of the likelihood function. Both terms are used in phylogenetics, but were not adopted in a general treatment of the topic of statistical evidence.\n\n\n=== Interpretations under different foundations ===\nAmong statisticians, there is no consensus about what the foundation of statistics should be. There are four main paradigms that have been proposed for the foundation: frequentism, Bayesianism, likelihoodism, and AIC-based. For each of the proposed foundations, the interpretation of likelihood is different. The four interpretations are described in the subsections below.\n\n\n==== Frequentist interpretation ====\n\n\n==== Bayesian interpretation ====\nIn Bayesian inference, although one can speak about the likelihood of any proposition or random variable given another random variable: for example the likelihood of a parameter value or of a statistical model (see marginal likelihood), given specified data or other evidence, the likelihood function remains the same entity, with the additional interpretations of (i) a conditional density of the data given the parameter (since the parameter is then a random variable) and (ii) a measure or amount of information brought by the data about the parameter value or even the model. Due to the introduction of a probability structure on the parameter space or on the collection of models, it is possible that a parameter value or a statistical model have a large likelihood value for given data, and yet have a low probability, or vice versa. This is often the case in medical contexts. Following Bayes' Rule, the likelihood when seen as a conditional density can be multiplied by the prior probability density of the parameter and then normalized, to give a posterior probability density. More generally, the likelihood of an unknown quantity \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given another unknown quantity \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is proportional to the probability of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  .\n\n\n==== Likelihoodist interpretation ====\nIn frequentist statistics, the likelihood function is itself a statistic that summarizes a single sample from a population, whose calculated value depends on a choice of several parameters \u03b81 ... \u03b8p, where p is the count of parameters in some already-selected statistical model. The value of the likelihood serves as a figure of merit for the choice used for the parameters, and the parameter set with maximum likelihood is the best choice, given the data available.\nThe specific calculation of the likelihood is the probability that the observed sample would be assigned, assuming that the model chosen and the values of the several parameters \u03b8 give an accurate approximation of the frequency distribution of the population that the observed sample was drawn from. Heuristically, it makes sense that a good choice of parameters is those which render the sample actually observed the maximum possible post-hoc probability of having happened. Wilks' theorem quantifies the heuristic rule by showing that the difference in the logarithm of the likelihood generated by the estimate's parameter values and the logarithm of the likelihood generated by population's \"true\" (but unknown) parameter values is asymptotically \u03c72 distributed.\nEach independent sample's maximum likelihood estimate is a separate estimate of the \"true\" parameter set describing the population sampled. Successive estimates from many independent samples will cluster together with the population's \"true\" set of parameter values hidden somewhere in their midst. The difference in the logarithms of the maximum likelihood and adjacent parameter sets' likelihoods may be used to draw a confidence region on a plot whose co-ordinates are the parameters \u03b81 ... \u03b8p. The region surrounds the maximum-likelihood estimate, and all points (parameter sets) within that region differ at most in log-likelihood by some fixed value. The \u03c72 distribution given by Wilks' theorem converts the region's log-likelihood differences into the \"confidence\" that the population's \"true\" parameter set lies inside. The art of choosing the fixed log-likelihood difference is to make the confidence acceptably high while keeping the region acceptably small (narrow range of estimates).\nAs more data are observed, instead of being used to make independent estimates, they can be combined with the previous samples to make a single combined sample, and that large sample may be used for a new maximum likelihood estimate. As the size of the combined sample increases, the size of the likelihood region with the same confidence shrinks. Eventually, either the size of the confidence region is very nearly a single point, or the entire population has been sampled; in both cases, the estimated parameter set is essentially the same as the population parameter set.\n\n\n==== AIC-based interpretation ====\nUnder the AIC paradigm, likelihood is interpreted within the context of information theory.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nAzzalini, Adelchi (1996). \"Likelihood\". Statistical Inference Based on the Likelihood. Chapman and Hall. pp. 17\u201350. ISBN 0-412-60650-X.\nBoos, Dennis D.; Stefanski, L. A. (2013). \"Likelihood Construction and Estimation\". Essential Statistical Inference : Theory and Methods. New York: Springer. pp. 27\u2013124. doi:10.1007/978-1-4614-4818-1_2. ISBN 978-1-4614-4817-4.\nEdwards, A. W. F. (1992) [1972]. Likelihood (Expanded ed.). Johns Hopkins University Press. ISBN 0-8018-4443-6.\nKing, Gary (1989). \"The Likelihood Model of Inference\". Unifying Political Methodology : the Likehood Theory of Statistical Inference. Cambridge University Press. pp. 59\u201394. ISBN 0-521-36697-6.\nLindsey, J. K. (1996). \"Likelihood\". Parametric Statistical Inference. Oxford University Press. pp. 69\u2013139. ISBN 0-19-852359-9.\nRohde, Charles A. (2014). Introductory Statistical Inference with the Likelihood Function. Berlin: Springer. ISBN 978-3-319-10460-7.\nRoyall, Richard (1997). Statistical Evidence : A Likelihood Paradigm. London: Chapman & Hall. ISBN 0-412-04411-0.\nWard, Michael D.; Ahlquist, John S. (2018). \"The Likelihood Function: A Deeper Dive\". Maximum Likelihood for Social Science : Strategies for Analysis. Cambridge University Press. pp. 21\u201328. ISBN 978-1-316-63682-4.\n\n\n== External links ==\n\nLikelihood function at Planetmath\n\"Log-likelihood\". Statlect.", "Decision tree": "A decision tree  is a decision support hierarchical model that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n\n\n== Overview ==\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\nIn decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.\nA decision tree consists of three types of nodes:\nDecision nodes \u2013 typically represented by squares\nChance nodes \u2013 typically represented by circles\nEnd nodes \u2013 typically represented by trianglesDecision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.\nDecision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods.\n\n\n== Decision-tree building blocks ==\n\n\n=== Decision-tree elements ===\n\nDrawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). So used manually they can grow very big and are then often hard to draw fully by hand. Traditionally, decision trees have been created manually \u2013 as the aside example shows \u2013 although increasingly, specialized software is employed.\n\n\n=== Decision rules ===\nThe decision tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form:\n\nif condition1 and condition2 and condition3 then outcome.Decision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.\n\n\n=== Decision tree using flowchart symbols ===\nCommonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. Note there is a conceptual error in the \"Proceed\" calculation of the tree shown below; the error relates to the calculation of \"costs\" awarded in a legal action.\n\n\n=== Analysis example ===\nAnalysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example:\n\nThe basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400K\u2014in that range of risk aversion, the company would need to model a third strategy, \"Neither A nor B\").\nAnother example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the \"Life's a Beach\" example). The example describes two beaches with lifeguards to be distributed on each beach. There is maximum budget B that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach.\n\nIn this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1.\n\nThe decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. But if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings.\n\n\n=== Influence diagram ===\nMuch of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.\n\n\n== Association rule induction ==\n\nDecision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or \"questions\"). Several algorithms to generate such optimal trees have been devised, such as ID3/4/5, CLS, ASSISTANT, and CART.\n\n\n== Advantages and disadvantages ==\nAmong decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:\n\nAre simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\nHave value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\nHelp determine worst, best, and expected values for different scenarios.\nUse a white box model. If a given result is provided by a model.\nCan be combined with other decision techniques.\nThe action of more than one decision-maker can be considered.Disadvantages of decision trees:\n\nThey are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.\nThey are often relatively inaccurate.  Many other predictors perform better with similar data.  This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\nFor data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels.\nCalculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n\n\n== Optimizing a decision tree ==\nA few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. Note that these things are not the only things to consider but only some.\nIncreasing the number of levels of the tree\nThe accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree\u2019s leaves are pure nodes. When a node is pure, it means that all the data in that node belongs to a single class. For example, if the classes in the data set are Cancer and Non-Cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. It is important to note that a deeper tree is not always better when optimizing the decision tree. A deeper tree can influence the runtime in a negative way. If a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm is significantly slower. There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. If the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. Occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. To summarize, observe the points below, we will define the number D as the depth of the tree.\nPossible advantages of increasing the number D:\n\nAccuracy of the decision-tree classification model increases.Possible disadvantages of increasing D\n\n Runtime issues\nDecrease in accuracy in general\nPure node splits while going deeper can cause issues.The ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.\nThe choice of node-splitting functions\nThe node splitting function used can have an impact on improving the accuracy of the decision tree. For example, using the information-gain function may yield better results than using the phi function. The phi function is known as a measure of \u201cgoodness\u201d of a candidate split  at a node in the decision tree. The information gain function is known as a measure of the \u201creduction in entropy\u201d. In the following, we will build two decision trees. One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes.\nThe main advantages and disadvantages of information gain and  phi function\n\nOne major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values.\nAn advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree. It is a very good measure for deciding the relevance of some features.\nThe phi function is also a good measure for deciding the relevance of some features based on \"goodness\".This is the information gain function formula. The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree. \n\n  \n    \n      \n        I\n        g\n        a\n        i\n        n\n        s\n        (\n        s\n        )\n        =\n        H\n        (\n        t\n        )\n        \u2212\n        H\n        (\n        s\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle Igains(s)=H(t)-H(s,t)}\n  \nThis is the phi function formula. The phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.\n\n  \n    \n      \n        \u03a6\n        (\n        s\n        ,\n        t\n        )\n        =\n        (\n        2\n        \u2217\n        \n          P\n          \n            L\n          \n        \n        \u2217\n        \n          P\n          \n            R\n          \n        \n        )\n        \u2217\n        Q\n        (\n        s\n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle \\Phi (s,t)=(2*P_{L}*P_{R})*Q(s|t)}\n  \nWe will set D, which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of cancer and non-cancer samples and the mutation features that the samples either have or do not have. If a sample has a feature mutation then the sample is positive for that mutation, and it will be represented by one. If a sample does not have a feature mutation then the sample is negative for that mutation, and it will be represented by zero.   \nTo summarize, C stands for cancer and NC stands for non-cancer. The letter M stands for mutation, and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.\n\nNow, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced. The first thing to be done is to select the root node. In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. Now assume that M1  has the highest phi function value and M4 has the highest information gain value. The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree. You can observe the root nodes below \n\nNow, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B. For example, if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4, NC3, NC1, C1 in group B.\nDisregarding the mutation chosen for the root node, proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.\n\nNow assume the classification results from both trees are given using a confusion matrix.\nInformation gain confusion matrix:\n\nPhi function confusion matrix:\n\nThe tree using information gain has  the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. For the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below. The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree.\nOther techniques\nThe above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build. One of the techniques is making our decision tree model from a bootstrapped dataset. The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. There are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible.\n\n\n== Evaluating a decision tree ==\nIt is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. All these measurements are derived from the number of true positives, false positives, True negatives, and false negatives obtained when running a set of samples through the decision tree classification model. Also, a confusion matrix can be made to display these results. All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. For example, A low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples.\nLet us take the confusion matrix below. The confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.\n\nWe will now calculate the values accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. \nAccuracy:\n\n  \n    \n      \n        A\n        c\n        c\n        u\n        r\n        a\n        c\n        y\n        =\n        (\n        T\n        P\n        +\n        T\n        N\n        )\n        \n          /\n        \n        (\n        T\n        P\n        +\n        T\n        N\n        +\n        F\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle Accuracy=(TP+TN)/(TP+TN+FP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        +\n        104\n        )\n        \u00f7\n        162\n        =\n        71.60\n        %\n      \n    \n    {\\displaystyle (11+104)\\div 162=71.60\\%}\n  \nSensitivity (TPR \u2013 true positive tate):\n  \n    \n      \n        T\n        P\n        R\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle TPR=TP/(TP+FN)}\n  \n\n  \n    \n      \n        (\n        11\n        )\n        \u00f7\n        (\n        11\n        +\n        45\n        )\n        =\n        19.64\n        %\n      \n    \n    {\\displaystyle (11)\\div (11+45)=19.64\\%}\n  \nSpecificity (TNR \u2013 true negative rate):\n\n  \n    \n      \n        T\n        N\n        R\n        =\n        T\n        N\n        \n          /\n        \n        (\n        T\n        N\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle TNR=TN/(TN+FP)}\n  \n\n  \n    \n      \n        105\n        \u00f7\n        (\n        105\n        +\n        1\n        )\n        =\n        99.06\n        %\n      \n    \n    {\\displaystyle 105\\div (105+1)=99.06\\%}\n  \nPrecision (PPV \u2013 positive predictive value):\n\n  \n    \n      \n        P\n        P\n        V\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        P\n        )\n      \n    \n    {\\displaystyle PPV=TP/(TP+FP)}\n  \n\n  \n    \n      \n        11\n        \n          /\n        \n        (\n        11\n        +\n        1\n        )\n        =\n        91.66\n        %\n      \n    \n    {\\displaystyle 11/(11+1)=91.66\\%}\n  \nMiss Rate (FNR \u2013 false negative rate):\n\n  \n    \n      \n        F\n        N\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FNR=FN/(FN+TP)}\n  \n\n  \n    \n      \n        45\n        \u00f7\n        (\n        45\n        +\n        11\n        )\n        =\n        80.35\n        %\n      \n    \n    {\\displaystyle 45\\div (45+11)=80.35\\%}\n  \nFalse discovery rate (FDR):\n\n  \n    \n      \n        F\n        D\n        R\n        =\n        F\n        P\n        \n          /\n        \n        (\n        F\n        P\n        +\n        T\n        P\n        )\n      \n    \n    {\\displaystyle FDR=FP/(FP+TP)}\n  \n\n  \n    \n      \n        1\n        \u00f7\n        (\n        1\n        +\n        11\n        )\n        =\n        8.30\n        %\n      \n    \n    {\\displaystyle 1\\div (1+11)=8.30\\%}\n  \nFalse omission rate (FOR):\n\n  \n    \n      \n        F\n        O\n        R\n        =\n        F\n        N\n        \n          /\n        \n        (\n        F\n        N\n        +\n        T\n        N\n        )\n      \n    \n    {\\displaystyle FOR=FN/(FN+TN)}\n  \n\n  \n    \n      \n        45\n        \u00f7\n        (\n        45\n        +\n        105\n        )\n        =\n        30.00\n        %\n      \n    \n    {\\displaystyle 45\\div (45+105)=30.00\\%}\n  \nOnce we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%. The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. The sensitivity value of 19.64% means that out of everyone who was actually positive for cancer tested positive. If we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative. When it comes to sensitivity and specificity it is important to have a balance between the two values ,so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial. These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nExtensive Decision Tree tutorials and examples\nGallery of example decision trees\nGradient Boosted Decision Trees", "Regression analysis": "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\nRegression analysis is primarily used for two conceptually distinct purposes.\nFirst, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\nSecond, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.\n\n\n== History ==\nThe earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss\u2013Markov theorem.\nThe term \"regression\" was coined by Francis Galton in the 19th century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).\nFor Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context. In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. This assumption was weakened by R.A. Fisher in his works of 1922 and 1925. Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.\nIn the 1950s and 1960s, economists used electromechanical desk \"calculators\" to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.Regression methods continue to be an area of active research. In recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, Bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.\n\n\n== Regression model ==\nIn practice, researchers first select a model they would like to estimate and then use their chosen method (e.g., ordinary least squares) to estimate the parameters of that model. Regression models involve the following components:\n\nThe unknown parameters, often denoted as a scalar or vector \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  .\nThe independent variables, which are observed in data and are often denoted as a vector \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n   (where \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   denotes a row of data).\nThe dependent variable, which are observed in data and often denoted using the scalar \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n  .\nThe error terms, which are not directly observed in data and are often denoted using the scalar \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n  .In various fields of application, different terminologies are used in place of dependent and independent variables.\nMost regression models propose that \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   is a function (regression function) of \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n   and \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  , with \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n   representing an additive error term that may stand in for un-modeled determinants of \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   or random statistical noise:\n\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \u03b2\n        )\n        +\n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}=f(X_{i},\\beta )+e_{i}}\n  The researchers' goal is to estimate the function \n  \n    \n      \n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \u03b2\n        )\n      \n    \n    {\\displaystyle f(X_{i},\\beta )}\n   that most closely fits the data. To carry out regression analysis, the form of the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   must be specified. Sometimes the form of this function is based on knowledge about the relationship between \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   and \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n   that does not rely on the data. If no such knowledge is available, a flexible or convenient form for \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is chosen. For example, a simple univariate regression may propose \n  \n    \n      \n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \u03b2\n        )\n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f(X_{i},\\beta )=\\beta _{0}+\\beta _{1}X_{i}}\n  , suggesting that the researcher believes \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          X\n          \n            i\n          \n        \n        +\n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}=\\beta _{0}+\\beta _{1}X_{i}+e_{i}}\n   to be a reasonable approximation for the statistical process generating the data.\nOnce researchers determine their preferred statistical model, different forms of regression analysis provide tools to estimate the parameters \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  . For example, least squares (including its most common variant, ordinary least squares) finds the value of \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   that minimizes the sum of squared errors \n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        (\n        \n          Y\n          \n            i\n          \n        \n        \u2212\n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \u03b2\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sum _{i}(Y_{i}-f(X_{i},\\beta ))^{2}}\n  . A given regression method will ultimately provide an estimate of \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  , usually denoted \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}}\n   to distinguish the estimate from the true (unknown) parameter value that generated the data. Using this estimate, the researcher can then use the fitted value \n  \n    \n      \n        \n          \n            \n              \n                Y\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        =\n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {Y_{i}}}=f(X_{i},{\\hat {\\beta }})}\n   for prediction or to assess the accuracy of the model in explaining the data. Whether the researcher is intrinsically interested in the estimate \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}}\n   or the predicted value \n  \n    \n      \n        \n          \n            \n              \n                Y\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {Y_{i}}}}\n   will depend on context and their goals. As described in ordinary least squares, least squares is widely used because the estimated function \n  \n    \n      \n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle f(X_{i},{\\hat {\\beta }})}\n   approximates the conditional expectation \n  \n    \n      \n        E\n        (\n        \n          Y\n          \n            i\n          \n        \n        \n          |\n        \n        \n          X\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle E(Y_{i}|X_{i})}\n  . However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when researchers want to model other functions \n  \n    \n      \n        f\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \u03b2\n        )\n      \n    \n    {\\displaystyle f(X_{i},\\beta )}\n  .\nIt is important to note that there must be sufficient data to estimate a regression model. For example, suppose that a researcher has access to \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   rows of data with one dependent and two independent variables: \n  \n    \n      \n        (\n        \n          Y\n          \n            i\n          \n        \n        ,\n        \n          X\n          \n            1\n            i\n          \n        \n        ,\n        \n          X\n          \n            2\n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (Y_{i},X_{1i},X_{2i})}\n  . Suppose further that the researcher wants to estimate a bivariate linear model via least squares: \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          X\n          \n            1\n            i\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          X\n          \n            2\n            i\n          \n        \n        +\n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}=\\beta _{0}+\\beta _{1}X_{1i}+\\beta _{2}X_{2i}+e_{i}}\n  . If the researcher only has access to \n  \n    \n      \n        N\n        =\n        2\n      \n    \n    {\\displaystyle N=2}\n   data points, then they could find infinitely many combinations \n  \n    \n      \n        (\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        ,\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\hat {\\beta }}_{0},{\\hat {\\beta }}_{1},{\\hat {\\beta }}_{2})}\n   that explain the data equally well: any combination can be chosen that satisfies \n  \n    \n      \n        \n          \n            \n              \n                Y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        \n          X\n          \n            1\n            i\n          \n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          X\n          \n            2\n            i\n          \n        \n      \n    \n    {\\displaystyle {\\hat {Y}}_{i}={\\hat {\\beta }}_{0}+{\\hat {\\beta }}_{1}X_{1i}+{\\hat {\\beta }}_{2}X_{2i}}\n  , all of which lead to \n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \n          \n            \n              \n                e\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            2\n          \n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        (\n        \n          \n            \n              \n                Y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        \u2212\n        (\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        \n          X\n          \n            1\n            i\n          \n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          X\n          \n            2\n            i\n          \n        \n        )\n        \n          )\n          \n            2\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\sum _{i}{\\hat {e}}_{i}^{2}=\\sum _{i}({\\hat {Y}}_{i}-({\\hat {\\beta }}_{0}+{\\hat {\\beta }}_{1}X_{1i}+{\\hat {\\beta }}_{2}X_{2i}))^{2}=0}\n   and are therefore valid solutions that minimize the sum of squared residuals. To understand why there are infinitely many options, note that the system of \n  \n    \n      \n        N\n        =\n        2\n      \n    \n    {\\displaystyle N=2}\n   equations is to be solved for 3 unknowns, which makes the system underdetermined. Alternatively, one can visualize infinitely many 3-dimensional planes that go through \n  \n    \n      \n        N\n        =\n        2\n      \n    \n    {\\displaystyle N=2}\n   fixed points.\nMore generally, to estimate a least squares model with \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   distinct parameters, one must have \n  \n    \n      \n        N\n        \u2265\n        k\n      \n    \n    {\\displaystyle N\\geq k}\n   distinct data points. If \n  \n    \n      \n        N\n        >\n        k\n      \n    \n    {\\displaystyle N>k}\n  , then there does not generally exist a set of parameters that will perfectly fit the data. The quantity \n  \n    \n      \n        N\n        \u2212\n        k\n      \n    \n    {\\displaystyle N-k}\n   appears often in regression analysis, and is referred to as the degrees of freedom in the model. Moreover, to estimate a least squares model, the independent variables \n  \n    \n      \n        (\n        \n          X\n          \n            1\n            i\n          \n        \n        ,\n        \n          X\n          \n            2\n            i\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          X\n          \n            k\n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (X_{1i},X_{2i},...,X_{ki})}\n   must be linearly independent: one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables. As discussed in ordinary least squares, this condition ensures that \n  \n    \n      \n        \n          X\n          \n            T\n          \n        \n        X\n      \n    \n    {\\displaystyle X^{T}X}\n   is an invertible matrix and therefore that a unique solution \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}}\n   exists.\n\n\n== Underlying assumptions ==\nBy itself, a regression is simply a calculation using the data. In order to interpret the output of regression as a meaningful statistical quantity that measures real-world relationships, researchers often rely on a number of classical assumptions. These assumptions often include:\n\nThe sample is representative of the population at large.\nThe independent variables are measured with no error.\nDeviations from the model have an expected value of zero, conditional on covariates: \n  \n    \n      \n        E\n        (\n        \n          e\n          \n            i\n          \n        \n        \n          |\n        \n        \n          X\n          \n            i\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle E(e_{i}|X_{i})=0}\n  \nThe variance of the residuals \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n   is constant across observations (homoscedasticity).\nThe residuals \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n   are uncorrelated with one another. Mathematically, the variance\u2013covariance matrix of the errors is diagonal.A handful of conditions are sufficient for the least-squares estimator to possess desirable properties: in particular, the Gauss\u2013Markov assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. Practitioners have developed a variety of methods to maintain some or all of these desirable properties in real-world settings, because these classical assumptions are unlikely to hold exactly. For example, modeling errors-in-variables can lead to reasonable estimates independent variables are measured with errors. Heteroscedasticity-consistent standard errors allow the variance of \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n   to change across values of \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  . Correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors, geographic weighted regression, or Newey\u2013West standard errors, among other techniques. When rows of data correspond to locations in space, the choice of how to model \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}}\n   within geographic units can have important consequences. The subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real-world conclusions in real-world settings, where classical assumptions do not hold exactly.\n\n\n== Linear regression ==\n\nIn linear regression, the model specification is that the dependent variable, \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   is a linear combination of the parameters (but need not be linear in the independent variables). For example, in simple linear regression for modeling \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   data points there is one independent variable: \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  , and two parameters, \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n   and \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  :\n\nstraight line: \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          x\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n        \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        .\n        \n      \n    \n    {\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i}+\\varepsilon _{i},\\quad i=1,\\dots ,n.\\!}\n  In multiple linear regression, there are several independent variables or functions of independent variables.\nAdding a term in \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{i}^{2}}\n   to the preceding regression gives:\n\nparabola: \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          x\n          \n            i\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          x\n          \n            i\n          \n          \n            2\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n         \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        .\n        \n      \n    \n    {\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i}+\\beta _{2}x_{i}^{2}+\\varepsilon _{i},\\ i=1,\\dots ,n.\\!}\n  This is still linear regression; although the expression on the right hand side is quadratic in the independent variable \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  , it is linear in the parameters \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n  , \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   and \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\beta _{2}.}\n  \nIn both cases, \n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}}\n   is an error term and the subscript \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   indexes a particular observation.\nReturning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:\n\n  \n    \n      \n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        \n          x\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {y}}_{i}={\\widehat {\\beta }}_{0}+{\\widehat {\\beta }}_{1}x_{i}.}\n  The residual, \n  \n    \n      \n        \n          e\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle e_{i}=y_{i}-{\\widehat {y}}_{i}}\n  , is the difference between the value of the dependent variable predicted by the model, \n  \n    \n      \n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {y}}_{i}}\n  , and the true value of the dependent variable, \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  . One method of estimation is ordinary least squares. This method obtains parameter estimates that minimize the sum of squared residuals, SSR:\n\n  \n    \n      \n        S\n        S\n        R\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          e\n          \n            i\n          \n          \n            2\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle SSR=\\sum _{i=1}^{n}e_{i}^{2}.\\,}\n  Minimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, \n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        ,\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\beta }}_{0},{\\widehat {\\beta }}_{1}}\n  .\n\nIn the case of simple regression, the formulas for the least squares estimates are\n\n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        =\n        \n          \n            \n              \u2211\n              (\n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    y\n                    \u00af\n                  \n                \n              \n              )\n            \n            \n              \u2211\n              (\n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\beta }}_{1}={\\frac {\\sum (x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum (x_{i}-{\\bar {x}})^{2}}}}\n  \n\n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            0\n          \n        \n        =\n        \n          \n            \n              y\n              \u00af\n            \n          \n        \n        \u2212\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\beta }}_{0}={\\bar {y}}-{\\widehat {\\beta }}_{1}{\\bar {x}}}\n  where \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   is the mean (average) of the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   values and \n  \n    \n      \n        \n          \n            \n              y\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {y}}}\n   is the mean of the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   values.\nUnder the assumption that the population error term has a constant variance, the estimate of that variance is given by:\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \u03b5\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              S\n              S\n              R\n            \n            \n              n\n              \u2212\n              2\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}_{\\varepsilon }^{2}={\\frac {SSR}{n-2}}.\\,}\n  This is called the mean square error (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, \n  \n    \n      \n        (\n        n\n        \u2212\n        p\n        )\n      \n    \n    {\\displaystyle (n-p)}\n   for \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   regressors or \n  \n    \n      \n        (\n        n\n        \u2212\n        p\n        \u2212\n        1\n        )\n      \n    \n    {\\displaystyle (n-p-1)}\n   if an intercept is used. In this case, \n  \n    \n      \n        p\n        =\n        1\n      \n    \n    {\\displaystyle p=1}\n   so the denominator is \n  \n    \n      \n        n\n        \u2212\n        2\n      \n    \n    {\\displaystyle n-2}\n  .\nThe standard errors of the parameter estimates are given by\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \n              \u03b2\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \u03b5\n          \n        \n        \n          \n            \n              1\n              \n                \u2211\n                (\n                \n                  x\n                  \n                    i\n                  \n                \n                \u2212\n                \n                  \n                    \n                      x\n                      \u00af\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}_{\\beta _{1}}={\\hat {\\sigma }}_{\\varepsilon }{\\sqrt {\\frac {1}{\\sum (x_{i}-{\\bar {x}})^{2}}}}}\n  \n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \n              \u03b2\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \u03b5\n          \n        \n        \n          \n            \n              \n                1\n                n\n              \n            \n            +\n            \n              \n                \n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                \n                  \u2211\n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        x\n                        \u00af\n                      \n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            \n              \u03b2\n              \n                1\n              \n            \n          \n        \n        \n          \n            \n              \n                \u2211\n                \n                  x\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n              \n              n\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}_{\\beta _{0}}={\\hat {\\sigma }}_{\\varepsilon }{\\sqrt {{\\frac {1}{n}}+{\\frac {{\\bar {x}}^{2}}{\\sum (x_{i}-{\\bar {x}})^{2}}}}}={\\hat {\\sigma }}_{\\beta _{1}}{\\sqrt {\\frac {\\sum x_{i}^{2}}{n}}}.}\n  Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.\n\n\n=== General linear model ===\n\nIn the more general multiple regression model, there are \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   independent variables:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          x\n          \n            i\n            1\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          x\n          \n            i\n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          \u03b2\n          \n            p\n          \n        \n        \n          x\n          \n            i\n            p\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle y_{i}=\\beta _{1}x_{i1}+\\beta _{2}x_{i2}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i},\\,}\n  where \n  \n    \n      \n        \n          x\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle x_{ij}}\n   is the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th observation on the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  -th independent variable.\nIf the first independent variable takes the value 1 for all \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  , \n  \n    \n      \n        \n          x\n          \n            i\n            1\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x_{i1}=1}\n  , then \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   is called the regression intercept.\nThe least squares parameter estimates are obtained from \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   normal equations. The residual can be written as\n\n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        \n          x\n          \n            i\n            1\n          \n        \n        \u2212\n        \u22ef\n        \u2212\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            p\n          \n        \n        \n          x\n          \n            i\n            p\n          \n        \n        .\n      \n    \n    {\\displaystyle \\varepsilon _{i}=y_{i}-{\\hat {\\beta }}_{1}x_{i1}-\\cdots -{\\hat {\\beta }}_{p}x_{ip}.}\n  The normal equations are\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            p\n          \n        \n        \n          x\n          \n            i\n            j\n          \n        \n        \n          x\n          \n            i\n            k\n          \n        \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            k\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            i\n            j\n          \n        \n        \n          y\n          \n            i\n          \n        \n        ,\n         \n        j\n        =\n        1\n        ,\n        \u2026\n        ,\n        p\n        .\n        \n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}\\sum _{k=1}^{p}x_{ij}x_{ik}{\\hat {\\beta }}_{k}=\\sum _{i=1}^{n}x_{ij}y_{i},\\ j=1,\\dots ,p.\\,}\n  In matrix notation, the normal equations are written as\n\n  \n    \n      \n        \n          (\n          \n            X\n            \n              \u22a4\n            \n          \n          X\n          )\n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          =\n          \n\n          \n          \n            X\n            \n              \u22a4\n            \n          \n          Y\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\mathbf {(X^{\\top }X){\\hat {\\boldsymbol {\\beta }}}={}X^{\\top }Y} ,\\,}\n  where the \n  \n    \n      \n        i\n        j\n      \n    \n    {\\displaystyle ij}\n   element of \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   is \n  \n    \n      \n        \n          x\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle x_{ij}}\n  , the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   element of the column vector \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  , and the \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   element of \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}}\n   is \n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}_{j}}\n  . Thus \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   is \n  \n    \n      \n        n\n        \u00d7\n        p\n      \n    \n    {\\displaystyle n\\times p}\n  , \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is \n  \n    \n      \n        n\n        \u00d7\n        1\n      \n    \n    {\\displaystyle n\\times 1}\n  , and \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}}\n   is \n  \n    \n      \n        p\n        \u00d7\n        1\n      \n    \n    {\\displaystyle p\\times 1}\n  . The solution is\n\n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          =\n          (\n          \n            X\n            \n              \u22a4\n            \n          \n          X\n          \n            )\n            \n              \u2212\n              1\n            \n          \n          \n            X\n            \n              \u22a4\n            \n          \n          Y\n        \n        .\n        \n      \n    \n    {\\displaystyle \\mathbf {{\\hat {\\boldsymbol {\\beta }}}=(X^{\\top }X)^{-1}X^{\\top }Y} .\\,}\n  \n\n\n=== Diagnostics ===\n\nOnce a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.\nInterpretations of these diagnostic tests rest heavily on the model's assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.\n\n\n=== Limited dependent variables ===\nLimited dependent variables, which are response variables that are categorical variables or are variables constrained to fall only in a certain range, often arise in econometrics.\nThe response variable may be non-continuous (\"limited\" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. Nonlinear models for binary dependent variables include the probit and logit model. The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For categorical variables with more than two values there is the multinomial logit. For ordinal variables with more than two values, there are the ordered logit and ordered probit models. Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used.\n\n\n== Nonlinear regression ==\n\nWhen the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in Differences between linear and non-linear least squares.\n\n\n== Prediction (interpolation and extrapolation) ==\n\nRegression models predict a value of the Y variable given known values of the X variables. Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction outside this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.\nIt is generally advised that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.\nFor such reasons and others, some tend to say that it might be unwise to undertake extrapolation.However, this does not cover the full set of modeling errors that may be made: in particular, the assumption of a particular form for the relation between Y and X. A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. Best-practice advice here is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model \u2013 even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is \"realistic\" (or in accord with what is known).\n\n\n== Power and sample size calculations ==\nThere are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One method conjectured by Good and Hardin is \n  \n    \n      \n        N\n        =\n        \n          m\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle N=m^{n}}\n  , where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the sample size, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the number of independent variables and \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is the number of observations needed to reach the desired precision if the model had only one independent variable. For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  ). If the researcher decides that five observations are needed to precisely define a straight line (\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  ), then the maximum number of independent variables the model can support is 4, because\n\n  \n    \n      \n        \n          \n            \n              log\n              \u2061\n              1000\n            \n            \n              log\n              \u2061\n              5\n            \n          \n        \n        =\n        4.29.\n      \n    \n    {\\displaystyle {\\frac {\\log 1000}{\\log 5}}=4.29.}\n  \n\n\n== Other methods ==\nAlthough the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:\n\nBayesian methods, e.g. Bayesian linear regression\nPercentage regression, for situations where reducing percentage errors is deemed more appropriate.\nLeast absolute deviations, which is more robust in the presence of outliers, leading to quantile regression\nNonparametric regression, requires a large number of observations and is computationally intensive\nScenario optimization, leading to  interval predictor models\nDistance metric learning, which is learned by the search of a meaningful distance metric in a given input space.\n\n\n== Software ==\n\nAll major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized. Different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nWilliam H. Kruskal and Judith M. Tanur, ed. (1978), \"Linear Hypotheses,\" International Encyclopedia of Statistics. Free Press,  v. 1,Evan J. Williams, \"I. Regression,\" pp. 523\u201341.\nJulian C. Stanley, \"II. Analysis of Variance,\" pp. 541\u2013554.Lindley, D.V. (1987). \"Regression and correlation analysis,\" New Palgrave: A Dictionary of Economics, v. 4, pp. 120\u201323.\nBirkes, David and Dodge, Y., Alternative Methods of Regression. ISBN 0-471-56881-3\nChatfield, C. (1993) \"Calculating Interval Forecasts,\" Journal of Business and Economic Statistics, 11. pp. 121\u2013135.\nDraper, N.R.; Smith, H. (1998). Applied Regression Analysis (3rd ed.). John Wiley. ISBN 978-0-471-17082-2.\nFox, J. (1997).  Applied Regression Analysis, Linear Models and Related Methods. Sage\nHardle, W., Applied Nonparametric Regression (1990), ISBN 0-521-42950-1\nMeade, Nigel; Islam, Towhidul (1995). \"Prediction intervals for growth curve forecasts\". Journal of Forecasting. 14 (5): 413\u2013430. doi:10.1002/for.3980140502.\nA. Sen, M. Srivastava, Regression Analysis \u2014 Theory, Methods, and Applications, Springer-Verlag, Berlin, 2011 (4th printing).\nT. Strutz: Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond). Vieweg+Teubner, ISBN 978-3-8348-1022-9.\nStulp, Freek, and Olivier Sigaud. Many Regression Algorithms, One Unified Model: A Review. Neural Networks, vol. 69, Sept. 2015, pp. 60\u201379. https://doi.org/10.1016/j.neunet.2015.05.005.\nMalakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley & Sons.\nChicco, Davide; Warrens, Matthijs J.; Jurman, Giuseppe (2021). \"The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation\". PeerJ Computer Science. 7 (e623): e623. doi:10.7717/peerj-cs.623. PMC 8279135. PMID 34307865.\n\n\n== External links ==\n\n\"Regression analysis\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nEarliest Uses: Regression \u2013 basic history and references\nWhat is multiple regression used for? \u2013 Multiple regression\nRegression of Weakly Correlated Data \u2013 how linear regression mistakes can appear when Y-range is much smaller than X-range", "Conditional probability distribution": "In probability theory and statistics, given two jointly distributed random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , the conditional probability distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is the probability distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   when \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   as a parameter. When both \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable.\nIf the conditional distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is a continuous distribution, then its probability density function is known as the conditional density function. The properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance.\nMore generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.\n\n\n== Conditional discrete distributions ==\nFor discrete random variables, the conditional probability mass function of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n   can be written according to its definition as:\n\nDue to the occurrence of \n  \n    \n      \n        P\n        (\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(X=x)}\n   in the denominator, this is defined only for non-zero (hence strictly positive) \n  \n    \n      \n        P\n        (\n        X\n        =\n        x\n        )\n        .\n      \n    \n    {\\displaystyle P(X=x).}\n  \nThe relation with the probability distribution of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is:\n\n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        \u2223\n        X\n        =\n        x\n        )\n        P\n        (\n        X\n        =\n        x\n        )\n        =\n        P\n        (\n        {\n        X\n        =\n        x\n        }\n        \u2229\n        {\n        Y\n        =\n        y\n        }\n        )\n        =\n        P\n        (\n        X\n        =\n        x\n        \u2223\n        Y\n        =\n        y\n        )\n        P\n        (\n        Y\n        =\n        y\n        )\n        .\n      \n    \n    {\\displaystyle P(Y=y\\mid X=x)P(X=x)=P(\\{X=x\\}\\cap \\{Y=y\\})=P(X=x\\mid Y=y)P(Y=y).}\n  \n\n\n=== Example ===\nConsider the roll of a fair die and let \n  \n    \n      \n        X\n        =\n        1\n      \n    \n    {\\displaystyle X=1}\n   if the number is even (i.e., 2, 4, or 6) and \n  \n    \n      \n        X\n        =\n        0\n      \n    \n    {\\displaystyle X=0}\n   otherwise. Furthermore, let \n  \n    \n      \n        Y\n        =\n        1\n      \n    \n    {\\displaystyle Y=1}\n   if the number is prime (i.e., 2, 3, or 5) and \n  \n    \n      \n        Y\n        =\n        0\n      \n    \n    {\\displaystyle Y=0}\n   otherwise.\n\nThen the unconditional probability that \n  \n    \n      \n        X\n        =\n        1\n      \n    \n    {\\displaystyle X=1}\n   is 3/6 = 1/2 (since there are six possible rolls of the dice, of which three are even), whereas the probability that \n  \n    \n      \n        X\n        =\n        1\n      \n    \n    {\\displaystyle X=1}\n   conditional on \n  \n    \n      \n        Y\n        =\n        1\n      \n    \n    {\\displaystyle Y=1}\n   is 1/3 (since there are three possible prime number rolls\u20142, 3, and 5\u2014of which one is even).\n\n\n== Conditional continuous distributions ==\nSimilarly for continuous random variables, the conditional probability density function of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given the occurrence of the value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   can be written as:\u200ap. 99\u200a\n\nwhere \n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n   gives the joint density of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , while \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n   gives the marginal density for \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . Also in this case it is necessary that \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        >\n        0\n      \n    \n    {\\displaystyle f_{X}(x)>0}\n  .\nThe relation with the probability distribution of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is given by:\n\n  \n    \n      \n        \n          f\n          \n            Y\n            \u2223\n            X\n          \n        \n        (\n        y\n        \u2223\n        x\n        )\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          f\n          \n            X\n            \n              |\n            \n            Y\n          \n        \n        (\n        x\n        \u2223\n        y\n        )\n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n        .\n      \n    \n    {\\displaystyle f_{Y\\mid X}(y\\mid x)f_{X}(x)=f_{X,Y}(x,y)=f_{X|Y}(x\\mid y)f_{Y}(y).}\n  The concept of the conditional distribution of a continuous random variable is not as intuitive as it might seem: Borel's paradox shows that conditional probability density functions need not be invariant under coordinate transformations.\n\n\n=== Example ===\n\nThe graph shows a bivariate normal joint density for random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  . To see the distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   conditional on \n  \n    \n      \n        X\n        =\n        70\n      \n    \n    {\\displaystyle X=70}\n  , one can first visualize the line \n  \n    \n      \n        X\n        =\n        70\n      \n    \n    {\\displaystyle X=70}\n   in the \n  \n    \n      \n        X\n        ,\n        Y\n      \n    \n    {\\displaystyle X,Y}\n   plane, and then visualize the plane containing that line and perpendicular to the \n  \n    \n      \n        X\n        ,\n        Y\n      \n    \n    {\\displaystyle X,Y}\n   plane. The intersection of that plane with the joint normal density, once rescaled to give unit area under the intersection, is the relevant conditional density of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\n\n  \n    \n      \n        Y\n        \u2223\n        X\n        =\n        70\n         \n        \u223c\n         \n        \n          \n            N\n          \n        \n        \n          (\n          \n            \n              \u03bc\n              \n                1\n              \n            \n            +\n            \n              \n                \n                  \u03c3\n                  \n                    1\n                  \n                \n                \n                  \u03c3\n                  \n                    2\n                  \n                \n              \n            \n            \u03c1\n            (\n            70\n            \u2212\n            \n              \u03bc\n              \n                2\n              \n            \n            )\n            ,\n            \n            (\n            1\n            \u2212\n            \n              \u03c1\n              \n                2\n              \n            \n            )\n            \n              \u03c3\n              \n                1\n              \n              \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle Y\\mid X=70\\ \\sim \\ {\\mathcal {N}}\\left(\\mu _{1}+{\\frac {\\sigma _{1}}{\\sigma _{2}}}\\rho (70-\\mu _{2}),\\,(1-\\rho ^{2})\\sigma _{1}^{2}\\right).}\n  \n\n\n== Relation to independence ==\nRandom variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are independent if and only if the conditional distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is, for all possible realizations of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , equal to the unconditional distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  . For discrete random variables this means \n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        \n          |\n        \n        X\n        =\n        x\n        )\n        =\n        P\n        (\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle P(Y=y|X=x)=P(Y=y)}\n   for all possible \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   with \n  \n    \n      \n        P\n        (\n        X\n        =\n        x\n        )\n        >\n        0\n      \n    \n    {\\displaystyle P(X=x)>0}\n  . For continuous random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , having a joint density function, it means \n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        \n          |\n        \n        X\n        =\n        x\n        )\n        =\n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{Y}(y|X=x)=f_{Y}(y)}\n   for all possible \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   with \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        >\n        0\n      \n    \n    {\\displaystyle f_{X}(x)>0}\n  .\n\n\n== Properties ==\nSeen as a function of \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   for given \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , \n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y=y|X=x)}\n   is a probability mass function and so the sum over all \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   (or integral if it is a conditional probability density) is 1.  Seen as a function of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   for given \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  , it is a likelihood function, so that the sum over all \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   need not be 1.\nAdditionally, a marginal of a joint distribution can be expressed as the expectation of the corresponding conditional distribution. For instance, \n  \n    \n      \n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        \n          E\n          \n            Y\n          \n        \n        [\n        \n          p\n          \n            X\n            \n              |\n            \n            Y\n          \n        \n        (\n        X\n         \n        \n          |\n        \n         \n        Y\n        )\n        ]\n      \n    \n    {\\displaystyle p_{X}(x)=E_{Y}[p_{X|Y}(X\\ |\\ Y)]}\n  .\n\n\n== Measure-theoretic formulation ==\nLet \n  \n    \n      \n        (\n        \u03a9\n        ,\n        \n          \n            F\n          \n        \n        ,\n        P\n        )\n      \n    \n    {\\displaystyle (\\Omega ,{\\mathcal {F}},P)}\n   be a probability space, \n  \n    \n      \n        \n          \n            G\n          \n        \n        \u2286\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {G}}\\subseteq {\\mathcal {F}}}\n   a \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  -field in \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  . Given \n  \n    \n      \n        A\n        \u2208\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle A\\in {\\mathcal {F}}}\n  , the Radon-Nikodym theorem implies that there is a  \n  \n    \n      \n        \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {G}}}\n  -measurable random variable \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        \n          \n            G\n          \n        \n        )\n        :\n        \u03a9\n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle P(A\\mid {\\mathcal {G}}):\\Omega \\to \\mathbb {R} }\n  , called the conditional probability, such thatfor every \n  \n    \n      \n        G\n        \u2208\n        \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle G\\in {\\mathcal {G}}}\n  , and such a random variable is uniquely defined up to sets of probability zero. A conditional probability is called regular if  \n  \n    \n      \n        P\n        \u2061\n        (\n        \u22c5\n        \u2223\n        \n          \n            G\n          \n        \n        )\n        (\n        \u03c9\n        )\n      \n    \n    {\\displaystyle \\operatorname {P} (\\cdot \\mid {\\mathcal {G}})(\\omega )}\n   is a probability measure on \n  \n    \n      \n        (\n        \u03a9\n        ,\n        \n          \n            F\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Omega ,{\\mathcal {F}})}\n   for all \n  \n    \n      \n        \u03c9\n        \u2208\n        \u03a9\n      \n    \n    {\\displaystyle \\omega \\in \\Omega }\n   a.e. \nSpecial cases: \n\nFor the trivial sigma algebra \n  \n    \n      \n        \n          \n            G\n          \n        \n        =\n        {\n        \u2205\n        ,\n        \u03a9\n        }\n      \n    \n    {\\displaystyle {\\mathcal {G}}=\\{\\emptyset ,\\Omega \\}}\n  , the conditional probability is the constant function \n  \n    \n      \n        P\n        \n        \n          (\n          \n            A\n            \u2223\n            {\n            \u2205\n            ,\n            \u03a9\n            }\n          \n          )\n        \n        =\n        P\n        \u2061\n        (\n        A\n        )\n        .\n      \n    \n    {\\displaystyle \\operatorname {P} \\!\\left(A\\mid \\{\\emptyset ,\\Omega \\}\\right)=\\operatorname {P} (A).}\n  \nIf \n  \n    \n      \n        A\n        \u2208\n        \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle A\\in {\\mathcal {G}}}\n  ,  then \n  \n    \n      \n        P\n        \u2061\n        (\n        A\n        \u2223\n        \n          \n            G\n          \n        \n        )\n        =\n        \n          1\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {P} (A\\mid {\\mathcal {G}})=1_{A}}\n  , the indicator function (defined below).Let \n  \n    \n      \n        X\n        :\n        \u03a9\n        \u2192\n        E\n      \n    \n    {\\displaystyle X:\\Omega \\to E}\n   be a \n  \n    \n      \n        (\n        E\n        ,\n        \n          \n            E\n          \n        \n        )\n      \n    \n    {\\displaystyle (E,{\\mathcal {E}})}\n  -valued random variable. For each \n  \n    \n      \n        B\n        \u2208\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle B\\in {\\mathcal {E}}}\n  , define For any \n  \n    \n      \n        \u03c9\n        \u2208\n        \u03a9\n      \n    \n    {\\displaystyle \\omega \\in \\Omega }\n  , the function \n  \n    \n      \n        \n          \u03bc\n          \n            X\n            \n            \n              |\n            \n            \n              \n                G\n              \n            \n          \n        \n        (\n        \u22c5\n        \n        \n          |\n        \n        \n          \n            G\n          \n        \n        )\n        (\n        \u03c9\n        )\n        :\n        \n          \n            E\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle \\mu _{X\\,|{\\mathcal {G}}}(\\cdot \\,|{\\mathcal {G}})(\\omega ):{\\mathcal {E}}\\to \\mathbb {R} }\n   is called the conditional probability distribution of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given \n  \n    \n      \n        \n          \n            G\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {G}}}\n  . If it is a probability measure on \n  \n    \n      \n        (\n        E\n        ,\n        \n          \n            E\n          \n        \n        )\n      \n    \n    {\\displaystyle (E,{\\mathcal {E}})}\n  , then it is called regular. \nFor a real-valued random variable (with respect to the Borel \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  -field \n  \n    \n      \n        \n          \n            \n              R\n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}^{1}}\n   on \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  ), every conditional probability distribution is regular. In this case,\n  \n    \n      \n        E\n        [\n        X\n        \u2223\n        \n          \n            G\n          \n        \n        ]\n        =\n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        x\n        \n        \u03bc\n        (\n        d\n        x\n        ,\n        \u22c5\n        )\n      \n    \n    {\\displaystyle E[X\\mid {\\mathcal {G}}]=\\int _{-\\infty }^{\\infty }x\\,\\mu (dx,\\cdot )}\n   almost surely. \n\n\n=== Relation to conditional expectation ===\nFor any event \n  \n    \n      \n        A\n        \u2208\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle A\\in {\\mathcal {F}}}\n  , define the indicator function:\n\n  \n    \n      \n        \n          \n            1\n          \n          \n            A\n          \n        \n        (\n        \u03c9\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  1\n                  \n                \n                \n                  \n                    if \n                  \n                  \u03c9\n                  \u2208\n                  A\n                  ,\n                \n              \n              \n                \n                  0\n                  \n                \n                \n                  \n                    if \n                  \n                  \u03c9\n                  \u2209\n                  A\n                  ,\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {1} _{A}(\\omega )={\\begin{cases}1\\;&{\\text{if }}\\omega \\in A,\\\\0\\;&{\\text{if }}\\omega \\notin A,\\end{cases}}}\n  which is a random variable. Note that the expectation of this random variable is equal to the probability of A itself:\n\n  \n    \n      \n        E\n        \u2061\n        (\n        \n          \n            1\n          \n          \n            A\n          \n        \n        )\n        =\n        P\n        \u2061\n        (\n        A\n        )\n        .\n        \n      \n    \n    {\\displaystyle \\operatorname {E} (\\mathbf {1} _{A})=\\operatorname {P} (A).\\;}\n  Given a  \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  -field \n  \n    \n      \n        \n          \n            G\n          \n        \n        \u2286\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {G}}\\subseteq {\\mathcal {F}}}\n  , the conditional probability \n  \n    \n      \n        P\n        \u2061\n        (\n        A\n        \u2223\n        \n          \n            G\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {P} (A\\mid {\\mathcal {G}})}\n   is a version of the conditional expectation of the indicator function for \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  :\n\n  \n    \n      \n        P\n        \u2061\n        (\n        A\n        \u2223\n        \n          \n            G\n          \n        \n        )\n        =\n        E\n        \u2061\n        (\n        \n          \n            1\n          \n          \n            A\n          \n        \n        \u2223\n        \n          \n            G\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle \\operatorname {P} (A\\mid {\\mathcal {G}})=\\operatorname {E} (\\mathbf {1} _{A}\\mid {\\mathcal {G}})\\;}\n  An expectation of a random variable with respect to a regular conditional probability is equal to its conditional expectation.\n\n\n== See also ==\nConditioning (probability)\nConditional probability\nRegular conditional probability\nBayes' theorem\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===", "Outlier": "In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to a variability in the measurement, an indication of novel data, or it may be the result of experimental error; the latter are sometimes excluded from the data set.  An outlier can be an indication of exciting possibility, but can also cause serious problems in statistical analyses.\nOutliers can occur by chance in any distribution, but they can indicate novel behaviour or structures in the data-set, measurement error, or that the population has a heavy-tailed distribution. In the case of measurement error, one wishes to discard them or use statistics that are robust to outliers, while in the case of heavy-tailed distributions, they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model.\nIn most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition).\nOutliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations.\nNaive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 \u00b0C, the median of the data will be between 20 and 25 \u00b0C but the mean temperature will be between 35.5 and 40 \u00b0C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as \"a typical sample\", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set.\nEstimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.\n\n\n== Occurrence and causes ==\n\nIn the case of normally distributed data, the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean, and 1 in 370 will deviate by three times the standard deviation. In a sample of 1000 observations, the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected, being less than twice the expected number and hence within 1 standard deviation of the expected number \u2013 see Poisson distribution \u2013 and not indicate an anomaly. If the sample size is only 100, however, just three such outliers are already reason for concern, being more than 11 times the expected number.\nIn general, if the nature of the population distribution is known a priori, it is possible to test if the number of outliers deviate significantly from what can be expected: for a given cutoff (so samples fall beyond the cutoff with probability p) of a given distribution, the number of outliers will follow a binomial distribution with parameter p, which can generally be well-approximated by the Poisson distribution with \u03bb = pn. Thus if one takes a normal distribution with cutoff 3 standard deviations from the mean, p is approximately 0.3%, and thus for 1000 trials one can approximate the number of samples whose deviation exceeds 3 sigmas by a Poisson distribution with \u03bb = 3.\n\n\n=== Causes ===\nOutliers can have many anomalous causes. A physical apparatus for taking measurements may have suffered a transient malfunction. There may have been an error in data transmission or transcription. Outliers arise due to changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. A sample may have been contaminated with elements from outside the population being examined. Alternatively, an outlier could be the result of a flaw in the assumed theory, calling for further investigation by the researcher.  Additionally, the pathological appearance of outliers of a certain form appears in a variety of datasets, indicating that the causative mechanism for the data might differ at the extreme end (King effect).\n\n\n== Definitions and detection ==\nThere is no rigid mathematical definition of what constitutes an outlier; determining whether or not an observation is an outlier is ultimately a subjective exercise.  There are various methods of outlier detection, some of which are treated as synonymous with novelty detection. Some are graphical such as normal probability plots.  Others are model-based. Box plots are a hybrid.\nModel-based methods which are commonly used for identification assume that the data are from a normal distribution, and identify observations which are deemed \"unlikely\" based on mean and standard deviation:\n\nChauvenet's criterion\nGrubbs's test for outliers\nDixon's Q test\nASTM E178: Standard Practice for Dealing With Outlying Observations\nMahalanobis distance and leverage are often used to detect outliers, especially in the development of linear regression models.\nSubspace and correlation based techniques for high-dimensional numerical data\n\n\n=== Peirce's criterion ===\n\nIt is proposed to determine in a series of \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   observations the limit of error, beyond which all observations involving so great an error may be rejected, provided there are as many as \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   such observations. The principle upon which it is proposed to solve this problem is, that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many, and no more, abnormal observations. (Quoted in the editorial note on page 516 to Peirce (1982 edition) from A Manual of Astronomy 2:558 by Chauvenet.)\n\n\n=== Tukey's fences ===\nOther methods flag observations based on measures such as the interquartile range. For example, if \n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle Q_{1}}\n   and \n  \n    \n      \n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{3}}\n   are the lower and upper quartiles respectively, then one could define an outlier to be any observation outside the range:\n\n  \n    \n      \n        \n          \n            [\n          \n        \n        \n          Q\n          \n            1\n          \n        \n        \u2212\n        k\n        (\n        \n          Q\n          \n            3\n          \n        \n        \u2212\n        \n          Q\n          \n            1\n          \n        \n        )\n        ,\n        \n          Q\n          \n            3\n          \n        \n        +\n        k\n        (\n        \n          Q\n          \n            3\n          \n        \n        \u2212\n        \n          Q\n          \n            1\n          \n        \n        )\n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\big [}Q_{1}-k(Q_{3}-Q_{1}),Q_{3}+k(Q_{3}-Q_{1}){\\big ]}}\n  for some nonnegative constant \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  .\nJohn Tukey proposed this test, where \n  \n    \n      \n        k\n        =\n        1.5\n      \n    \n    {\\displaystyle k=1.5}\n   indicates an \"outlier\", and \n  \n    \n      \n        k\n        =\n        3\n      \n    \n    {\\displaystyle k=3}\n   indicates data that is \"far out\".\n\n\n=== In anomaly detection ===\n\nIn various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, networking and data mining, the task of anomaly detection may take other approaches. Some of these may be distance-based and density-based such as Local Outlier Factor (LOF). Some approaches may use the distance to the k-nearest neighbors to label observations as outliers or non-outliers.\n\n\n=== Modified Thompson Tau test ===\n\nThe modified Thompson Tau test is a method used to determine if an outlier exists in a data set. The strength of this method lies in the fact that it takes into account a data set's standard deviation, average and provides a statistically determined rejection zone; thus providing an objective method to determine if a data point is an outlier.\nHow it works:\nFirst, a data set's average is determined. Next the absolute deviation between each data point and the average are determined. Thirdly, a rejection region is determined using the formula: \n\n  \n    \n      \n        \n          Rejection Region\n        \n        \n          =\n        \n        \n          \n            \n              \n                \n                  t\n                  \n                    \u03b1\n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                \n                  (\n                  \n                    n\n                    \u2212\n                    1\n                  \n                  )\n                \n              \n            \n            \n              \n                \n                  n\n                \n              \n              \n                \n                  n\n                  \u2212\n                  2\n                  +\n                  \n                    \n                      t\n                      \n                        \u03b1\n                        \n                          /\n                        \n                        2\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Rejection Region}}{=}{\\frac {{t_{\\alpha /2}}{\\left(n-1\\right)}}{{\\sqrt {n}}{\\sqrt {n-2+{t_{\\alpha /2}^{2}}}}}}}\n  ;where \n  \n    \n      \n        \n          \n            \n              t\n              \n                \u03b1\n                \n                  /\n                \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {t_{\\alpha /2}}}\n   is the critical value from the Student t distribution with n-2 degrees of freedom, n is the sample size, and s is the sample standard deviation.\nTo determine if a value is an outlier:\nCalculate \n  \n    \n      \n        \n          \u03b4\n          =\n          \n            |\n          \n          (\n          X\n          \u2212\n          m\n          e\n          a\n          n\n          (\n          X\n          )\n          )\n          \n            /\n          \n          s\n          \n            |\n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle \\delta =|(X-mean(X))/s|}\n  .\nIf \u03b4 > Rejection Region, the data point is an outlier.\nIf \u03b4 \u2264 Rejection Region, the data point is not an outlier.\nThe modified Thompson Tau test is used to find one outlier at a time (largest value of \u03b4 is removed if it is an outlier). Meaning, if a data point is found to be an outlier, it is removed from the data set and the test is applied again with a new average and rejection region. This process is continued until no outliers remain in a data set.\nSome work has also examined outliers for nominal (or categorical) data. In the context of a set of examples (or instances) in a data set, instance hardness measures the probability that an instance will be misclassified ( \n  \n    \n      \n        1\n        \u2212\n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle 1-p(y|x)}\n   where y is the assigned class label and x represent the input attribute value for an instance in the training set t). Ideally, instance hardness would be calculated by summing over the set of all possible hypotheses H:\n\n  \n    \n      \n        \n          \n            \n              \n                I\n                H\n                (\n                \u27e8\n                x\n                ,\n                y\n                \u27e9\n                )\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    H\n                  \n                \n                (\n                1\n                \u2212\n                p\n                (\n                y\n                ,\n                x\n                ,\n                h\n                )\n                )\n                p\n                (\n                h\n                \n                  |\n                \n                t\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    H\n                  \n                \n                p\n                (\n                h\n                \n                  |\n                \n                t\n                )\n                \u2212\n                p\n                (\n                y\n                ,\n                x\n                ,\n                h\n                )\n                p\n                (\n                h\n                \n                  |\n                \n                t\n                )\n              \n            \n            \n              \n              \n                \n                =\n                1\n                \u2212\n                \n                  \u2211\n                  \n                    H\n                  \n                \n                p\n                (\n                y\n                ,\n                x\n                ,\n                h\n                )\n                p\n                (\n                h\n                \n                  |\n                \n                t\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}IH(\\langle x,y\\rangle )&=\\sum _{H}(1-p(y,x,h))p(h|t)\\\\&=\\sum _{H}p(h|t)-p(y,x,h)p(h|t)\\\\&=1-\\sum _{H}p(y,x,h)p(h|t).\\end{aligned}}}\n  Practically, this formulation is unfeasible as H is potentially infinite and calculating \n  \n    \n      \n        p\n        (\n        h\n        \n          |\n        \n        t\n        )\n      \n    \n    {\\displaystyle p(h|t)}\n   is unknown for many algorithms. Thus, instance hardness can be approximated using a diverse subset \n  \n    \n      \n        L\n        \u2282\n        H\n      \n    \n    {\\displaystyle L\\subset H}\n  :\n\n  \n    \n      \n        I\n        \n          H\n          \n            L\n          \n        \n        (\n        \u27e8\n        x\n        ,\n        y\n        \u27e9\n        )\n        =\n        1\n        \u2212\n        \n          \n            1\n            \n              \n                |\n              \n              L\n              \n                |\n              \n            \n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            \n              |\n            \n            L\n            \n              |\n            \n          \n        \n        p\n        (\n        y\n        \n          |\n        \n        x\n        ,\n        \n          g\n          \n            j\n          \n        \n        (\n        t\n        ,\n        \u03b1\n        )\n        )\n      \n    \n    {\\displaystyle IH_{L}(\\langle x,y\\rangle )=1-{\\frac {1}{|L|}}\\sum _{j=1}^{|L|}p(y|x,g_{j}(t,\\alpha ))}\n  where \n  \n    \n      \n        \n          g\n          \n            j\n          \n        \n        (\n        t\n        ,\n        \u03b1\n        )\n      \n    \n    {\\displaystyle g_{j}(t,\\alpha )}\n   is the hypothesis induced by learning algorithm \n  \n    \n      \n        \n          g\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle g_{j}}\n   trained on training set t with hyperparameters \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  . Instance hardness provides a continuous value for determining if an instance is an outlier instance.\n\n\n== Working with outliers ==\nThe choice of how to deal with an outlier should depend on the cause. Some estimators are highly sensitive to outliers, notably estimation of covariance matrices.\n\n\n=== Retention ===\nEven when a normal distribution model is appropriate to the data being analyzed, outliers are expected for large sample sizes and should not automatically be discarded if that is the case .  Instead, one should use a method that is robust to outliers to model or analyze data with naturally occurring outliers .\n\n\n=== Exclusion ===\nWhen deciding whether to remove an outlier, the cause has to be considered.  As mentioned earlier, if the outlier's origin can be attributed to an experimental error, or if it can be otherwise determined that the outlying data point is erroneous, it is generally recommended to remove it . However, it is more desirable to correct the erroneous value, if possible. \nRemoving a data point solely because it is an outlier, on the other hand,  is a controversial practice, often frowned upon by many scientists and science instructors, as it typically invalidates statistical results . While mathematical criteria provide an objective and quantitative method for data rejection, they do not make the practice more scientifically or methodologically sound, especially in small sets or where a normal distribution cannot be assumed. Rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of measurement error are confidently known. \nThe two common approaches to exclude outliers are truncation (or trimming) and Winsorising. Trimming discards the outliers whereas Winsorising replaces the outliers with the nearest \"nonsuspect\" data. Exclusion can also be a consequence of the measurement process, such as when an experiment is not entirely capable of measuring such extreme values, resulting in censored data.In regression problems, an alternative approach may be to only exclude points which exhibit a large degree of influence on the estimated coefficients, using a measure such as Cook's distance.If a data point (or points) is excluded from the data analysis, this should be clearly stated on any subsequent report.\n\n\n=== Non-normal distributions ===\nThe possibility should be considered that the underlying distribution of the data is not approximately normal, having \"fat tails\". For instance, when sampling from a Cauchy distribution, the sample variance increases with the sample size, the sample mean fails to converge as the sample size increases, and outliers are expected at far larger rates than for a normal distribution. Even a slight difference in the fatness of the tails can make a large difference in the expected number of extreme values.\n\n\n=== Set-membership uncertainties ===\nA set membership approach considers that the uncertainty corresponding to the ith measurement of an unknown random vector x is represented by a set Xi (instead of a probability density function). If no outliers occur, x should belong to the intersection of all Xi's. When outliers occur, this intersection could be empty, and we should relax a small number of the sets Xi (as small as possible) in order to avoid any inconsistency. This can be done using the notion of q-relaxed intersection. As illustrated by the figure, the q-relaxed intersection corresponds to the set of all x which belong to all sets except q of them. Sets Xi that do not intersect the  q-relaxed intersection could be suspected to be outliers.\n\n\n=== Alternative models ===\nIn cases where the cause of the outliers is known, it may be possible to incorporate this effect into the model structure, for example by using a hierarchical Bayes model, or a mixture model.\n\n\n== See also ==\nAnomaly (natural sciences)\nNovelty detection\nAnscombe's quartet\nData transformation (statistics)\nExtreme value theory\nInfluential observation\nRandom sample consensus\nRobust regression\nStudentized residual\nWinsorizing\n\n\n== References ==\n\n\n== External links ==\n\nRenze, John. \"Outlier\". MathWorld.\nBalakrishnan, N.; Childs, A. (2001) [1994], \"Outlier\", Encyclopedia of Mathematics, EMS Press\nGrubbs test described by NIST manual", "Decision tree learning": "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\nTree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).\n\n\n== General ==\n\nDecision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables.\nA decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the \"classification\". Each element of the domain of the classification is called a class.\nA decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes).\nA tree is built by splitting the source set, constituting the root node of the tree, into subsets\u2014which constitute the successor children. The splitting is based on a set of splitting rules based on classification features.  This process is repeated on each derived subset in a recursive manner called recursive partitioning.\nThe recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data.In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.\nData comes in records of the form:\n\n  \n    \n      \n        (\n        \n          \n            x\n          \n        \n        ,\n        Y\n        )\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            k\n          \n        \n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle ({\\textbf {x}},Y)=(x_{1},x_{2},x_{3},...,x_{k},Y)}\n  The dependent variable, \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , is the target variable that we are trying to understand, classify or generalize. The vector \n  \n    \n      \n        \n          \n            x\n          \n        \n      \n    \n    {\\displaystyle {\\textbf {x}}}\n   is composed of the features, \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},x_{3}}\n   etc., that are used for that task.\n\n\n== Decision tree types ==\nDecision trees used in data mining are of two main types:\n\nClassification tree analysis is when the predicted outcome is the class (discrete) to which the data belongs.\nRegression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient's length of stay in a hospital).The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al. in 1984. Trees used for regression and trees used for classification have some similarities \u2013 but also some differences, such as the procedure used to determine where to split.Some techniques, often called ensemble methods, construct more than one decision tree:\n\nBoosted trees Incrementally building an ensemble by training each new instance to emphasize the training instances previously mis-modeled. A typical example is AdaBoost. These can be used for regression-type and classification-type problems.\nBootstrap aggregated (or bagged) decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction.A random forest classifier is a specific type of bootstrap aggregating\nRotation forest \u2013 in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features.A special case of a decision tree is a decision list, which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node).  While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods and monotonic constraints to be imposed.Notable decision tree algorithms include:\n\nID3 (Iterative Dichotomiser 3)\nC4.5 (successor of ID3)\nCART (Classification And Regression Tree)\nChi-square automatic interaction detection (CHAID). Performs multi-level splits when computing classification trees.\nMARS: extends decision trees to handle numerical data better.\nConditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning.ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning a decision tree from training tuples.\nIt has also been proposed to leverage concepts of fuzzy set theory for the definition of a special version of decision tree, known as Fuzzy Decision Tree (FDT).\nIn this type of fuzzy classification, generally, an input vector \n  \n    \n      \n        \n          \n            x\n          \n        \n      \n    \n    {\\displaystyle {\\textbf {x}}}\n   is associated with multiple classes, each with a different confidence value.\nBoosted ensembles of FDTs have been recently investigated as well, and they have shown performances comparable to those of other very efficient fuzzy classifiers.\n\n\n== Metrics ==\nAlgorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. Different algorithms use different metrics for measuring \"best\".  These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split. Depending on the underlying metric, the performance of various heuristic algorithms for decision tree learning may vary significantly.\n\n\n=== Estimate of Positive Correctness ===\nA simple and effective metric can be used to identify the degree to which true positives outweigh true negatives (see Confusion matrix). This metric, \"Estimate of Positive Correctness\" is defined below:\n\n  \n    \n      \n        \n          E\n          \n            P\n          \n        \n        =\n        T\n        P\n        \u2212\n        F\n        P\n      \n    \n    {\\displaystyle E_{P}=TP-FP}\n  \nIn this equation, the total false positives (FP) are subtracted from the total true positives (TP). The resulting number gives an estimate on how many positive examples the feature could correctly identify within the data, with higher numbers meaning that the feature could correctly classify more positive samples. Below is an example of how to use the metric when the full confusion matrix of a certain feature is given:\nFeature A Confusion Matrix\n\nHere we can see that the TP value would be 8 and the FP value would be 2 (the underlined numbers in the table). When we plug these numbers in the equation we are able to calculate the estimate: \n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n        =\n        T\n        P\n        \u2212\n        F\n        P\n        =\n        8\n        \u2212\n        2\n        =\n        6\n      \n    \n    {\\displaystyle E_{p}=TP-FP=8-2=6}\n  . This means that using the estimate on this feature would have it receive a score of 6.\nHowever, it should be worth noting that this number is only an estimate. For example, if two features both had a FP value of 2 while one of the features had a higher TP value, that feature would be ranked higher than the other because the resulting estimate when using the equation would give a higher value. This could lead to some inaccuracies when using the metric if some features have more positive samples than others. To combat this, one could use a more powerful metric known as Sensitivity that takes into account the proportions of the values from the confusion matrix to give the actual true positive rate (TPR). The difference between these metrics is shown in the example below:\n\nIn this example, Feature A had an estimate of 6 and a TPR of approximately 0.73 while Feature B had an estimate of 4 and a TPR of 0.75. This shows that although the positive estimate for some feature may be higher, the more accurate TPR value for that feature may be lower when compared to other features that have a lower positive estimate. Depending on the situation and knowledge of the data and decision trees, one may opt to use the positive estimate for a quick and easy solution to their problem. On the other hand, a more experienced user would most likely prefer to use the TPR value to rank the features because it takes into account the proportions of the data and all the samples that should have been classified as positive.\n\n\n=== Gini impurity ===\nGini impurity, Gini's diversity index, or Gini-Simpson Index in biodiversity research, is named after Italian mathematician Corrado Gini and used by the CART (classification and regression tree) algorithm for classification trees. Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it was labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category.\nFor a set of items with \n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n   classes and relative frequencies \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  , \n  \n    \n      \n        i\n        \u2208\n        {\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        J\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,2,...,J\\}}\n  , the probability of choosing an item with label \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   is \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  , and the probability of miscategorizing that item is \n  \n    \n      \n        \n          \u2211\n          \n            k\n            \u2260\n            i\n          \n        \n        \n          p\n          \n            k\n          \n        \n        =\n        1\n        \u2212\n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sum _{k\\neq i}p_{k}=1-p_{i}}\n  . The Gini impurity is computed by summing pairwise products of these probabilities for each class label:\n\n  \n    \n      \n        \n          I\n          \n            G\n          \n        \n        \u2061\n        (\n        p\n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          (\n          \n            \n              p\n              \n                i\n              \n            \n            \n              \u2211\n              \n                k\n                \u2260\n                i\n              \n            \n            \n              p\n              \n                k\n              \n            \n          \n          )\n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        (\n        1\n        \u2212\n        \n          p\n          \n            i\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        (\n        \n          p\n          \n            i\n          \n        \n        \u2212\n        \n          p\n          \n            i\n          \n          \n            2\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n          \n            2\n          \n        \n        =\n        1\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {I} _{G}(p)=\\sum _{i=1}^{J}\\left(p_{i}\\sum _{k\\neq i}p_{k}\\right)=\\sum _{i=1}^{J}p_{i}(1-p_{i})=\\sum _{i=1}^{J}(p_{i}-p_{i}^{2})=\\sum _{i=1}^{J}p_{i}-\\sum _{i=1}^{J}p_{i}^{2}=1-\\sum _{i=1}^{J}p_{i}^{2}.}\n  The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient \n  \n    \n      \n        q\n        =\n        2\n      \n    \n    {\\displaystyle q=2}\n  , which in physics is associated with the lack of information in out-of-equilibrium, non-extensive, dissipative and quantum systems. For the limit \n  \n    \n      \n        q\n        \u2192\n        1\n      \n    \n    {\\displaystyle q\\to 1}\n   one recovers the usual Boltzmann-Gibbs or Shannon entropy. In this sense, the Gini impurity is nothing but a variation of the usual entropy measure for decision trees.\n\n\n=== Information gain ===\n\nUsed by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.\nEntropy is defined as below\n\n  \n    \n      \n        \n          H\n        \n        (\n        T\n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        \u2061\n        \n          (\n          \n            \n              p\n              \n                1\n              \n            \n            ,\n            \n              p\n              \n                2\n              \n            \n            ,\n            \u2026\n            ,\n            \n              p\n              \n                J\n              \n            \n          \n          )\n        \n        =\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {H} (T)=\\operatorname {I} _{E}\\left(p_{1},p_{2},\\ldots ,p_{J}\\right)=-\\sum _{i=1}^{J}p_{i}\\log _{2}p_{i}}\n  where \n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle p_{1},p_{2},\\ldots }\n   are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.\n\n  \n    \n      \n        =\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          p\n          \n            i\n          \n        \n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \u2212\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n      \n    \n    {\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}p_{i}-\\sum _{i=1}^{J}-\\Pr(i\\mid a)\\log _{2}\\Pr(i\\mid a)}\n  Averaging over the possible values of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  ,\n\n  \n    \n      \n        =\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          p\n          \n            i\n          \n        \n        \u2212\n        \n          \u2211\n          \n            a\n          \n        \n        p\n        (\n        a\n        )\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \u2212\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n      \n    \n    {\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}p_{i}-\\sum _{a}p(a)\\sum _{i=1}^{J}-\\Pr(i\\mid a)\\log _{2}\\Pr(i\\mid a)}\n  \nWhere weighted sum of entropies is given by,\n\n  \n    \n      \n        \n          \n            H\n          \n          (\n          T\n          \u2223\n          A\n          )\n        \n        =\n        \n          \u2211\n          \n            a\n          \n        \n        p\n        (\n        a\n        )\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \u2212\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        Pr\n        (\n        i\n        \u2223\n        a\n        )\n      \n    \n    {\\displaystyle {\\mathrm {H} (T\\mid A)}=\\sum _{a}p(a)\\sum _{i=1}^{J}-\\Pr(i\\mid a)\\log _{2}\\Pr(i\\mid a)}\n  That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information.\nInformation gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value \"represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node\".Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes each have consistent data, or until the information gain is 0.\nTo find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's.\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        9\n        ,\n        5\n        ]\n        )\n        =\n        \u2212\n        \n          \n            9\n            14\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            9\n            14\n          \n        \n        \u2212\n        \n          \n            5\n            14\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            5\n            14\n          \n        \n        =\n        0.94\n      \n    \n    {\\displaystyle I_{E}([9,5])=-{\\frac {9}{14}}\\log _{2}{\\frac {9}{14}}-{\\frac {5}{14}}\\log _{2}{\\frac {5}{14}}=0.94}\n  The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above. Since there is an equal number of yes's and no's in this node, we have\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        )\n        =\n        \u2212\n        \n          \n            3\n            6\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            3\n            6\n          \n        \n        \u2212\n        \n          \n            3\n            6\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            3\n            6\n          \n        \n        =\n        \u2212\n        \n          \n            1\n            2\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            1\n            2\n          \n        \n        \u2212\n        \n          \n            1\n            2\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            1\n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle I_{E}([3,3])=-{\\frac {3}{6}}\\log _{2}{\\frac {3}{6}}-{\\frac {3}{6}}\\log _{2}{\\frac {3}{6}}=-{\\frac {1}{2}}\\log _{2}{\\frac {1}{2}}-{\\frac {1}{2}}\\log _{2}{\\frac {1}{2}}=1}\n  For the node where windy=false there were eight data points, six yes's and two no's. Thus we have\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        \u2212\n        \n          \n            6\n            8\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            6\n            8\n          \n        \n        \u2212\n        \n          \n            2\n            8\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            2\n            8\n          \n        \n        =\n        \u2212\n        \n          \n            3\n            4\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            3\n            4\n          \n        \n        \u2212\n        \n          \n            1\n            4\n          \n        \n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          \n            1\n            4\n          \n        \n        =\n        0.81\n      \n    \n    {\\displaystyle I_{E}([6,2])=-{\\frac {6}{8}}\\log _{2}{\\frac {6}{8}}-{\\frac {2}{8}}\\log _{2}{\\frac {2}{8}}=-{\\frac {3}{4}}\\log _{2}{\\frac {3}{4}}-{\\frac {1}{4}}\\log _{2}{\\frac {1}{4}}=0.81}\n  To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node.\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        ,\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        (\n        \n          windy or not\n        \n        )\n        =\n        \n          \n            6\n            14\n          \n        \n        \u22c5\n        1\n        +\n        \n          \n            8\n            14\n          \n        \n        \u22c5\n        0.81\n        =\n        0.89\n      \n    \n    {\\displaystyle I_{E}([3,3],[6,2])=I_{E}({\\text{windy or not}})={\\frac {6}{14}}\\cdot 1+{\\frac {8}{14}}\\cdot 0.81=0.89}\n  Now we can calculate the information gain achieved by splitting on the windy feature.\n\n  \n    \n      \n        IG\n        \u2061\n        (\n        \n          windy\n        \n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        9\n        ,\n        5\n        ]\n        )\n        \u2212\n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        ,\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        0.94\n        \u2212\n        0.89\n        =\n        0.05\n      \n    \n    {\\displaystyle \\operatorname {IG} ({\\text{windy}})=I_{E}([9,5])-I_{E}([3,3],[6,2])=0.94-0.89=0.05}\n  To build the tree, the information gain of each possible first split would need to be calculated. The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al.Information gain is also known as Shannon index in bio diversity research.\n\n\n=== Variance reduction ===\nIntroduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable Y due to the split at this node:\n\n  \n    \n      \n        \n          I\n          \n            V\n          \n        \n        (\n        N\n        )\n        =\n        \n          \n            1\n            \n              \n                |\n              \n              S\n              \n                \n                  |\n                \n                \n                  2\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            \u2208\n            S\n          \n        \n        \n          \u2211\n          \n            j\n            \u2208\n            S\n          \n        \n        \n          \n            1\n            2\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n        \u2212\n        \n          (\n          \n            \n              \n                \n                  \n                    |\n                  \n                  \n                    S\n                    \n                      t\n                    \n                  \n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    |\n                  \n                  S\n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              \n                1\n                \n                  \n                    |\n                  \n                  \n                    S\n                    \n                      t\n                    \n                  \n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              \u2211\n              \n                i\n                \u2208\n                \n                  S\n                  \n                    t\n                  \n                \n              \n            \n            \n              \u2211\n              \n                j\n                \u2208\n                \n                  S\n                  \n                    t\n                  \n                \n              \n            \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              y\n              \n                i\n              \n            \n            \u2212\n            \n              y\n              \n                j\n              \n            \n            \n              )\n              \n                2\n              \n            \n            +\n            \n              \n                \n                  \n                    |\n                  \n                  \n                    S\n                    \n                      f\n                    \n                  \n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    |\n                  \n                  S\n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              \n                1\n                \n                  \n                    |\n                  \n                  \n                    S\n                    \n                      f\n                    \n                  \n                  \n                    \n                      |\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              \u2211\n              \n                i\n                \u2208\n                \n                  S\n                  \n                    f\n                  \n                \n              \n            \n            \n              \u2211\n              \n                j\n                \u2208\n                \n                  S\n                  \n                    f\n                  \n                \n              \n            \n            \n              \n                1\n                2\n              \n            \n            (\n            \n              y\n              \n                i\n              \n            \n            \u2212\n            \n              y\n              \n                j\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle I_{V}(N)={\\frac {1}{|S|^{2}}}\\sum _{i\\in S}\\sum _{j\\in S}{\\frac {1}{2}}(y_{i}-y_{j})^{2}-\\left({\\frac {|S_{t}|^{2}}{|S|^{2}}}{\\frac {1}{|S_{t}|^{2}}}\\sum _{i\\in S_{t}}\\sum _{j\\in S_{t}}{\\frac {1}{2}}(y_{i}-y_{j})^{2}+{\\frac {|S_{f}|^{2}}{|S|^{2}}}{\\frac {1}{|S_{f}|^{2}}}\\sum _{i\\in S_{f}}\\sum _{j\\in S_{f}}{\\frac {1}{2}}(y_{i}-y_{j})^{2}\\right)}\n  where \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  , \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  , and \n  \n    \n      \n        \n          S\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle S_{f}}\n   are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean.\nBy replacing \n  \n    \n      \n        (\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          y\n          \n            j\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (y_{i}-y_{j})^{2}}\n   in the formula above with the dissimilarity \n  \n    \n      \n        \n          d\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle d_{ij}}\n   between two objects \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   and \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  , the variance reduction criterion applies to any kind of object for which pairwise dissimilarities can be computed.\n\n\n=== Measure of \"goodness\" ===\nUsed by CART in 1984, the measure of \"goodness\" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete. The function \n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        t\n        )\n      \n    \n    {\\displaystyle \\varphi (s\\mid t)}\n  , where \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   is a candidate split at node \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  , is defined as below\n\n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        t\n        )\n        =\n        2\n        \n          P\n          \n            L\n          \n        \n        \n          P\n          \n            R\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            class count\n          \n        \n        \n          |\n        \n        P\n        (\n        j\n        \u2223\n        \n          t\n          \n            L\n          \n        \n        )\n        \u2212\n        P\n        (\n        j\n        \u2223\n        \n          t\n          \n            R\n          \n        \n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle \\varphi (s\\mid t)=2P_{L}P_{R}\\sum _{j=1}^{\\text{class count}}|P(j\\mid t_{L})-P(j\\mid t_{R})|}\n  where \n  \n    \n      \n        \n          t\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle t_{L}}\n   and \n  \n    \n      \n        \n          t\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle t_{R}}\n   are the left and right children of node \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   using split \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , respectively; \n  \n    \n      \n        \n          P\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle P_{L}}\n   and \n  \n    \n      \n        \n          P\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle P_{R}}\n   are the proportions of records in \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   in \n  \n    \n      \n        \n          t\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle t_{L}}\n   and \n  \n    \n      \n        \n          t\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle t_{R}}\n  , respectively; and \n  \n    \n      \n        P\n        (\n        j\n        \u2223\n        \n          t\n          \n            L\n          \n        \n        )\n      \n    \n    {\\displaystyle P(j\\mid t_{L})}\n   and \n  \n    \n      \n        P\n        (\n        j\n        \u2223\n        \n          t\n          \n            R\n          \n        \n        )\n      \n    \n    {\\displaystyle P(j\\mid t_{R})}\n   are the proportions of class \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   records in \n  \n    \n      \n        \n          t\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle t_{L}}\n   and \n  \n    \n      \n        \n          t\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle t_{R}}\n  , respectively.\nConsider an example data set with three attributes: savings(low, medium, high), assets(low, medium, high), income(numerical value), and a binary target variable credit risk(good, bad) and 8 data points. The full data is presented in the table below. To start a decision tree, we will calculate the maximum value of \n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        t\n        )\n      \n    \n    {\\displaystyle \\varphi (s\\mid t)}\n   using each feature to find which one will split the root node. This process will continue until all children are pure or all \n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        t\n        )\n      \n    \n    {\\displaystyle \\varphi (s\\mid t)}\n   values are below a set threshold.\n\nTo find \n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        t\n        )\n      \n    \n    {\\displaystyle \\varphi (s\\mid t)}\n   of the feature savings, we need to note the quantity of each value. The original data contained three low's, three medium's, and two high's. Out of the low's, one had a good credit risk while out of the medium's and high's, 4 had a good credit risk. Assume a candidate split \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   such that records with a low savings will be put in the left child and all other records will be put into the right child.\n\n  \n    \n      \n        \u03c6\n        (\n        s\n        \u2223\n        \n          root\n        \n        )\n        =\n        2\n        \u22c5\n        \n          \n            3\n            8\n          \n        \n        \u22c5\n        \n          \n            5\n            8\n          \n        \n        \u22c5\n        \n          (\n          \n            \n              |\n              \n                (\n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                  \u2212\n                  \n                    \n                      4\n                      5\n                    \n                  \n                \n                )\n              \n              |\n            \n            +\n            \n              |\n              \n                (\n                \n                  \n                    \n                      2\n                      3\n                    \n                  \n                  \u2212\n                  \n                    \n                      1\n                      5\n                    \n                  \n                \n                )\n              \n              |\n            \n          \n          )\n        \n        =\n        0.44\n      \n    \n    {\\displaystyle \\varphi (s\\mid {\\text{root}})=2\\cdot {\\frac {3}{8}}\\cdot {\\frac {5}{8}}\\cdot \\left(\\left|\\left({\\frac {1}{3}}-{\\frac {4}{5}}\\right)\\right|+\\left|\\left({\\frac {2}{3}}-{\\frac {1}{5}}\\right)\\right|\\right)=0.44}\n  To build the tree, the \"goodness\" of all candidate splits for the root node need to be calculated. The candidate with the maximum value will split the root node, and the process will continue for each impure node until the tree is complete.\nCompared to other metrics such as information gain, the measure of \"goodness\" will attempt to create a more balanced tree, leading to more-consistent decision time. However, it sacrifices some priority for creating pure children which can lead to additional splits that are not present with other metrics.\n\n\n== Uses ==\n\n\n=== Advantages ===\nAmongst other data mining methods, decision trees have various advantages:\n\nSimple to understand and interpret. People are able to understand decision tree models after a brief explanation. Trees can also be displayed graphically in a way that is easy for non-experts to interpret.\nAble to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables or categoricals converted to 0-1 values.) Early decision trees were only capable of handling categorical variables, but more recent versions, such as C4.5, do not have this limitation.\nRequires little data preparation. Other techniques often require data normalization. Since trees can handle qualitative predictors, there is no need to create dummy variables.\nUses a white box or open-box model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand, for example with an artificial neural network.\nPossible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\nNon-parametric approach that makes no assumptions of the training data or prediction residuals; e.g., no distributional, independence, or constant variance assumptions\nPerforms well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.\nAccuracy with flexible modeling. These methods may be applied to healthcare research with increased accuracy.\nMirrors human decision making more closely than other approaches. This could be useful when modeling human decisions/behavior.\nRobust against co-linearity, particularly boosting.\nIn built feature selection. Additional irrelevant feature will be less used so that they can be removed on subsequent runs. The hierarchy of attributes in a decision tree reflects the importance of attributes. It means that the features on top are the most informative.\nDecision trees can approximate any Boolean function e.g. XOR.\n\n\n=== Limitations ===\nTrees can be very non-robust. A small change in the training data can result in a large change in the tree and consequently the final predictions.\nThe problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. To reduce the greedy effect of local optimality, some methods such as the dual information distance (DID) tree were proposed.\nDecision-tree learners can create over-complex trees that do not generalize well from the training data.  (This is known as overfitting.)  Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, that does not require pruning).\nThe average depth of the tree that is defined by the number of nodes or tests till classification is not guaranteed to be minimal or small under various splitting criteria.\nFor data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of attributes with more levels. To counter this problem,  instead of choosing the attribute with highest information gain, one can choose the attribute with the highest information gain ratio among the attributes whose information gain is greater than the mean information gain.  This biases the decision tree against considering attributes with a large number of distinct values, while not giving an unfair advantage to attributes with very low information gain. Alternatively, the issue of biased predictor selection can be avoided by the Conditional Inference approach, a two-stage approach, or adaptive leave-one-out feature selection.\n\n\n=== Implementations ===\nMany data mining software packages provide implementations of one or more decision tree algorithms.\nExamples include\n\nSalford Systems CART (which licensed the proprietary code of the original CART authors),\nIBM SPSS Modeler,\nRapidMiner,\nSAS Enterprise Miner,\nMatlab,\nR (an open-source software environment for statistical computing, which includes several CART implementations such as rpart, party and randomForest packages),\nWeka (a free and open-source data-mining suite, contains many decision tree algorithms),\nOrange,\nKNIME,\nMicrosoft SQL Server [1], and\nscikit-learn (a free and open-source machine learning library for the Python programming language).\n\n\n== Extensions ==\n\n\n=== Decision graphs ===\nIn a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML).  Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.  In general, decision graphs infer models with fewer leaves than decision trees.\n\n\n=== Alternative search methods ===\nEvolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.It is also possible for a tree to be sampled using MCMC.The tree can be searched for in a bottom-up fashion. Or several trees can be constructed parallelly to reduce the expected number of tests till classification.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nJames, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2017). \"Tree-Based Methods\" (PDF). An Introduction to Statistical Learning: with Applications in R. New York: Springer. pp. 303\u2013336. ISBN 978-1-4614-7137-0.\n\n\n== External links ==\nEvolutionary Learning of Decision Trees in C++\nA very detailed explanation of information gain as splitting criterion", "Joint probability distribution": "Given two random variables that are defined on the same probability space, the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs. The joint distribution can just as well be considered for any given number of random variables. The joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables. It also encodes the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s).\nIn the formal mathematical setup of measure theory, the joint distribution is given by the pushforward measure, by the map obtained by pairing together the given random variables, of the sample space's probability measure.\nIn the case of real-valued random variables, the joint distribution, as a particular multivariate distribution, may be expressed by a multivariate cumulative distribution function, or by a multivariate probability density function together with a multivariate probability mass function. In the special case of continuous random variables, it is sufficient to consider probability density functions, and in the case of discrete random variables, it is sufficient to consider probability mass functions.\n\n\n== Examples ==\n\n\n=== Draws from an urn ===\nEach of two urns contains twice as many red balls as blue balls, and no others, and one ball is randomly selected from each urn, with the two draws independent of each other. Let \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   be discrete random variables associated with the outcomes of the draw from the first urn and second urn respectively. The probability of drawing a red ball from either of the urns is 2/3, and the probability of drawing a blue ball is 1/3. The joint probability distribution is presented in the following table:\n\nEach of the four inner cells shows the probability of a particular combination of results from the two draws; these probabilities are the joint distribution. In any one cell the probability of a particular combination occurring is (since the draws are independent) the product of the probability of the specified result for A and the probability of the specified result for B. The probabilities in these four cells sum to 1, as with all probability distributions.\nMoreover, the final row and the final column give the marginal probability distribution for A and the marginal probability distribution for B respectively. For example, for A the first of these cells gives the sum of the probabilities for A being red, regardless of which possibility for B in the column above the cell occurs, as 2/3. Thus the marginal probability distribution for \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   gives \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  's probabilities unconditional on \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , in a margin of the table.\n\n\n=== Coin flips ===\nConsider the flip of two fair coins; let \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   be discrete random variables associated with the outcomes of the first and second coin flips respectively. Each coin flip is a Bernoulli trial and has a Bernoulli distribution. If a coin displays \"heads\" then the associated random variable takes the value 1, and it takes the value 0 otherwise. The probability of each of these outcomes is 1/2, so the marginal (unconditional) density functions are\n\n  \n    \n      \n        P\n        (\n        A\n        )\n        =\n        1\n        \n          /\n        \n        2\n        \n        \n          for\n        \n        \n        A\n        \u2208\n        {\n        0\n        ,\n        1\n        }\n        ;\n      \n    \n    {\\displaystyle P(A)=1/2\\quad {\\text{for}}\\quad A\\in \\{0,1\\};}\n  \n\n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        1\n        \n          /\n        \n        2\n        \n        \n          for\n        \n        \n        B\n        \u2208\n        {\n        0\n        ,\n        1\n        }\n        .\n      \n    \n    {\\displaystyle P(B)=1/2\\quad {\\text{for}}\\quad B\\in \\{0,1\\}.}\n  The joint probability mass function of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   defines probabilities for each pair of outcomes. All possible outcomes are\n\n  \n    \n      \n        (\n        A\n        =\n        0\n        ,\n        B\n        =\n        0\n        )\n        ,\n        (\n        A\n        =\n        0\n        ,\n        B\n        =\n        1\n        )\n        ,\n        (\n        A\n        =\n        1\n        ,\n        B\n        =\n        0\n        )\n        ,\n        (\n        A\n        =\n        1\n        ,\n        B\n        =\n        1\n        )\n        .\n      \n    \n    {\\displaystyle (A=0,B=0),(A=0,B=1),(A=1,B=0),(A=1,B=1).}\n  Since each outcome is equally likely the joint probability mass function becomes\n\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        )\n        =\n        1\n        \n          /\n        \n        4\n        \n        \n          for\n        \n        \n        A\n        ,\n        B\n        \u2208\n        {\n        0\n        ,\n        1\n        }\n        .\n      \n    \n    {\\displaystyle P(A,B)=1/4\\quad {\\text{for}}\\quad A,B\\in \\{0,1\\}.}\n  Since the coin flips are independent, the joint probability mass function is the product\nof the marginals:\n\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n        \n        \n          for\n        \n        \n        A\n        ,\n        B\n        \u2208\n        {\n        0\n        ,\n        1\n        }\n        .\n      \n    \n    {\\displaystyle P(A,B)=P(A)P(B)\\quad {\\text{for}}\\quad A,B\\in \\{0,1\\}.}\n  \n\n\n=== Rolling a die ===\nConsider the roll of a fair die and let \n  \n    \n      \n        A\n        =\n        1\n      \n    \n    {\\displaystyle A=1}\n   if the number is even (i.e. 2, 4, or 6) and \n  \n    \n      \n        A\n        =\n        0\n      \n    \n    {\\displaystyle A=0}\n   otherwise. Furthermore, let \n  \n    \n      \n        B\n        =\n        1\n      \n    \n    {\\displaystyle B=1}\n   if the number is prime (i.e. 2, 3, or 5) and \n  \n    \n      \n        B\n        =\n        0\n      \n    \n    {\\displaystyle B=0}\n   otherwise.\n\nThen, the joint distribution of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , expressed as a probability mass function, is\n\n  \n    \n      \n        \n          P\n        \n        (\n        A\n        =\n        0\n        ,\n        B\n        =\n        0\n        )\n        =\n        P\n        {\n        1\n        }\n        =\n        \n          \n            1\n            6\n          \n        \n        ,\n        \n        \n        \n          P\n        \n        (\n        A\n        =\n        1\n        ,\n        B\n        =\n        0\n        )\n        =\n        P\n        {\n        4\n        ,\n        6\n        }\n        =\n        \n          \n            2\n            6\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathrm {P} (A=0,B=0)=P\\{1\\}={\\frac {1}{6}},\\quad \\quad \\mathrm {P} (A=1,B=0)=P\\{4,6\\}={\\frac {2}{6}},}\n  \n\n  \n    \n      \n        \n          P\n        \n        (\n        A\n        =\n        0\n        ,\n        B\n        =\n        1\n        )\n        =\n        P\n        {\n        3\n        ,\n        5\n        }\n        =\n        \n          \n            2\n            6\n          \n        \n        ,\n        \n        \n        \n          P\n        \n        (\n        A\n        =\n        1\n        ,\n        B\n        =\n        1\n        )\n        =\n        P\n        {\n        2\n        }\n        =\n        \n          \n            1\n            6\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {P} (A=0,B=1)=P\\{3,5\\}={\\frac {2}{6}},\\quad \\quad \\mathrm {P} (A=1,B=1)=P\\{2\\}={\\frac {1}{6}}.}\n  These probabilities necessarily sum to 1, since the probability of some combination of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   occurring is 1.\n\n\n== Marginal probability distribution ==\nIf more than one random variable is defined in a random experiment, it is important to distinguish between the joint probability distribution of X and Y and the probability distribution of each variable individually. The individual probability distribution of a random variable is referred to as its marginal probability distribution. In general, the marginal probability distribution of X can be determined from the joint probability distribution of X and other random variables.\nIf the joint probability density function of random variable X and Y is  \n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n   , the marginal probability density function of X and Y, which defines the marginal distribution, is given by:\n\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        \u222b\n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n        d\n        y\n      \n    \n    {\\displaystyle f_{X}(x)=\\int f_{X,Y}(x,y)\\;dy}\n  \n\n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n        =\n        \u222b\n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle f_{Y}(y)=\\int f_{X,Y}(x,y)\\;dx}\n  \nwhere the first integral is over all points in the range of (X,Y) for which X=x and the second integral is over all points in the range of (X,Y) for which Y=y.\n\n\n== Joint cumulative distribution function ==\nFor a pair of random variables \n  \n    \n      \n        X\n        ,\n        Y\n      \n    \n    {\\displaystyle X,Y}\n  , the joint cumulative distribution function (CDF) \n  \n    \n      \n        \n          F\n          \n            X\n            Y\n          \n        \n      \n    \n    {\\displaystyle F_{XY}}\n   is given by:\u200ap. 89\u200a\n\nwhere the right-hand side represents the probability that the random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   takes on a value less than or equal to \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and that \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   takes on a value less than or equal to \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  .\nFor \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   random variables \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\ldots ,X_{N}}\n  , the joint CDF \n  \n    \n      \n        \n          F\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \u2026\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{X_{1},\\ldots ,X_{N}}}\n   is given by\n\nInterpreting the \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   random variables as a random vector \n  \n    \n      \n        \n          X\n        \n        =\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            N\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} =(X_{1},\\ldots ,X_{N})^{T}}\n   yields a shorter notation:\n\n  \n    \n      \n        \n          F\n          \n            \n              X\n            \n          \n        \n        (\n        \n          x\n        \n        )\n        =\n        P\n        \u2061\n        (\n        \n          X\n          \n            1\n          \n        \n        \u2264\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            N\n          \n        \n        \u2264\n        \n          x\n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle F_{\\mathbf {X} }(\\mathbf {x} )=\\operatorname {P} (X_{1}\\leq x_{1},\\ldots ,X_{N}\\leq x_{N})}\n  \n\n\n== Joint density function or mass function ==\n\n\n=== Discrete case ===\nThe joint probability mass function of two discrete random variables \n  \n    \n      \n        X\n        ,\n        Y\n      \n    \n    {\\displaystyle X,Y}\n   is:\n\nor written in terms of conditional distributions\n\n  \n    \n      \n        \n          p\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          P\n        \n        (\n        Y\n        =\n        y\n        \u2223\n        X\n        =\n        x\n        )\n        \u22c5\n        \n          P\n        \n        (\n        X\n        =\n        x\n        )\n        =\n        \n          P\n        \n        (\n        X\n        =\n        x\n        \u2223\n        Y\n        =\n        y\n        )\n        \u22c5\n        \n          P\n        \n        (\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (Y=y\\mid X=x)\\cdot \\mathrm {P} (X=x)=\\mathrm {P} (X=x\\mid Y=y)\\cdot \\mathrm {P} (Y=y)}\n  where \n  \n    \n      \n        \n          P\n        \n        (\n        Y\n        =\n        y\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {P} (Y=y\\mid X=x)}\n   is the probability of \n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n   given that \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  .\nThe generalization of the preceding two-variable case is the joint probability distribution of \n  \n    \n      \n        n\n        \n      \n    \n    {\\displaystyle n\\,}\n   discrete random variables \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2},\\dots ,X_{n}}\n   which is:\n\nor equivalently\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  p\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ,\n                    \u2026\n                    ,\n                    \n                      X\n                      \n                        n\n                      \n                    \n                  \n                \n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    n\n                  \n                \n                )\n              \n              \n                \n                =\n                \n                  P\n                \n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                =\n                \n                  x\n                  \n                    1\n                  \n                \n                )\n                \u22c5\n                \n                  P\n                \n                (\n                \n                  X\n                  \n                    2\n                  \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n                \u2223\n                \n                  X\n                  \n                    1\n                  \n                \n                =\n                \n                  x\n                  \n                    1\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \u22c5\n                \n                  P\n                \n                (\n                \n                  X\n                  \n                    3\n                  \n                \n                =\n                \n                  x\n                  \n                    3\n                  \n                \n                \u2223\n                \n                  X\n                  \n                    1\n                  \n                \n                =\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \n                  X\n                  \n                    2\n                  \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \u2026\n              \n            \n            \n              \n              \n                \n                \u22c5\n                P\n                (\n                \n                  X\n                  \n                    n\n                  \n                \n                =\n                \n                  x\n                  \n                    n\n                  \n                \n                \u2223\n                \n                  X\n                  \n                    1\n                  \n                \n                =\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \n                  X\n                  \n                    2\n                  \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  X\n                  \n                    n\n                    \u2212\n                    1\n                  \n                \n                =\n                \n                  x\n                  \n                    n\n                    \u2212\n                    1\n                  \n                \n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}p_{X_{1},\\ldots ,X_{n}}(x_{1},\\ldots ,x_{n})&=\\mathrm {P} (X_{1}=x_{1})\\cdot \\mathrm {P} (X_{2}=x_{2}\\mid X_{1}=x_{1})\\\\&\\cdot \\mathrm {P} (X_{3}=x_{3}\\mid X_{1}=x_{1},X_{2}=x_{2})\\\\&\\dots \\\\&\\cdot P(X_{n}=x_{n}\\mid X_{1}=x_{1},X_{2}=x_{2},\\dots ,X_{n-1}=x_{n-1}).\\end{aligned}}}\n  .This identity is known as the chain rule of probability.\nSince these are probabilities, in the two-variable case\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u2211\n          \n            j\n          \n        \n        \n          P\n        \n        (\n        X\n        =\n        \n          x\n          \n            i\n          \n        \n         \n        \n          a\n          n\n          d\n        \n         \n        Y\n        =\n        \n          y\n          \n            j\n          \n        \n        )\n        =\n        1\n        ,\n        \n      \n    \n    {\\displaystyle \\sum _{i}\\sum _{j}\\mathrm {P} (X=x_{i}\\ \\mathrm {and} \\ Y=y_{j})=1,\\,}\n  which generalizes for \n  \n    \n      \n        n\n        \n      \n    \n    {\\displaystyle n\\,}\n   discrete random variables \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2},\\dots ,X_{n}}\n   to\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u2211\n          \n            j\n          \n        \n        \u2026\n        \n          \u2211\n          \n            k\n          \n        \n        \n          P\n        \n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n            i\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            2\n            j\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n            k\n          \n        \n        )\n        =\n        1.\n        \n      \n    \n    {\\displaystyle \\sum _{i}\\sum _{j}\\dots \\sum _{k}\\mathrm {P} (X_{1}=x_{1i},X_{2}=x_{2j},\\dots ,X_{n}=x_{nk})=1.\\;}\n  \n\n\n=== Continuous case ===\nThe joint probability density function \n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n   for two continuous random variables is defined as the derivative of the joint cumulative distribution function (see Eq.1):\n\nThis is equal to:\n\n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          f\n          \n            Y\n            \u2223\n            X\n          \n        \n        (\n        y\n        \u2223\n        x\n        )\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        \n          f\n          \n            X\n            \u2223\n            Y\n          \n        \n        (\n        x\n        \u2223\n        y\n        )\n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)=f_{Y\\mid X}(y\\mid x)f_{X}(x)=f_{X\\mid Y}(x\\mid y)f_{Y}(y)}\n  where \n  \n    \n      \n        \n          f\n          \n            Y\n            \u2223\n            X\n          \n        \n        (\n        y\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle f_{Y\\mid X}(y\\mid x)}\n   and \n  \n    \n      \n        \n          f\n          \n            X\n            \u2223\n            Y\n          \n        \n        (\n        x\n        \u2223\n        y\n        )\n      \n    \n    {\\displaystyle f_{X\\mid Y}(x\\mid y)}\n   are the conditional distributions of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n   and of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given \n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n   respectively, and \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n   and \n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{Y}(y)}\n   are the marginal distributions for \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   respectively.\nThe definition extends naturally to more than two random variables:\n\nAgain, since these are probability distributions, one has\n\n  \n    \n      \n        \n          \u222b\n          \n            x\n          \n        \n        \n          \u222b\n          \n            y\n          \n        \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n        d\n        y\n        \n        d\n        x\n        =\n        1\n      \n    \n    {\\displaystyle \\int _{x}\\int _{y}f_{X,Y}(x,y)\\;dy\\;dx=1}\n  respectively\n\n  \n    \n      \n        \n          \u222b\n          \n            \n              x\n              \n                1\n              \n            \n          \n        \n        \u2026\n        \n          \u222b\n          \n            \n              x\n              \n                n\n              \n            \n          \n        \n        \n          f\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \u2026\n            ,\n            \n              X\n              \n                n\n              \n            \n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \n        d\n        \n          x\n          \n            n\n          \n        \n        \u2026\n        \n        d\n        \n          x\n          \n            1\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\int _{x_{1}}\\ldots \\int _{x_{n}}f_{X_{1},\\ldots ,X_{n}}(x_{1},\\ldots ,x_{n})\\;dx_{n}\\ldots \\;dx_{1}=1}\n  \n\n\n=== Mixed case ===\nThe \"mixed joint density\" may be defined where one or more random variables are continuous and the other random variables are discrete. With one variable of each type\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                    X\n                    ,\n                    Y\n                  \n                \n                (\n                x\n                ,\n                y\n                )\n                =\n                \n                  f\n                  \n                    X\n                    \u2223\n                    Y\n                  \n                \n                (\n                x\n                \u2223\n                y\n                )\n                \n                  P\n                \n                (\n                Y\n                =\n                y\n                )\n                =\n                \n                  P\n                \n                (\n                Y\n                =\n                y\n                \u2223\n                X\n                =\n                x\n                )\n                \n                  f\n                  \n                    X\n                  \n                \n                (\n                x\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f_{X,Y}(x,y)=f_{X\\mid Y}(x\\mid y)\\mathrm {P} (Y=y)=\\mathrm {P} (Y=y\\mid X=x)f_{X}(x).\\end{aligned}}}\n  One example of a situation in which one may wish to find the cumulative distribution of one random variable which is continuous and another random variable which is discrete arises when one wishes to use a logistic regression in predicting the probability of a binary outcome Y conditional on the value of a continuously distributed outcome \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . One must use the \"mixed\" joint density when finding the cumulative distribution of this binary outcome because the input variables \n  \n    \n      \n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle (X,Y)}\n   were initially defined in such a way that one could not collectively assign it either a probability density function or a probability mass function.  Formally, \n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n   is the probability density function of \n  \n    \n      \n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle (X,Y)}\n   with respect to the product measure on the respective supports of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  . Either of these two decompositions can then be used to recover the joint cumulative distribution function:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    X\n                    ,\n                    Y\n                  \n                \n                (\n                x\n                ,\n                y\n                )\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    t\n                    \u2264\n                    y\n                  \n                \n                \n                  \u222b\n                  \n                    s\n                    =\n                    \u2212\n                    \u221e\n                  \n                  \n                    x\n                  \n                \n                \n                  f\n                  \n                    X\n                    ,\n                    Y\n                  \n                \n                (\n                s\n                ,\n                t\n                )\n                \n                d\n                s\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{X,Y}(x,y)&=\\sum \\limits _{t\\leq y}\\int _{s=-\\infty }^{x}f_{X,Y}(s,t)\\;ds.\\end{aligned}}}\n  The definition generalizes to a mixture of arbitrary numbers of discrete and continuous random variables.\n\n\n== Additional properties ==\n\n\n=== Joint distribution for independent variables ===\nIn general two random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are independent if and only if the joint cumulative distribution function satisfies\n\n  \n    \n      \n        \n          F\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        \u22c5\n        \n          F\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle F_{X,Y}(x,y)=F_{X}(x)\\cdot F_{Y}(y)}\n  Two discrete random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are independent if and only if the joint probability mass function satisfies\n\n  \n    \n      \n        P\n        (\n        X\n        =\n        x\n         \n        \n          \n            and\n          \n        \n         \n        Y\n        =\n        y\n        )\n        =\n        P\n        (\n        X\n        =\n        x\n        )\n        \u22c5\n        P\n        (\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle P(X=x\\ {\\mbox{and}}\\ Y=y)=P(X=x)\\cdot P(Y=y)}\n  for all \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  .\nWhile the number of independent random events grows, the related joint probability value decreases rapidly to zero, according to a negative exponential law.\nSimilarly, two absolutely continuous random variables are independent if and only if\n\n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \u22c5\n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)=f_{X}(x)\\cdot f_{Y}(y)}\n  for all \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . This means that acquiring any information about the value of one or more of the random variables leads to a conditional distribution of any other variable that is identical to its unconditional (marginal) distribution; thus no variable provides any information about any other variable.\n\n\n=== Joint distribution for conditionally dependent variables ===\nIf a subset \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   of the variables \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \u22ef\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\cdots ,X_{n}}\n   is conditionally dependent given another subset \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   of these variables, then the probability mass function of the joint distribution is \n  \n    \n      \n        \n          P\n        \n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathrm {P} (X_{1},\\ldots ,X_{n})}\n  .  \n  \n    \n      \n        \n          P\n        \n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathrm {P} (X_{1},\\ldots ,X_{n})}\n   is equal to \n  \n    \n      \n        P\n        (\n        B\n        )\n        \u22c5\n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(B)\\cdot P(A\\mid B)}\n  . Therefore, it can be efficiently represented by the lower-dimensional probability distributions \n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n   and \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  . Such conditional independence relations can be represented with a Bayesian network or copula functions.\n\n\n=== Covariance ===\nWhen two or more random variables are defined on a probability space, it is useful to describe how they vary together; that is, it is useful to measure the relationship between the variables. A common measure of the relationship between two random variables is the covariance. Covariance is a measure of linear relationship between the random variables. If the relationship between the random variables is nonlinear, the covariance might not be sensitive to the relationship, which means, it does not relate the correlation between two variables.\nThe covariance between the random variable X and Y, denoted as cov(X,Y), is :\n\n  \n    \n      \n        \n          \u03c3\n          \n            X\n            Y\n          \n        \n        =\n        E\n        [\n        (\n        X\n        \u2212\n        \n          \u03bc\n          \n            x\n          \n        \n        )\n        (\n        Y\n        \u2212\n        \n          \u03bc\n          \n            y\n          \n        \n        )\n        ]\n        =\n        E\n        (\n        X\n        Y\n        )\n        \u2212\n        \n          \u03bc\n          \n            x\n          \n        \n        \n          \u03bc\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{XY}=E[(X-\\mu _{x})(Y-\\mu _{y})]=E(XY)-\\mu _{x}\\mu _{y}}\n  \n\n\n=== Correlation ===\nThere is another measure of the relationship between two random variables that is often easier to interpret than the covariance.\nThe correlation just scales the covariance by the product of the standard deviation of each variable. Consequently, the correlation is a dimensionless quantity that can be used to compare the linear relationships between pairs of variables in different units. If the points in the joint probability distribution of X and Y that receive positive probability tend to fall along a line of positive (or negative) slope, \u03c1XY is near +1 (or \u22121). If \u03c1XY equals +1 or \u22121, it can be shown that the points in the joint probability distribution that receive positive probability fall exactly along a straight line. Two random variables with nonzero correlation are said to be correlated. Similar to covariance, the correlation is a measure of the linear relationship between random variables.\nThe correlation between random variable X and Y, denoted as\n\n  \n    \n      \n        \n          \u03c1\n          \n            X\n            Y\n          \n        \n        =\n        \n          \n            \n              c\n              o\n              v\n              (\n              X\n              ,\n              Y\n              )\n            \n            \n              V\n              (\n              X\n              )\n              V\n              (\n              Y\n              )\n            \n          \n        \n        =\n        \n          \n            \n              \u03c3\n              \n                X\n                Y\n              \n            \n            \n              \n                \u03c3\n                \n                  X\n                \n              \n              \n                \u03c3\n                \n                  Y\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho _{XY}={\\frac {cov(X,Y)}{\\sqrt {V(X)V(Y)}}}={\\frac {\\sigma _{XY}}{\\sigma _{X}\\sigma _{Y}}}}\n  \n\n\n== Important named distributions ==\nNamed joint distributions that arise frequently in statistics include the multivariate normal distribution, the multivariate stable distribution, the multinomial distribution, the negative multinomial distribution, the multivariate hypergeometric distribution, and the elliptical distribution.\n\n\n== See also ==\nBayesian programming\nChow\u2013Liu tree\nConditional probability\nCopula (probability theory)\nDisintegration theorem\nMultivariate statistics\nStatistical interference\nPairwise independent distribution\n\n\n== References ==\n\n\n== External links ==\n\"Joint distribution\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\n\"Multi-dimensional distribution\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nA modern introduction to probability and statistics : understanding why and how. Dekking, Michel, 1946-. London: Springer. 2005. ISBN 978-1-85233-896-1. OCLC 262680588.\n\"Joint continuous density function\". PlanetMath.\nMathworld: Joint Distribution Function", "Probability density function": "In probability theory, a probability density function (PDF), or density of an absolutely continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample. Probability density  is the  probability per unit length,  in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample.\nIn a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range\u2014that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and the area under the entire curve is equal to 1.\nThe terms probability distribution function and probability function have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, \"probability distribution function\" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. \"Density function\" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.\n\n\n== Example ==\nSuppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.00... hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable. Suppose the answer is 0.02 (i.e., 2%). Then, the probability that the bacterium dies between 5 hours and 5.001 hours should be about 0.002, since this time interval is one-tenth as long as the previous. The probability that the bacterium dies between 5 hours and 5.0001 hours should be about 0.0002, and so on.\nIn this example, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to 2 per hour (or 2 hour\u22121). For example, there is 0.02 probability of dying in the 0.01-hour interval between 5 and 5.01 hours, and (0.02 probability / 0.01 hours) = 2 hour\u22121. This quantity 2 hour\u22121 is called the probability density for dying at around 5 hours. Therefore, the probability that the bacterium dies at 5 hours can be written as (2 hour\u22121) dt. This is the probability that the bacterium dies within an infinitesimal window of time around 5 hours, where dt is the duration of this window. For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour\u22121)\u00d7(1 nanosecond) \u2248 6\u00d710\u221213 (using the unit conversion 3.6\u00d71012 nanoseconds = 1 hour).\nThere is a probability density function f with f(5 hours) = 2 hour\u22121. The integral of f over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window.\n\n\n== Absolutely continuous univariate distributions ==\nA probability density function is most commonly associated with absolutely continuous univariate distributions. A random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   has density \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle f_{X}}\n  , where \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle f_{X}}\n   is a non-negative Lebesgue-integrable function, if:\n\nHence, if \n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle F_{X}}\n   is the cumulative distribution function of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , then:\n\nand (if \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle f_{X}}\n   is continuous at \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  )\n\nIntuitively, one can think of \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle f_{X}(x)\\,dx}\n   as being the probability of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   falling within the infinitesimal interval \n  \n    \n      \n        [\n        x\n        ,\n        x\n        +\n        d\n        x\n        ]\n      \n    \n    {\\displaystyle [x,x+dx]}\n  .\n\n\n== Formal definition ==\n(This definition may be extended to any probability distribution using the measure-theoretic definition of probability.)\nA random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   with values in a measurable space \n  \n    \n      \n        (\n        \n          \n            X\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {X}},{\\mathcal {A}})}\n   (usually \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n   with the Borel sets as measurable subsets) has as probability distribution the measure X\u2217P on \n  \n    \n      \n        (\n        \n          \n            X\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {X}},{\\mathcal {A}})}\n  : the density of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   with respect to a reference measure \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   on \n  \n    \n      \n        (\n        \n          \n            X\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {X}},{\\mathcal {A}})}\n   is the Radon\u2013Nikodym derivative:\n\nThat is, f is any measurable function with the property that:\n\nfor any measurable set \n  \n    \n      \n        A\n        \u2208\n        \n          \n            A\n          \n        \n        .\n      \n    \n    {\\displaystyle A\\in {\\mathcal {A}}.}\n  \n\n\n=== Discussion ===\nIn the continuous univariate case above, the reference measure is the Lebesgue measure. The probability mass function of a discrete random variable is the density with respect to the counting measure over the sample space (usually the set of integers, or some subset thereof).\nIt is not possible to define a density with reference to an arbitrary measure (e.g. one can't choose the counting measure as a reference for a continuous random variable). Furthermore, when it does exist, the density is almost unique, meaning that any two such densities coincide almost everywhere.\n\n\n== Further details ==\nUnlike a probability, a probability density function can take on values greater than one; for example, the uniform distribution on the interval [0, 1/2] has probability density f(x) = 2 for 0 \u2264 x \u2264 1/2 and f(x) = 0 elsewhere.\nThe standard normal distribution has probability density\n\nIf a random variable X is given and its distribution admits a probability density function f, then the expected value of X (if the expected value exists) can be calculated as\n\nNot every probability distribution has a density function: the distributions of discrete random variables do not; nor does the Cantor distribution, even though it has no discrete component, i.e., does not assign positive probability to any individual point.\nA distribution has a density function if and only if its cumulative distribution function F(x) is absolutely continuous. In this case: F is almost everywhere differentiable, and its derivative can be used as probability density:\n\nIf a probability distribution admits a density, then the probability of every one-point set {a} is zero; the same holds for finite and countable sets.\nTwo probability densities f and g represent the same probability distribution precisely if they differ only on a set of Lebesgue measure zero.\nIn the field of statistical physics, a non-formal reformulation of the relation above between the derivative of the cumulative distribution function and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:\nIf dt is an infinitely small number, the probability that X is included within the interval (t, t + dt) is equal to f(t) dt, or:\n\n\n== Link between discrete and continuous distributions ==\nIt is possible to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a generalized probability density function using the Dirac delta function. (This is not possible with a probability density function in the sense defined above, it may be done with a distribution.) For example, consider a binary discrete random variable having the Rademacher distribution\u2014that is, taking \u22121 or 1 for values, with probability 1\u20442 each. The density of probability associated with this variable is:\n\nMore generally, if a discrete variable can take n different values among real numbers, then the associated probability density function is:\n\nwhere \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n   are the discrete values accessible to the variable and \n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle p_{1},\\ldots ,p_{n}}\n   are the probabilities associated with these values.\nThis substantially unifies the treatment of discrete and continuous probability distributions. The above expression allows for determining statistical characteristics of such a discrete variable (such as the mean, variance, and kurtosis), starting from the formulas given for a continuous distribution of the probability.\n\n\n== Families of densities ==\nIt is common for probability density functions (and probability mass functions) to be parametrized\u2014that is, to be characterized by unspecified parameters. For example, the normal distribution is parametrized in terms of the mean and the variance, denoted by \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   and \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n   respectively, giving the family of densities\n\nDifferent values of the parameters describe different distributions of different random variables on the same sample space (the same set of all possible values of the variable); this sample space is the domain of the family of random variables that this family of distributions describes.  A given set of parameters describes a single distribution within the family sharing the functional form of the density. From the perspective of a given distribution, the parameters are constants, and terms in a density function that contain only parameters, but not variables, are part of the normalization factor of a distribution (the multiplicative factor that ensures that the area under the density\u2014the probability of something in the domain occurring\u2014 equals 1). This normalization factor is outside the kernel of the distribution.\nSince the parameters are constants, reparametrizing a density in terms of different parameters to give a characterization of a different random variable in the family, means simply substituting the new parameter values into the formula in place of the old ones.\n\n\n== Densities associated with multiple variables ==\nFor continuous random variables X1, ..., Xn, it is also possible to define a probability density function associated to the set as a whole, often called joint probability density function. This density function is defined as a function of the n variables, such that, for any domain D in the n-dimensional space of the values of the variables X1, ..., Xn, the probability that a realisation of the set variables falls inside the domain D is\n\nIf F(x1, ..., xn) = Pr(X1 \u2264 x1, ..., Xn \u2264 xn) is the cumulative distribution function of the vector (X1, ..., Xn), then the joint probability density function can be computed as a partial derivative\n\n\n=== Marginal densities ===\nFor i = 1, 2, ..., n, let fXi(xi) be the probability density function associated with variable Xi alone.  This is called the marginal density function, and can be deduced from the probability density associated with the random variables X1, ..., Xn by integrating over all values of the other n \u2212 1 variables:\n\n\n=== Independence ===\nContinuous random variables X1, ..., Xn admitting a joint density are all independent from each other if and only if\n\n\n=== Corollary ===\nIf the joint probability density function of a vector of n random variables can be factored into a product of n functions of one variable\n\n(where each fi is not necessarily a density) then the n variables in the set are all independent from each other, and the marginal probability density function of each of them is given by\n\n\n=== Example ===\nThis elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call \n  \n    \n      \n        \n          \n            \n              R\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {R}}}\n   a 2-dimensional random vector of coordinates (X, Y): the probability to obtain \n  \n    \n      \n        \n          \n            \n              R\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {R}}}\n   in the quarter plane of positive x and y is\n\n\n== Function of random variables and change of variables in the probability density function ==\nIf the probability density function of a random variable (or vector) X is given as fX(x), it is possible (but often not necessary; see below) to calculate the probability density function of some variable Y = g(X). This is also called a \u201cchange of variable\u201d and is in practice used to generate a random variable of arbitrary shape fg(X) = fY using a known (for instance, uniform) random number generator.\nIt is tempting to think that in order to find the expected value E(g(X)), one must first find the probability density fg(X) of the new random variable Y = g(X).  However, rather than computing\n\none may find instead\n\nThe values of the two integrals are the same in all cases in which both X and g(X) actually have probability density functions.  It is not necessary that g be a one-to-one function.  In some cases the latter integral is computed much more easily than the former. See Law of the unconscious statistician.\n\n\n=== Scalar to scalar ===\nLet \n  \n    \n      \n        g\n        :\n        \n          R\n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle g:\\mathbb {R} \\to \\mathbb {R} }\n   be a monotonic function, then the resulting density function is\n\nHere g\u22121 denotes the inverse function.\nThis follows from the fact that the probability contained in a differential area must be invariant under change of variables. That is,\n\nor\n\nFor functions that are not monotonic, the probability density function for y is\n\nwhere n(y) is the number of solutions in x for the equation \n  \n    \n      \n        g\n        (\n        x\n        )\n        =\n        y\n      \n    \n    {\\displaystyle g(x)=y}\n  , and \n  \n    \n      \n        \n          g\n          \n            k\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle g_{k}^{-1}(y)}\n   are these solutions.\n\n\n=== Vector to vector ===\nSuppose x is an n-dimensional random variable with joint density f. If y = H(x), where H is a bijective, differentiable function, then y has density g:\n\nwith the differential regarded as the Jacobian of the inverse of H(\u22c5), evaluated at y.For example, in the 2-dimensional case x = (x1, x2), suppose the transform H is given as y1 = H1(x1, x2), y2 = H2(x1, x2) with inverses x1 = H1\u22121(y1, y2), x2 = H2\u22121(y1, y2).  The joint distribution for y = (y1, y2) has density\n\n\n=== Vector to scalar ===\nLet \n  \n    \n      \n        V\n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle V:\\mathbb {R} ^{n}\\to \\mathbb {R} }\n   be a differentiable function and \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   be a random vector taking values in \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  , \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle f_{X}}\n   be the probability density function of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        \u03b4\n        (\n        \u22c5\n        )\n      \n    \n    {\\displaystyle \\delta (\\cdot )}\n    be the Dirac delta function. It is possible to use the formulas above to determine \n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle f_{Y}}\n  , the probability density function of \n  \n    \n      \n        Y\n        =\n        V\n        (\n        X\n        )\n      \n    \n    {\\displaystyle Y=V(X)}\n  , which will be given by\n\nThis result leads to the law of the unconscious statistician:\n\nProof:\nLet \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   be a collapsed random variable with probability density function \n  \n    \n      \n        \n          p\n          \n            Z\n          \n        \n        (\n        z\n        )\n        =\n        \u03b4\n        (\n        z\n        )\n      \n    \n    {\\displaystyle p_{Z}(z)=\\delta (z)}\n   (i.e., a constant equal to zero). Let the random vector \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n   and the transform \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   be defined as\n\nIt is clear that \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   is a bijective mapping, and the Jacobian of \n  \n    \n      \n        \n          H\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle H^{-1}}\n   is given by:\n\nwhich is an upper triangular matrix with ones on the main diagonal, therefore its determinant is 1. Applying the change of variable theorem from the previous section we obtain that\n\nwhich if marginalized over \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   leads to the desired probability density function.\n\n\n== Sums of independent random variables ==\n\nThe probability density function of the sum of two independent random variables U and V, each of which has a probability density function, is the convolution of their separate density functions:\n\nIt is possible to generalize the previous relation to a sum of N independent random variables, with densities U1, ..., UN:\n\nThis can be derived from a two-way change of variables involving Y = U + V and Z = V, similarly to the example below for the quotient of independent random variables.\n\n\n== Products and quotients of independent random variables ==\n\nGiven two independent random variables U and V, each of which has a probability density function, the density of the product Y = UV and quotient Y = U/V can be computed by a change of variables.\n\n\n=== Example: Quotient distribution ===\nTo compute the quotient Y = U/V of two independent random variables U and V, define the following transformation:\n\nThen, the joint density p(y,z) can be computed by a change of variables from U,V to Y,Z, and Y can be derived by marginalizing out Z from the joint density.\nThe inverse transformation is\n\nThe absolute value of the Jacobian matrix determinant \n  \n    \n      \n        J\n        (\n        U\n        ,\n        V\n        \u2223\n        Y\n        ,\n        Z\n        )\n      \n    \n    {\\displaystyle J(U,V\\mid Y,Z)}\n   of this transformation is:\n\nThus:\n\nAnd the distribution of Y can be computed by marginalizing out Z:\n\nThis method crucially requires that the transformation from U,V to Y,Z be bijective.  The above transformation meets this because Z can be mapped directly back to V, and for a given V the quotient U/V is monotonic.  This is similarly the case for the sum U + V, difference U \u2212 V and product UV.\nExactly the same method can be used to compute the distribution of other functions of multiple independent random variables.\n\n\n=== Example: Quotient of two standard normals ===\nGiven two standard normal variables U and V, the quotient can be computed as follows.  First, the variables have the following density functions:\n\nWe transform as described above:\n\nThis leads to:\n\nThis is the density of a standard Cauchy distribution.\n\n\n== See also ==\nDensity estimation\nKernel density estimation\nLikelihood function\nList of probability distributions\nProbability amplitude\nProbability mass function\nSecondary measure\nUses as position probability density:\nAtomic orbital\nHome range\n\n\n== References ==\n\n\n== Further reading ==\nBillingsley, Patrick (1979). Probability and Measure. New York, Toronto, London: John Wiley and Sons. ISBN 0-471-00710-2.\nCasella, George; Berger, Roger L. (2002). Statistical Inference (Second ed.). Thomson Learning. pp. 34\u201337. ISBN 0-534-24312-6.\nStirzaker, David (2003). Elementary Probability. ISBN 0-521-42028-8. Chapters 7 to 9 are about continuous variables.\n\n\n== External links ==\nUshakov, N.G. (2001) [1994], \"Density of a probability distribution\", Encyclopedia of Mathematics, EMS Press\nWeisstein, Eric W. \"Probability density function\". MathWorld.", "Principal component analysis": "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.\nThe principal components of a collection of points in a real coordinate space are a sequence of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   unit vectors, where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th vector is the direction of a line that best fits the data while being orthogonal to the first \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\nIn data analysis, the first principal component of a set of  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through  \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.\nPCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th principal component can be taken as a direction orthogonal to the first \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   principal components that maximizes the variance of the projected data.\nFor either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.\n\n\n== History ==\nPCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen\u2013Lo\u00e8ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 20th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart\u2013Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science (Lorenz, 1956), empirical eigenfunction decomposition (Sirovich, 1987), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.\n\n\n== Intuition ==\nPCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.\nTo find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually-orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform the covariance matrix into a diagonalized form, in which the diagonal elements represent the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.\nBiplots and scree plots (degree of explained variance) are used to explain findings of the PCA. \n\n\n== Details ==\nPCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.Consider an \n  \n    \n      \n        n\n        \u00d7\n        p\n      \n    \n    {\\displaystyle n\\times p}\n   data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).\nMathematically, the transformation is defined by a set of size \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   of p-dimensional vectors of weights or coefficients \n  \n    \n      \n        \n          \n            w\n          \n          \n            (\n            k\n            )\n          \n        \n        =\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            p\n          \n        \n        \n          )\n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}}\n   that map each row vector \n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            i\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{(i)}}\n   of X to a new vector of principal component scores \n  \n    \n      \n        \n          \n            t\n          \n          \n            (\n            i\n            )\n          \n        \n        =\n        (\n        \n          t\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          t\n          \n            l\n          \n        \n        \n          )\n          \n            (\n            i\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {t} _{(i)}=(t_{1},\\dots ,t_{l})_{(i)}}\n  , given by\n\n  \n    \n      \n        \n          \n            \n              t\n              \n                k\n              \n            \n          \n          \n            (\n            i\n            )\n          \n        \n        =\n        \n          \n            x\n          \n          \n            (\n            i\n            )\n          \n        \n        \u22c5\n        \n          \n            w\n          \n          \n            (\n            k\n            )\n          \n        \n        \n        \n          f\n          o\n          r\n        \n        \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        \n        k\n        =\n        1\n        ,\n        \u2026\n        ,\n        l\n      \n    \n    {\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,l}\n  in such a way that the individual variables \n  \n    \n      \n        \n          t\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          t\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle t_{1},\\dots ,t_{l}}\n   of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector (where \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   is usually selected to be strictly less than \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   to reduce dimensionality).\n\n\n=== First component ===\nIn order to maximize variance, the first weight vector w(1) thus has to satisfy\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        arg\n        \u2061\n        \n          max\n          \n            \u2016\n            \n              w\n            \n            \u2016\n            =\n            1\n          \n        \n        \n        \n          {\n          \n            \n              \u2211\n              \n                i\n              \n            \n            (\n            \n              t\n              \n                1\n              \n            \n            \n              )\n              \n                (\n                i\n                )\n              \n              \n                2\n              \n            \n          \n          }\n        \n        =\n        arg\n        \u2061\n        \n          max\n          \n            \u2016\n            \n              w\n            \n            \u2016\n            =\n            1\n          \n        \n        \n        \n          {\n          \n            \n              \u2211\n              \n                i\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                    \n                    \n                      (\n                      i\n                      )\n                    \n                  \n                  \u22c5\n                  \n                    w\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max _{\\Vert \\mathbf {w} \\Vert =1}\\,\\left\\{\\sum _{i}(t_{1})_{(i)}^{2}\\right\\}=\\arg \\max _{\\Vert \\mathbf {w} \\Vert =1}\\,\\left\\{\\sum _{i}\\left(\\mathbf {x} _{(i)}\\cdot \\mathbf {w} \\right)^{2}\\right\\}}\n  Equivalently, writing this in matrix form gives\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        arg\n        \u2061\n        \n          max\n          \n            \n              \u2016\n              \n                w\n              \n              \u2016\n            \n            =\n            1\n          \n        \n        \n          {\n          \n            \n              \u2016\n              \n                X\n                w\n              \n              \u2016\n            \n            \n              2\n            \n          \n          }\n        \n        =\n        arg\n        \u2061\n        \n          max\n          \n            \n              \u2016\n              \n                w\n              \n              \u2016\n            \n            =\n            1\n          \n        \n        \n          {\n          \n            \n              \n                w\n              \n              \n                \n                  T\n                \n              \n            \n            \n              \n                X\n              \n              \n                \n                  T\n                \n              \n            \n            \n              X\n              w\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\left\\|\\mathbf {Xw} \\right\\|^{2}\\right\\}=\\arg \\max _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\mathbf {w} ^{\\mathsf {T}}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {Xw} \\right\\}}\n  Since w(1) has been defined to be a unit vector, it equivalently also satisfies\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            (\n            1\n            )\n          \n        \n        =\n        arg\n        \u2061\n        max\n        \n          {\n          \n            \n              \n                \n                  \n                    w\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    X\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  X\n                  w\n                \n              \n              \n                \n                  \n                    w\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  w\n                \n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{(1)}=\\arg \\max \\left\\{{\\frac {\\mathbf {w} ^{\\mathsf {T}}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {Xw} }{\\mathbf {w} ^{\\mathsf {T}}\\mathbf {w} }}\\right\\}}\n  The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.\nWith w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) \u22c5 w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) \u22c5 w(1)} w(1).\n\n\n=== Further components ===\nThe k-th component can be found by subtracting the first k \u2212 1 principal components from X:\n\n  \n    \n      \n        \n          \n            \n              \n                X\n                ^\n              \n            \n          \n          \n            k\n          \n        \n        =\n        \n          X\n        \n        \u2212\n        \n          \u2211\n          \n            s\n            =\n            1\n          \n          \n            k\n            \u2212\n            1\n          \n        \n        \n          X\n        \n        \n          \n            w\n          \n          \n            (\n            s\n            )\n          \n        \n        \n          \n            w\n          \n          \n            (\n            s\n            )\n          \n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {X}} _{k}=\\mathbf {X} -\\sum _{s=1}^{k-1}\\mathbf {X} \\mathbf {w} _{(s)}\\mathbf {w} _{(s)}^{\\mathsf {T}}}\n  and then finding the weight vector which extracts the maximum variance from this new data matrix\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            (\n            k\n            )\n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n          \n          \n            \n              \u2016\n              \n                w\n              \n              \u2016\n            \n            =\n            1\n          \n        \n        \u2061\n        \n          {\n          \n            \n              \u2016\n              \n                \n                  \n                    \n                      \n                        X\n                        ^\n                      \n                    \n                  \n                  \n                    k\n                  \n                \n                \n                  w\n                \n              \n              \u2016\n            \n            \n              2\n            \n          \n          }\n        \n        =\n        arg\n        \u2061\n        max\n        \n          {\n          \n            \n              \n                \n                  \n                    \n                      w\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    \n                      \n                        \n                          X\n                          ^\n                        \n                      \n                    \n                    \n                      k\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    \n                      \n                        \n                          X\n                          ^\n                        \n                      \n                    \n                    \n                      k\n                    \n                  \n                  \n                    w\n                  \n                \n                \n                  \n                    \n                      w\n                    \n                    \n                      T\n                    \n                  \n                  \n                    w\n                  \n                \n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{(k)}=\\mathop {\\operatorname {arg\\,max} } _{\\left\\|\\mathbf {w} \\right\\|=1}\\left\\{\\left\\|\\mathbf {\\hat {X}} _{k}\\mathbf {w} \\right\\|^{2}\\right\\}=\\arg \\max \\left\\{{\\tfrac {\\mathbf {w} ^{\\mathsf {T}}\\mathbf {\\hat {X}} _{k}^{\\mathsf {T}}\\mathbf {\\hat {X}} _{k}\\mathbf {w} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}}\n  It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of XTX.\nThe k-th principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) \u22c5 w(k) in the transformed coordinates, or as the corresponding vector in the space of the original variables, {x(i) \u22c5 w(k)} w(k), where w(k) is the kth eigenvector of XTX.\nThe full principal components decomposition of X can therefore be given as\n\n  \n    \n      \n        \n          T\n        \n        =\n        \n          X\n        \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {T} =\\mathbf {X} \\mathbf {W} }\n  where W is a p-by-p matrix of weights whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation. Columns of W multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.\n\n\n=== Covariances ===\nXTX itself can be recognized as proportional to the empirical sample covariance matrix of the dataset XT.:\u200a30\u201331\u200aThe sample covariance Q between two of the different principal components over the dataset is given by:\n\n  \n    \n      \n        \n          \n            \n              \n                Q\n                (\n                \n                  \n                    P\n                    C\n                  \n                  \n                    (\n                    j\n                    )\n                  \n                \n                ,\n                \n                  \n                    P\n                    C\n                  \n                  \n                    (\n                    k\n                    )\n                  \n                \n                )\n              \n              \n                \n                \u221d\n                (\n                \n                  X\n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    j\n                    )\n                  \n                \n                \n                  )\n                  \n                    \n                      T\n                    \n                  \n                \n                (\n                \n                  X\n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    k\n                    )\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    w\n                  \n                  \n                    (\n                    j\n                    )\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    X\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  X\n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    k\n                    )\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    w\n                  \n                  \n                    (\n                    j\n                    )\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \u03bb\n                  \n                    (\n                    k\n                    )\n                  \n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    k\n                    )\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u03bb\n                  \n                    (\n                    k\n                    )\n                  \n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    j\n                    )\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    w\n                  \n                  \n                    (\n                    k\n                    )\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}Q(\\mathrm {PC} _{(j)},\\mathrm {PC} _{(k)})&\\propto (\\mathbf {X} \\mathbf {w} _{(j)})^{\\mathsf {T}}(\\mathbf {X} \\mathbf {w} _{(k)})\\\\&=\\mathbf {w} _{(j)}^{\\mathsf {T}}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {X} \\mathbf {w} _{(k)}\\\\&=\\mathbf {w} _{(j)}^{\\mathsf {T}}\\lambda _{(k)}\\mathbf {w} _{(k)}\\\\&=\\lambda _{(k)}\\mathbf {w} _{(j)}^{\\mathsf {T}}\\mathbf {w} _{(k)}\\end{aligned}}}\n  where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.\nAnother way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.\nIn matrix form, the empirical covariance matrix for the original variables can be written\n\n  \n    \n      \n        \n          Q\n        \n        \u221d\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        =\n        \n          W\n        \n        \n          \u039b\n        \n        \n          \n            W\n          \n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Q} \\propto \\mathbf {X} ^{\\mathsf {T}}\\mathbf {X} =\\mathbf {W} \\mathbf {\\Lambda } \\mathbf {W} ^{\\mathsf {T}}}\n  The empirical covariance matrix between the principal components becomes\n\n  \n    \n      \n        \n          \n            W\n          \n          \n            \n              T\n            \n          \n        \n        \n          Q\n        \n        \n          W\n        \n        \u221d\n        \n          \n            W\n          \n          \n            \n              T\n            \n          \n        \n        \n          W\n        \n        \n        \n          \u039b\n        \n        \n        \n          \n            W\n          \n          \n            \n              T\n            \n          \n        \n        \n          W\n        \n        =\n        \n          \u039b\n        \n      \n    \n    {\\displaystyle \\mathbf {W} ^{\\mathsf {T}}\\mathbf {Q} \\mathbf {W} \\propto \\mathbf {W} ^{\\mathsf {T}}\\mathbf {W} \\,\\mathbf {\\Lambda } \\,\\mathbf {W} ^{\\mathsf {T}}\\mathbf {W} =\\mathbf {\\Lambda } }\n  where \u039b is the diagonal matrix of eigenvalues \u03bb(k) of XTX. \u03bb(k) is equal to the sum of the squares over the dataset associated with each component k, that is, \u03bb(k) = \u03a3i tk2(i) = \u03a3i (x(i) \u22c5 w(k))2.\n\n\n=== Dimensionality reduction ===\nThe transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L eigenvectors, gives the truncated transformation\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            L\n          \n        \n        =\n        \n          X\n        \n        \n          \n            W\n          \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{L}=\\mathbf {X} \\mathbf {W} _{L}}\n  where the matrix TL now has n rows but only L columns. In other words, PCA learns a linear transformation \n  \n    \n      \n        t\n        =\n        \n          W\n          \n            L\n          \n          \n            \n              T\n            \n          \n        \n        x\n        ,\n        x\n        \u2208\n        \n          \n            R\n          \n          \n            p\n          \n        \n        ,\n        t\n        \u2208\n        \n          \n            R\n          \n          \n            L\n          \n        \n        ,\n      \n    \n    {\\displaystyle t=W_{L}^{\\mathsf {T}}x,x\\in \\mathbb {R} ^{p},t\\in \\mathbb {R} ^{L},}\n   where the columns of p \u00d7 L matrix \n  \n    \n      \n        \n          W\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle W_{L}}\n   form an orthogonal basis for the L features (the components of representation t) that are decorrelated. By construction, of all the transformed data matrices with only L columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error \n  \n    \n      \n        \u2016\n        \n          T\n        \n        \n          \n            W\n          \n          \n            T\n          \n        \n        \u2212\n        \n          \n            T\n          \n          \n            L\n          \n        \n        \n          \n            W\n          \n          \n            L\n          \n          \n            T\n          \n        \n        \n          \u2016\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|\\mathbf {T} \\mathbf {W} ^{T}-\\mathbf {T} _{L}\\mathbf {W} _{L}^{T}\\|_{2}^{2}}\n   or \n  \n    \n      \n        \u2016\n        \n          X\n        \n        \u2212\n        \n          \n            X\n          \n          \n            L\n          \n        \n        \n          \u2016\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|\\mathbf {X} -\\mathbf {X} _{L}\\|_{2}^{2}}\n  .\n\nSuch dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L = 2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.\nSimilarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.\nDimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less\u2014the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss. If the dataset is not too large, the significance of the principal components can be tested using parametric bootstrap, as an aid in determining how many principal components to retain.\n\n\n=== Singular value decomposition ===\n\nThe principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,\n\n  \n    \n      \n        \n          X\n        \n        =\n        \n          U\n        \n        \n          \u03a3\n        \n        \n          \n            W\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} =\\mathbf {U} \\mathbf {\\Sigma } \\mathbf {W} ^{T}}\n  Here \u03a3 is an n-by-p rectangular diagonal matrix of positive numbers \u03c3(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p matrix whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.\nIn terms of this factorization, the matrix XTX can be written\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    X\n                  \n                  \n                    T\n                  \n                \n                \n                  X\n                \n              \n              \n                \n                =\n                \n                  W\n                \n                \n                  \n                    \u03a3\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    U\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  U\n                \n                \n                  \u03a3\n                \n                \n                  \n                    W\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  W\n                \n                \n                  \n                    \u03a3\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \u03a3\n                \n                \n                  \n                    W\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  W\n                \n                \n                  \n                    \n                      \n                        \u03a3\n                        ^\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                \n                  \n                    W\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {X} ^{T}\\mathbf {X} &=\\mathbf {W} \\mathbf {\\Sigma } ^{\\mathsf {T}}\\mathbf {U} ^{\\mathsf {T}}\\mathbf {U} \\mathbf {\\Sigma } \\mathbf {W} ^{\\mathsf {T}}\\\\&=\\mathbf {W} \\mathbf {\\Sigma } ^{\\mathsf {T}}\\mathbf {\\Sigma } \\mathbf {W} ^{\\mathsf {T}}\\\\&=\\mathbf {W} \\mathbf {\\hat {\\Sigma }} ^{2}\\mathbf {W} ^{\\mathsf {T}}\\end{aligned}}}\n  where  \n  \n    \n      \n        \n          \n            \n              \u03a3\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {\\Sigma }} }\n   is the square diagonal matrix with the singular values of X and the excess zeros chopped off that satisfies \n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03a3\n                  ^\n                \n              \n            \n            \n              2\n            \n          \n        \n        =\n        \n          \n            \u03a3\n          \n          \n            \n              T\n            \n          \n        \n        \n          \u03a3\n        \n      \n    \n    {\\displaystyle \\mathbf {{\\hat {\\Sigma }}^{2}} =\\mathbf {\\Sigma } ^{\\mathsf {T}}\\mathbf {\\Sigma } }\n  . Comparison with the eigenvector factorization of XTX establishes that the right singular vectors W of X are equivalent to the eigenvectors of XTX, while the singular values \u03c3(k) of  \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   are equal to the square-root of the eigenvalues \u03bb(k) of XTX.\nUsing the singular value decomposition the score matrix T can be written\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  T\n                \n              \n              \n                \n                =\n                \n                  X\n                \n                \n                  W\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  U\n                \n                \n                  \u03a3\n                \n                \n                  \n                    W\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  W\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  U\n                \n                \n                  \u03a3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {T} &=\\mathbf {X} \\mathbf {W} \\\\&=\\mathbf {U} \\mathbf {\\Sigma } \\mathbf {W} ^{\\mathsf {T}}\\mathbf {W} \\\\&=\\mathbf {U} \\mathbf {\\Sigma } \\end{aligned}}}\n  so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.\nEfficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix, unless only a handful of components are required.\nAs with the eigen-decomposition, a truncated n \u00d7 L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            L\n          \n        \n        =\n        \n          \n            U\n          \n          \n            L\n          \n        \n        \n          \n            \u03a3\n          \n          \n            L\n          \n        \n        =\n        \n          X\n        \n        \n          \n            W\n          \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{L}=\\mathbf {U} _{L}\\mathbf {\\Sigma } _{L}=\\mathbf {X} \\mathbf {W} _{L}}\n  The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart\u2013Young theorem [1936].\n\n\n== Further considerations ==\nThe singular values (in \u03a3) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest \"variance\" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the \"DCT\". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.\nPCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are completely correlated, then the PCA will entail a rotation by 45\u00b0 and the \"weights\" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Pearson's original paper was entitled \"On Lines and Planes of Closest Fit to Systems of Points in Space\" \u2013 \"in space\" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.\nMean subtraction (a.k.a. \"mean centering\") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on \"Mean-centering in Moderated Regression: Much Ado About Nothing\". Since covariances are correlations of normalized variables (Z- or standard-scores) a PCA based on the correlation matrix of  X is equal to a PCA based on the covariance matrix of  Z, the standardized version of  X.\nPCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability. However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes. The linear discriminant analysis is an alternative which is optimized for class separability.\n\n\n== Table of symbols and abbreviations ==\n\n\n== Properties and limitations of PCA ==\n\n\n=== Properties ===\nSome properties of PCA include:\nProperty 1: For any integer q, 1 \u2264 q \u2264 p, consider the orthogonal linear transformation\n\n  \n    \n      \n        y\n        =\n        \n          \n            B\n            \u2032\n          \n        \n        x\n      \n    \n    {\\displaystyle y=\\mathbf {B'} x}\n  \nwhere \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is a q-element vector and \n  \n    \n      \n        \n          \n            B\n            \u2032\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B'} }\n   is a (q \u00d7 p) matrix, and let \n  \n    \n      \n        \n          \n            \u03a3\n          \n          \n            y\n          \n        \n        =\n        \n          \n            B\n            \u2032\n          \n        \n        \n          \u03a3\n        \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } _{y}=\\mathbf {B'} \\mathbf {\\Sigma } \\mathbf {B} }\n   be the variance-covariance matrix for \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . Then the trace of \n  \n    \n      \n        \n          \n            \u03a3\n          \n          \n            y\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } _{y}}\n  , denoted \n  \n    \n      \n        tr\n        \u2061\n        (\n        \n          \n            \u03a3\n          \n          \n            y\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {tr} (\\mathbf {\\Sigma } _{y})}\n  , is maximized by taking \n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            A\n          \n          \n            q\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} =\\mathbf {A} _{q}}\n  , where \n  \n    \n      \n        \n          \n            A\n          \n          \n            q\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} _{q}}\n   consists of the first q columns of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n   \n  \n    \n      \n        (\n        \n          \n            B\n            \u2032\n          \n        \n      \n    \n    {\\displaystyle (\\mathbf {B'} }\n   is the transpose of \n  \n    \n      \n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {B} )}\n  .Property 2: Consider again the orthonormal transformation\n\n  \n    \n      \n        y\n        =\n        \n          \n            B\n            \u2032\n          \n        \n        x\n      \n    \n    {\\displaystyle y=\\mathbf {B'} x}\n  \nwith \n  \n    \n      \n        x\n        ,\n        \n          B\n        \n        ,\n        \n          A\n        \n      \n    \n    {\\displaystyle x,\\mathbf {B} ,\\mathbf {A} }\n   and \n  \n    \n      \n        \n          \n            \u03a3\n          \n          \n            y\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } _{y}}\n   defined as before. Then \n  \n    \n      \n        tr\n        \u2061\n        (\n        \n          \n            \u03a3\n          \n          \n            y\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {tr} (\\mathbf {\\Sigma } _{y})}\n   is minimized by taking \n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            A\n          \n          \n            q\n          \n          \n            \u2217\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {B} =\\mathbf {A} _{q}^{*},}\n   where \n  \n    \n      \n        \n          \n            A\n          \n          \n            q\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} _{q}^{*}}\n   consists of the last q columns of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  .The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.\n\nProperty 3: (Spectral decomposition of \u03a3)\n\n  \n    \n      \n        \n          \u03a3\n        \n        =\n        \n          \u03bb\n          \n            1\n          \n        \n        \n          \u03b1\n          \n            1\n          \n        \n        \n          \u03b1\n          \n            1\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          \u03bb\n          \n            p\n          \n        \n        \n          \u03b1\n          \n            p\n          \n        \n        \n          \u03b1\n          \n            p\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } =\\lambda _{1}\\alpha _{1}\\alpha _{1}'+\\cdots +\\lambda _{p}\\alpha _{p}\\alpha _{p}'}\n  Before we look at its usage, we first look at diagonal elements,\n\n  \n    \n      \n        Var\n        \u2061\n        (\n        \n          x\n          \n            j\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            P\n          \n        \n        \n          \u03bb\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n            j\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {Var} (x_{j})=\\sum _{k=1}^{P}\\lambda _{k}\\alpha _{kj}^{2}}\n  Then, perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of x into decreasing contributions due to each PC, but we can also decompose the whole covariance matrix into contributions \n  \n    \n      \n        \n          \u03bb\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\lambda _{k}\\alpha _{k}\\alpha _{k}'}\n   from each PC. Although not strictly decreasing, the elements of \n  \n    \n      \n        \n          \u03bb\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\lambda _{k}\\alpha _{k}\\alpha _{k}'}\n   will tend to become smaller as \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   increases, as \n  \n    \n      \n        \n          \u03bb\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n        \n        \n          \u03b1\n          \n            k\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\lambda _{k}\\alpha _{k}\\alpha _{k}'}\n   is nonincreasing for increasing \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , whereas the elements of \n  \n    \n      \n        \n          \u03b1\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}}\n   tend to stay about the same size because of the normalization constraints: \n  \n    \n      \n        \n          \u03b1\n          \n            k\n          \n          \u2032\n        \n        \n          \u03b1\n          \n            k\n          \n        \n        =\n        1\n        ,\n        k\n        =\n        1\n        ,\n        \u2026\n        ,\n        p\n      \n    \n    {\\displaystyle \\alpha _{k}'\\alpha _{k}=1,k=1,\\dots ,p}\n  .\n\n\n=== Limitations ===\nAs noted above, the results of PCA depend on the scaling of the variables. This can be cured by scaling each feature by its standard deviation, so that one ends up with dimensionless features with unital variance.The applicability of PCA as described above is limited by certain (tacit) assumptions made in its derivation. In particular, PCA can capture linear correlations between the features but fails when this assumption is violated (see Figure 6a in the reference). In some cases, coordinate transformations can restore the linearity assumption and PCA can then be applied (see kernel PCA).\nAnother limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes, and forward modeling has to be performed to recover the true magnitude of the signals. As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations. See more at Relation between PCA and Non-negative Matrix Factorization.\nPCA is at a disadvantage if the data has not been standardized before applying the algorithm to it. PCA transforms original data into data that is relevant to the principal components of that data, which means that the new data variables cannot be interpreted in the same ways that the originals were. They are linear interpretations of the original variables. Also, if PCA is not performed properly, there is a high likelihood of information loss.PCA relies on a linear model. If a dataset has a pattern hidden inside it that is nonlinear, then PCA can actually steer the analysis in the complete opposite direction of progress. Researchers at Kansas State University discovered that the sampling error in their experiments impacted the bias of PCA results. \"If the number of subjects or blocks is smaller than 30, and/or the researcher is interested in PC's beyond the first, it may be better to first correct for the serial correlation, before PCA is conducted\". The researchers at Kansas State also found that PCA could be \"seriously biased if the autocorrelation structure of the data is not correctly handled\".\n\n\n=== PCA and information theory ===\nDimensionality reduction results in a loss of information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.\nUnder the assumption that\n\n  \n    \n      \n        \n          x\n        \n        =\n        \n          s\n        \n        +\n        \n          n\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {x} =\\mathbf {s} +\\mathbf {n} ,}\n  that is, that the data vector \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   is the sum of the desired information-bearing signal \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   and a noise signal \n  \n    \n      \n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {n} }\n   one can show that PCA can be optimal for dimensionality reduction, from an information-theoretic point-of-view.\nIn particular, Linsker showed that if \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   is Gaussian and \n  \n    \n      \n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {n} }\n   is Gaussian noise with a covariance matrix proportional to the identity matrix, the PCA maximizes the mutual information \n  \n    \n      \n        I\n        (\n        \n          y\n        \n        ;\n        \n          s\n        \n        )\n      \n    \n    {\\displaystyle I(\\mathbf {y} ;\\mathbf {s} )}\n   between the desired information \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   and the dimensionality-reduced output \n  \n    \n      \n        \n          y\n        \n        =\n        \n          \n            W\n          \n          \n            L\n          \n          \n            T\n          \n        \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {W} _{L}^{T}\\mathbf {x} }\n  .If the noise is still Gaussian and has a covariance matrix proportional to the identity matrix (that is, the components of the vector \n  \n    \n      \n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {n} }\n   are iid), but the information-bearing signal \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   is non-Gaussian (which is a common scenario), PCA at least minimizes an upper bound on the information loss, which is defined as\n\n  \n    \n      \n        I\n        (\n        \n          x\n        \n        ;\n        \n          s\n        \n        )\n        \u2212\n        I\n        (\n        \n          y\n        \n        ;\n        \n          s\n        \n        )\n        .\n      \n    \n    {\\displaystyle I(\\mathbf {x} ;\\mathbf {s} )-I(\\mathbf {y} ;\\mathbf {s} ).}\n  The optimality of PCA is also preserved if the noise \n  \n    \n      \n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {n} }\n   is iid and at least more Gaussian (in terms of the Kullback\u2013Leibler divergence) than the information-bearing signal \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n  . In general, even if the above signal model holds, PCA loses its information-theoretic optimality as soon as the noise \n  \n    \n      \n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {n} }\n   becomes dependent.\n\n\n== Computing PCA using the covariance method ==\nThe following is a detailed description of PCA using the covariance method (see also here) as opposed to the correlation method.The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen\u2013Lo\u00e8ve transform (KLT) of matrix X:\n\n  \n    \n      \n        \n          Y\n        \n        =\n        \n          K\n          L\n          T\n        \n        {\n        \n          X\n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {Y} =\\mathbb {KLT} \\{\\mathbf {X} \\}}\n  Organize the data setSuppose you have data comprising a set of observations of p variables, and you want to reduce the data so that each observation can be described with only L variables, L < p. Suppose further, that the data are arranged as a set of n data vectors \n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        \u2026\n        \n          \n            x\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{1}\\ldots \\mathbf {x} _{n}}\n   with each \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   representing a single grouped observation of the p variables.\n\nWrite \n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        \u2026\n        \n          \n            x\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{1}\\ldots \\mathbf {x} _{n}}\n   as row vectors, each with p elements.\nPlace the row vectors into a single matrix X of dimensions n \u00d7 p.Calculate the empirical meanFind the empirical mean along each column j = 1, ..., p.\nPlace the calculated mean values into an empirical mean vector u of dimensions p \u00d7 1.\n\n  \n    \n      \n        \n          u\n          \n            j\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          X\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle u_{j}={\\frac {1}{n}}\\sum _{i=1}^{n}X_{ij}}\n  Calculate the deviations from the meanMean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data. Hence we proceed by centering the data as follows:\n\nSubtract the empirical mean vector \n  \n    \n      \n        \n          \n            u\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {u} ^{T}}\n   from each row of the data matrix X.\nStore mean-subtracted data in the n \u00d7 p matrix B.\n\n  \n    \n      \n        \n          B\n        \n        =\n        \n          X\n        \n        \u2212\n        \n          h\n        \n        \n          \n            u\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} =\\mathbf {X} -\\mathbf {h} \\mathbf {u} ^{T}}\n  \nwhere h is an n \u00d7 1 column vector of all 1s:\n\n  \n    \n      \n        \n          h\n          \n            i\n          \n        \n        =\n        1\n        \n        \n        \n        \n          for \n        \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle h_{i}=1\\,\\qquad \\qquad {\\text{for }}i=1,\\ldots ,n}\n  In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score).  This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.\n\nFind the covariance matrixFind the p \u00d7 p empirical covariance matrix C from matrix B:  where \n  \n    \n      \n        \u2217\n      \n    \n    {\\displaystyle *}\n   is the conjugate transpose operator. If B consists entirely of real numbers, which is the case in many applications, the \"conjugate transpose\" is the same as the regular transpose.\nThe reasoning behind using n \u2212 1 instead of n to calculate the covariance is Bessel's correction.Find the eigenvectors and eigenvalues of the covariance matrixCompute the matrix V of eigenvectors which diagonalizes the covariance matrix C:  where D is the diagonal matrix of eigenvalues of C. This step will typically involve the use of a computer-based algorithm for computing eigenvectors and eigenvalues. These algorithms are readily available as sub-components of most matrix algebra systems, such as SAS, R, MATLAB, Mathematica, SciPy, IDL (Interactive Data Language), or GNU Octave as well as OpenCV.\nMatrix D will take the form of an p \u00d7 p diagonal matrix, where  is the jth eigenvalue of the covariance matrix C, and \nMatrix V, also of dimension p \u00d7 p, contains p column vectors, each of length p, which represent the p eigenvectors of the covariance matrix C.\nThe eigenvalues and eigenvectors are ordered and paired. The jth eigenvalue corresponds to the jth eigenvector.\nMatrix V denotes the matrix of right eigenvectors (as opposed to left eigenvectors). In general, the matrix of right eigenvectors need not be the (conjugate) transpose of the matrix of left eigenvectors.Rearrange the eigenvectors and eigenvaluesSort the columns of the eigenvector matrix V and eigenvalue matrix D in order of decreasing eigenvalue.\nMake sure to maintain the correct pairings between the columns in each matrix.Compute the cumulative energy content for each eigenvectorThe eigenvalues represent the distribution of the source data's energy among each of the eigenvectors, where the eigenvectors form a basis for the data. The cumulative energy content g for the jth eigenvector is the sum of the energy content across all of the eigenvalues from 1 through j:\n\n  \n    \n      \n        \n          g\n          \n            j\n          \n        \n        =\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            j\n          \n        \n        \n          D\n          \n            k\n            k\n          \n        \n        \n        \n          for \n        \n        j\n        =\n        1\n        ,\n        \u2026\n        ,\n        p\n      \n    \n    {\\displaystyle g_{j}=\\sum _{k=1}^{j}D_{kk}\\qquad {\\text{for }}j=1,\\dots ,p}\n  Select a subset of the eigenvectors as basis vectorsSave the first L columns of V as the p \u00d7 L matrix W:  where \nUse the vector g as a guide in choosing an appropriate value for L. The goal is to choose a value of L as small as possible while achieving a reasonably high value of g on a percentage basis. For example, you may want to choose L so that the cumulative energy g is above a certain threshold, like 90 percent. In this case, choose the smallest value of L such that Project the data onto the new basisThe projected data points are the rows of the matrix That is, the first column of \n  \n    \n      \n        \n          T\n        \n      \n    \n    {\\displaystyle \\mathbf {T} }\n   is the projection of the data points onto the first principal component, the second column is the projection onto the second principal component, etc.\n\n\n== Derivation of PCA using the covariance method ==\nLet X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.\nWe want to find \n  \n    \n      \n        (\n        \u2217\n        )\n      \n    \n    {\\displaystyle (\\ast )}\n   a d \u00d7 d orthonormal transformation matrix P so that PX has a diagonal covariance matrix (that is, PX is a random vector with all its distinct components pairwise uncorrelated).\nA quick computation assuming \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   were unitary yields:\n\n  \n    \n      \n        \n          \n            \n              \n                cov\n                \u2061\n                (\n                P\n                X\n                )\n              \n              \n                \n                =\n                E\n                \u2061\n                [\n                P\n                X\n                 \n                (\n                P\n                X\n                \n                  )\n                  \n                    \u2217\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                E\n                \u2061\n                [\n                P\n                X\n                 \n                \n                  X\n                  \n                    \u2217\n                  \n                \n                \n                  P\n                  \n                    \u2217\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                P\n                E\n                \u2061\n                [\n                X\n                \n                  X\n                  \n                    \u2217\n                  \n                \n                ]\n                \n                  P\n                  \n                    \u2217\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                P\n                cov\n                \u2061\n                (\n                X\n                )\n                \n                  P\n                  \n                    \u2212\n                    1\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {cov} (PX)&=\\operatorname {E} [PX~(PX)^{*}]\\\\&=\\operatorname {E} [PX~X^{*}P^{*}]\\\\&=P\\operatorname {E} [XX^{*}]P^{*}\\\\&=P\\operatorname {cov} (X)P^{-1}\\\\\\end{aligned}}}\n  Hence \n  \n    \n      \n        (\n        \u2217\n        )\n      \n    \n    {\\displaystyle (\\ast )}\n   holds if and only if \n  \n    \n      \n        cov\n        \u2061\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {cov} (X)}\n   were diagonalisable by \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  .\nThis is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.\n\n\n== Covariance-free computation ==\nIn practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.\n\n\n=== Iterative computation ===\nOne way to compute the first principal component efficiently is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.\n\nr = a random vector of length p\nr = r / norm(r)\ndo c times:\n      s = 0 (a vector of length p)\n      for each row x in X\n            s = s + (x \u22c5 r) x\n      \u03bb = rTs // \u03bb is the eigenvalue\n      error = |\u03bb \u22c5 r \u2212 s|\n      r = s / norm(s)\n      exit if error < tolerance\nreturn \u03bb, r\n\nThis power iteration algorithm simply calculates the vector XT(X r), normalizes, and places the result back in r. The eigenvalue is approximated by rT (XTX) r, which is the Rayleigh quotient on the unit vector r for the covariance matrix XTX . If the largest singular value is well separated from the next largest one, the vector r gets close to the first principal component of X within the number of iterations c, which is small relative to p, at the total cost 2cnp. The power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix-free methods, such as the Lanczos algorithm or the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.\nSubsequent principal components can be computed one-by-one via deflation or simultaneously as a block. In the former approach, imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components, thus increasing the error with every new computation. The latter approach in the block power method replaces single-vectors r and s with block-vectors, matrices R and S. Every column of R approximates one of the leading principal components, while all columns are iterated simultaneously. The main calculation is evaluation of the product XT(X R). Implemented, for example, in LOBPCG, efficient blocking eliminates the accumulation of the errors, allows using high-level BLAS matrix-matrix product functions, and typically leads to faster convergence, compared to the single-vector one-by-one technique.\n\n\n=== The NIPALS method ===\nNon-linear iterative partial least squares (NIPALS) is a variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (for example, genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm updates iterative approximations to the leading scores and loadings t1 and r1T by the power iteration multiplying on every iteration by X on the left and on the right, that is, calculation of the covariance matrix is avoided, just as in the matrix-free implementation of the power iterations to XTX, based on the function evaluating the product XT(X r) = ((X r)TX)T.\nThe matrix deflation by subtraction is performed by subtracting the outer product, t1r1T from X leaving the deflated residual matrix used to calculate the subsequent leading PCs.\nFor large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality of PCs due to machine precision round-off errors accumulated in each iteration and matrix deflation by subtraction. A Gram\u2013Schmidt re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality. NIPALS reliance on single-vector multiplications cannot take advantage of high-level BLAS and results in slow convergence for clustered leading singular values\u2014both these deficiencies are resolved in more sophisticated matrix-free block solvers, such as the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.\n\n\n=== Online/sequential estimation ===\nIn an \"online\" or \"streaming\" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.\n\n\n== PCA and qualitative variables ==\nIn PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species.\nFor this, the following results are produced.\n\nIdentification, on the factorial planes, of the different species, for example, using different colors.\nRepresentation, on the factorial planes, of the centers of gravity of plants belonging to the same species.\nFor each center of gravity and each axis, p-value to judge the significance of the difference between the center of gravity and origin.These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, L\u00ea & Pag\u00e8s 2009 and Pag\u00e8s 2013.\nFew software offer this option in an \"automatic\" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.\n\n\n== Applications ==\n\n\n=== Intelligence ===\nThe earliest application of factor analysis was in locating and measuring components of human intelligence. it was believed that intelligence had various uncorrelated components such as spatial intelligence, verbal intelligence, induction, deduction etc and that scores on these could be adduced by factor analysis from results on various tests, to give a single index known as the Intelligence Quotient (IQ). The pioneering statistical psychologist Spearman actually developed factor analysis in 1904 for his two-factor theory of intelligence, adding a formal technique to the science of psychometrics. In 1924 Thurstone looked for 56 factors of intelligence, developing the notion of Mental Age. Standard IQ tests today are based on this early work.\n\n\n=== Residential differentiation ===\nIn 1949, Shevky and Williams introduced the theory of factorial ecology, which dominated studies of residential differentiation  from the 1950s to the 1970s. Neighbourhoods in a city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis. These were known as 'social rank' (an index of occupational status), 'familism' or family size, and 'ethnicity'; Cluster analysis could then be applied to divide the city into  clusters or precincts according to values of the three key factor variables. An extensive literature developed around factorial ecology in urban geography, but the approach went out of fashion after 1980 as being methodologically primitive and having little place in postmodern geographical paradigms.\nOne of the problems with factor analysis has always been finding convincing names for the various artificial factors. In 2000, Flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly, without resorting to factor rotation. The principal components were actually dual variables or shadow prices of 'forces' pushing people together or apart in cities. The first component was 'accessibility', the classic trade-off between demand for travel and demand for space, around which classical urban economics is based. The next two components were 'disadvantage', which keeps people of similar status in separate neighbourhoods (mediated by planning), and ethnicity, where people of similar ethnic backgrounds try to co-locate.About the same time, the Australian Bureau of Statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were  thought to be important. These SEIFA indexes are regularly published for various jurisdictions, and are used frequently in spatial analysis.\n\n\n=== Development indexes ===\nPCA has been the only formal method available for the development of indexes, which are otherwise a hit-or-miss ad hoc undertaking.\nThe City Development Index was developed by PCA from about 200 indicators of city outcomes in a 1996 survey of 254 global cities. The first principal component was subject to iterative regression, adding the original variables singly until about 90% of its variation was accounted for. The index ultimately used about 15 indicators but was a good predictor of many more variables. Its comparative value agreed very well with a subjective assessment of the condition of each city. The coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services, suggesting the Index was actually a measure of effective physical and social investment in the city.\nThe country-level Human Development Index (HDI) from UNDP, which has been published since 1990 and is very extensively used in development studies, has very similar coefficients on similar indicators, strongly suggesting it was originally constructed using PCA.\n\n\n=== Population genetics ===\nIn 1978 Cavalli-Sforza and others pioneered the use of principal components analysis (PCA) to summarise data on variation in human gene frequencies across regions. The components showed distinctive patterns, including gradients and sinusoidal waves. They interpreted these patterns as resulting from specific ancient migration events.\nSince then, PCA has been ubiquitous in population genetics, with thousands of papers using PCA as a display mechanism. Genetics varies largely according to proximity, so the first two principal components actually show spatial distribution and may be used to map the  relative geographical location of different population groups, thereby showing individuals who have wandered from their original locations.PCA in genetics has been technically controversial, in that the technique has been performed on discrete non-normal variables and often on binary allele markers. The lack of any measures of standard error in PCA are also an impediment to more consistent usage. In August 2022, the molecular biologist Eran Elhaik published a theoretical paper in Scientific Reports analyzing 12 PCA applications. He concluded that it was easy to manipulate the method, which, in his view, generated results that were 'erroneous, contradictory, and absurd.' Specifically, he argued, the results achieved in population genetics were characterized by  cherry-picking and circular reasoning.\n\n\n=== Market research and indexes of attitude ===\nMarket research has been an extensive user of PCA. It is used to develop customer satisfaction or customer loyalty scores for products, and with clustering, to develop market segments that may be targeted with advertising campaigns, in much the same way as factorial ecology will locate geographical areas with similar characteristics.PCA rapidly transforms large amounts of data into smaller, easier-to-digest variables that can be more rapidly and readily analyzed. In any consumer questionnaire, there are series of questions designed to elicit consumer attitudes, and principal components seek out latent variables underlying these attitudes. For example, the Oxford Internet Survey in 2013 asked 2000 people about their attitudes and beliefs, and from these analysts extracted four principal component dimensions, which they identified as 'escape', 'social networking', 'efficiency', and 'problem creating'.Another example from Joe Flood in 2008 extracted an attitudinal index toward housing from 28 attitude questions in a national survey of 2697 households in Australia. The first principal component represented a general attitude toward property and home ownership. The index, or the attitude questions it embodied, could be fed into a General Linear Model of  tenure choice. The strongest determinant of private renting by far was the attitude index, rather than income, marital status or household type.\n\n\n=== Quantitative finance ===\n\nPCA has also been applied to equity portfolios, both to portfolio risk and to risk return. One application is to reduce portfolio risk, where allocation strategies are applied to the \"principal portfolios\" instead of the underlying stocks. A second is to enhance portfolio return, using the principal components to select stocks with upside potential.\n\n\n=== Neuroscience ===\nA variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increases a neuron's probability of generating an action potential. This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.\nIn neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.\nPCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, that is, order parameters, during phase transitions in the brain.\n\n\n== Relation with other methods ==\n\n\n=== Correspondence analysis ===\nCorrespondence analysis (CA)\nwas developed by Jean-Paul Benz\u00e9cri\nand is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables.\nCA decomposes the chi-squared statistic associated to this table into orthogonal factors.\nBecause CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not.\nSeveral variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.\n\n\n=== Factor analysis ===\n\nPrincipal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (that is, translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.\nFactor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors \"represent the common variance of variables, excluding unique variance\". In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (that is, shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.:\u200a158\u200a Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (that is, latent constructs or factors) or causal modeling. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results.\n\n\n=== K-means clustering ===\nIt has been asserted that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result, and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\n\n\n=== Non-negative matrix factorization ===\n Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy, in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.\nIn PCA, the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue, which is equivalent to the fractional residual variance (FRV) in analyzing empirical data. For NMF, its components are ranked based only on the empirical FRV curves. The residual fractional eigenvalue plots, that is, \n  \n    \n      \n        1\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            k\n          \n        \n        \n          \u03bb\n          \n            i\n          \n        \n        \n          \n            /\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u03bb\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle 1-\\sum _{i=1}^{k}\\lambda _{i}{\\Big /}\\sum _{j=1}^{n}\\lambda _{j}}\n   as a function of component number \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   given a total of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   components, for PCA has a flat plateau, where no data is captured to remove the quasi-static noise, then the curves dropped quickly as an indication of over-fitting and captures random noise. The FRV curves for NMF is decreasing continuously when the NMF components are constructed sequentially, indicating the continuous capturing of quasi-static noise; then converge to higher levels than PCA, indicating the less over-fitting property of NMF.\n\n\n=== Iconography of correlations ===\nIt is often difficult to interpret the principal components when the data include many variables of various origins, or when some variables are qualitative. This leads the PCA user to a delicate elimination of several variables. If observations or variables have an excessive impact on the direction of the axes, they should be removed and then projected as supplementary elements. In addition, it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane.\n\nThe iconography of correlations, on the contrary, which is not a projection on a system of axes, does not have these drawbacks. We can therefore keep all the variables.\nThe principle of the diagram is to underline the \"remarkable\" correlations of the correlation matrix, by a solid line (positive correlation) or dotted line (negative correlation).\nA strong correlation is not \"remarkable\" if it is not direct, but caused by the effect of a third variable. Conversely, weak correlations can be \"remarkable\". For example, if a variable Y depends on several independent variables, the correlations of Y with each of them are weak and yet \"remarkable\".\n\n\n== Generalizations ==\n\n\n=== Sparse PCA ===\n\nA particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.\nSeveral approaches have been proposed, including\n\na regression framework,\na convex relaxation/semidefinite programming framework,\na generalized power method framework\nan alternating maximization framework\nforward-backward greedy search and exact methods using branch-and-bound techniques,\nBayesian formulation framework.The methodological and theoretical developments of Sparse PCA as well as its applications in scientific studies were recently reviewed in a survey paper.\n\n\n=== Nonlinear PCA ===\n\nMost of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be \"the best fit\" to a set of data points. Trevor Hastie expanded on this concept by proposing Principal curves as the natural extension for the geometric interpretation of PCA, which explicitly constructs a manifold for data approximation followed by projecting the points onto it, as is illustrated by Fig.\nSee also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.\nIn multilinear subspace learning, PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.\nN-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.\n\n\n=== Robust PCA ===\nWhile PCA finds the mathematically optimal method (as in minimizing the squared error), it is still sensitive to outliers in the data that produce large errors, something that the method tries to avoid in the first place. It is therefore common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand.\nA recently proposed generalization of PCA based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.\nOutlier-resistant variants of PCA have also been proposed, based on L1-norm formulations (L1-PCA).Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of PCA that works well with respect to grossly corrupted observations.\n\n\n== Similar techniques ==\n\n\n=== Independent component analysis ===\nIndependent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.\n\n\n=== Network component analysis ===\nGiven a matrix \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  , it tries to decompose it into two matrices such that \n  \n    \n      \n        E\n        =\n        A\n        P\n      \n    \n    {\\displaystyle E=AP}\n  . A key difference from techniques such as PCA and ICA is that some of the entries of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   are constrained to be 0. Here \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   is termed the regulatory layer. While in general such a decomposition can have multiple solutions, they prove that if the following conditions are satisfied :\n\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   has full column rank\nEach column of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   must have at least \n  \n    \n      \n        L\n        \u2212\n        1\n      \n    \n    {\\displaystyle L-1}\n   zeroes where \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   is the number of columns of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   (or alternatively the number of rows of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  ). The justification for this criterion is that if a node is removed from the regulatory layer along with all the output nodes connected to it, the result must still be characterized by a connectivity matrix with full column rank.\n\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   must have full row rank.then the decomposition is unique up to multiplication by a scalar.\n\n\n=== Discriminant analysis of principal components ===\nDiscriminant analysis of principal components (DAPC) is a multivariate method used to identify and describe clusters of genetically related individuals. Genetic variation is partitioned into two components: variation between groups and within groups, and it maximizes the former. Linear discriminants are linear combinations of alleles which best separate the clusters. Alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups. The contributions of alleles to the groupings identified by DAPC can allow identifying regions of the genome driving the genetic divergence among groups\nIn DAPC, data is first transformed using a principal components analysis (PCA) and subsequently clusters are identified using discriminant analysis (DA).\nA DAPC can be realized on R using the package Adegenet. (more info: adegenet on the web)\n\n\n=== Directional component analysis ===\nDirectional component analysis (DCA) is a method used in the atmospheric sciences for analysing multivariate datasets.\nLike PCA, it allows for dimension reduction, improved visualization and improved interpretability of large data-sets.\nAlso like PCA, it is based on a covariance matrix derived from the input dataset.\nThe difference between PCA and DCA is that DCA additionally requires the input of a vector direction, referred to as the impact. \nWhereas PCA maximises explained variance, DCA maximises probability density given impact.\nThe motivation for DCA is to find components of a multivariate dataset that are both likely (measured using probability density) and important (measured using the impact).\nDCA has been used to find the most likely and most serious heat-wave patterns in weather prediction ensembles\n, and the most likely and most impactful changes in rainfall due to climate change\n.\n\n\n== Software/source code ==\nALGLIB - a C++ and C# library that implements PCA and truncated PCA\nAnalytica \u2013 The built-in EigenDecomp function computes principal components.\nELKI \u2013 includes PCA for projection, including robust variants of PCA, as well as PCA-based clustering algorithms.\nGretl \u2013 principal component analysis can be performed either via the pca command or via the princomp() function.\nJulia \u2013 Supports PCA with the pca function in the MultivariateStats package\nKNIME \u2013 A java based nodal arranging software for Analysis, in this the nodes called PCA, PCA compute, PCA Apply, PCA inverse make it easily.\nMaple (software) \u2013 The PCA command is used to perform a principal component analysis on a set of data.\nMathematica \u2013 Implements principal component analysis with the PrincipalComponents command using both covariance and correlation methods.\nMathPHP \u2013 PHP mathematics library with support for PCA.\nMATLAB - The SVD function is part of the basic system.  In the Statistics Toolbox, the functions princomp and pca (R2012b) give the principal components, while the function pcares gives the residuals and reconstructed matrix for a low-rank PCA approximation.\nMatplotlib \u2013 Python library have a PCA package in the .mlab module.\nmlpack \u2013 Provides an implementation of principal component analysis in C++.\nmrmath - A high performance math library for Delphi and FreePascal can perform PCA; including robust variants.\nNAG Library \u2013 Principal components analysis is implemented via the g03aa routine (available in both the Fortran versions of the Library).\nNMath \u2013 Proprietary numerical library containing PCA for the .NET Framework.\nGNU Octave \u2013 Free software computational environment mostly compatible with MATLAB, the function princomp gives the principal component.\nOpenCV\nOracle Database 12c \u2013 Implemented via DBMS_DATA_MINING.SVDS_SCORING_MODE by specifying setting value SVDS_SCORING_PCA\nOrange (software) \u2013 Integrates PCA in its visual programming environment. PCA displays a scree plot (degree of explained variance) where user can interactively select the number of principal components.\nOrigin \u2013 Contains PCA in its Pro version.\nQlucore \u2013 Commercial software for analyzing multivariate data with instant response using PCA.\nR \u2013 Free statistical package, the functions princomp and prcomp can be used for principal component analysis; prcomp uses singular value decomposition which generally gives better numerical accuracy. Some packages that implement PCA in R, include, but are not limited to: ade4, vegan, ExPosition, dimRed, and FactoMineR.\nSAS \u2013 Proprietary software; for example, see\nscikit-learn \u2013 Python library for machine learning which contains PCA, Probabilistic PCA, Kernel PCA, Sparse PCA and other techniques in the decomposition module.\nScilab \u2013  Free and open-source, cross-platform numerical computational package, the function princomp computes principal component analysis, the function pca computes principal component analysis with standardized variables.\nSPSS \u2013 Proprietary software most commonly used by social scientists for PCA, factor analysis and associated cluster analysis.\nWeka \u2013 Java library for machine learning which contains modules for computing principal components.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nJackson, J.E. (1991). A User's Guide to Principal Components (Wiley).\nJolliffe, I. T. (1986). Principal Component Analysis. Springer Series in Statistics. Springer-Verlag. pp. 487. CiteSeerX 10.1.1.149.8828. doi:10.1007/b98835. ISBN 978-0-387-95442-4.\nJolliffe, I. T. (2002). Principal Component Analysis. Springer Series in Statistics. New York: Springer-Verlag. doi:10.1007/b98835. ISBN 978-0-387-95442-4.\nHusson Fran\u00e7ois, L\u00ea S\u00e9bastien & Pag\u00e8s J\u00e9r\u00f4me (2009). Exploratory Multivariate Analysis by Example Using R. Chapman & Hall/CRC The R Series, London. 224p. ISBN 978-2-7535-0938-2\nPag\u00e8s J\u00e9r\u00f4me (2014). Multiple Factor Analysis by Example Using R. Chapman & Hall/CRC The R Series London 272 p\n\n\n== External links ==\n\nUniversity of Copenhagen video by Rasmus Bro on YouTube\nStanford University video by Andrew Ng on YouTube\nA Tutorial on Principal Component Analysis\nA layman's introduction to principal component analysis on YouTube (a video of less than 100 seconds.)\nStatQuest: StatQuest: Principal Component Analysis (PCA), Step-by-Step on YouTube\nSee also the list of Software implementations", "Web search engine": "A web search engine is a software system designed to carry out web searches.  They search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories and social bookmarking sites, which are maintained by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Any internet-based content that cannot be indexed and searched by a web search engine falls under the category of deep web.\n\n\n== History ==\n\n\n=== Pre-1990s ===\nA system for locating published information intended to overcome the ever increasing difficulty of locating information in ever-growing centralized indices of scientific work was described in 1945 by Vannevar Bush, who wrote an article in The Atlantic Monthly titled \"As We May Think\" in which he envisioned libraries of research with connected annotations not unlike modern hyperlinks. Link analysis would eventually become a crucial component of search engines through algorithms such as Hyper Search and PageRank.\n\n\n=== 1990s: Birth of search engines ===\nThe first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982, and the Knowbot Information Service multi-network user search was first implemented in 1989. The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains, but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".The first tool used for searching content (as opposed to users) on the Internet was Archie. The name stands for \"archive\" without the \"v\". It was created by Alan Emtage, computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\nThe rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor.\nIn the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.\nJumpStation (created in December 1993 by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.\nOne of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor.\nThe first popular search engine on the Web was Yahoo! Search. The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory. It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages.\nSoon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search.\nIn 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking and received a US patent for the technology. It was the first search engine that used hyperlinks to measure the quality of websites it was indexing, predating the very similar algorithm patent filed by Google two years later in 1998. Larry Page referenced Li's work in some of his U.S. patents for PageRank. Li later used his Rankdex technology for the Baidu search engine, which was founded by him in China and launched in 2000.\nIn 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.Google adopted the idea of selling search terms in 1998, from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s. Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000.\n\n\n=== 2000s\u2013present: Post dot-com bubble ===\nAround 2000, Google's search engine rose to prominence. The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google. This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker.\nBy 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.\nMicrosoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999 the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot).\nMicrosoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology.\nAs of 2019, active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex.\n\n\n== Approach ==\n\nA search engine maintains the following processes in near real time:\n\nWeb crawling\nIndexing\nSearchingWeb search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are made in a public database, made available for web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible. Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis.\nBetween visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed. The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot.\n\nTypically when a user enters a query into a search engine it is a few keywords. The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes. Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing.\nBeyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results.\nFor example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range. It's also possible to weight by date because each page has a modification time. Most search engines support the use of the boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords. There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for.\nThe usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another. The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.\nMost Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.\n\n\n=== Local search ===\nLocal search is the process that optimizes the efforts of local businesses. They focus on change to make sure all searches are consistent. It's important because many people determine where they plan to go and what to buy based on their searches.\n\n\n== Market share ==\nAs of January 2022, Google is by far the world's most used search engine, with a market share of 90.6%, and the world's other most used search engines were Bing, Yahoo!, Baidu, Yandex, and DuckDuckGo.\n\n\n=== Russia and East Asia ===\nIn Russia, Yandex has a market share of 62.6%, compared to Google's 28.3%. And Yandex is the second most used search engine on smartphones in Asia and Europe. In China, Baidu is the most popular search engine. South Korea's homegrown search portal, Naver, is used for 62.8% of online searches in the country. Yahoo! Japan and Yahoo! Taiwan are the most popular avenues for Internet searches in Japan and Taiwan, respectively. China is one of few countries where Google is not in the top three web search engines for market share. Google was previously a top search engine in China, but withdrew after a disagreement with the government over censorship, and a cyberattack. But Bing is in top three web search engine with a market share of 14.95%. Baidu is on top with 49.1\u2105 market share.\n\n\n=== Europe ===\nMost countries' markets in the European Union are dominated by Google, except for the Czech Republic, where Seznam is a strong competitor.The search engine Qwant is based in Paris, France, where it attracts most of its 50 million monthly registered users from.\n\n\n== Search engine bias ==\nAlthough search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide and the underlying assumptions about the technology. These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws). For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal.\nBiases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results. Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons.\nSeveral scholars have studied the cultural changes triggered by search engines, and the representation of certain controversial topics in their results, such as terrorism in Ireland, climate change denial, and conspiracy theories.\n\n\n== Customized results and filter bubbles ==\nThere has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011. The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble. On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalisation in search, that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.\n\n\n== Religious search engines ==\nThe global growth of the Internet and electronic media in the Arab and Muslim World during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of the \"Law of Islam\". ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).While lack of investment and slow pace in technologies in the Muslim World has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim, a Muslim lifestyle site, did receive millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google, and SeekFind.org, which is Christian. SeekFind filters sites that attack or degrade their faith.\n\n\n== Search engine submission ==\nWeb search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign.\nSome search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.\n\n\n== Comparison to social bookmarking ==\n\nIn comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders. Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this.\n\n\n== Technology ==\n\n\n=== Archie ===\nThe first web search engines was Archie, created in 1990 by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives,\" but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.\nThe primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\nInitially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them.\nEven with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\nArchie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.\n\n\n=== Veronica ===\nIn 1993, the University of Nevada System Computing Services group developed Veronica. It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.\n\n\n=== The Lone Wanderer ===\nThe World Wide Web Wanderer, developed by Matthew Gray in 1993 was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\nMatthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\nIn response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\nALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.\n\n\n=== Excite ===\nExcite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.Excite was the first serious commercial search engine which launched in 1995. It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\nSome of the first analysis of web searching was conducted on search logs from Excite\n\n\n=== Yahoo! ===\nIn April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\nAs the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\nThe Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\n\n\n=== Lycos ===\nAt Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine.\n\n\n=== Types of web search engines ===\nSearch engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.\nFinding and selecting full or partial content based on the keywords provided.\nMaintaining index of the content and referencing to the location they find\nAllowing users to look for words or combinations of words found in that index.The process begins when a user enters a query statement into the system through the interface provided.\n\nThere are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\nCrawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\nHuman-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\nIn both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created \u2014you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\nSo why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\nOne of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\nAnother common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\nModern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\nAnother category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nSteve Lawrence; C. Lee Giles (1999). \"Accessibility of information on the web\". Nature. 400 (6740): 107\u20139. Bibcode:1999Natur.400..107L. doi:10.1038/21987. PMID 10428673. S2CID 4347646.\nBing Liu (2007), Web Data Mining: Exploring Hyperlinks, Contents and Usage Data. Springer,ISBN 3-540-37881-2\nBar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231\u2013288.\nLevene, Mark (2005). An Introduction to Search Engines and Web Navigation. Pearson.\nHock, Randolph (2007). The Extreme Searcher's Handbook.ISBN 978-0-910965-76-7\nJaved Mostafa (February 2005). \"Seeking Better Web Searches\". Scientific American. 292 (2): 66\u201373. Bibcode:2005SciAm.292b..66M. doi:10.1038/scientificamerican0205-66.\nRoss, Nancy; Wolfram, Dietmar (2000). \"End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine\". Journal of the American Society for Information Science. 51 (10): 949\u2013958. doi:10.1002/1097-4571(2000)51:10<949::AID-ASI70>3.0.CO;2-5.\nXie, M.;  et al. (1998). \"Quality dimensions of Internet search engines\". Journal of Information Science. 24 (5): 365\u2013372. doi:10.1177/016555159802400509. S2CID 34686531.\nInformation Retrieval: Implementing and Evaluating Search Engines. MIT Press. 2010. Archived from the original on 2020-10-05. Retrieved 2010-08-07.\n\n\n== External links ==\n\nSearch Engines at Curlie", "Statistical significance": "In statistical hypothesis testing, a result has statistical significance when a result at least as \"extreme\" would be very infrequent if the null hypothesis were true. More precisely, a study's defined significance level, denoted by \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  , is the probability of the study rejecting the null hypothesis, given that the null hypothesis is true; and the p-value of a result, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , is the probability of obtaining a result at least as extreme, given that the null hypothesis is true. The result is statistically significant, by the standards of the study, when \n  \n    \n      \n        p\n        \u2264\n        \u03b1\n      \n    \n    {\\displaystyle p\\leq \\alpha }\n  . The significance level for a study is chosen before data collection, and is typically set to 5% or much lower\u2014depending on the field of study.In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the p-value of an observed effect is less than (or equal to) the significance level, an investigator may conclude that the effect reflects the characteristics of the whole population, thereby rejecting the null hypothesis.This technique for testing the statistical significance of results was developed in the early 20th century. The term significance does not imply importance here, and the term statistical significance is not the same as research significance, theoretical significance, or practical significance. For example, the term clinical significance refers to the practical importance of a treatment effect.\n\n\n== History ==\n\nStatistical significance dates to the 18th century, in the work of John Arbuthnot and Pierre-Simon Laplace, who computed the p-value for the human sex ratio at birth, assuming a null hypothesis of equal probability of male and female births; see p-value \u00a7 History for details.In 1925, Ronald Fisher advanced the idea of statistical hypothesis testing, which he called \"tests of significance\", in his publication Statistical Methods for Research Workers. Fisher suggested a probability of one in twenty (0.05) as a convenient cutoff level to reject the null hypothesis. In a 1933 paper, Jerzy Neyman and Egon Pearson called this cutoff the significance level, which they named \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  . They recommended that \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   be set ahead of time, prior to any data collection.Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed. In his 1956 publication Statistical Methods and Scientific Inference, he recommended that significance levels be set according to specific circumstances.\n\n\n=== Related concepts ===\nThe significance level \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is the threshold for \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   below which the null hypothesis is rejected even though by assumption it were true, and something else is going on. This means that \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is also the probability of mistakenly rejecting the null hypothesis, if the null hypothesis is true. This is also called false positive and type I error.\nSometimes researchers talk about the confidence level \u03b3 = (1 \u2212 \u03b1) instead. This is the probability of not rejecting the null hypothesis given that it is true. Confidence levels and confidence intervals were introduced by Neyman in 1937.\n\n\n== Role in statistical hypothesis testing ==\n\nStatistical significance plays a pivotal role in statistical hypothesis testing. It is used to determine whether the null hypothesis should be rejected or retained. The null hypothesis is the default assumption that nothing happened or changed. For the null hypothesis to be rejected, an observed result has to be statistically significant, i.e. the observed p-value is less than the pre-specified significance level \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  .\nTo determine whether a result is statistically significant, a researcher calculates a p-value, which is the probability of observing an effect of the same magnitude or more extreme given that the null hypothesis is true. The null hypothesis is rejected if the p-value is less than (or equal to) a predetermined level, \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  .  \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is also called the significance level, and is the probability of rejecting the null hypothesis given that it is true (a type I error). It is usually set at or below 5%.\nFor example, when \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is set to 5%, the conditional probability of a type I error, given that the null hypothesis is true, is 5%, and a statistically significant result is one where the observed p-value is less than (or equal to) 5%. When drawing data from a sample, this means that the rejection region comprises 5% of the sampling distribution. These 5% can be allocated to one side of the sampling distribution, as in a one-tailed test, or partitioned to both sides of the distribution, as in a two-tailed test, with each tail (or rejection region) containing 2.5% of the distribution.\nThe use of a one-tailed test is dependent on whether the research question or alternative hypothesis specifies a direction such as whether a group of objects is heavier or the performance of students on an assessment is better. A two-tailed test may still be used but it will be less powerful than a one-tailed test, because the rejection region for a one-tailed test is concentrated on one end of the null distribution and is twice the size (5% vs. 2.5%) of each rejection region for a two-tailed test. As a result, the null hypothesis can be rejected with a less extreme result if a one-tailed test was used. The one-tailed test is only more powerful than a two-tailed test if the specified direction of the alternative hypothesis is correct. If it is wrong, however, then the one-tailed test has no power.\n\n\n=== Significance thresholds in specific fields ===\n\nIn specific fields such as particle physics and manufacturing, statistical significance is often expressed in multiples of the standard deviation or sigma (\u03c3) of a normal distribution, with significance thresholds set at a much stricter level (for example 5\u03c3). For instance, the certainty of the Higgs boson particle's existence was based on the 5\u03c3 criterion, which corresponds to a p-value of about 1 in 3.5 million.In other fields of scientific research such as genome-wide association studies, significance levels as low as 5\u00d710\u22128 are not uncommon\u2014as the number of tests performed is extremely large.\n\n\n== Limitations ==\nResearchers focusing solely on whether their results are statistically significant might report findings that are not substantive and not replicable. There is also a difference between statistical significance and practical significance. A study that is found to be statistically significant may not necessarily be practically significant.\n\n\n=== Effect size ===\n\nEffect size is a measure of a study's practical significance. A statistically significant result may have a weak effect. To gauge the research significance of their result, researchers are encouraged to always report an effect size along with p-values. An effect size measure quantifies the strength of an effect, such as the distance between two means in units of standard deviation (cf. Cohen's d), the correlation coefficient between two variables or its square, and other measures.\n\n\n=== Reproducibility ===\n\nA statistically significant result may not be easy to reproduce. In particular, some statistically significant results will in fact be false positives. Each failed attempt to reproduce a result increases the likelihood that the result was a false positive.\n\n\n== Challenges ==\n\n\n=== Overuse in some journals ===\nStarting in the 2010s, some journals began questioning whether significance testing, and particularly using a threshold of \u03b1=5%, was being relied on too heavily as the primary measure of validity of a hypothesis. Some journals encouraged authors to do more detailed analysis than just a statistical significance test. In social psychology, the journal Basic and Applied Social Psychology banned the use of significance testing altogether from papers it published, requiring authors to use other measures to evaluate hypotheses and impact.Other editors, commenting on this ban have noted: \"Banning the reporting of p-values, as Basic and Applied Social Psychology recently did, is not going to solve the problem because it is merely treating a symptom of the problem. There is nothing wrong with hypothesis testing and p-values per se as long as authors, reviewers, and action editors use them correctly.\" Some statisticians prefer to use alternative measures of evidence, such as likelihood ratios or Bayes factors. Using Bayesian statistics can avoid confidence levels, but also requires making additional assumptions, and may not necessarily improve practice regarding statistical testing.The widespread abuse of statistical significance represents an important topic of research in metascience.\n\n\n=== Redefining significance ===\nIn 2016, the American Statistical Association (ASA) published a statement on p-values, saying that \"the widespread use of 'statistical significance' (generally interpreted as 'p \u2264 0.05') as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process\". In 2017, a group of 72 authors proposed to enhance reproducibility by changing the p-value threshold for statistical significance from 0.05 to 0.005. Other researchers responded that imposing a more stringent significance threshold would aggravate problems such as data dredging; alternative propositions are thus to select and justify flexible p-value thresholds before collecting data, or to interpret p-values as continuous indices, thereby discarding thresholds and statistical significance. Additionally, the change to 0.005 would increase the likelihood of false negatives, whereby the effect being studied is real, but the test fails to show it.\nIn 2019, over 800 statisticians and scientists signed a message calling for the abandonment of the term \"statistical significance\" in science, and the ASA published a further official statement  declaring (page 2): We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term \"statistically significant\" entirely. Nor should variants such as \"significantly different,\" \"\n  \n    \n      \n        p\n        \u2264\n        0.05\n      \n    \n    {\\displaystyle p\\leq 0.05}\n  ,\" and \"nonsignificant\" survive, whether expressed in words, by asterisks in a table, or in some other way.\n\n\n== See also ==\nA/B testing, ABX test\nEstimation statistics\nFisher's method for combining independent tests of significance\nLook-elsewhere effect\nMultiple comparisons problem\nSample size\nTexas sharpshooter fallacy (gives examples of tests where the significance level was set too high)\n\n\n== References ==\n\n\n== Further reading ==\nLydia Denworth, \"A Significant Problem: Standard scientific methods are under fire. Will anything change?\", Scientific American, vol. 321, no. 4 (October 2019), pp. 62\u201367. \"The use of p values for nearly a century [since 1925] to determine statistical significance of experimental results has contributed to an illusion of certainty and [to] reproducibility crises in many scientific fields. There is growing determination to reform statistical analysis... Some [researchers] suggest changing statistical methods, whereas others would do away with a threshold for defining \"significant\" results.\" (p. 63.)\nZiliak, Stephen and Deirdre McCloskey (2008), The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives. Ann Arbor, University of Michigan Press, 2009. ISBN 978-0-472-07007-7. Reviews and reception: (compiled by Ziliak)\nThompson, Bruce (2004). \"The \"significance\" crisis in psychology and education\". Journal of Socio-Economics. 33 (5): 607\u2013613. doi:10.1016/j.socec.2004.09.034.\nChow, Siu L., (1996). Statistical Significance: Rationale, Validity and Utility, Volume 1 of series Introducing Statistical Methods, Sage Publications Ltd, ISBN 978-0-7619-5205-3 \u2013 argues that statistical significance is useful in certain circumstances.\nKline, Rex, (2004). Beyond Significance Testing: Reforming Data Analysis Methods in Behavioral Research Washington, DC: American Psychological Association.\nNuzzo, Regina (2014). Scientific method: Statistical errors. Nature Vol. 506, p. 150-152 (open access). Highlights common misunderstandings about the p value.\nCohen, Joseph (1994). [1] Archived 2017-07-13 at the Wayback Machine. The earth is round (p<.05). American Psychologist. Vol 49, p. 997-1003. Reviews problems with null hypothesis statistical testing.\nAmrhein, Valentin; Greenland, Sander; McShane, Blake (2019-03-20). \"Scientists rise up against statistical significance\". Nature. 567 (7748): 305\u2013307. Bibcode:2019Natur.567..305A. doi:10.1038/d41586-019-00857-9. PMID 30894741.\n\n\n== External links ==\n\nThe article \"Earliest Known Uses of Some of the Words of Mathematics (S)\" contains an entry on Significance that provides some historical information.\n\"The Concept of Statistical Significance Testing\" (February 1994): article by Bruce Thompon hosted by the ERIC Clearinghouse on Assessment and Evaluation, Washington, D.C.\n\"What does it mean for a result to be \"statistically significant\"?\" (no date): an article from the Statistical Assessment Service at George Mason University, Washington, D.C.", "Data analysis": "Data analysis  is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\n\n\n== The process of data analysis ==\n\nAnalysis, refers to dividing a whole into its separate components for individual examination. Data analysis, is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data, is collected and analyzed to answer questions, test hypotheses, or disprove theories.\nStatistician John Tukey, defined data analysis in 1961, as:\"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.\n\n\n=== Data requirements ===\nThe data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analytics (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\n\n\n=== Data collection ===\nData is collected from a variety of sources. A list of data sources are available for study & research. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\n\n\n=== Data processing ===\n\nData, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.\n\n\n=== Data cleaning ===\n\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers can be used to lessen the amount of mis-typed words. However, it is harder to tell if the words themselves are correct.\n\n\n=== Exploratory data analysis ===\nOnce the datasets are cleaned, they can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.\n\n\n=== Modeling and algorithms ===\nMathematical formulas or models (also known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model's accuracy (e.g., Data = Model + Error).Inferential statistics, includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.\n\n\n=== Data product ===\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\n\n\n=== Communication ===\n\nOnce data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.When determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\n\n\n== Quantitative messages ==\n\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\nTime-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.\nFrequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0\u201310%, 11\u201320%, etc. A histogram, a type of bar chart, may be used for this analysis.\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.\n\n\n== Techniques for analyzing quantitative data ==\n\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:\n\nCheck raw data for anomalies prior to performing an analysis;\nRe-perform important calculations, such as verifying columns of data that are formula driven;\nConfirm main totals are the sum of subtotals;\nCheck relationships between numbers that should be related in a predictable way, such as ratios over time;\nNormalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\nBreak problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\n The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.Necessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\n\n\n== Analytical activities of data users ==\nUsers may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\n\n\n== Barriers to effective analysis ==\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\n\n\n=== Confusing fact and opinion ===\n\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011\u20132020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\n\n\n=== Cognitive biases ===\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\n\n\n=== Innumeracy ===\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.Analysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\n\n\n== Other topics ==\n\n\n=== Smart buildings ===\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\n\n\n=== Analytics and business intelligence ===\n\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\n\n\n=== Education ===\n\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators\u2019 data analyses.\n\n\n== Practitioner notes ==\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\n\n\n=== Initial data analysis ===\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\n\n\n==== Quality of data ====\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.\nAnalysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.\nComparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\nTest for common-method variance.The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\n\n\n==== Quality of measurements ====\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\nThere are two ways to assess measurement quality:\n\nConfirmatory factor analysis\nAnalysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's \u03b1 of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale\n\n\n==== Initial transformations ====\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\nPossible transformations of variables are:\nSquare root transformation (if the distribution differs moderately from normal)\nLog-transformation (if the distribution differs substantially from normal)\nInverse transformation (if the distribution differs severely from normal)\nMake categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\n\n\n==== Did the implementation of the study fulfill the intentions of the research design? ====\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\n\ndropout (this should be identified during the initial data analysis phase)\nItem non-response (whether this is random or not should be assessed during the initial data analysis phase)\nTreatment quality (using manipulation checks).\n\n\n==== Characteristics of data sample ====\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\n\nBasic statistics of important variables\nScatter plots\nCorrelations and associations\nCross-tabulations\n\n\n==== Final stage of the initial data analysis ====\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\n\nIn the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\nIn the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\nIn the case of outliers: should one use robust analysis techniques?\nIn case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\nIn the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\nIn case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?\n\n\n==== Analysis ====\nSeveral analyses can be used during the initial data analysis phase:\nUnivariate statistics (single variable)\nBivariate associations (correlations)\nGraphical techniques (scatter plots)It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\nNominal and ordinal variables\nFrequency counts (numbers and percentages)\nAssociations\ncircumambulations (crosstabulations)\nhierarchical loglinear analysis (restricted to a maximum of 8 variables)\nloglinear analysis (to identify relevant/important variables and possible confounders)\nExact tests or bootstrapping (in case subgroups are small)\nComputation of new variables\nContinuous variables\nDistribution\nStatistics (M, SD, variance, skewness, kurtosis)\nStem-and-leaf displays\nBox plots\n\n\n==== Nonlinear analysis ====\nNonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.\n\n\n=== Main data analysis ===\nIn the main analysis phase, analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\n\n\n==== Exploratory and confirmatory approaches ====\nIn the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\n\n\n==== Stability of results ====\nIt is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.\nCross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.\nSensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.\n\n\n== Free software for data analysis ==\nNotable free software for data analysis include:\n\nDevInfo \u2013 A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\nELKI \u2013 Data mining framework in Java with data mining oriented visualization functions.\nKNIME \u2013 The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\nOrange \u2013 A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\nPandas \u2013 Python library for data analysis.\nPAW \u2013 FORTRAN/C data analysis framework developed at CERN.\nR \u2013 A programming language and software environment for statistical computing and graphics.\nROOT \u2013  C++ data analysis framework developed at CERN.\nSciPy \u2013 Python library for data analysis.\nJulia - A programming language well-suited for numerical analysis and computational science.\n\n\n== International data analysis contests ==\nDifferent companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows:\nKaggle competition, which is held by Kaggle.\nLTPP data analysis contest held by FHWA and ASCE.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAd\u00e8r, Herman J. (2008a). \"Chapter 14: Phases and initial steps in data analysis\".  In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333\u2013356. ISBN 9789079418015. OCLC 905799857.\nAd\u00e8r, Herman J. (2008b). \"Chapter 15: The main analysis phase\".  In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357\u2013386. ISBN 9789079418015. OCLC 905799857.\nTabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60\u2013116). Boston: Pearson Education, Inc. / Allyn and Bacon.\n\n\n== Further reading ==\n\nAd\u00e8r, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant's Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.  ISBN 978-90-79418-01-5\nChambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X\nFandango, Armando (2017). Python Data Analysis, 2nd Edition. Packt Publishers. ISBN 978-1787127487\nJuran, Joseph M.; Godfrey, A. Blanton (1999). Juran's Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X\nLewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6\nNIST/SEMATECH (2008) Handbook of Statistical Methods,\nPyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7\nRichard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7\nTabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4", "Association rule learning": "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\nIn addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nThe association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.\n\n\n== Definition ==\n\nFollowing the original definition by Agrawal, Imieli\u0144ski, Swami the problem of association rule mining is defined as:\nLet \n  \n    \n      \n        I\n        =\n        {\n        \n          i\n          \n            1\n          \n        \n        ,\n        \n          i\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          i\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle I=\\{i_{1},i_{2},\\ldots ,i_{n}\\}}\n   be a set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   binary attributes called items.\nLet \n  \n    \n      \n        D\n        =\n        {\n        \n          t\n          \n            1\n          \n        \n        ,\n        \n          t\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          t\n          \n            m\n          \n        \n        }\n      \n    \n    {\\displaystyle D=\\{t_{1},t_{2},\\ldots ,t_{m}\\}}\n   be a set of transactions called the database.\nEach transaction in \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   has a unique transaction ID and contains a subset of the items in \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  .\nA rule is defined as an implication of the form:\n\n  \n    \n      \n        X\n        \u21d2\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n  , where \n  \n    \n      \n        X\n        ,\n        Y\n        \u2286\n        I\n      \n    \n    {\\displaystyle X,Y\\subseteq I}\n  .\nIn Agrawal, Imieli\u0144ski, Swami a rule is defined only between a set and a single item, \n  \n    \n      \n        X\n        \u21d2\n        \n          i\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X\\Rightarrow i_{j}}\n   for \n  \n    \n      \n        \n          i\n          \n            j\n          \n        \n        \u2208\n        I\n      \n    \n    {\\displaystyle i_{j}\\in I}\n  .\nEvery rule is composed by two different sets of items, also known as itemsets, \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , where \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is called antecedent or left-hand-side (LHS) and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement \n  \n    \n      \n        X\n        \u21d2\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n   is often read as if \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   then \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , where the antecedent (\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   ) is the if and the consequent (\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  ) is the then. This simply implies that, in theory, whenever \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   occurs in a dataset, then \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   will as well.\n\n\n== Process ==\nAssociation rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.\nAssociation rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn\u2019t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.\nThere are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis. What technique you should use depends on what you are looking for with your data. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior. Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data. Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.Benefits\nThere are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.Downsides\nHowever, Association rules also lead to many different downsides such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downside of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn\u2019t have a good concept of data mining, this might cause them to have trouble understanding it.\nThresholdsWhen using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:\nA minimum Support threshold to find all the frequent itemsets that are in the database.\nA minimum Confidence threshold to the frequent itemsets found to create rules.The Support Threshold is 30%, Confidence Threshold is 50%\nThe Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.\nTo find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle 2^{n}-1}\n   , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support (also called anti-monotonicity). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori and Eclat) can find all frequent itemsets.\n\n\n== Useful Concepts ==\nTo illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is \n  \n    \n      \n        I\n        =\n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n          ,\n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          e\n          e\n          r\n          ,\n          d\n          i\n          a\n          p\n          e\n          r\n          s\n          ,\n          e\n          g\n          g\n          s\n          ,\n          f\n          r\n          u\n          i\n          t\n        \n        }\n      \n    \n    {\\displaystyle I=\\{\\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \\}}\n  .\nAn example rule for the supermarket could be \n  \n    \n      \n        {\n        \n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n        }\n        \u21d2\n        {\n        \n          m\n          i\n          l\n          k\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {butter,bread} \\}\\Rightarrow \\{\\mathrm {milk} \\}}\n   meaning that if butter and bread are bought, customers also buy milk.\nIn order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.\nLet \n  \n    \n      \n        X\n        ,\n        Y\n      \n    \n    {\\displaystyle X,Y}\n   be itemsets, \n  \n    \n      \n        X\n        \u21d2\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n   an association rule and T a set of transactions of a given database.\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n=== Support ===\nSupport is an indication of how frequently the itemset appears in the dataset.\nIn our example, it can be easier to explain support by writing \n  \n    \n      \n        s\n        u\n        p\n        p\n        o\n        r\n        t\n        =\n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        \n          \n            \n              (\n              \n                number of transactions containing \n              \n              A\n              \n                 and \n              \n              B\n              )\n            \n             (total number of transactions)\n          \n        \n      \n    \n    {\\displaystyle support=P(A\\cap B)={\\frac {({\\text{number of transactions containing }}A{\\text{ and }}B)}{\\text{ (total number of transactions)}}}}\n    where A and B are separate item sets that occur in at the same time in a transaction.\nUsing Table 2 as an example, the itemset \n  \n    \n      \n        X\n        =\n        {\n        \n          b\n          e\n          e\n          r\n          ,\n          d\n          i\n          a\n          p\n          e\n          r\n          s\n        \n        }\n      \n    \n    {\\displaystyle X=\\{\\mathrm {beer,diapers} \\}}\n   has a support of \n  \n    \n      \n        1\n        \n          /\n        \n        5\n        =\n        0.2\n      \n    \n    {\\displaystyle 1/5=0.2}\n   since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of support of X is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).Furthermore, the itemset \n  \n    \n      \n        Y\n        =\n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n          ,\n          b\n          u\n          t\n          t\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle Y=\\{\\mathrm {milk,bread,butter} \\}}\n   has a support of \n  \n    \n      \n        1\n        \n          /\n        \n        5\n        =\n        0.2\n      \n    \n    {\\displaystyle 1/5=0.2}\n   as it appears in 20% of all transactions as well.\nWhen using antecedents and consequents, it allows a data miner to determine the support of multiple items being bought together in comparison to the whole data set. For example, Table 2 shows that if milk is bought, then bread is bought has a support of 0.4 or 40%. This because in 2 out 5 of the transactions, milk as well as bread are bought. In smaller data sets like this example, it is harder to see a strong correlation when there are few samples, but when the data set grows larger, support can be used to find correlation between two or more products in the supermarket example.\nMinimum support thresholds are useful for determining which itemsets are preferred or interesting.\nIf we set the support threshold to \u22650.4 in Table 3, then the \n  \n    \n      \n        {\n        \n          m\n          i\n          l\n          k\n        \n        }\n        \u21d2\n        {\n        \n          e\n          g\n          g\n          s\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {milk} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n   would be removed since it did not meet the minimum threshold of 0.4. Minimum threshold is used to remove samples where there is not a strong enough support or confidence to deem the sample as important or interesting in the dataset.\nAnother way of finding interesting samples is to find the value of (support)X(confidence); this allows a data miner to see the samples where support and confidence are high enough to be highlighted in the dataset and prompt a closer look at the sample to find more information on the connection between the items.\nSupport can be beneficial for finding the connection between products in comparison to the whole dataset, whereas confidence looks at the connection between one or more items and another item. Below is a table that shows the comparison and contrast between support and support x confidence, using the information from Table 4 to derive the confidence values.\n\nThe support of X with respect to T is defined as the proportion of transactions in the dataset which contains the itemset X. Denoting a transaction by \n  \n    \n      \n        (\n        i\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle (i,t)}\n   where i is the unique identifier of the transaction and t is its itemset, the support may be written as:\n\n  \n    \n      \n        \n          s\n          u\n          p\n          p\n          o\n          r\n          t\n          \n          o\n          f\n          \n          X\n        \n        =\n        \n          \n            \n              \n                |\n              \n              {\n              (\n              i\n              ,\n              t\n              )\n              \u2208\n              T\n              :\n              X\n              \u2286\n              t\n              }\n              \n                |\n              \n            \n            \n              \n                |\n              \n              T\n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {support\\,of\\,X} ={\\frac {|\\{(i,t)\\in T:X\\subseteq t\\}|}{|T|}}}\n  \nThis notation can be used when defining more complicated datasets where the items and itemsets may not be as easy as our supermarket example above. Other examples of where support can be used is in finding groups of genetic mutations that work collectively to cause a disease, investigating the number of subscribers that respond to upgrade offers, and discovering which products in a drug store are never bought together.\n\n\n=== Confidence ===\nConfidence is the percentage of all transactions satisfying X that also satisfy Y.With respect to T, the confidence value of an association rule, often denoted as \n  \n    \n      \n        X\n        \u21d2\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n  , is the ratio of transactions containing both X and Y to the total amount of X values present, where X is the antecedent and Y is the consequent.\nConfidence can also be interpreted as an estimate of the conditional probability \n  \n    \n      \n        P\n        (\n        \n          E\n          \n            Y\n          \n        \n        \n          |\n        \n        \n          E\n          \n            X\n          \n        \n        )\n      \n    \n    {\\displaystyle P(E_{Y}|E_{X})}\n  , the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.It is commonly depicted as:\n\n  \n    \n      \n        \n          c\n          o\n          n\n          f\n        \n        (\n        X\n        \u21d2\n        Y\n        )\n        =\n        P\n        (\n        Y\n        \n          |\n        \n        X\n        )\n        =\n        \n          \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              \u2229\n              Y\n              )\n            \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              )\n            \n          \n        \n        =\n        \n          \n            \n              \n                number of transactions containing \n              \n              X\n              \n                 and \n              \n              Y\n            \n            \n              \n                number of transactions containing \n              \n              X\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {conf} (X\\Rightarrow Y)=P(Y|X)={\\frac {\\mathrm {supp} (X\\cap Y)}{\\mathrm {supp} (X)}}={\\frac {{\\text{number of transactions containing }}X{\\text{ and }}Y}{{\\text{number of transactions containing }}X}}}\n  \nThe equation illustrates that confidence can be computed by calculating the co-occurrence of transactions X and Y within the dataset in ratio to transactions containing only X. This means that the number of transactions in both  X and Y  is divided by those just in X .\nFor example, Table 2 shows the rule \n  \n    \n      \n        {\n        \n          b\n          u\n          t\n          t\n          e\n          r\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n        }\n        \u21d2\n        {\n        \n          m\n          i\n          l\n          k\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {butter,bread} \\}\\Rightarrow \\{\\mathrm {milk} \\}}\n   which has a confidence of \n  \n    \n      \n        \n          \n            \n              1\n              \n                /\n              \n              5\n            \n            \n              1\n              \n                /\n              \n              5\n            \n          \n        \n        =\n        \n          \n            0.2\n            0.2\n          \n        \n        =\n        1.0\n      \n    \n    {\\displaystyle {\\frac {1/5}{1/5}}={\\frac {0.2}{0.2}}=1.0}\n   in the dataset, which denotes that every time a customer buys butter and bread, they also buy milk. This particular example demonstrates the rule being correct 100% of the time for transactions containing both butter and bread. The rule \n  \n    \n      \n        {\n        \n          f\n          r\n          u\n          i\n          t\n        \n        }\n        \u21d2\n        {\n        \n          e\n          g\n          g\n          s\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {fruit} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n  , however, has a confidence of \n  \n    \n      \n        \n          \n            \n              2\n              \n                /\n              \n              5\n            \n            \n              3\n              \n                /\n              \n              5\n            \n          \n        \n        =\n        \n          \n            0.4\n            0.6\n          \n        \n        =\n        0.67\n      \n    \n    {\\displaystyle {\\frac {2/5}{3/5}}={\\frac {0.4}{0.6}}=0.67}\n  . This suggests that eggs are bought 67% of the times that fruit is brought. Within this particular dataset, fruit is purchased a total of 3 times, with two of those times consisting of egg purchases.\nFor larger datasets, a minimum threshold, or a percentage cutoff, for the confidence can be useful for determining item relationships. When applying this method to some of the data in Table 2, information that does not meet the requirements are removed. Table 4 shows association rule examples where the minimum threshold for confidence is 0.5 (50%). Any data that does not have a confidence of at least 0.5 is omitted. Generating thresholds allow for the association between items to become stronger as the data is further researched by emphasizing those that co-occur the most. The table uses the confidence information from Table 3 to implement the Support x Confidence column, where the relationship between items via their both confidence and support, instead of just one concept, is highlighted. Ranking the rules by Support x Confidence multiples the confidence of a particular rule to its support and is often implemented for a more in-depth understanding of the relationship between the items.\n\nOverall, using confidence in association rule mining is great way to bring awareness to data relations. Its greatest benefit is highlighting the relationship between particular items to one another within the set, as it compares co-occurrences of items to the total occurrence of the antecedent in the specific rule. However, confidence is not the optimal method for every concept in association rule mining. The disadvantage of using it is that it does not offer multiple difference outlooks on the associations. Unlike support, for instance, confidence does not provide the perspective of relationships between certain items in comparison to the entire dataset, so while milk and bread, for example, may occur 100% of the time for confidence, it only has a support of 0.4 (40%). This is why it is important to look at other viewpoints, such as Support x Confidence, instead of solely relying on one concept incessantly to define the relationships.\n\n\n=== Lift ===\nThe lift of a rule is defined as:\n\n  \n    \n      \n        \n          l\n          i\n          f\n          t\n        \n        (\n        X\n        \u21d2\n        Y\n        )\n        =\n        \n          \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              \u2229\n              Y\n              )\n            \n            \n              \n                s\n                u\n                p\n                p\n              \n              (\n              X\n              )\n              \u00d7\n              \n                s\n                u\n                p\n                p\n              \n              (\n              Y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {lift} (X\\Rightarrow Y)={\\frac {\\mathrm {supp} (X\\cap Y)}{\\mathrm {supp} (X)\\times \\mathrm {supp} (Y)}}}\n  \nor the ratio of the observed support to that expected if X and Y were independent.\nFor example, the rule \n  \n    \n      \n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n   has a lift of \n  \n    \n      \n        \n          \n            0.2\n            \n              0.4\n              \u00d7\n              0.4\n            \n          \n        \n        =\n        1.25\n      \n    \n    {\\displaystyle {\\frac {0.2}{0.4\\times 0.4}}=1.25}\n  .\nIf the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\nIf the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\nIf the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.\nThe value of lift is that it considers both the support of the rule and the overall data set.\n\n\n=== Conviction ===\nThe conviction of a rule is defined as \n  \n    \n      \n        \n          c\n          o\n          n\n          v\n        \n        (\n        X\n        \u21d2\n        Y\n        )\n        =\n        \n          \n            \n              1\n              \u2212\n              \n                s\n                u\n                p\n                p\n              \n              (\n              Y\n              )\n            \n            \n              1\n              \u2212\n              \n                c\n                o\n                n\n                f\n              \n              (\n              X\n              \u21d2\n              Y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {conv} (X\\Rightarrow Y)={\\frac {1-\\mathrm {supp} (Y)}{1-\\mathrm {conf} (X\\Rightarrow Y)}}}\n  .For example, the rule \n  \n    \n      \n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n   has a conviction of \n  \n    \n      \n        \n          \n            \n              1\n              \u2212\n              0.4\n            \n            \n              1\n              \u2212\n              0.5\n            \n          \n        \n        =\n        1.2\n      \n    \n    {\\displaystyle {\\frac {1-0.4}{1-0.5}}=1.2}\n  , and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule \n  \n    \n      \n        {\n        \n          m\n          i\n          l\n          k\n          ,\n          b\n          r\n          e\n          a\n          d\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          t\n          t\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n   would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.\n\n\n=== Alternative measures of interestingness ===\nIn addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:\n\nAll-confidence\nCollective strength\nLeverageSeveral more measures are presented and compared by Tan et al. and by Hahsler. Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of \"Subjective Interestingness.\"\n\n\n== History ==\nThe concept of association rules was popularized particularly due to the 1993 article of Agrawal et al., which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called \"association rules\" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr H\u00e1jek et al.An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with \n  \n    \n      \n        \n          s\n          u\n          p\n          p\n        \n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\mathrm {supp} (X)}\n   and \n  \n    \n      \n        \n          c\n          o\n          n\n          f\n        \n        (\n        X\n        \u21d2\n        Y\n        )\n      \n    \n    {\\displaystyle \\mathrm {conf} (X\\Rightarrow Y)}\n   greater than user defined constraints.\n\n\n== Statistically sound associations ==\nOne limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.\n\n\n== Algorithms ==\nMany algorithms for generating association rules have been proposed.\nSome well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.\n\n\n=== Apriori algorithm ===\nApriori is given by R. Agrawal and R. Srikant in 1994 for frequent item set mining and association rule learning. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often. The name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.\n\nOverview: Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length  from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\nExample: Assume that each row is a cancer sample with a certain combination of mutations labeled by a character in the alphabet. For example a row could have {a, c} which means it is affected by mutation 'a' and mutation 'c'. \n\nNow we will generate the frequent item set by counting the number of occurrences of each character. This is also known as finding the support values. Then we will prune the item set by picking a minimum support threshold. For this pass of the algorithm we will pick 3. \n\nSince all support values are three or above there is no pruning. The frequent item set is {a}, {b}, {c}, and {d}. After this we will repeat the process by counting pairs of mutations in the input set. \n\nNow we will make our minimum support value 4 so only {a, d} and {c, d} will remain after pruning. Now we will use the frequent item set to make combinations of triplets.  We will then repeat the process by counting occurrences of triplets of mutations in the input set. \n\nSince we only have one item the next set of combinations of quadruplets is empty so the algorithm will stop.\nAdvantages and Limitations:\nApriori has some limitations. Candidate generation can result in large candidate sets. For example a 10^4 frequent 1-itemset will generate a 10^7 candidate 2-itemset. The algorithm also needs to frequently scan the database, to be specific n+1 scans where n is the length of the longest pattern. Apriori is slower than the Eclat algorithm. However, Apriori performs well compared to Eclat when the dataset is large. This is because in the Eclat algorithm if the dataset is too large the tid-lists become too large for memory. FP-growth outperforms the Apriori and Eclat. This is due to the FP-growth algorithm not having candidate generation or test, using a compact data structure, and only having one database scan.\n\n\n=== Eclat algorithm ===\nEclat (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.\nTo illustrate this, let there be a frequent itemset {a, b, c}. a DFS may check the nodes in the frequent itemset lattice in the following order: {a} \u2192 {a, b} \u2192 {a, b, c}, at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. BFS would explore each subset of {a, b, c} before finally checking it. As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.\nIt is suitable for both sequential as well as parallel execution with locality-enhancing properties.\n\n\n=== FP-growth algorithm ===\nFP stands for frequent pattern.In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.\nItems in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.\nItems in each transaction that do not meet the minimum support requirement are discarded.\nIf many transactions share most frequent items, the FP-tree provides high compression close to tree root.\nRecursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).\nGrowth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  .\nA new conditional tree is created which is the original FP-tree projected onto \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n  . The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   meet the minimum support threshold. The resulting paths from root to \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.\nOnce the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.\n\n\n=== Others ===\n\n\n==== ASSOC ====\nThe ASSOC procedure is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example \"items\" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.\n\n\n==== OPUS search ====\nOPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support. Initially used to find rules for a fixed consequent it has subsequently been extended to find rules with any item as a consequent. OPUS search is the core technology in the popular Magnum Opus association discovery system.\n\n\n== Lore ==\nA famous story about association rule mining is the \"beer and diaper\" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true. Daniel Powers says:\nIn 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis \"did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers\". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.\n\n\n== Other types of association rule mining ==\nMulti-Relation Association Rules (MRAR): These are association rules where each item may have several relations. These relations indicate indirect relationships between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: \u201cThose who live in a place which is nearby a city with humid climate type and also are younger than 20 \n  \n    \n      \n        \n        \u27f9\n        \n      \n    \n    {\\displaystyle \\implies }\n   their health condition is good\u201d. Such association rules can be extracted from RDBMS data or semantic web data.Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.Weighted class learning is another form of associative learning where weights may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.\nHigh-order pattern discovery facilitates the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.\nK-optimal pattern discovery provides an alternative to the standard approach to association rule learning which requires that each pattern appear frequently in the data.\nApproximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.Generalized Association Rules hierarchical taxonomy (concept hierarchy)\nQuantitative Association Rules categorical and quantitative data\nInterval Data Association Rules e.g. partition the age into 5-year-increment ranged\nSequential pattern mining  discovers subsequences that are common to more than minsup (minimum support threshold) sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.Subspace Clustering, a specific type of clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.Warmr, shipped as part of the ACE data mining suite, allows association rule learning for first order relational rules.\n\n\n== See also ==\nSequence mining\nProduction system (computer science)\nLearning classifier system\nRule-based machine learning\n\n\n== References ==\n\n\n=== Bibliographies ===\nAnnotated Bibliography on Association Rules by M. Hahsler", "Student's t-test": "A t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. It is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known (typically, the scaling term is unknown and therefore a nuisance parameter). When the scaling term is estimated based on the data, the test statistic\u2014under certain conditions\u2014follows a Student's t distribution. The t-test's most common application is to test whether the means of two populations are different.\n\n\n== History ==\n\nThe term \"t-statistic\" is abbreviated from \"hypothesis test statistic\". In statistics, the t-distribution was first derived as a posterior distribution in 1876 by Helmert and L\u00fcroth. The t-distribution also appeared in a more general form as Pearson Type IV distribution in Karl Pearson's 1895 paper. However, the T-Distribution, also known as Student's t-distribution, gets its name from William Sealy Gosset who first published it in English in 1908 in the scientific journal Biometrika using the pseudonym \"Student\" because his employer preferred staff to use pen names when publishing scientific papers. Gosset worked at the Guinness Brewery in Dublin, Ireland, and was interested in the problems of small samples \u2013 for example, the chemical properties of barley with small sample sizes. Hence a second version of the etymology of the term Student is that Guinness did not want their competitors to know that they were using the t-test to determine the quality of raw material (see Student's t-distribution for a detailed history of this pseudonym, which is not to be confused with the literal term student). Although it was William Gosset after whom the term \"Student\" is penned, it was actually through the work of Ronald Fisher that the distribution became well known as \"Student's distribution\" and \"Student's t-test\".\nGosset had been hired owing to Claude Guinness's policy of recruiting the best graduates from Oxford and Cambridge to apply biochemistry and statistics to Guinness's industrial processes. Gosset devised the t-test as an economical way to monitor the quality of stout. The t-test work was submitted to and accepted in the journal Biometrika and published in 1908.Guinness had a policy of allowing technical staff leave for study (so-called \"study leave\"), which Gosset used during the first two terms of the 1906\u20131907 academic year in Professor Karl Pearson's Biometric Laboratory at University College London. Gosset's identity was then known to fellow statisticians and to editor-in-chief Karl Pearson.\n\n\n== Uses ==\nThe most frequently used t-tests are one-sample and two-sample tests:\n\nA one-sample location test of whether the mean of a population has a value specified in a null hypothesis.\nA two-sample location test of the null hypothesis such that the means of two populations are equal. All such tests are usually called Student's t-tests, though strictly speaking that name should only be used if the variances of the two populations are also assumed to be equal; the form of the test used when this assumption is dropped is sometimes called Welch's t-test. These tests are often referred to as unpaired or independent samples t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping.\n\n\n== AssumptionsMost test statistics have the form t = Z/s, where Z and s are functions of the data. ==\nZ may be sensitive to the alternative hypothesis (i.e., its magnitude tends to be larger when the alternative hypothesis is true), whereas s is a scaling parameter that allows the distribution of t to be determined.\nAs an example, in the one-sample t-test\n\n  \n    \n      \n        t\n        =\n        \n          \n            Z\n            s\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    X\n                    \u00af\n                  \n                \n              \n              \u2212\n              \u03bc\n            \n            \n              \n                \n                  \n                    \u03c3\n                    ^\n                  \n                \n              \n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {Z}{s}}={\\frac {{\\bar {X}}-\\mu }{{\\widehat {\\sigma }}/{\\sqrt {n}}}}}\n  where X is the sample mean from a sample X1, X2, \u2026, Xn, of size n, s is the standard error of the mean, \n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\widehat {\\sigma }}}\n   is the estimate of the standard deviation of the population, and \u03bc is the population mean.\nThe assumptions underlying a t-test in the simplest form above are that:\n\nX follows a normal distribution with mean \u03bc and variance \u03c32/n\ns2(n \u2212 1)/\u03c32 follows a \u03c72 distribution with n \u2212 1 degrees of freedom. This assumption is met when the observations used for estimating s2 come from a normal distribution (and i.i.d for each group).\nZ and s are independent.In the t-test comparing the means of two independent samples, the following assumptions should be met:\n\nThe means of the two populations being compared should follow normal distributions. Under weak assumptions, this follows in large samples from the central limit theorem, even when the distribution of observations in each group is non-normal.\nIf using Student's original definition of the t-test, the two populations being compared should have the same variance (testable using F-test, Levene's test, Bartlett's test, or the Brown\u2013Forsythe test; or assessable graphically using a Q\u2013Q plot).  If the sample sizes in the two groups being compared are equal, Student's original t-test is highly robust to the presence of unequal variances. Welch's t-test is insensitive to equality of the variances regardless of whether the sample sizes are similar.\nThe data used to carry out the test should either be sampled independently from the two populations being compared or be fully paired. This is in general not testable from the data, but if the data are known to be dependent (e.g. paired by test design), a dependent test has to be applied. For partially paired data, the classical independent t-tests may give invalid results as the test statistic might not follow a t distribution, while the dependent t-test is sub-optimal as it discards the unpaired data.Most two-sample t-tests are robust to all but large deviations from the assumptions.For exactness, the t-test and Z-test require normality of the sample means, and the t-test additionally requires that the sample variance follows a scaled \u03c72 distribution, and that the sample mean and sample variance be statistically independent.  Normality of the individual data values is not required if these conditions are met.  By the central limit theorem, sample means of moderately large samples are often well-approximated by a normal distribution even if the data are not normally distributed.  For non-normal data, the distribution of the sample variance may deviate substantially from a \u03c72 distribution. \nHowever, if the sample size is large, Slutsky's theorem implies that the distribution of the sample variance has little effect on the distribution of the test statistic. That is as sample size \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   increases:\n\n  \n    \n      \n        \n          \n            n\n          \n        \n        (\n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n        \u2212\n        \u03bc\n        )\n        \n          \u2192\n          \n            d\n          \n        \n        N\n        \n          (\n          \n            0\n            ,\n            \n              \u03c3\n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\sqrt {n}}({\\bar {X}}-\\mu )\\xrightarrow {d} N\\left(0,\\sigma ^{2}\\right)}\n   as per the Central limit theorem.\n\n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n        \n          \u2192\n          \n            p\n          \n        \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s^{2}\\xrightarrow {p} \\sigma ^{2}}\n   as per the Law of large numbers.\n\n  \n    \n      \n        \u2234\n        \n          \n            \n              \n                \n                  n\n                \n              \n              (\n              \n                \n                  \n                    X\n                    \u00af\n                  \n                \n              \n              \u2212\n              \u03bc\n              )\n            \n            s\n          \n        \n        \n          \u2192\n          \n            d\n          \n        \n        N\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle \\therefore {\\frac {{\\sqrt {n}}({\\bar {X}}-\\mu )}{s}}\\xrightarrow {d} N(0,1)}\n  \n\n\n== Unpaired and paired two-sample t-tests ==\n\nTwo-sample t-tests for a difference in means involve independent samples (unpaired samples) or paired samples.  Paired t-tests are a form of blocking, and have greater power (probability of avoiding a type II error, also known as a false negative) than unpaired tests when the paired units are similar with respect to \"noise factors\" (see confounder) that are independent of membership in the two groups being compared. In a different context, paired t-tests can be used to reduce the effects of confounding factors in an observational study.\n\n\n=== Independent (unpaired) samples ===\nThe independent samples t-test is used when two separate sets of independent and identically distributed samples are obtained, and one variable from each of the two populations is compared. For example, suppose we are evaluating the effect of a medical treatment, and we enroll 100 subjects into our study, then randomly assign 50 subjects to the treatment group and 50 subjects to the control group. In this case, we have two independent samples and would use the unpaired form of the t-test.\n\n\n=== Paired samples ===\n\nPaired samples t-tests typically consist of a sample of matched pairs of similar units, or one group of units that has been tested twice (a \"repeated measures\" t-test).\nA typical example of the repeated measures t-test would be where subjects are tested prior to a treatment, say for high blood pressure, and the same subjects are tested again after treatment with a blood-pressure-lowering medication. By comparing the same patient's numbers before and after treatment, we are effectively using each patient as their own control. That way the correct rejection of the null hypothesis (here: of no difference made by the treatment) can become much more likely, with statistical power increasing simply because the random interpatient variation has now been eliminated. However, an increase of statistical power comes at a price: more tests are required, each subject having to be tested twice. Because half of the sample now depends on the other half, the paired version of Student's t-test has only n/2 \u2212 1 degrees of freedom (with n being the total number of observations). Pairs become individual test units, and the sample has to be doubled to achieve the same number of degrees of freedom. Normally, there are n \u2212 1 degrees of freedom (with n being the total number of observations).A paired samples t-test based on a \"matched-pairs sample\" results from an unpaired sample that is subsequently used to form a paired sample, by using additional variables that were measured along with the variable of interest. The matching is carried out by identifying pairs of values consisting of one observation from each of the two samples, where the pair is similar in terms of other measured variables. This approach is sometimes used in observational studies to reduce or eliminate the effects of confounding factors.\nPaired samples t-tests are often referred to as \"dependent samples t-tests\".\n\n\n== Calculations ==\nExplicit expressions that can be used to carry out various t-tests are given below.  In each case, the formula for a test statistic that either exactly follows or closely approximates a t-distribution under the null hypothesis is given.  Also, the appropriate degrees of freedom are given in each case.  Each of these statistics can be used to carry out either a one-tailed or two-tailed test.\nOnce the t value and degrees of freedom are determined, a p-value can be found using a table of values from Student's t-distribution.  If the calculated p-value is below the threshold chosen for statistical significance (usually the 0.10, the 0.05, or 0.01 level), then the null hypothesis is rejected in favor of the alternative hypothesis.\n\n\n=== One-sample t-test ===\nIn testing the null hypothesis that the population mean is equal to a specified value \u03bc0, one uses the statistic\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  0\n                \n              \n            \n            \n              s\n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {x}}-\\mu _{0}}{s/{\\sqrt {n}}}}}\n  where \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   is the sample mean, s is the sample standard deviation and n is the sample size. The degrees of freedom used in this test are n \u2212 1. \nAlthough the parent population does not need to be normally distributed, the distribution of the population of sample means \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   is assumed to be normal.\nBy the central limit theorem, if the observations are independent and the second moment exists, then \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   will be approximately normal N(0;1).\n\n\n=== Slope of a regression line ===\nSuppose one is fitting the model\n\n  \n    \n      \n        Y\n        =\n        \u03b1\n        +\n        \u03b2\n        x\n        +\n        \u03b5\n      \n    \n    {\\displaystyle Y=\\alpha +\\beta x+\\varepsilon }\n  where x is known, \u03b1 and \u03b2 are unknown, \u03b5 is a normally distributed random variable with mean 0 and unknown variance \u03c32, and Y is the outcome of interest. We want to test the null hypothesis that the slope \u03b2 is equal to some specified value \u03b20 (often taken to be 0, in which case the null hypothesis is that x and y are uncorrelated).\nLet\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \u03b1\n                      ^\n                    \n                  \n                \n                ,\n                \n                  \n                    \n                      \u03b2\n                      ^\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  least-squares estimators\n                \n                ,\n              \n            \n            \n              \n                S\n                \n                  E\n                  \n                    \n                      \n                        \u03b1\n                        ^\n                      \n                    \n                  \n                \n                ,\n                S\n                \n                  E\n                  \n                    \n                      \n                        \u03b2\n                        ^\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  the standard errors of least-squares estimators\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\widehat {\\alpha }},{\\widehat {\\beta }}&={\\text{least-squares estimators}},\\\\SE_{\\widehat {\\alpha }},SE_{\\widehat {\\beta }}&={\\text{the standard errors of least-squares estimators}}.\\end{aligned}}}\n  Then\n\n  \n    \n      \n        \n          t\n          \n            score\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    \u03b2\n                    ^\n                  \n                \n              \n              \u2212\n              \n                \u03b2\n                \n                  0\n                \n              \n            \n            \n              S\n              \n                E\n                \n                  \n                    \n                      \u03b2\n                      ^\n                    \n                  \n                \n              \n            \n          \n        \n        \u223c\n        \n          \n            \n              T\n            \n          \n          \n            n\n            \u2212\n            2\n          \n        \n      \n    \n    {\\displaystyle t_{\\text{score}}={\\frac {{\\widehat {\\beta }}-\\beta _{0}}{SE_{\\widehat {\\beta }}}}\\sim {\\mathcal {T}}_{n-2}}\n  has a t-distribution with n \u2212 2 degrees of freedom if the null hypothesis is true. The standard error of the slope coefficient:\n\n  \n    \n      \n        S\n        \n          E\n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    1\n                    \n                      n\n                      \u2212\n                      2\n                    \n                  \n                \n              \n              \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        y\n                        \n                          i\n                        \n                      \n                      \u2212\n                      \n                        \n                          \n                            \n                              y\n                              ^\n                            \n                          \n                        \n                        \n                          i\n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \n                    (\n                    \n                      \n                        x\n                        \n                          i\n                        \n                      \n                      \u2212\n                      \n                        \n                          \n                            x\n                            \u00af\n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle SE_{\\widehat {\\beta }}={\\frac {\\sqrt {{\\dfrac {1}{n-2}}\\displaystyle \\sum _{i=1}^{n}\\left(y_{i}-{\\widehat {y}}_{i}\\right)^{2}}}{\\sqrt {\\displaystyle \\sum _{i=1}^{n}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}}}\n  can be written in terms of the residuals. Let\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \n                        \u03b5\n                        ^\n                      \n                    \n                  \n                  \n                    i\n                  \n                \n              \n              \n                \n                =\n                \n                  y\n                  \n                    i\n                  \n                \n                \u2212\n                \n                  \n                    \n                      \n                        y\n                        ^\n                      \n                    \n                  \n                  \n                    i\n                  \n                \n                =\n                \n                  y\n                  \n                    i\n                  \n                \n                \u2212\n                \n                  (\n                  \n                    \n                      \n                        \n                          \u03b1\n                          ^\n                        \n                      \n                    \n                    +\n                    \n                      \n                        \n                          \u03b2\n                          ^\n                        \n                      \n                    \n                    \n                      x\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                =\n                \n                  residuals\n                \n                =\n                \n                  estimated errors\n                \n                ,\n              \n            \n            \n              \n                \n                  SSR\n                \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \n                    \n                      \n                        \n                          \n                            \u03b5\n                            ^\n                          \n                        \n                      \n                      \n                        i\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                =\n                \n                  sum of squares of residuals\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\widehat {\\varepsilon }}_{i}&=y_{i}-{\\widehat {y}}_{i}=y_{i}-\\left({\\widehat {\\alpha }}+{\\widehat {\\beta }}x_{i}\\right)={\\text{residuals}}={\\text{estimated errors}},\\\\{\\text{SSR}}&=\\sum _{i=1}^{n}{{\\widehat {\\varepsilon }}_{i}}^{2}={\\text{sum of squares of residuals}}.\\end{aligned}}}\n  Then tscore is given by:\n\n  \n    \n      \n        \n          t\n          \n            score\n          \n        \n        =\n        \n          \n            \n              \n                (\n                \n                  \n                    \n                      \n                        \u03b2\n                        ^\n                      \n                    \n                  \n                  \u2212\n                  \n                    \u03b2\n                    \n                      0\n                    \n                  \n                \n                )\n              \n              \n                \n                  n\n                  \u2212\n                  2\n                \n              \n            \n            \n              \n                \n                  S\n                  S\n                  R\n                \n                \n                  \n                    \u2211\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    \n                      (\n                      \n                        \n                          x\n                          \n                            i\n                          \n                        \n                        \u2212\n                        \n                          \n                            \n                              x\n                              \u00af\n                            \n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle t_{\\text{score}}={\\frac {\\left({\\widehat {\\beta }}-\\beta _{0}\\right){\\sqrt {n-2}}}{\\sqrt {\\frac {SSR}{\\sum _{i=1}^{n}\\left(x_{i}-{\\bar {x}}\\right)^{2}}}}}.}\n  Another way to determine the tscore is:\n\n  \n    \n      \n        \n          t\n          \n            score\n          \n        \n        =\n        \n          \n            \n              r\n              \n                \n                  n\n                  \u2212\n                  2\n                \n              \n            \n            \n              1\n              \u2212\n              \n                r\n                \n                  2\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle t_{\\text{score}}={\\frac {r{\\sqrt {n-2}}}{\\sqrt {1-r^{2}}}},}\n  where r is the Pearson correlation coefficient.\nThe tscore, intercept can be determined from the tscore, slope:\n\n  \n    \n      \n        \n          t\n          \n            score,intercept\n          \n        \n        =\n        \n          \n            \u03b1\n            \u03b2\n          \n        \n        \n          \n            \n              t\n              \n                score,slope\n              \n            \n            \n              \n                s\n                \n                  x\n                \n                \n                  2\n                \n              \n              +\n              \n                \n                  \n                    \n                      x\n                      \u00af\n                    \n                  \n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t_{\\text{score,intercept}}={\\frac {\\alpha }{\\beta }}{\\frac {t_{\\text{score,slope}}}{\\sqrt {s_{\\text{x}}^{2}+{\\bar {x}}^{2}}}}}\n  where sx2 is the sample variance.\n\n\n=== Independent two-sample t-test ===\n\n\n==== Equal sample sizes and variance ====\nGiven two groups (1, 2), this test is only applicable when:\n\nthe two sample sizes are equal;\nit can be assumed that the two distributions have the same variance;Violations of these assumptions are discussed below.\nThe t statistic to test whether the means are different can be calculated as follows:\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  1\n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  2\n                \n              \n            \n            \n              \n                s\n                \n                  p\n                \n              \n              \n                \n                  \n                    2\n                    n\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{p}{\\sqrt {\\frac {2}{n}}}}}}\n  where\n\n  \n    \n      \n        \n          s\n          \n            p\n          \n        \n        =\n        \n          \n            \n              \n                \n                  s\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  s\n                  \n                    \n                      X\n                      \n                        2\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n              \n              2\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s_{p}={\\sqrt {\\frac {s_{X_{1}}^{2}+s_{X_{2}}^{2}}{2}}}.}\n  Here sp is the pooled standard deviation for n = n1 = n2 and s 2X1 and s 2X2 are the unbiased estimators of the population variance. The denominator of t is the standard error of the difference between two means.\nFor significance testing, the degrees of freedom for this test is 2n \u2212 2 where n is sample size.\n\n\n==== Equal or unequal sample sizes, similar variances (1/2 < sX1/sX2 < 2) ====\nThis test is used only when it can be assumed that the two distributions have the same variance. (When this assumption is violated, see below.) \nThe previous formulae are a special case of the formulae below, one recovers them when both samples are equal in size: n = n1 = n2.\nThe t statistic to test whether the means are different can be calculated as follows:\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  1\n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  2\n                \n              \n            \n            \n              \n                s\n                \n                  p\n                \n              \n              \u22c5\n              \n                \n                  \n                    \n                      1\n                      \n                        n\n                        \n                          1\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      1\n                      \n                        n\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{p}\\cdot {\\sqrt {{\\frac {1}{n_{1}}}+{\\frac {1}{n_{2}}}}}}}}\n  where\n\n  \n    \n      \n        \n          s\n          \n            p\n          \n        \n        =\n        \n          \n            \n              \n                \n                  (\n                  \n                    \n                      n\n                      \n                        1\n                      \n                    \n                    \u2212\n                    1\n                  \n                  )\n                \n                \n                  s\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  (\n                  \n                    \n                      n\n                      \n                        2\n                      \n                    \n                    \u2212\n                    1\n                  \n                  )\n                \n                \n                  s\n                  \n                    \n                      X\n                      \n                        2\n                      \n                    \n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                  n\n                  \n                    1\n                  \n                \n                +\n                \n                  n\n                  \n                    2\n                  \n                \n                \u2212\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s_{p}={\\sqrt {\\frac {\\left(n_{1}-1\\right)s_{X_{1}}^{2}+\\left(n_{2}-1\\right)s_{X_{2}}^{2}}{n_{1}+n_{2}-2}}}}\n  is the pooled standard deviation of the two samples: it is defined in this way so that its square is an unbiased estimator of the common variance whether or not the population means are the same. In these formulae, ni \u2212 1 is the number of degrees of freedom for each group, and the total sample size minus two (that is, n1 + n2 \u2212 2) is the total number of degrees of freedom, which is used in significance testing.\n\n\n==== Equal or unequal sample sizes, unequal variances (sX1 > 2sX2 or sX2 > 2sX1) ====\n\nThis test, also known as Welch's t-test, is used only when the two population variances are not assumed to be equal (the two sample sizes may or may not be equal) and hence must be estimated separately. The t statistic to test whether the population means are different is calculated as:\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  1\n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  2\n                \n              \n            \n            \n              s\n              \n                \n                  \n                    \u0394\n                    \u00af\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{s_{\\bar {\\Delta }}}}}\n  where\n\n  \n    \n      \n        \n          s\n          \n            \n              \n                \u0394\n                \u00af\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  s\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                \n                  n\n                  \n                    1\n                  \n                \n              \n            \n            +\n            \n              \n                \n                  s\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                \n                  n\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle s_{\\bar {\\Delta }}={\\sqrt {{\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}}}.}\n  Here si2 is the unbiased estimator of the variance of each of the two samples with ni = number of participants in group i (i = 1 or 2). In this case\n\n  \n    \n      \n        (\n        \n          s\n          \n            \n              \n                \u0394\n                \u00af\n              \n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\textstyle (s_{\\bar {\\Delta }})^{2}}\n  \nis not a pooled variance. For use in significance testing, the distribution of the test statistic is approximated as an ordinary Student's t-distribution with the degrees of freedom calculated using\n\n  \n    \n      \n        \n          d\n          .\n          f\n          .\n        \n        =\n        \n          \n            \n              \n                (\n                \n                  \n                    \n                      \n                        s\n                        \n                          1\n                        \n                        \n                          2\n                        \n                      \n                      \n                        n\n                        \n                          1\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      \n                        s\n                        \n                          2\n                        \n                        \n                          2\n                        \n                      \n                      \n                        n\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n            \n              \n                \n                  \n                    \n                      (\n                      \n                        \n                          s\n                          \n                            1\n                          \n                          \n                            2\n                          \n                        \n                        \n                          /\n                        \n                        \n                          n\n                          \n                            1\n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                  \n                    \n                      n\n                      \n                        1\n                      \n                    \n                    \u2212\n                    1\n                  \n                \n              \n              +\n              \n                \n                  \n                    \n                      (\n                      \n                        \n                          s\n                          \n                            2\n                          \n                          \n                            2\n                          \n                        \n                        \n                          /\n                        \n                        \n                          n\n                          \n                            2\n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                  \n                    \n                      n\n                      \n                        2\n                      \n                    \n                    \u2212\n                    1\n                  \n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {d.f.} ={\\frac {\\left({\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}\\right)^{2}}{{\\frac {\\left(s_{1}^{2}/n_{1}\\right)^{2}}{n_{1}-1}}+{\\frac {\\left(s_{2}^{2}/n_{2}\\right)^{2}}{n_{2}-1}}}}.}\n  This is known as the Welch\u2013Satterthwaite equation. The true distribution of the test statistic actually depends (slightly) on the two unknown population variances (see Behrens\u2013Fisher problem).\n\n\n=== Exact method for unequal variances and sample sizes ===\nThe test deals with the famous Behrens\u2013Fisher problem, i.e., comparing the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal, based on two independent samples. \nThe test is developed as an exact test that allows for unequal sample sizes and unequal variances of two populations. The exact property still holds even with small extremely small and unbalanced sample sizes (e.g. \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n        =\n        5\n        ,\n        \n          n\n          \n            2\n          \n        \n        =\n        50\n      \n    \n    {\\displaystyle n_{1}=5,n_{2}=50}\n  ).\nThe statistic to test whether the means are different can be calculated as follows:\nLet \n  \n    \n      \n        X\n        =\n        [\n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            m\n          \n        \n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X=[X_{1},X_{2},\\ldots ,X_{m}]^{T}}\n   and \n  \n    \n      \n        Y\n        =\n        [\n        \n          Y\n          \n            1\n          \n        \n        ,\n        \n          Y\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          Y\n          \n            n\n          \n        \n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle Y=[Y_{1},Y_{2},\\ldots ,Y_{n}]^{T}}\n   be the i.i.d. sample vectors (\n  \n    \n      \n        m\n        >\n        n\n      \n    \n    {\\displaystyle m>n}\n  ) from \n  \n    \n      \n        N\n        (\n        \n          \u03bc\n          \n            1\n          \n        \n        ,\n        \n          \u03c3\n          \n            1\n          \n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle N(\\mu _{1},\\sigma _{1}^{2})}\n   and \n  \n    \n      \n        N\n        (\n        \n          \u03bc\n          \n            2\n          \n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle N(\\mu _{2},\\sigma _{2}^{2})}\n   separately.\nLet \n  \n    \n      \n        (\n        \n          P\n          \n            T\n          \n        \n        \n          )\n          \n            n\n            \u00d7\n            n\n          \n        \n      \n    \n    {\\displaystyle (P^{T})_{n\\times n}}\n   be an \n  \n    \n      \n        n\n        \u00d7\n        n\n      \n    \n    {\\displaystyle n\\times n}\n   orthogonal matrix whose elements of the first row are all \n  \n    \n      \n        1\n        \n          /\n        \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 1/{\\sqrt {n}}}\n  , similarly, let \n  \n    \n      \n        (\n        \n          Q\n          \n            T\n          \n        \n        \n          )\n          \n            n\n            \u00d7\n            m\n          \n        \n      \n    \n    {\\displaystyle (Q^{T})_{n\\times m}}\n   be the first n rows of an \n  \n    \n      \n        m\n        \u00d7\n        m\n      \n    \n    {\\displaystyle m\\times m}\n   orthogonal matrix (whose elements of the first row are all \n  \n    \n      \n        1\n        \n          /\n        \n        \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle 1/{\\sqrt {m}}}\n  ).\nThen \n  \n    \n      \n        Z\n        :=\n        (\n        \n          Q\n          \n            T\n          \n        \n        \n          )\n          \n            n\n            \u00d7\n            m\n          \n        \n        X\n        \n          /\n        \n        \n          \n            m\n          \n        \n        \u2212\n        (\n        \n          P\n          \n            T\n          \n        \n        \n          )\n          \n            n\n            \u00d7\n            n\n          \n        \n        Y\n        \n          /\n        \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle Z:=(Q^{T})_{n\\times m}X/{\\sqrt {m}}-(P^{T})_{n\\times n}Y/{\\sqrt {n}}}\n   is an n-dimensional normal random vector.\n\n  \n    \n      \n        Z\n        \u223c\n        N\n        (\n        (\n        \n          \u03bc\n          \n            1\n          \n        \n        \u2212\n        \n          \u03bc\n          \n            2\n          \n        \n        ,\n        0\n        ,\n        .\n        .\n        .\n        ,\n        0\n        \n          )\n          \n            T\n          \n        \n        ,\n        (\n        \n          \n            \n              \u03c3\n              \n                1\n              \n              \n                2\n              \n            \n            m\n          \n        \n        +\n        \n          \n            \n              \u03c3\n              \n                2\n              \n              \n                2\n              \n            \n            n\n          \n        \n        )\n        \n          I\n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle Z\\sim N((\\mu _{1}-\\mu _{2},0,...,0)^{T},({\\frac {\\sigma _{1}^{2}}{m}}+{\\frac {\\sigma _{2}^{2}}{n}})I_{n}).}\n  From the above distribution we see that \n\n  \n    \n      \n        \n          Z\n          \n            1\n          \n        \n        \u2212\n        (\n        \n          \u03bc\n          \n            1\n          \n        \n        \u2212\n        \n          \u03bc\n          \n            2\n          \n        \n        )\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \n            \n              \u03c3\n              \n                1\n              \n              \n                2\n              \n            \n            m\n          \n        \n        +\n        \n          \n            \n              \u03c3\n              \n                2\n              \n              \n                2\n              \n            \n            n\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle Z_{1}-(\\mu _{1}-\\mu _{2})\\sim N(0,{\\frac {\\sigma _{1}^{2}}{m}}+{\\frac {\\sigma _{2}^{2}}{n}}),}\n  \n  \n    \n      \n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  2\n                \n                \n                  n\n                \n              \n              \n                Z\n                \n                  i\n                \n                \n                  2\n                \n              \n            \n            \n              n\n              \u2212\n              1\n            \n          \n        \n        \u223c\n        \n          \n            \n              \u03c7\n              \n                n\n                \u2212\n                1\n              \n              \n                2\n              \n            \n            \n              n\n              \u2212\n              1\n            \n          \n        \n        \u00d7\n        (\n        \n          \n            \n              \u03c3\n              \n                1\n              \n              \n                2\n              \n            \n            m\n          \n        \n        +\n        \n          \n            \n              \u03c3\n              \n                2\n              \n              \n                2\n              \n            \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\frac {\\sum _{i=2}^{n}Z_{i}^{2}}{n-1}}\\sim {\\frac {\\chi _{n-1}^{2}}{n-1}}\\times ({\\frac {\\sigma _{1}^{2}}{m}}+{\\frac {\\sigma _{2}^{2}}{n}})}\n  \n  \n    \n      \n        \n          Z\n          \n            1\n          \n        \n        \u2212\n        (\n        \n          \u03bc\n          \n            1\n          \n        \n        \u2212\n        \n          \u03bc\n          \n            2\n          \n        \n        )\n        \u22a5\n        \n          \u2211\n          \n            i\n            =\n            2\n          \n          \n            n\n          \n        \n        \n          Z\n          \n            i\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle Z_{1}-(\\mu _{1}-\\mu _{2})\\perp \\sum _{i=2}^{n}Z_{i}^{2}.}\n  \n  \n    \n      \n        \n          T\n          \n            e\n          \n        \n        :=\n        \n          \n            \n              \n                Z\n                \n                  1\n                \n              \n              \u2212\n              (\n              \n                \u03bc\n                \n                  1\n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  2\n                \n              \n              )\n            \n            \n              (\n              \n                \u2211\n                \n                  i\n                  =\n                  2\n                \n                \n                  n\n                \n              \n              \n                Z\n                \n                  i\n                \n                \n                  2\n                \n              \n              )\n              \n                /\n              \n              (\n              n\n              \u2212\n              1\n              )\n            \n          \n        \n        \u223c\n        \n          t\n          \n            n\n            \u2212\n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle T_{e}:={\\frac {Z_{1}-(\\mu _{1}-\\mu _{2})}{\\sqrt {(\\sum _{i=2}^{n}Z_{i}^{2})/(n-1)}}}\\sim t_{n-1}.}\n  \n\n\n=== Dependent t-test for paired samples ===\nThis test is used when the samples are dependent; that is, when there is only one sample that has been tested twice (repeated measures) or when there are two samples that have been matched or \"paired\". This is an example of a paired difference test. The t statistic is calculated as\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  D\n                \n              \n              \u2212\n              \n                \u03bc\n                \n                  0\n                \n              \n            \n            \n              \n                s\n                \n                  D\n                \n              \n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {X}}_{D}-\\mu _{0}}{s_{D}/{\\sqrt {n}}}}}\n  where \n  \n    \n      \n        \n          \n            \n              \n                X\n                \u00af\n              \n            \n          \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}_{D}}\n   and \n  \n    \n      \n        \n          s\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle s_{D}}\n   are the average and standard deviation of the differences between all pairs. The pairs are e.g. either one person's pre-test and post-test scores or between-pairs of persons matched into meaningful groups (for instance drawn from the same family or age group: see table). The constant \u03bc0 is zero if we want to test whether the average of the difference is significantly different. The degree of freedom used is n \u2212 1, where n represents the number of pairs.\n\n\n== Worked examples ==\nLet A1 denote a set obtained by drawing a random sample of six measurements:\n\n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        =\n        {\n        30.02\n        ,\n         \n        29.99\n        ,\n         \n        30.11\n        ,\n         \n        29.97\n        ,\n         \n        30.01\n        ,\n         \n        29.99\n        }\n      \n    \n    {\\displaystyle A_{1}=\\{30.02,\\ 29.99,\\ 30.11,\\ 29.97,\\ 30.01,\\ 29.99\\}}\n  and let A2 denote a second set obtained similarly:\n\n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n        =\n        {\n        29.89\n        ,\n         \n        29.93\n        ,\n         \n        29.72\n        ,\n         \n        29.98\n        ,\n         \n        30.02\n        ,\n         \n        29.98\n        }\n      \n    \n    {\\displaystyle A_{2}=\\{29.89,\\ 29.93,\\ 29.72,\\ 29.98,\\ 30.02,\\ 29.98\\}}\n  These could be, for example, the weights of screws that were chosen out of a bucket.\nWe will carry out tests of the null hypothesis that the means of the populations from which the two samples were taken are equal.\nThe difference between the two sample means, each denoted by Xi, which appears in the numerator for all the two-sample testing approaches discussed above, is\n\n  \n    \n      \n        \n          \n            \n              \n                X\n                \u00af\n              \n            \n          \n          \n            1\n          \n        \n        \u2212\n        \n          \n            \n              \n                X\n                \u00af\n              \n            \n          \n          \n            2\n          \n        \n        =\n        0.095.\n      \n    \n    {\\displaystyle {\\bar {X}}_{1}-{\\bar {X}}_{2}=0.095.}\n  The sample standard deviations for the two samples are approximately 0.05 and 0.11, respectively. For such small samples, a test of equality between the two population variances would not be very powerful.  Since the sample sizes are equal, the two forms of the two-sample t-test will perform similarly in this example.\n\n\n=== Unequal variances ===\nIf the approach for unequal variances (discussed above) is followed, the results are\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  s\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                \n                  n\n                  \n                    1\n                  \n                \n              \n            \n            +\n            \n              \n                \n                  s\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                \n                  n\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        \u2248\n        0.04849\n      \n    \n    {\\displaystyle {\\sqrt {{\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}}}\\approx 0.04849}\n  and the degrees of freedom\n\n  \n    \n      \n        \n          d.f.\n        \n        \u2248\n        7.031.\n      \n    \n    {\\displaystyle {\\text{d.f.}}\\approx 7.031.}\n  The test statistic is approximately 1.959, which gives a two-tailed test p-value of 0.09077.\n\n\n=== Equal variances ===\nIf the approach for equal variances (discussed above) is followed, the results are\n\n  \n    \n      \n        \n          s\n          \n            p\n          \n        \n        \u2248\n        0.08396\n      \n    \n    {\\displaystyle s_{p}\\approx 0.08396}\n  and the degrees of freedom\n\n  \n    \n      \n        \n          d.f.\n        \n        =\n        10.\n      \n    \n    {\\displaystyle {\\text{d.f.}}=10.}\n  The test statistic is approximately equal to 1.959, which gives a two-tailed p-value of 0.07857.\n\n\n== Related statistical tests ==\n\n\n=== Alternatives to the t-test for location problems ===\nThe t-test provides an exact test for the equality of the means of two i.i.d. normal populations with unknown, but equal, variances. (Welch's t-test is a nearly exact test for the case where the data are normal but the variances may differ.)  For moderately large samples and a one tailed test, the t-test is relatively robust to moderate violations of the normality assumption. In large enough samples, the t-test asymptotically approaches the z-test, and becomes robust even to large deviations from normality.If the data are substantially non-normal and the sample size is small, the t-test can give misleading results. See Location test for Gaussian scale mixture distributions for some theory related to one particular family of non-normal distributions.\nWhen the normality assumption does not hold, a non-parametric alternative to the t-test may have better statistical power. However, when data are non-normal with differing variances between groups, a t-test may have better type-1 error control than some non-parametric alternatives. Furthermore, non-parametric methods, such as the Mann-Whitney U test discussed below, typically do not test for a difference of means, so should be used carefully if a difference of means is of primary scientific interest. For example, Mann-Whitney U test will keep the type 1 error at the desired level alpha if both groups have the same distribution. It will also have power in detecting an alternative by which group B has the same distribution as A but after some shift by a constant (in which case there would indeed be a difference in the means of the two groups). However, there could be cases where group A and B will have different distributions but with the same means (such as two distributions, one with positive skewness and the other with a negative one, but shifted so to have the same means). In such cases, MW could have more than alpha level power in rejecting the Null hypothesis but attributing the interpretation of difference in means to such a result would be incorrect.\nIn the presence of an outlier, the t-test is not robust. For example, for two independent samples when the data distributions are asymmetric  (that is, the distributions are skewed) or the distributions have large tails, then the Wilcoxon rank-sum test (also known as the Mann\u2013Whitney U test) can have three to four times higher power than the t-test. The nonparametric counterpart to the paired samples t-test is the Wilcoxon signed-rank test for paired samples. For a discussion on choosing between the t-test and nonparametric alternatives, see Lumley, et al. (2002).One-way analysis of variance (ANOVA) generalizes the two-sample t-test when the data belong to more than two groups.\n\n\n=== A design which includes both paired observations and independent observations ===\nWhen both paired observations and independent observations are present in the two sample design, assuming  data are missing completely at random (MCAR), the paired observations or independent observations may be discarded in order to proceed with the standard tests above. Alternatively making use of all of the available data, assuming normality and MCAR, the generalized partially overlapping samples t-test could be used.\n\n\n=== Multivariate testing ===\n\nA generalization of Student's t statistic, called Hotelling's t-squared statistic, allows for the testing of hypotheses on multiple (often correlated) measures within the same sample. For instance, a researcher might submit a number of subjects to a personality test consisting of multiple personality scales (e.g. the Minnesota Multiphasic Personality Inventory). Because measures of this type are usually positively correlated, it is not advisable to conduct separate univariate t-tests to test hypotheses, as these would neglect the covariance among measures and inflate the chance of falsely rejecting at least one hypothesis (Type I error). In this case a single multivariate test is preferable for hypothesis testing. Fisher's Method for combining multiple tests with alpha reduced for positive correlation among tests is one. Another is Hotelling's T2 statistic follows a T2 distribution. However, in practice the distribution is rarely used, since tabulated values for T2 are hard to find. Usually, T2 is converted instead to an F statistic.\nFor a one-sample multivariate test, the hypothesis is that the mean vector (\u03bc) is equal to a given vector (\u03bc0). The test statistic is Hotelling's t2:\n\n  \n    \n      \n        \n          t\n          \n            2\n          \n        \n        =\n        n\n        (\n        \n          \n            \n              \n                x\n              \n              \u00af\n            \n          \n        \n        \u2212\n        \n          \n            \n              \u03bc\n            \n            \n              0\n            \n          \n        \n        \n          )\n          \u2032\n        \n        \n          \n            \n              S\n            \n          \n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            \n              \n                x\n              \n              \u00af\n            \n          \n        \n        \u2212\n        \n          \n            \n              \u03bc\n            \n            \n              0\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle t^{2}=n({\\bar {\\mathbf {x} }}-{{\\boldsymbol {\\mu }}_{0}})'{\\mathbf {S} }^{-1}({\\bar {\\mathbf {x} }}-{{\\boldsymbol {\\mu }}_{0}})}\n  where n is the sample size, x is the vector of column means and S is an m \u00d7 m sample covariance matrix.\nFor a two-sample multivariate test, the hypothesis is that the mean vectors (\u03bc1, \u03bc2) of two samples are equal. The test statistic is Hotelling's two-sample t2:\n\n  \n    \n      \n        \n          t\n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  1\n                \n              \n              \n                n\n                \n                  2\n                \n              \n            \n            \n              \n                n\n                \n                  1\n                \n              \n              +\n              \n                n\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        x\n                      \n                      \u00af\n                    \n                  \n                \n                \n                  1\n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      \n                        x\n                      \n                      \u00af\n                    \n                  \n                \n                \n                  2\n                \n              \n            \n            )\n          \n          \u2032\n        \n        \n          \n            \n              \n                S\n              \n              \n                pooled\n              \n            \n          \n          \n            \u2212\n            1\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      x\n                    \n                    \u00af\n                  \n                \n              \n              \n                1\n              \n            \n            \u2212\n            \n              \n                \n                  \n                    \n                      x\n                    \n                    \u00af\n                  \n                \n              \n              \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle t^{2}={\\frac {n_{1}n_{2}}{n_{1}+n_{2}}}\\left({\\bar {\\mathbf {x} }}_{1}-{\\bar {\\mathbf {x} }}_{2}\\right)'{\\mathbf {S} _{\\text{pooled}}}^{-1}\\left({\\bar {\\mathbf {x} }}_{1}-{\\bar {\\mathbf {x} }}_{2}\\right).}\n  \n\n\n=== The two-sample t-test is a special case of simple linear regression ===\nThe two-sample t-test is a special case of simple linear regression as illustrated by the following example. \nA clinical trial examines 6 patients given drug or placebo. 3 patients get 0 units of drug (the placebo group). 3 patients get 1 unit of drug (the active treatment group). At the end of treatment, the researchers measure the change from baseline in the number of words that each patient can recall in a memory test. \n\nData and code are given for the analysis using the R programming language with the t.test and lmfunctions for the t-test and linear regression. Here are the (fictitious) data generated in R.\n  \n> word.recall.data=data.frame(drug.dose=c(0,0,0,1,1,1), word.recall=c(1,2,3,5,6,7)) \n\nPerform the t-test. Notice that the assumption of equal variance, var.equal=T, is required to make the analysis exactly equivalent to simple linear regression. \n\nRunning the R code gives the following results.\n\nThe mean word.recall in the 0 drug.dose group is 2.\nThe mean word.recall in the 1 drug.dose group is 6.\nThe difference between treatment groups in the mean word.recall is 6 \u2013 2 = 4.\nThe difference in word.recall between drug doses is significant (p=0.00805).Perform a linear regression of the same data. Calculations may be performed using the R function lm() for a linear model. \n\nThe linear regression provides a table of coefficients and p-values.\n\nThe table of coefficients gives the following results.\n\nThe estimate value of 2 for the intercept is the mean value of the word recall when the drug dose is 0.\nThe estimate value of 4 for the drug dose indicates that for a 1-unit change in drug dose (from 0 to 1) there is a 4-unit change in mean word recall (from 2 to 6). This is the slope of the line joining the two group means.\nThe p-value that the slope of 4 is different from 0 is p = 0.00805.The coefficients for the linear regression specify the slope and intercept of the line that joins the two group means, as illustrated in the graph. The intercept is 2 and the slope is 4.\n\nCompare the result from the linear regression to the result from the t-test.\n\nFrom the t-test, the difference between the group means is 6-2=4.\nFrom the regression, the slope is also 4 indicating that a 1-unit change in drug dose (from 0 to 1) gives a 4-unit change in mean word recall (from 2 to 6).\nThe t-test p-value for the difference in means, and the regression p-value for the slope, are both 0.00805. The methods give identical results.This example shows that, for the special case of a simple linear regression where there is a single x-variable that has values 0 and 1, the t-test gives the same results as the linear regression. The relationship can also be shown algebraically.\nRecognizing this relationship between the t-test and linear regression facilitates the use of multiple linear regression and multi-way analysis of variance . These alternatives to t-tests allow for the inclusion of additional explanatory variables  that are associated with the response. Including such additional explanatory variables using regression or anova reduces the otherwise unexplained variance, and commonly yields greater power to detect differences than do two-sample t-tests.\n\n\n== Software implementations ==\nMany spreadsheet programs and statistics packages, such as QtiPlot, LibreOffice Calc, Microsoft Excel, SAS, SPSS, Stata, DAP, gretl, R, Python, PSPP, MATLAB and Minitab, include implementations of Student's t-test.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== Further reading ==\nBoneau, C. Alan (1960). \"The effects of violations of assumptions underlying the t test\". Psychological Bulletin. 57 (1): 49\u201364. doi:10.1037/h0041412. PMID 13802482.\nEdgell, Stephen E.; Noon, Sheila M. (1984). \"Effect of violation of normality on the t test of the correlation coefficient\". Psychological Bulletin. 95 (3): 576\u2013583. doi:10.1037/0033-2909.95.3.576.\n\n\n== External links ==\n\n\"Student test\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nTrochim, William M.K. \"The T-Test\", Research Methods Knowledge Base, conjoint.ly\nEconometrics lecture (topic: hypothesis testing) on YouTube by Mark Thoma", "Signal-to-noise ratio": "Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.\nSNR is an important parameter that affects the performance and quality of systems that process or transmit signals, such as communication systems, audio systems, radar systems, imaging systems, and data acquisition systems. A high SNR means that the signal is clear and easy to detect or interpret, while a low SNR means that the signal is corrupted or obscured by noise and may be difficult to distinguish or recover. SNR can be improved by various methods, such as increasing the signal strength, reducing the noise level, filtering out unwanted noise, or using error correction techniques.\nSNR also determines the maximum possible amount of data that can be transmitted reliably over a given channel, which depends on its bandwidth and SNR. This relationship is described by the Shannon\u2013Hartley theorem, which is a fundamental law of information theory.\nSNR can be calculated using different formulas depending on how the signal and noise are measured and defined. The most common way to express SNR is in decibels, which is a logarithmic scale that makes it easier to compare large or small values. Other definitions of SNR may use different factors or bases for the logarithm, depending on the context and application.\n\n\n== Definition ==\nSignal-to-noise ratio is defined as the ratio of the power of a signal (meaningful input) to the power of background noise (meaningless or unwanted input):\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              P\n              \n                \n                  s\n                  i\n                  g\n                  n\n                  a\n                  l\n                \n              \n            \n            \n              P\n              \n                \n                  n\n                  o\n                  i\n                  s\n                  e\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {P_{\\mathrm {signal} }}{P_{\\mathrm {noise} }}},}\n  where P is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.\nDepending on whether the signal is a constant (s) or a random variable (S), the signal-to-noise ratio for random noise N becomes:\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              s\n              \n                2\n              \n            \n            \n              \n                E\n              \n              [\n              \n                N\n                \n                  2\n                \n              \n              ]\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {s^{2}}{\\mathrm {E} [N^{2}]}}}\n  where E refers to the expected value, i.e. in this case the mean square of N,\nor\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              \n                E\n              \n              [\n              \n                S\n                \n                  2\n                \n              \n              ]\n            \n            \n              \n                E\n              \n              [\n              \n                N\n                \n                  2\n                \n              \n              ]\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {\\mathrm {E} [S^{2}]}{\\mathrm {E} [N^{2}]}}}\n  If the noise has expected value of zero, as is common, the denominator is its variance, the square of its standard deviation \u03c3N.\n\nThe signal and the noise must be measured the same way, for example as voltages across the same impedance. The root mean squares can alternatively be used in the ratio:\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              P\n              \n                \n                  s\n                  i\n                  g\n                  n\n                  a\n                  l\n                \n              \n            \n            \n              P\n              \n                \n                  n\n                  o\n                  i\n                  s\n                  e\n                \n              \n            \n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  A\n                  \n                    \n                      s\n                      i\n                      g\n                      n\n                      a\n                      l\n                    \n                  \n                \n                \n                  A\n                  \n                    \n                      n\n                      o\n                      i\n                      s\n                      e\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {P_{\\mathrm {signal} }}{P_{\\mathrm {noise} }}}=\\left({\\frac {A_{\\mathrm {signal} }}{A_{\\mathrm {noise} }}}\\right)^{2},}\n  where A is root mean square (RMS) amplitude (for example, RMS voltage).\n\n\n=== Decibels ===\nBecause many signals have a very wide dynamic range, signals are often expressed using the logarithmic decibel scale. Based upon the definition of decibel, signal and noise may be expressed in decibels (dB) as\n\n  \n    \n      \n        \n          P\n          \n            \n              s\n              i\n              g\n              n\n              a\n              l\n              ,\n              d\n              B\n            \n          \n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            P\n            \n              \n                s\n                i\n                g\n                n\n                a\n                l\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P_{\\mathrm {signal,dB} }=10\\log _{10}\\left(P_{\\mathrm {signal} }\\right)}\n  and\n\n  \n    \n      \n        \n          P\n          \n            \n              n\n              o\n              i\n              s\n              e\n              ,\n              d\n              B\n            \n          \n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            P\n            \n              \n                n\n                o\n                i\n                s\n                e\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle P_{\\mathrm {noise,dB} }=10\\log _{10}\\left(P_{\\mathrm {noise} }\\right).}\n  In a similar manner, SNR may be expressed in decibels as\n\n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            S\n            N\n            R\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} =10\\log _{10}\\left(\\mathrm {SNR} \\right).}\n  Using the definition of SNR\n\n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            \n              \n                P\n                \n                  \n                    s\n                    i\n                    g\n                    n\n                    a\n                    l\n                  \n                \n              \n              \n                P\n                \n                  \n                    n\n                    o\n                    i\n                    s\n                    e\n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} =10\\log _{10}\\left({\\frac {P_{\\mathrm {signal} }}{P_{\\mathrm {noise} }}}\\right).}\n  Using the quotient rule for logarithms\n\n  \n    \n      \n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            \n              \n                P\n                \n                  \n                    s\n                    i\n                    g\n                    n\n                    a\n                    l\n                  \n                \n              \n              \n                P\n                \n                  \n                    n\n                    o\n                    i\n                    s\n                    e\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            P\n            \n              \n                s\n                i\n                g\n                n\n                a\n                l\n              \n            \n          \n          )\n        \n        \u2212\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            P\n            \n              \n                n\n                o\n                i\n                s\n                e\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle 10\\log _{10}\\left({\\frac {P_{\\mathrm {signal} }}{P_{\\mathrm {noise} }}}\\right)=10\\log _{10}\\left(P_{\\mathrm {signal} }\\right)-10\\log _{10}\\left(P_{\\mathrm {noise} }\\right).}\n  Substituting the definitions of SNR, signal, and noise in decibels into the above equation results in an important formula for calculating the signal to noise ratio in decibels, when the signal and noise are also in decibels:\n\n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        \n          \n            P\n            \n              \n                s\n                i\n                g\n                n\n                a\n                l\n                ,\n                d\n                B\n              \n            \n          \n          \u2212\n          \n            P\n            \n              \n                n\n                o\n                i\n                s\n                e\n                ,\n                d\n                B\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} ={P_{\\mathrm {signal,dB} }-P_{\\mathrm {noise,dB} }}.}\n  In the above formula, P is measured in units of power, such as watts (W) or milliwatts (mW), and the signal-to-noise ratio is a pure number.\nHowever, when the signal and noise are measured in volts (V) or amperes (A), which are measures of amplitude, they must first be squared to obtain a quantity proportional to power, as shown below:\n\n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          [\n          \n            \n              (\n              \n                \n                  \n                    A\n                    \n                      \n                        s\n                        i\n                        g\n                        n\n                        a\n                        l\n                      \n                    \n                  \n                  \n                    A\n                    \n                      \n                        n\n                        o\n                        i\n                        s\n                        e\n                      \n                    \n                  \n                \n              \n              )\n            \n            \n              2\n            \n          \n          ]\n        \n        =\n        20\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        \n          (\n          \n            \n              \n                A\n                \n                  \n                    s\n                    i\n                    g\n                    n\n                    a\n                    l\n                  \n                \n              \n              \n                A\n                \n                  \n                    n\n                    o\n                    i\n                    s\n                    e\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        2\n        \n          (\n          \n            \n              A\n              \n                \n                  s\n                  i\n                  g\n                  n\n                  a\n                  l\n                  ,\n                  d\n                  B\n                \n              \n            \n            \u2212\n            \n              A\n              \n                \n                  n\n                  o\n                  i\n                  s\n                  e\n                  ,\n                  d\n                  B\n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} =10\\log _{10}\\left[\\left({\\frac {A_{\\mathrm {signal} }}{A_{\\mathrm {noise} }}}\\right)^{2}\\right]=20\\log _{10}\\left({\\frac {A_{\\mathrm {signal} }}{A_{\\mathrm {noise} }}}\\right)=2\\left({A_{\\mathrm {signal,dB} }-A_{\\mathrm {noise,dB} }}\\right).}\n  \n\n\n=== Dynamic range ===\nThe concepts of signal-to-noise ratio and dynamic range are closely related. Dynamic range measures the ratio between the strongest un-distorted signal on a channel and the minimum discernible signal, which for most purposes is the noise level. SNR measures the ratio between an arbitrary signal level (not necessarily the most powerful signal possible) and noise. Measuring signal-to-noise ratios requires the selection of a representative or reference signal. In audio engineering, the reference signal is usually a sine wave at a standardized nominal or alignment level, such as 1 kHz at +4 dBu (1.228 VRMS).\nSNR is usually taken to indicate an average signal-to-noise ratio, as it is possible that instantaneous signal-to-noise ratios will be considerably different. The concept can be understood as normalizing the noise level to 1 (0 dB) and measuring how far the signal 'stands out'.\n\n\n=== Difference from conventional power ===\nIn physics, the average power of an AC signal is defined as the average value of voltage times current; for resistive (non-reactive) circuits, where voltage and current are in phase, this is equivalent to the product of the rms voltage and current:\n\n  \n    \n      \n        \n          P\n        \n        =\n        \n          V\n          \n            \n              r\n              m\n              s\n            \n          \n        \n        \n          I\n          \n            \n              r\n              m\n              s\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {P} =V_{\\mathrm {rms} }I_{\\mathrm {rms} }}\n  \n  \n    \n      \n        \n          P\n        \n        =\n        \n          \n            \n              V\n              \n                \n                  r\n                  m\n                  s\n                \n              \n              \n                2\n              \n            \n            R\n          \n        \n        =\n        \n          I\n          \n            \n              r\n              m\n              s\n            \n          \n          \n            2\n          \n        \n        R\n      \n    \n    {\\displaystyle \\mathrm {P} ={\\frac {V_{\\mathrm {rms} }^{2}}{R}}=I_{\\mathrm {rms} }^{2}R}\n  But in signal processing and communication, one usually assumes that \n  \n    \n      \n        R\n        =\n        1\n        \u03a9\n      \n    \n    {\\displaystyle R=1\\Omega }\n    so that factor is usually not included while measuring power or energy of a signal. This may cause some confusion among readers, but the resistance factor is not significant for typical operations performed in signal processing, or for computing power ratios. For most cases, the power of a signal would be considered to be simply\n\n  \n    \n      \n        \n          P\n        \n        =\n        \n          V\n          \n            \n              r\n              m\n              s\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {P} =V_{\\mathrm {rms} }^{2}}\n  \n\n\n== Alternative definition ==\nAn alternative definition of SNR is as the reciprocal of the coefficient of variation, i.e., the ratio of mean to standard deviation of a signal or measurement:\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \u03bc\n            \u03c3\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {\\mu }{\\sigma }}}\n  where \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the signal mean or expected value and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is the standard deviation of the noise, or an estimate thereof. Notice that such an alternative definition is only useful for variables that are always non-negative (such as photon counts and luminance), and it is only an approximation since \n  \n    \n      \n        E\n        \u2061\n        \n          [\n          \n            X\n            \n              2\n            \n          \n          ]\n        \n        =\n        \n          \u03c3\n          \n            2\n          \n        \n        +\n        \n          \u03bc\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} \\left[X^{2}\\right]=\\sigma ^{2}+\\mu ^{2}}\n  . It is commonly used in image processing, where the SNR of an image is usually calculated as the ratio of the mean pixel value to the standard deviation of the pixel values over a given neighborhood.\nSometimes SNR is defined as the square of the alternative definition above, in which case it is equivalent to the more common definition:\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              \u03bc\n              \n                2\n              \n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {SNR} ={\\frac {\\mu ^{2}}{\\sigma ^{2}}}}\n  This definition is closely related to the sensitivity index or d', when assuming that the signal has two states separated by signal amplitude \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  , and the noise standard deviation \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   does not change between the two states.\nThe Rose criterion (named after Albert Rose) states that an SNR of at least 5 is needed to be able to distinguish image features with certainty. An SNR less than 5 means less than 100% certainty in identifying image details.Yet another alternative, very specific, and distinct definition of SNR is employed to characterize sensitivity of imaging systems; see Signal-to-noise ratio (imaging).\nRelated measures are the \"contrast ratio\" and the \"contrast-to-noise ratio\".\n\n\n== Modulation system measurements ==\n\n\n=== Amplitude modulation ===\nChannel signal-to-noise ratio is given by\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              C\n              ,\n              A\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  C\n                \n                \n                  2\n                \n              \n              (\n              1\n              +\n              \n                k\n                \n                  a\n                \n                \n                  2\n                \n              \n              P\n              )\n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{C,AM}} ={\\frac {A_{C}^{2}(1+k_{a}^{2}P)}{2WN_{0}}}}\n  where W is the bandwidth and \n  \n    \n      \n        \n          k\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle k_{a}}\n   is modulation index\nOutput signal-to-noise ratio (of AM receiver) is given by\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              O\n              ,\n              A\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  c\n                \n                \n                  2\n                \n              \n              \n                k\n                \n                  a\n                \n                \n                  2\n                \n              \n              P\n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{O,AM}} ={\\frac {A_{c}^{2}k_{a}^{2}P}{2WN_{0}}}}\n  \n\n\n=== Frequency modulation ===\nChannel signal-to-noise ratio is given by\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              C\n              ,\n              F\n              M\n            \n          \n        \n        =\n        \n          \n            \n              A\n              \n                c\n              \n              \n                2\n              \n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{C,FM}} ={\\frac {A_{c}^{2}}{2WN_{0}}}}\n  Output signal-to-noise ratio is given by\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              O\n              ,\n              F\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  c\n                \n                \n                  2\n                \n              \n              \n                k\n                \n                  f\n                \n                \n                  2\n                \n              \n              P\n            \n            \n              2\n              \n                N\n                \n                  0\n                \n              \n              \n                W\n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{O,FM}} ={\\frac {A_{c}^{2}k_{f}^{2}P}{2N_{0}W^{3}}}}\n  \n\n\n== Noise reduction ==\n\nAll real measurements are disturbed by noise. This includes electronic noise, but can also include external events that affect the measured phenomenon \u2014 wind, vibrations, the gravitational attraction of the moon, variations of temperature, variations of humidity, etc., depending on what is measured and of the sensitivity of the device. It is often possible to reduce the noise by controlling the environment. \nInternal electronic noise of measurement systems can be reduced through the use of low-noise amplifiers.\nWhen the characteristics of the noise are known and are different from the signal, it is possible to use a filter to reduce the noise. For example, a lock-in amplifier can extract a narrow bandwidth signal from broadband noise a million times stronger.\nWhen the signal is constant or periodic and the noise is random, it is possible to enhance the SNR by averaging the measurements. In this case the noise goes down as the square root of the number of averaged samples.\n\n\n== Digital signals ==\nWhen a measurement is digitized, the number of bits used to represent the measurement determines the maximum possible signal-to-noise ratio. This is because the minimum possible noise level is the error caused by the quantization of the signal, sometimes called quantization noise. This noise level is non-linear and signal-dependent; different calculations exist for different signal models. Quantization noise is modeled as an analog error signal summed with the signal before quantization (\"additive noise\").\nThis theoretical maximum SNR assumes a perfect input signal. If the input signal is already noisy (as is usually the case), the signal's noise may be larger than the quantization noise. Real analog-to-digital converters also have other sources of noise that further decrease the SNR compared to the theoretical maximum from the idealized quantization noise, including the intentional addition of dither.\nAlthough noise levels in a digital system can be expressed using SNR, it is more common to use Eb/No, the energy per bit per noise power spectral density.\nThe modulation error ratio (MER) is a measure of the SNR in a digitally modulated signal.\n\n\n=== Fixed point ===\n\nFor n-bit integers with equal distance between quantization levels (uniform quantization) the dynamic range (DR) is also determined.\nAssuming a uniform distribution of input signal values, the quantization noise is a uniformly distributed random signal with a peak-to-peak amplitude of one quantization level, making the amplitude ratio 2n/1. The formula is then:\n\n  \n    \n      \n        \n          D\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        20\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        (\n        \n          2\n          \n            n\n          \n        \n        )\n        \u2248\n        6.02\n        \u22c5\n        n\n      \n    \n    {\\displaystyle \\mathrm {DR_{dB}} =\\mathrm {SNR_{dB}} =20\\log _{10}(2^{n})\\approx 6.02\\cdot n}\n  This relationship is the origin of statements like \"16-bit audio has a dynamic range of 96 dB\". Each extra quantization bit increases the dynamic range by roughly 6 dB.\nAssuming a full-scale sine wave signal (that is, the quantizer is designed such that it has the same minimum and maximum values as the input signal), the quantization noise approximates a sawtooth wave with peak-to-peak amplitude of one quantization level and uniform distribution. In this case, the SNR is approximately\n\n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        \u2248\n        20\n        \n          log\n          \n            10\n          \n        \n        \u2061\n        (\n        \n          2\n          \n            n\n          \n        \n        \n          \n            \n              \n                3\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        )\n        \u2248\n        6.02\n        \u22c5\n        n\n        +\n        1.761\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} \\approx 20\\log _{10}(2^{n}{\\textstyle {\\sqrt {3/2}}})\\approx 6.02\\cdot n+1.761}\n  \n\n\n=== Floating point ===\nFloating-point numbers provide a way to trade off signal-to-noise ratio for an increase in dynamic range. For n bit floating-point numbers, with n-m bits in the mantissa and m bits in the exponent:\n\n  \n    \n      \n        \n          D\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        6.02\n        \u22c5\n        \n          2\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {DR_{dB}} =6.02\\cdot 2^{m}}\n  \n  \n    \n      \n        \n          S\n          N\n          \n            R\n            \n              d\n              B\n            \n          \n        \n        =\n        6.02\n        \u22c5\n        (\n        n\n        \u2212\n        m\n        )\n      \n    \n    {\\displaystyle \\mathrm {SNR_{dB}} =6.02\\cdot (n-m)}\n  Note that the dynamic range is much larger than fixed-point, but at a cost of a worse signal-to-noise ratio. This makes floating-point preferable in situations where the dynamic range is large or unpredictable. Fixed-point's simpler implementations can be used with no signal quality disadvantage in systems where dynamic range is less than 6.02m. The very large dynamic range of floating-point can be a disadvantage, since it requires more forethought in designing algorithms.\n\n\n== Optical signals ==\nOptical signals have a carrier frequency (about 200 THz and more) that is much higher than the modulation frequency. This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20 dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.\n\n\n== Types and abbreviations ==\nSignal to noise ratio may be abbreviated as SNR and less commonly as S/N. PSNR stands for peak signal-to-noise ratio. GSNR stands for geometric signal-to-noise ratio. SINR is the signal-to-interference-plus-noise ratio.\n\n\n== Other uses ==\nWhile SNR is commonly quoted for electrical signals, it can be applied to any form of signal, for example isotope levels in an ice core, biochemical signaling between cells, or financial trading signals. The term is sometimes used metaphorically to refer to the ratio of useful information to false or irrelevant data in a conversation or exchange. For example, in online discussion forums and other online communities, off-topic posts and spam are regarded as noise that interferes with the signal of appropriate discussion.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nWalt Kester, Taking the Mystery out of the Infamous Formula,\"SNR = 6.02N + 1.76dB,\" and Why You Should Care (PDF), Analog Devices, archived (PDF) from the original on 2022-10-09, retrieved 2019-04-10\nADC and DAC Glossary \u2013 Maxim Integrated Products\nUnderstand SINAD, ENOB, SNR, THD, THD + N, and SFDR so you don't get lost in the noise floor \u2013 Analog Devices\nThe Relationship of dynamic range to data word size in digital audio processing\nCalculation of signal-to-noise ratio, noise voltage, and noise level\nLearning by simulations \u2013 a simulation showing the improvement of the SNR by time averaging\nDynamic Performance Testing of Digital Audio D/A Converters\nFundamental theorem of analog circuits: a minimum level of power must be dissipated to maintain a level of SNR\nInteractive webdemo of visualization of SNR in a QAM constellation diagram Institute of Telecommunicatons, University of Stuttgart\nBernard Widrow,Istv\u00e1n Koll\u00e1r (2008-07-03), Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control, and Communications, Cambridge University Press, Cambridge, UK, 2008. 778 p., ISBN 9780521886710\nQuantization Noise Widrow & Koll\u00e1r Quantization book page with sample chapters and additional material\nSignal-to-noise ratio online audio demonstrator - Virtual Communications Lab", "Spectral clustering": "In multivariate statistics, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\nIn application to image segmentation, spectral clustering is known as segmentation-based object categorization.\n\n\n== Definitions ==\nGiven an enumerated set of data points, the similarity matrix may be defined as a symmetric matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , where \n  \n    \n      \n        \n          A\n          \n            i\n            j\n          \n        \n        \u2265\n        0\n      \n    \n    {\\displaystyle A_{ij}\\geq 0}\n   represents a measure of the similarity between data points with indices \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   and \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  . The general approach to spectral clustering is to use a standard clustering method (there are many such methods, k-means is discussed below) on relevant eigenvectors of a Laplacian matrix of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . There are many different ways to define a Laplacian which have different mathematical interpretations, and so the clustering will also have different interpretations. The eigenvectors that are relevant are the ones that correspond to smallest several eigenvalues of the Laplacian except for the smallest eigenvalue which will have a value of 0. For computational efficiency, these eigenvectors are often computed as the eigenvectors corresponding to the largest several eigenvalues of a function of the Laplacian.\n\n\n=== Laplacian matrix ===\n\nSpectral clustering is well known to relate to partitioning of a mass-spring system, where each mass is associated with a data point and each spring stiffness corresponds to a weight of an edge describing a similarity of the two related data points, as in the spring system. Specifically, the classical reference  explains that the eigenvalue problem describing transversal vibration modes of a mass-spring system is exactly the same as the eigenvalue problem for the graph Laplacian matrix defined as \n\n  \n    \n      \n        L\n        :=\n        D\n        \u2212\n        A\n      \n    \n    {\\displaystyle L:=D-A}\n  ,where \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   is the diagonal matrix\n\n  \n    \n      \n        \n          D\n          \n            i\n            i\n          \n        \n        =\n        \n          \u2211\n          \n            j\n          \n        \n        \n          A\n          \n            i\n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle D_{ii}=\\sum _{j}A_{ij},}\n  and A is the adjacency matrix.\nThe masses that are tightly connected by the springs in the mass-spring system evidently move together from the equilibrium position in low-frequency vibration modes, so that the components of the eigenvectors corresponding to the smallest eigenvalues of the graph Laplacian can be used for meaningful clustering of the masses. For example, assuming that all the springs and the masses are identical in the 2-dimensional spring system pictured, one would intuitively expect that the loosest connected masses on the right-hand side of the system would move with the largest amplitude and in the opposite direction to the rest of the masses when the system is shaken \u2014 and this expectation will be confirmed by analyzing components of the eigenvectors of the graph Laplacian corresponding to the smallest eigenvalues, i.e., the smallest vibration frequencies.\n\n\n=== Laplacian matrix normalization ===\nThe goal of normalization is making the diagonal entries of the Laplacian matrix to be all unit, also scaling off-diagonal entries correspondingly. In a weighted graph, a vertex may have a large degree because of a small number of connected edges but with large weights just as well as due to a large number of connected edges with unit weights.\nA popular normalized spectral clustering technique is the normalized cuts algorithm or Shi\u2013Malik algorithm introduced by Jianbo Shi and Jitendra Malik, commonly used for image segmentation. It partitions points into two sets \n  \n    \n      \n        (\n        \n          B\n          \n            1\n          \n        \n        ,\n        \n          B\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (B_{1},B_{2})}\n   based on the eigenvector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   corresponding to the second-smallest eigenvalue of the symmetric normalized Laplacian defined as\n\n  \n    \n      \n        \n          L\n          \n            norm\n          \n        \n        :=\n        I\n        \u2212\n        \n          D\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        A\n        \n          D\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle L^{\\text{norm}}:=I-D^{-1/2}AD^{-1/2}.}\n  The vector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   is also the eigenvector corresponding to the second-largest eigenvalue of the symmetrically normalized  adjacency matrix \n  \n    \n      \n        \n          D\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        A\n        \n          D\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle D^{-1/2}AD^{-1/2}.}\n  \nThe random walk (or left) normalized Laplacian is defined as\n\n  \n    \n      \n        \n          L\n          \n            rw\n          \n        \n        :=\n        \n          D\n          \n            \u2212\n            1\n          \n        \n        L\n        =\n        I\n        \u2212\n        \n          D\n          \n            \u2212\n            1\n          \n        \n        A\n      \n    \n    {\\displaystyle L^{\\text{rw}}:=D^{-1}L=I-D^{-1}A}\n  and can also be used for spectral clustering. A mathematically equivalent algorithm  takes the eigenvector \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n   corresponding to the largest eigenvalue of the random walk normalized adjacency matrix \n  \n    \n      \n        P\n        =\n        \n          D\n          \n            \u2212\n            1\n          \n        \n        A\n      \n    \n    {\\displaystyle P=D^{-1}A}\n  .\nThe eigenvector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   of the symmetrically normalized Laplacian and the eigenvector \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n   of the left normalized Laplacian are related by the identity \n  \n    \n      \n        \n          D\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        v\n        =\n        u\n        .\n      \n    \n    {\\displaystyle D^{-1/2}v=u.}\n  \n\n\n=== Cluster analysis via Spectral Embedding ===\nKnowing the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   matrix \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n   of selected eigenvectors, mapping \u2014 called spectral embedding \u2014 of the original \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   data points is performed to a \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -dimensional vector space using the rows of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  . Now the analysis is reduced to clustering vectors with \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   components, which may be done in various ways. \nIn the simplest case \n  \n    \n      \n        k\n        =\n        1\n      \n    \n    {\\displaystyle k=1}\n  , the selected single eigenvector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  , called the Fiedler vector, corresponds to the second smallest eigenvalue. Using the components of \n  \n    \n      \n        v\n        ,\n      \n    \n    {\\displaystyle v,}\n   one can place all points whose component in \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   is positive in the set \n  \n    \n      \n        \n          B\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle B_{+}}\n   and the rest in \n  \n    \n      \n        \n          B\n          \n            \u2212\n          \n        \n      \n    \n    {\\displaystyle B_{-}}\n  , thus bi-partitioning the graph and labeling the data points with two labels. This sign-based approach follows the intuitive explanation of spectral clustering via the mass-spring model \u2014 in the low frequency vibration mode that the Fiedler vector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   represents, one cluster data points identified with mutually strongly connected masses would move together in one direction, while in the complement cluster data points identified with remaining masses would move together in the opposite direction. The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in the same fashion.\nIn the general case \n  \n    \n      \n        k\n        >\n        1\n      \n    \n    {\\displaystyle k>1}\n  , any vector clustering technique can be used, e.g., DBSCAN.\n\n\n== Algorithms ==\nBasic AlgorithmCalculate the Laplacian  \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   (or the normalized Laplacian)\nCalculate the first \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   eigenvectors (the eigenvectors corresponding to the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   smallest eigenvalues of \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  )\nConsider the matrix formed by the first  \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n    eigenvectors; the  \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  -th row defines the features of graph node \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \nCluster the graph nodes based on these features (e.g., using k-means clustering)If the similarity matrix \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   has not already been explicitly constructed, the efficiency of spectral clustering may be improved if the solution to the corresponding eigenvalue problem is performed in a matrix-free fashion (without explicitly manipulating or even computing the similarity matrix), as in the Lanczos algorithm.\nFor large-sized graphs, the second eigenvalue  of the (normalized) graph Laplacian matrix is often ill-conditioned, leading to slow convergence of iterative eigenvalue solvers. Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Spectral clustering has been successfully applied on large graphs by first identifying their community structure, and then clustering communities.Spectral clustering is closely related to nonlinear dimensionality reduction, and dimension reduction techniques such as locally-linear embedding can be used to reduce errors from noise or outliers.\n\n\n== Costs ==\nDenoting the number of the data points ny \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , it is important to estimate the memory footprint and compute time, or number of arithmetic operations (AO) performed, as a function of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . No matter the algorithm of the spectral clustering, the two main costly items are the construction of the graph Laplacian and determining its \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   eigenvectors for the spectral embedding. The last step \u2014 determining the labels from the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   matrix of eigenvectors \u2014 is typically the least expensive requiring only \n  \n    \n      \n        k\n        n\n      \n    \n    {\\displaystyle kn}\n   AO and creating just a \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   vector of the labels in memory.\nThe need to construct the graph Laplacian is common for all distance- or correlation-based clustering methods. Computing the eigenvectors is specific to spectral clustering only. \n\n\n=== Constructing graph Laplacian ===\nThe graph Laplacian can be and commonly is constructed from the adjacency matrix. The construction can be performed matrix-free, i.e., without explicitly forming the matrix of the graph Laplacian and no AO. It can also be performed in-place of the adjacency matrix without increasing the memory footprint. Either way, the costs of constructing the graph Laplacian is essentially determined by the costs of constructing the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   graph adjacency matrix.\nMoreover, a normalized Laplacian has exactly the same eigenvectors as the normalized adjacency matrix, but with the order of the eigenvalues reversed. Thus, instead of computing the eigenvectors corresponding to the smallest eigenvalues of the normalized Laplacian, one can equivalently compute the eigenvectors corresponding to the largest eigenvalues of the normalized adjacency matrix, without even talking about the Laplacian matrix.\nNaive constructions of the graph adjacency matrix, e.g., using the RBF kernel, make it dense, thus requiring \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n   memory and \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n   AO to determine each of the \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n   entries of the matrix. Nystrom method can be used to approximate the similarity matrix, but the approximate matrix is not elementwise positive, i.e. cannot be interpreted as a distance-based similarity.\nAlgorithms to construct the graph adjacency matrix as a sparse matrix are typically based on a nearest neighbor search, which estimate or sample a neighborhood of a given data point for nearest neighbors, and compute non-zero entries of the adjacency matrix by comparing only pairs of the neighbors. The number of the selected nearest neighbors thus determines the number of non-zero entries, and is often fixed so that the memory footprint of the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   graph adjacency matrix is only \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  , only \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   sequential arithmetic operations are needed to compute the \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   non-zero entries, and the calculations can be trivially run in parallel.\n\n\n=== Computing eigenvectors ===\nThe cost of computing the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   (with \n  \n    \n      \n        k\n        \u226a\n        n\n      \n    \n    {\\displaystyle k\\ll n}\n  ) matrix of selected eigenvectors of the graph Laplacian is normally proportional to the cost of multiplication of the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   graph Laplacian matrix by a vector, which varies greatly whether the graph Laplacian matrix is dense or sparse. For the dense case the cost thus is \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  . The very commonly cited in the literature cost \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3})}\n   comes from choosing \n  \n    \n      \n        k\n        =\n        n\n      \n    \n    {\\displaystyle k=n}\n   and is clearly misleading, since, e.g., in a hierarchical spectral clustering \n  \n    \n      \n        k\n        =\n        1\n      \n    \n    {\\displaystyle k=1}\n   as determined by the Fiedler vector.\nIn the sparse case of the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   graph Laplacian matrix with \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   non-zero entries, the cost of the matrix-vector product and thus of computing the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -by-\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   with \n  \n    \n      \n        k\n        \u226a\n        n\n      \n    \n    {\\displaystyle k\\ll n}\n   matrix of selected eigenvectors is \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  , with the memory footprint also only \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   \u2014 both are the optimal low bounds of complexity of clustering \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   data points. Moreover, matrix-free eigenvalue solvers such as LOBPCG can efficiently run in parallel, e.g., on multiple GPUs with distributed memory, resulting not only in high quality clusters, which spectral clustering is famous for, but also top performance. \n\n\n== Software ==\nFree software implementing spectral clustering is available in large open source projects like scikit-learn using LOBPCG with multigrid preconditioning  or ARPACK, MLlib for pseudo-eigenvector clustering using the power iteration method, and R.\n\n\n== Relationship with other clustering methods ==\nThe ideas behind spectral clustering may not be immediately obvious. It may be useful to highlight relationships with other methods. In particular, it can be described in the context of kernel clustering methods, which reveals several similarities with other approaches.\n\n\n=== Relationship with k-means ===\nThe weighted kernel k-means problem\nshares the objective function with the spectral clustering problem, which can be optimized directly by multi-level methods.\n\n\n=== Relationship to DBSCAN ===\nIn the trivial case of determining connected graph components \u2014 the optimal clusters with no edges cut \u2014 spectral clustering is also related to a spectral version of DBSCAN clustering that finds density-connected components.\n\n\n== Measures to compare clusterings ==\nRavi Kannan, Santosh Vempala and Adrian Vetta proposed a bicriteria measure to define the quality of a given clustering. They said that a clustering was an (\u03b1, \u03b5)-clustering if the conductance of each cluster (in the clustering) was at least \u03b1 and the weight of the inter-cluster edges was at most \u03b5 fraction of the total weight of all the edges in the graph. They also look at two approximation algorithms in the same paper.\n\n\n== History and related literatures ==\nSpectral clustering has a long history. Spectral clustering as a machine learning method was popularized by Shi & Malik and Ng, Jordan, & Weiss.Ideas and network measures related to spectral clustering also play an important role in a number of applications apparently different from clustering problems. For instance, networks with stronger spectral partitions take longer to converge in opinion-updating models used in sociology and economics.\n\n\n== See also ==\nAffinity propagation\nKernel principal component analysis\nCluster analysis\nSpectral graph theory\n\n\n== References ==", "Sampling (statistics)": "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population in question. Sampling has lower costs and faster data collection than measuring the entire population and can provide insights in cases where it is infeasible to measure an entire population. \nEach observation measures one or more properties (such as weight, location, colour or mass) of independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. Acceptance sampling is used to determine if a production lot of material meets the governing specifications.\n\n\n== Population definition ==\nSuccessful statistical practice is based on focused problem definition. In sampling, this includes defining the \"population\" from which our sample is drawn. A population can be defined as including all people or items with the characteristics one wishes to understand. Because there is very rarely enough time or money to gather information from everyone or everything in a population, the goal becomes finding a representative sample (or subset) of that population.\nSometimes what defines a population is obvious. For example, a manufacturer needs to decide whether a batch of material from production is of high enough quality to be released to the customer or should be scrapped or reworked due to poor quality. In this case, the batch is the population.\nAlthough the population of interest often consists of physical objects, sometimes it is necessary to sample over time, space, or some combination of these dimensions. For instance, an investigation of supermarket staffing could examine checkout line length at various times, or a study on endangered penguins might aim to understand their usage of various hunting grounds over time. For the time dimension, the focus may be on periods or discrete occasions.\nIn other cases, the examined 'population' may be even less tangible. For example, Joseph Jagger studied the behaviour of roulette wheels at a casino in Monte Carlo, and used this to identify a biased wheel. In this case, the 'population' Jagger wanted to investigate was the overall behaviour of the wheel (i.e. the probability distribution of its results over infinitely many trials), while his 'sample' was formed from observed results from that wheel. Similar considerations arise when taking repeated measurements of some physical characteristic such as the electrical conductivity of copper.\nThis situation often arises when seeking knowledge about the cause system of which the observed population is an outcome. In such cases, sampling theory may treat the observed population as a sample from a larger 'superpopulation'. For example, a researcher might study the success rate of a new 'quit smoking' program on a test group of 100 patients, in order to predict the effects of the program if it were made available nationwide. Here the superpopulation is \"everybody in the country, given access to this treatment\" \u2013 a group that does not yet exist since the program isn't yet available to all.\nThe population from which the sample is drawn may not be the same as the population from which information is desired. Often there is a large but not complete overlap between these two groups due to frame issues etc. (see below). Sometimes they may be entirely separate \u2013 for instance, one might study rats in order to get a better understanding of human health, or one might study records from people born in 2008 in order to make predictions about people born in 2009.\nTime spent in making the sampled population and population of concern precise is often well spent because it raises many issues, ambiguities, and questions that would otherwise have been overlooked at this stage.\n\n\n== Sampling frame ==\n\nIn the most straightforward case, such as the sampling of a batch of material from production (acceptance sampling by lots), it would be most desirable to identify and measure every single item in the population and to include any one of them in our sample. However, in the more general case this is not usually possible or practical. There is no way to identify all rats in the set of all rats. Where voting is not compulsory, there is no way to identify which people will vote at a forthcoming election (in advance of the election). These imprecise populations are not amenable to sampling in any of the ways below and to which we could apply statistical theory.\nAs a remedy, we seek a sampling frame which has the property that we can identify every single element and include any in our sample. The most straightforward type of frame is a list of elements of the population (preferably the entire population) with appropriate contact information. For example, in an opinion poll, possible sampling frames include an electoral register and a telephone directory.\nA probability sample is a sample in which every unit in the population has a chance (greater than zero) of being selected in the sample, and this probability can be accurately determined. The combination of these traits makes it possible to produce unbiased estimates of population totals, by weighting sampled units according to their probability of selection.\n\nExample: We want to estimate the total income of adults living in a given street. We visit each household in that street, identify all adults living there, and randomly select one adult from each household. (For example, we can allocate each person a random number, generated from a uniform distribution between 0 and 1, and select the person with the highest number in each household). We then interview the selected person and find their income.\nPeople living on their own are certain to be selected, so we simply add their income to our estimate of the total. But a person living in a household of two adults has only a one-in-two chance of selection. To reflect this, when we come to such a household, we would count the selected person's income twice towards the total. (The person who is selected from that household can be loosely viewed as also representing the person who isn't selected.)\n\nIn the above example, not everybody has the same probability of selection; what makes it a probability sample is the fact that each person's probability is known. When every element in the population does have the same probability of selection, this is known as an 'equal probability of selection' (EPS) design. Such designs are also referred to as 'self-weighting' because all sampled units are given the same weight.\nProbability sampling includes: Simple Random Sampling, Systematic Sampling, Stratified Sampling, Probability Proportional to Size Sampling, and Cluster or Multistage Sampling. These various ways of probability sampling have two things in common:\n\nEvery element has a known nonzero probability of being sampled and\ninvolves random selection at some point.\n\n\n=== Nonprobability sampling ===\n\nNonprobability sampling is any sampling method where some elements of the population have no chance of selection (these are sometimes referred to as 'out of coverage'/'undercovered'), or where the probability of selection can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population.\n\nExample: We visit every household in a given street, and interview the first person to answer the door. In any household with more than one occupant, this is a nonprobability sample, because some people are more likely to answer the door (e.g. an unemployed person who spends most of their time at home is more likely to answer than an employed housemate who might be at work when the interviewer calls) and it's not practical to calculate these probabilities.\n\nNonprobability sampling methods include convenience sampling, quota sampling, and purposive sampling. In addition, nonresponse effects may turn any probability design into a nonprobability design if the characteristics of nonresponse are not well understood, since nonresponse effectively modifies each element's probability of being sampled.\n\n\n== Sampling methods ==\nWithin any of the types of frames identified above, a variety of sampling methods can be employed individually or in combination. Factors commonly influencing the choice between these designs include:\n\nNature and quality of the frame\nAvailability of auxiliary information about units on the frame\nAccuracy requirements, and the need to measure accuracy\nWhether detailed analysis of the sample is expected\nCost/operational concerns\n\n\n=== Simple random sampling ===\n\nIn a simple random sample (SRS) of a given size, all subsets of a sampling frame have an equal probability of being selected. Each element of the frame thus has an equal probability of selection: the frame is not subdivided or partitioned. Furthermore, any given pair of elements has the same chance of selection as any other such pair (and similarly for triples, and so on). This minimizes bias and simplifies analysis of results. In particular, the variance between individual results within the sample is a good indicator of variance in the overall population, which makes it relatively easy to estimate the accuracy of results.\nSimple random sampling can be vulnerable to sampling error because the randomness of the selection may result in a sample that doesn't reflect the makeup of the population. For instance, a simple random sample of ten people from a given country will on average produce five men and five women, but any given trial is likely to over represent one sex and underrepresent the other. Systematic and stratified techniques attempt to overcome this problem by \"using information about the population\" to choose a more \"representative\" sample.\nAlso, simple random sampling can be cumbersome and tedious when sampling from a large target population. In some cases, investigators are interested in research questions specific to subgroups of the population. For example, researchers might be interested in examining whether cognitive ability as a predictor of job performance is equally applicable across racial groups. Simple random sampling cannot accommodate the needs of researchers in this situation, because it does not provide subsamples of the population, and other sampling strategies, such as stratified sampling, can be used instead.\n\n\n=== Systematic sampling ===\n\nSystematic sampling (also known as interval sampling) relies on arranging the study population according to some ordering scheme and then selecting elements at regular intervals through that ordered list. Systematic sampling involves a random start and then proceeds with the selection of every kth element from then onwards. In this case, k=(population size/sample size). It is important that the starting point is not automatically the first in the list, but is instead randomly chosen from within the first to the kth element in the list. A simple example would be to select every 10th name from the telephone directory (an 'every 10th' sample, also referred to as 'sampling with a skip of 10').\nAs long as the starting point is randomized, systematic sampling is a type of probability sampling. It is easy to implement and the stratification induced can make it efficient, if the variable by which the list is ordered is correlated with the variable of interest. 'Every 10th' sampling is especially useful for efficient sampling from databases.\nFor example, suppose we wish to sample people from a long street that starts in a poor area (house No. 1) and ends in an expensive district (house No. 1000). A simple random selection of addresses from this street could easily end up with too many from the high end and too few from the low end (or vice versa), leading to an unrepresentative sample. Selecting (e.g.) every 10th street number along the street ensures that the sample is spread evenly along the length of the street, representing all of these districts. (Note that if we always start at house #1 and end at #991, the sample is slightly biased towards the low end; by randomly selecting the start between #1 and #10, this bias is eliminated.)\nHowever, systematic sampling is especially vulnerable to periodicities in the list. If periodicity is present and the period is a multiple or factor of the interval used, the sample is especially likely to be unrepresentative of the overall population, making the scheme less accurate than simple random sampling.\nFor example, consider a street where the odd-numbered houses are all on the north (expensive) side of the road, and the even-numbered houses are all on the south (cheap) side. Under the sampling scheme given above, it is impossible to get a representative sample; either the houses sampled will all be from the odd-numbered, expensive side, or they will all be from the even-numbered, cheap side, unless the researcher has previous knowledge of this bias and avoids it by a using a skip which ensures jumping between the two sides (any odd-numbered skip).\nAnother drawback of systematic sampling is that even in scenarios where it is more accurate than SRS, its theoretical properties make it difficult to quantify that accuracy. (In the two examples of systematic sampling that are given above, much of the potential sampling error is due to variation between neighbouring houses \u2013 but because this method never selects two neighbouring houses, the sample will not give us any information on that variation.)\nAs described above, systematic sampling is an EPS method, because all elements have the same probability of selection (in the example given, one in ten). It is not 'simple random sampling' because different subsets of the same size have different selection probabilities \u2013 e.g. the set {4,14,24,...,994} has a one-in-ten probability of selection, but the set {4,13,24,34,...} has zero probability of selection.\nSystematic sampling can also be adapted to a non-EPS approach; for an example, see discussion of PPS samples below.\n\n\n=== Stratified sampling ===\n\nWhen the population embraces a number of distinct categories, the frame can be organized by these categories into separate \"strata.\" Each stratum is then sampled as an independent sub-population, out of which individual elements can be randomly selected. The ratio of the size of this random selection (or sample) to the size of the population is called a sampling fraction.  There are several potential benefits to stratified sampling.First, dividing the population into distinct, independent strata can enable researchers to draw inferences about specific subgroups that may be lost in a more generalized random sample.\nSecond, utilizing a stratified sampling method can lead to more efficient statistical estimates (provided that strata are selected based upon relevance to the criterion in question, instead of availability of the samples). Even if a stratified sampling approach does not lead to increased statistical efficiency, such a tactic will not result in less efficiency than would simple random sampling, provided that each stratum is proportional to the group's size in the population.\nThird, it is sometimes the case that data are more readily available for individual, pre-existing strata within a population than for the overall population; in such cases, using a stratified sampling approach may be more convenient than aggregating data across groups (though this may potentially be at odds with the previously noted importance of utilizing criterion-relevant strata).\nFinally, since each stratum is treated as an independent population, different sampling approaches can be applied to different strata, potentially enabling researchers to use the approach best suited (or most cost-effective) for each identified subgroup within the population.\nThere are, however, some potential drawbacks to using stratified sampling. First, identifying strata and implementing such an approach can increase the cost and complexity of sample selection, as well as leading to increased complexity of population estimates. Second, when examining multiple criteria, stratifying variables may be related to some, but not to others, further complicating the design, and potentially reducing the utility of the strata. Finally, in some cases (such as designs with a large number of strata, or those with a specified minimum sample size per group), stratified sampling can potentially require a larger sample than would other methods (although in most cases, the required sample size would be no larger than would be required for simple random sampling).\n\nA stratified sampling approach is most effective when three conditions are met\nVariability within strata are minimized\nVariability between strata are maximized\nThe variables upon which the population is stratified are strongly correlated with the desired dependent variable.Advantages over other sampling methodsFocuses on important subpopulations and ignores irrelevant ones.\nAllows use of different sampling techniques for different subpopulations.\nImproves the accuracy/efficiency of estimation.\nPermits greater balancing of statistical power of tests of differences between strata by sampling equal numbers from strata varying widely in size.DisadvantagesRequires selection of relevant stratification variables which can be difficult.\nIs not useful when there are no homogeneous subgroups.\nCan be expensive to implement.PoststratificationStratification is sometimes introduced after the sampling phase in a process called \"poststratification\". This approach is typically implemented due to a lack of prior knowledge of an appropriate stratifying variable or when the experimenter lacks the necessary information to create a stratifying variable during the sampling phase. Although the method is susceptible to the pitfalls of post hoc approaches, it can provide several benefits in the right situation. Implementation usually follows a simple random sample. In addition to allowing for stratification on an ancillary variable, poststratification can be used to implement weighting, which can improve the precision of a sample's estimates.\nOversamplingChoice-based sampling is one of the stratified sampling strategies. In choice-based sampling, the data are stratified on the target and a sample is taken from each stratum so that the rare target class will be more represented in the sample. The model is then built on this biased sample. The effects of the input variables on the target are often estimated with more precision with the choice-based sample even when a smaller overall sample size is taken, compared to a random sample. The results usually must be adjusted to correct for the oversampling.\n\n\n=== Probability-proportional-to-size sampling ===\n\nIn some cases the sample designer has access to an \"auxiliary variable\" or \"size measure\", believed to be correlated to the variable of interest, for each element in the population. These data can be used to improve accuracy in sample design. One option is to use the auxiliary variable as a basis for stratification, as discussed above.\nAnother option is probability proportional to size ('PPS') sampling, in which the selection probability for each element is set to be proportional to its size measure, up to a maximum of 1. In a simple PPS design, these selection probabilities can then be used as the basis for Poisson sampling. However, this has the drawback of variable sample size, and different portions of the population may still be over- or under-represented due to chance variation in selections.\nSystematic sampling theory can be used to create a probability proportionate to size sample. This is done by treating each count within the size variable as a single sampling unit. Samples are then identified by selecting at even intervals among these counts within the size variable. This method is sometimes called PPS-sequential or monetary unit sampling in the case of audits or forensic sampling.\n\nExample: Suppose we have six schools with populations of 150, 180, 200, 220, 260, and 490 students respectively (total 1500 students), and we want to use student population as the basis for a PPS sample of size three. To do this, we could allocate the first school numbers 1 to 150, the second school 151 to 330 (= 150 + 180), the third school 331 to 530, and so on to the last school (1011 to 1500). We then generate a random start between 1 and 500 (equal to 1500/3) and count through the school populations by multiples of 500. If our random start was 137, we would select the schools which have been allocated numbers 137, 637, and 1137, i.e. the first, fourth, and sixth schools.\n\nThe PPS approach can improve accuracy for a given sample size by concentrating sample on large elements that have the greatest impact on population estimates. PPS sampling is commonly used for surveys of businesses, where element size varies greatly and auxiliary information is often available \u2013 for instance, a survey attempting to measure the number of guest-nights spent in hotels might use each hotel's number of rooms as an auxiliary variable. In some cases, an older measurement of the variable of interest can be used as an auxiliary variable when attempting to produce more current estimates.\n\n\n=== Cluster sampling ===\n\nSometimes it is more cost-effective to select respondents in groups ('clusters'). Sampling is often clustered by geography, or by time periods. (Nearly all samples are in some sense 'clustered' in time \u2013 although this is rarely taken into account in the analysis.) For instance, if surveying households within a city, we might choose to select 100 city blocks and then interview every household within the selected blocks.\nClustering can reduce travel and administrative costs. In the example above, an interviewer can make a single trip to visit several households in one block, rather than having to drive to a different block for each household.\nIt also means that one does not need a sampling frame listing all elements in the target population. Instead, clusters can be chosen from a cluster-level frame, with an element-level frame created only for the selected clusters. In the example above, the sample only requires a block-level city map for initial selections, and then a household-level map of the 100 selected blocks, rather than a household-level map of the whole city.\nCluster sampling (also known as clustered sampling) generally increases the variability of sample estimates above that of simple random sampling, depending on how the clusters differ between one another as compared to the within-cluster variation. For this reason, cluster sampling requires a larger sample than SRS to achieve the same level of accuracy \u2013 but cost savings from clustering might still make this a cheaper option.\nCluster sampling is commonly implemented as multistage sampling. This is a complex form of cluster sampling in which two or more levels of units are embedded one in the other. The first stage consists of constructing the clusters that will be used to sample from. In the second stage, a sample of primary units is randomly selected from each cluster (rather than using all units contained in all selected clusters). In following stages, in each of those selected clusters, additional samples of units are selected, and so on. All ultimate units (individuals, for instance) selected at the last step of this procedure are then surveyed. This technique, thus, is essentially the process of taking random subsamples of preceding random samples.\nMultistage sampling can substantially reduce sampling costs, where the complete population list would need to be constructed (before other sampling methods could be applied). By eliminating the work involved in describing clusters that are not selected, multistage sampling can reduce the large costs associated with traditional cluster sampling. However, each sample may not be a full representative of the whole population.\n\n\n=== Quota sampling ===\n\nIn quota sampling, the population is first segmented into mutually exclusive sub-groups, just as in stratified sampling. Then judgement is used to select the subjects or units from each segment based on a specified proportion. For example, an interviewer may be told to sample 200 females and 300 males between the age of 45 and 60.\nIt is this second step which makes the technique one of non-probability sampling. In quota sampling the selection of the sample is non-random. For example, interviewers might be tempted to interview those who look most helpful. The problem is that these samples may be biased because not everyone gets a chance of selection. This random element is its greatest weakness and quota versus probability has been a matter of controversy for several years.\n\n\n=== Minimax sampling ===\nIn imbalanced datasets, where the sampling ratio does not follow the population statistics, one can resample the dataset in a conservative manner called minimax sampling. The minimax sampling has its origin in Anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be minimax ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the sampling ratio of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the best.\n\n\n=== Accidental sampling ===\nAccidental sampling (sometimes known as grab, convenience or opportunity sampling) is a type of nonprobability sampling which involves the sample being drawn from that part of the population which is close to hand. That is, a population is selected because it is readily available and convenient. It may be through meeting the person or including a person in the sample when one meets them or chosen by finding them through technological means such as the internet or through phone. The researcher using such a sample cannot scientifically make generalizations about the total population from this sample because it would not be representative enough. For example, if the interviewer were to conduct such a survey at a shopping center early in the morning on a given day, the people that they could interview would be limited to those given there at that given time, which would not represent the views of other members of society in such an area, if the survey were to be conducted at different times of day and several times per week. This type of sampling is most useful for pilot testing. Several important considerations for researchers using convenience samples include:\n\nAre there controls within the research design or experiment which can serve to lessen the impact of a non-random convenience sample, thereby ensuring the results will be more representative of the population?\nIs there good reason to believe that a particular convenience sample would or should respond or behave differently than a random sample from the same population?\nIs the question being asked by the research one that can adequately be answered using a convenience sample?In social science research, snowball sampling is a similar technique, where existing study subjects are used to recruit more subjects into the sample. Some variants of snowball sampling, such as respondent driven sampling, allow calculation of selection probabilities and are probability sampling methods under certain conditions.\n\n\n=== Voluntary Sampling ===\n\nThe voluntary sampling method is a type of non-probability sampling. Volunteers choose to complete a survey.\nVolunteers may be invited through advertisements in social media. The target population for advertisements can be selected by characteristics like location, age, sex, income, occupation, education, or interests using tools provided by the social medium. The advertisement may include a message about the research and link to a survey. After following the link and completing the survey, the volunteer submits the data to be included in the sample population. This method can reach a global population but is limited by the campaign budget. Volunteers outside the invited population may also be included in the sample.\nIt is difficult to make generalizations from this sample because it may not represent the total population. Often, volunteers have a strong interest in the main topic of the survey.\n\n\n=== Line-intercept sampling ===\nLine-intercept sampling is a method of sampling elements in a region whereby an element is sampled if a chosen line segment, called a \"transect\", intersects the element.\n\n\n=== Panel sampling ===\nPanel sampling is the method of first selecting a group of participants through a random sampling method and then asking that group for (potentially the same) information several times over a period of time. Therefore, each participant is interviewed at two or more time points; each period of data collection is called a \"wave\". The method was developed by sociologist Paul Lazarsfeld in 1938 as a means of studying political campaigns. This longitudinal sampling-method allows estimates of changes in the population, for example with regard to chronic illness to job stress to weekly food expenditures. Panel sampling can also be used to inform researchers about within-person health changes due to age or to help explain changes in continuous dependent variables such as spousal interaction. There have been several proposed methods of analyzing panel data, including MANOVA, growth curves, and structural equation modeling with lagged effects.\n\n\n=== Snowball sampling ===\nSnowball sampling involves finding a small group of initial respondents and using them to recruit more respondents. It is particularly useful in cases where the population is hidden or difficult to enumerate.\n\n\n=== Theoretical sampling ===\nTheoretical sampling occurs when samples are selected on the basis of the results of the data collected so far with a goal of developing a deeper understanding of the area or develop theories. Extreme or very specific cases might be selected in order to maximize the likelihood a phenomenon will actually be observable.\n\n\n== Replacement of selected units ==\nSampling schemes may be without replacement ('WOR' \u2013 no element can be selected more than once in the same sample) or with replacement ('WR' \u2013 an element may appear multiple times in the one sample). For example, if we catch fish, measure them, and immediately return them to the water before continuing with the sample, this is a WR design, because we might end up catching and measuring the same fish more than once. However, if we do not return the fish to the water or tag and release each fish after catching it, this becomes a WOR design.\n\n\n== Sample size determination ==\n\nFormulas, tables, and power function charts are well known approaches to determine sample size.\nSteps for using sample size tables:\n\nPostulate the effect size of interest, \u03b1, and \u03b2.\nCheck sample size tableSelect the table corresponding to the selected \u03b1\nLocate the row corresponding to the desired power\nLocate the column corresponding to the estimated effect size.\nThe intersection of the column and row is the minimum sample size required.\n\n\n== Sampling and data collection ==\nGood data collection involves:\n\nFollowing the defined sampling process\nKeeping the data in time order\nNoting comments and other contextual events\nRecording non-responses\n\n\n== Applications of sampling ==\nSampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. It is not necessary to look at all of them to determine the topics that are discussed during the day, nor is it necessary to look at all the tweets to determine the sentiment on each of the topics. A theoretical formulation for sampling Twitter data has been developed.In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.\n\n\n== Errors in sample surveys ==\n\nSurvey results are typically subject to some error. Total errors can be classified into sampling errors and non-sampling errors. The term \"error\" here includes systematic biases as well as random errors.\n\n\n=== Sampling errors and biases ===\nSampling errors and biases are induced by the sample design. They include:\n\nSelection bias: When the true selection probabilities differ from those assumed in calculating the results.\nRandom sampling error: Random variation in the results due to the elements in the sample being selected at random.\n\n\n=== Non-sampling error ===\nNon-sampling errors are other errors which can impact final survey estimates, caused by problems in data collection, processing, or sample design. Such errors may include:\n\nOver-coverage: inclusion of data from outside of the population\nUnder-coverage: sampling frame does not include elements in the population.\nMeasurement error: e.g. when respondents misunderstand a question, or find it difficult to answer\nProcessing error: mistakes in data coding\nNon-response or Participation bias: failure to obtain complete data from all selected individualsAfter sampling, a review should be held of the exact process followed in sampling, rather than that intended, in order to study any effects that any divergences might have on subsequent analysis.\nA particular problem involves non-response. Two major types of non-response exist:\nunit nonresponse (lack of completion of any part of the survey)\nitem non-response (submission or participation in survey but failing to complete one or more components/questions of the survey)In survey sampling, many of the individuals identified as part of the sample may be unwilling to participate, not have the time to participate (opportunity cost), or survey administrators may not have been able to contact them. In this case, there is a risk of differences between respondents and nonrespondents, leading to biased estimates of population parameters. This is often addressed by improving survey design, offering incentives, and conducting follow-up studies which make a repeated attempt to contact the unresponsive and to characterize their similarities and differences with the rest of the frame. The effects can also be mitigated by weighting the data (when population benchmarks are available) or by imputing data based on answers to other questions. Nonresponse is particularly a problem in internet sampling. Reasons for this problem may include improperly designed surveys, over-surveying (or survey fatigue),\nand the fact that potential participants may have multiple e-mail addresses, which they don't use anymore or don't check regularly.\n\n\n== Survey weights ==\nIn many situations the sample fraction may be varied by stratum and data will have to be weighted to correctly represent the population. Thus for example, a simple random sample of individuals in the United Kingdom might not include some in remote Scottish islands who would be inordinately expensive to sample. A cheaper method would be to use a stratified sample with urban and rural strata. The rural sample could be under-represented in the sample, but weighted up appropriately in the analysis to compensate.\nMore generally, data should usually be weighted if the sample design does not give each individual an equal chance of being selected. For instance, when households have equal selection probabilities but one person is interviewed from within each household, this gives people from large households a smaller chance of being interviewed. This can be accounted for using survey weights. Similarly, households with more than one telephone line have a greater chance of being selected in a random digit dialing sample, and weights can adjust for this.\nWeights can also serve other purposes, such as helping to correct for non-response.\n\n\n== Methods of producing random samples ==\nRandom number table\nMathematical algorithms for pseudo-random number generators\nPhysical randomization devices such as coins, playing cards or sophisticated devices such as ERNIE\n\n\n== History ==\nRandom sampling by using lots is an old idea, mentioned several times in the Bible. In 1786 Pierre Simon Laplace estimated the population of France by using a sample, along with ratio estimator. He also computed probabilistic estimates of the error. These were not expressed as modern confidence intervals but as the sample size that would be needed to achieve a particular upper bound on the sampling error with probability 1000/1001. His estimates used Bayes' theorem with a uniform prior probability and assumed that his sample was random. Alexander Ivanovich Chuprov introduced sample surveys to Imperial Russia in the 1870s.In the US the 1936 Literary Digest prediction of a Republican win in the presidential election went badly awry, due to severe bias [1]. More than two million people responded to the study with their names obtained through magazine subscription lists and telephone directories. It was not appreciated that these lists were heavily biased towards Republicans and the resulting sample, though very large, was deeply flawed.\n\n\n== See also ==\n\n\n== Notes ==\nThe textbook by Groves et alia provides an overview of survey methodology, including recent literature on questionnaire development (informed by cognitive psychology) :\n\nRobert Groves, et alia. Survey methodology (2010 2nd ed. [2004]) ISBN 0-471-48348-6.The other books focus on the statistical theory of survey sampling and require some knowledge of basic statistics, as discussed in the following  textbooks:\n\nDavid S. Moore and George P. McCabe (February 2005). \"Introduction to the practice of statistics\" (5th edition). W.H. Freeman & Company. ISBN 0-7167-6282-X.\nFreedman, David; Pisani, Robert; Purves, Roger (2007). Statistics (4th ed.). New York: Norton. ISBN 978-0-393-92972-0.The elementary book by Scheaffer et alia uses quadratic equations from high-school algebra:\n\nScheaffer, Richard L., William Mendenhal and R. Lyman Ott. Elementary survey sampling, Fifth Edition. Belmont: Duxbury Press, 1996.More mathematical statistics is required for Lohr, for S\u00e4rndal et alia, and for Cochran (classic):\n\nCochran, William G. (1977). Sampling techniques (Third ed.). Wiley. ISBN 978-0-471-16240-7.\nLohr, Sharon L. (1999). Sampling: Design and analysis. Duxbury. ISBN 978-0-534-35361-2.\nS\u00e4rndal, Carl-Erik; Swensson, Bengt; Wretman, Jan (1992). Model assisted survey sampling. Springer-Verlag. ISBN 978-0-387-40620-6.The historically important books by Deming and Kish remain valuable for insights for social scientists (particularly about the U.S. census and the Institute for Social Research at the University of Michigan):\n\nDeming, W. Edwards (1966). Some Theory of Sampling. Dover Publications. ISBN 978-0-486-64684-8. OCLC 166526.\nKish, Leslie (1995) Survey Sampling, Wiley, ISBN 0-471-10949-5\n\n\n== References ==\n\n\n== Further reading ==\nSingh, G N, Jaiswal, A. K., and Pandey A. K. (2021), Improved Imputation Methods for Missing Data in Two-Occasion Successive Sampling, Communications in Statistics: Theory and Methods. DOI:10.1080/03610926.2021.1944211\nChambers, R L, and Skinner, C J (editors) (2003), Analysis of Survey Data, Wiley, ISBN 0-471-89987-9\nDeming, W. Edwards (1975) On probability as a basis for action, The American Statistician, 29(4), pp. 146\u2013152.\nGy, P (2012) Sampling of Heterogeneous and Dynamic Material Systems: Theories of Heterogeneity, Sampling and Homogenizing, Elsevier Science, ISBN 978-0444556066\nKorn, E.L., and Graubard, B.I. (1999) Analysis of Health Surveys, Wiley, ISBN 0-471-13773-1\nLucas, Samuel R. (2012). doi:10.1007%2Fs11135-012-9775-3 \"Beyond the Existence Proof: Ontological Conditions, Epistemological Implications, and In-Depth Interview Research.\"], Quality & Quantity, doi:10.1007/s11135-012-9775-3.\nStuart, Alan (1962) Basic Ideas of Scientific Sampling, Hafner Publishing Company, New York\nSmith, T. M. F. (1984). \"Present Position and Potential Developments: Some Personal Views: Sample surveys\". Journal of the Royal Statistical Society, Series A. 147 (The 150th Anniversary of the Royal Statistical Society, number 2): 208\u2013221. doi:10.2307/2981677. JSTOR 2981677.\nSmith, T. M. F. (1993). \"Populations and Selection: Limitations of Statistics (Presidential address)\". Journal of the Royal Statistical Society, Series A. 156 (2): 144\u2013166. doi:10.2307/2982726. JSTOR 2982726. (Portrait of T. M. F. Smith on page 144)\nSmith, T. M. F. (2001). \"Centenary: Sample surveys\". Biometrika. 88 (1): 167\u2013243. doi:10.1093/biomet/88.1.167.\nSmith, T. M. F. (2001). \"Biometrika centenary: Sample surveys\".  In D. M. Titterington and D. R. Cox (ed.). Biometrika: One Hundred Years. Oxford University Press. pp. 165\u2013194. ISBN 978-0-19-850993-6.\nWhittle, P. (May 1954). \"Optimum preventative sampling\". Journal of the Operations Research Society of America. 2 (2): 197\u2013203. doi:10.1287/opre.2.2.197. JSTOR 166605.\n\n\n== Standards ==\n\n\n=== ISO ===\nISO 2859 series\nISO 3951 series\n\n\n=== ASTM ===\nASTM E105 Standard Practice for Probability Sampling Of Materials\nASTM E122 Standard Practice for Calculating Sample Size to Estimate, With a Specified Tolerable Error, the Average for Characteristic of a Lot or Process\nASTM E141 Standard Practice for Acceptance of Evidence Based on the Results of Probability Sampling\nASTM E1402 Standard Terminology Relating to Sampling\nASTM E1994 Standard Practice for Use of Process Oriented AOQL and LTPD Sampling Plans\nASTM E2234 Standard Practice for Sampling a Stream of Product by Attributes Indexed by AQL\n\n\n=== ANSI, ASQ ===\nANSI/ASQ Z1.4\n\n\n=== U.S. federal and military standards ===\nMIL-STD-105\nMIL-STD-1916\n\n\n== External links ==\n Media related to Sampling (statistics) at Wikimedia Commons", "Ensemble learning": "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n\n\n== Overview ==\nSupervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner.\nThe broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning on one non-ensemble system. An ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method.  Fast algorithms such as decision trees are commonly used in ensemble methods (for example, random forests), although slower algorithms can benefit from ensemble techniques as well.\nBy analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.\n\n\n== Ensemble theory ==\nEmpirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees). Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity. It is possible to increase diversity in the training stage of the model using correlation for regression tasks  or using information measures such as cross entropy for classification tasks.\n\n\n== Ensemble size ==\nWhile the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called \"the law of diminishing returns in ensemble construction.\" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.\n\n\n== Common types of ensembles ==\n\n\n=== Bayes optimal classifier ===\n\nThe Bayes optimal classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it. The naive Bayes optimal classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes optimal classifier can be expressed with the following equation:\n\n  \n    \n      \n        y\n        =\n        \n          \n            \n              a\n              r\n              g\n              m\n              a\n              x\n            \n            \n              \n                c\n                \n                  j\n                \n              \n              \u2208\n              C\n            \n          \n        \n        \n          \u2211\n          \n            \n              h\n              \n                i\n              \n            \n            \u2208\n            H\n          \n        \n        \n          P\n          (\n          \n            c\n            \n              j\n            \n          \n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          T\n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          \n            h\n            \n              i\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle y={\\underset {c_{j}\\in C}{\\mathrm {argmax} }}\\sum _{h_{i}\\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}\n  where \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is the predicted class, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is the set of all possible classes, \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   is the hypothesis space, \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   refers to a probability, and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is the training data. As an ensemble, the Bayes optimal classifier represents a hypothesis that is not necessarily in \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  . The hypothesis represented by the Bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  ).\nThis formula can be restated using Bayes' theorem, which says that the posterior is proportional to the likelihood times the prior:\n\n  \n    \n      \n        P\n        (\n        \n          h\n          \n            i\n          \n        \n        \n          |\n        \n        T\n        )\n        \u221d\n        P\n        (\n        T\n        \n          |\n        \n        \n          h\n          \n            i\n          \n        \n        )\n        P\n        (\n        \n          h\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle P(h_{i}|T)\\propto P(T|h_{i})P(h_{i})}\n  hence,\n\n  \n    \n      \n        y\n        =\n        \n          \n            \n              a\n              r\n              g\n              m\n              a\n              x\n            \n            \n              \n                c\n                \n                  j\n                \n              \n              \u2208\n              C\n            \n          \n        \n        \n          \u2211\n          \n            \n              h\n              \n                i\n              \n            \n            \u2208\n            H\n          \n        \n        \n          P\n          (\n          \n            c\n            \n              j\n            \n          \n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          \n            h\n            \n              i\n            \n          \n          \n            |\n          \n          T\n          )\n        \n      \n    \n    {\\displaystyle y={\\underset {c_{j}\\in C}{\\mathrm {argmax} }}\\sum _{h_{i}\\in H}{P(c_{j}|h_{i})P(h_{i}|T)}}\n  \n\n\n=== Bootstrap aggregating (bagging) ===\nBootstrap aggregation (bagging) involves training an ensemble on bootstrapped data sets. A bootstrapped set is created by selecting from original training data set with replacement. Thus, a bootstrap set may contain a given example zero, one, or multiple times. Ensemble members can also have limits on the features (e.g., nodes of a decision tree), to encourage exploring of diverse features. The variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble, and can strengthen the ensemble. To reduce overfitting, a member can be validated using the out-of-bag set (the examples that are not in its bootstrap set).Inference is done by voting of predictions of ensemble members, called aggregation. It is illustrated below with an ensemble of four decision trees. The query example is classified by each tree. Because three of the four predict the positive class, the ensemble's overall classification is positive. Random forests like the one shown are a common application of bagging.\n\n\n=== Boosting ===\n\nBoosting involves training successively models by emphasizing training data mis-classified by previously learned models. Initially, all data (D1) has equal weight and is used to learn a base model M1. The examples mis-classified by M1 are assigned a weight greater than correctly classified examples. This boosted data (D2) is used to train a second base model M2, and so on. Inference is done by voting.\nIn some cases, boosting has yielded better accuracy than bagging, but tends to over-fit more. The most common implementation of boosting is Adaboost, but some newer algorithms are reported to achieve better results.\n\n\n=== Bayesian model averaging ===\nBayesian model averaging (BMA) makes predictions by averaging the predictions of models weighted by their posterior probabilities given the data. BMA is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently.\nThe question with any use of Bayes' theorem is the prior, i.e., the probability (perhaps subjective) that each model is the best to use for a given purpose.  Conceptually, BMA can be used with any prior.  R packages ensembleBMA and BMA use the prior implied by the Bayesian information criterion, (BIC), following Raftery (1995). R package BAS supports the use of the priors implied by Akaike information criterion (AIC) and other criteria over the alternative models as well as priors over the coefficients.The difference between BIC and AIC is the strength of preference for parsimony.  BIC's penalty for model complexity is \n  \n    \n      \n        ln\n        \u2061\n        (\n        n\n        )\n        k\n      \n    \n    {\\displaystyle \\ln(n)k}\n   , while AIC's is \n  \n    \n      \n        2\n        k\n      \n    \n    {\\displaystyle 2k}\n  . Large-sample asymptotic theory establishes that if there is a best model, then with increasing sample sizes, BIC is strongly consistent, i.e., will almost certainly find it, while AIC may not, because AIC may continue to place excessive posterior probability on models that are more complicated than they need to be. On the other hand, AIC and AICc are asymptotically \u201cefficient\u201d (i.e., minimum mean square prediction error), while BIC is not .Haussler et al. (1994) showed that when BMA is used for classification, its expected error is at most twice the expected error of the Bayes optimal classifier. Burnham and Anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of Bayesian model averaging and popularizing the methodology. The availability of software, including other free open-source packages for R beyond those mentioned above, helped make the methods accessible to a wider audience.\n\n\n=== Bayesian model combination ===\nBayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weights drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. BMC has been shown to be better on average (with statistical significance) than BMA and bagging.Use of Bayes' law to compute model weights requires computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but this is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection.\nThe possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.\nThe results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.\n\n\n=== Bucket of models ===\nA \"bucket of models\" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\nThe most common approach used for model-selection is cross-validation selection (sometimes called a \"bake-off contest\"). It is described with the following pseudo-code:\n\nFor each model m in the bucket:\n    Do c times: (where 'c' is some constant)\n        Randomly divide the training dataset into two sets: A and B\n        Train m with A\n        Test m with B\nSelect the model that obtains the highest average score\n\nCross-Validation Selection can be summed up as: \"try them all with the training set, and pick the one that works best\".Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the \"best\" model, or it can be used to give a linear weight to the predictions from each model in the bucket.\nWhen a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.\n\n\n=== Stacking ===\nStacking (sometimes called stacked generalization) involves training a model to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm (final estimator) is trained to make a final prediction using all the predictions of the other algorithms (base estimators) as additional inputs or using cross-validated predictions from the base estimators which can prevent overfitting. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.\nStacking typically yields performance better than any single one of the trained models. It has been successfully used on both supervised learning tasks (regression, classification and distance learning ) and unsupervised learning (density estimation). It has also been used to estimate bagging's error rate. It has been reported to out-perform Bayesian model-averaging. The two top-performers in the Netflix competition utilized blending, which may be considered a form of stacking.\n\n\n=== Voting ===\nVoting is another form of ensembling. See e.g. Weighted majority algorithm (machine learning).\n\n\n== Implementations in statistics packages ==\nR: at least three packages offer Bayesian model averaging tools, including the BMS (an acronym for Bayesian Model Selection) package, the BAS (an acronym for Bayesian Adaptive Sampling) package, and the BMA package.\nPython: scikit-learn, a package for machine learning in Python offers packages for ensemble learning including packages for bagging, voting and averaging methods.\nMATLAB: classification ensembles are implemented in Statistics and Machine Learning Toolbox.\n\n\n== Ensemble learning applications ==\nIn recent years, due to growing computational power, which allows for training in large ensemble learning in a reasonable time frame, the number of ensemble learning applications has grown increasingly. Some of the applications of ensemble classifiers include:\n\n\n=== Remote sensing ===\n\n\n==== Land cover mapping ====\nLand cover mapping is one of the major applications of Earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. Generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation. Some different ensemble learning approaches based on artificial neural networks, kernel principal component analysis (KPCA), decision trees with boosting, random forest and automatic design of multiple classifier systems, are proposed to efficiently identify land cover objects.\n\n\n==== Change detection ====\nChange detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time. Change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring.\nThe earliest applications of ensemble classifiers in change detection are designed with the majority voting, Bayesian average and the maximum posterior probability.\n\n\n=== Computer security ===\n\n\n==== Distributed denial of service ====\nDistributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider. By combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds.\n\n\n==== Malware Detection ====\nClassification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem. Ensemble learning systems have shown a proper efficacy in this area.\n\n\n==== Intrusion detection ====\nAn intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. Ensemble learning successfully aids such monitoring systems to reduce their total error.\n\n\n=== Face recognition ===\n\nFace recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images.Hierarchical ensembles based on Gabor Fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field.\n\n\n=== Emotion recognition ===\n\nWhile speech recognition is mainly based on deep learning because most of the industry players in this field like Google, Microsoft and IBM reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning.It is also being successfully used in facial emotion recognition.\n\n\n=== Fraud detection ===\n\nFraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning. Because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems.\n\n\n=== Financial decision-making ===\nThe accuracy of prediction of business failure is a very crucial issue in financial decision-making. Therefore, different ensemble classifiers are proposed to predict financial crises and financial distress. Also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation.\n\n\n=== Medicine ===\nEnsemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. Alzheimer or myotonic dystrophy) detection based on MRI datasets, and cervical cytology classification.\n\n\n== See also ==\nEnsemble averaging (machine learning)\nBayesian structural time series (BSTS)\n\n\n== References ==\n\n\n== Further reading ==\nZhou Zhihua (2012). Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC. ISBN 978-1-439-83003-1.\nRobert Schapire; Yoav Freund (2012). Boosting: Foundations and Algorithms. MIT. ISBN 978-0-262-01718-3.\n\n\n== External links ==\nRobi Polikar (ed.). \"Ensemble learning\". Scholarpedia.\nThe Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques", "Anomaly detection": "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.Anomaly detection finds application in many domains including cyber security, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.\nThree broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.\n\n\n== Definition ==\nMany attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include:\n\nAn outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\nAnomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.\nAn outlier is an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.\nAn anomaly is a point or collection of points that is relatively distant from other points in multi-dimensional space of features.\nAnomalies are patterns in data that do not conform to a well defined notion of normal behaviour.\nLet T be observations from a univariate Gaussian distribution and O a point from T. Then the z-score for O is greater than a pre-selected threshold if and only if O is an outlier.\n\n\n== Applications ==\nAnomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.  The counterpart of anomaly detection in intrusion detection is misuse detection.\nIt is often used in preprocessing to remove anomalous data from the dataset. This is done for a number of reasons. Statistics of data such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy. Anomalies are also often the most important observations in the data to be found such as in intrusion detection or detecting abnormalities in medical images.\n\n\n== Popular techniques ==\nMany anomaly detection techniques have been proposed in literature. Some of the popular techniques are:\n\nStatistical (Z-score, Tukey's range test and Grubbs's test)\nDensity-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\nSubspace-, correlation-based and tensor-based  outlier detection for high-dimensional data\nOne-class support vector machines\nReplicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks\nBayesian networks\nHidden Markov models (HMMs)\nMinimum Covariance Determinant\nClustering: Cluster analysis-based outlier detection\nDeviations from association rules and frequent itemsets\nFuzzy logic-based outlier detection\nEnsemble techniques, using feature bagging, score normalization and different sources of diversityThe performance of methods depends on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.\n\n\n== Explainable Anomaly Detection ==\nMany of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:\n\nThe Subspace Outlier Degree (SOD) identifies attributes where a sample is normal, and attributes in which the sample deviates from the expected.\nCorrelation Outlier Probabilities (COP) compute an error vector how a sample point deviates from an expected location, which can be interpreted as a counterfactual explanation: the sample would be normal if it were moved to that location.\n\n\n== Software ==\nELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.\nPyOD is an open-source Python library developed specifically for anomaly detection.\nscikit-learn is an open-source Python library that contains some algorithms for unsupervised anomaly detection.\nWolfram Mathematica provides functionality for unsupervised anomaly detection across multiple data types \n\n\n== Datasets ==\nAnomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen; Mirror at University of S\u00e3o Paulo.\nODDS \u2013 ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.\nUnsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.\nKMASH Data Repository  at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth.\n\n\n== See also ==\nChange detection\nStatistical process control\nNovelty detection\nHierarchical temporal memory\n\n\n== References ==", "Maximum likelihood": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when all observed outcomes are assumed to have normal distributions with the same variance.From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions (or a normal prior distribution with a standard deviation of infinity). In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.\n\n\n== Principles ==\nWe model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. We write the parameters governing the joint distribution as a vector \n  \n    \n      \n        \n        \u03b8\n        =\n        \n          \n            [\n            \n              \n                \u03b8\n                \n                  1\n                \n              \n              ,\n              \n              \n                \u03b8\n                \n                  2\n                \n              \n              ,\n              \n              \u2026\n              ,\n              \n              \n                \u03b8\n                \n                  k\n                \n              \n            \n            ]\n          \n          \n            \n              T\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\;\\theta =\\left[\\theta _{1},\\,\\theta _{2},\\,\\ldots ,\\,\\theta _{k}\\right]^{\\mathsf {T}}\\;}\n   so that this distribution falls within a parametric family \n  \n    \n      \n        \n        {\n        f\n        (\n        \u22c5\n        \n        ;\n        \u03b8\n        )\n        \u2223\n        \u03b8\n        \u2208\n        \u0398\n        }\n        \n        ,\n      \n    \n    {\\displaystyle \\;\\{f(\\cdot \\,;\\theta )\\mid \\theta \\in \\Theta \\}\\;,}\n   where \n  \n    \n      \n        \n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\Theta \\,}\n   is called the parameter space, a finite-dimensional subset of Euclidean space. Evaluating the joint density at the observed data sample \n  \n    \n      \n        \n        \n          y\n        \n        =\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle \\;\\mathbf {y} =(y_{1},y_{2},\\ldots ,y_{n})\\;}\n   gives a real-valued function,\n\n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n        (\n        \u03b8\n        )\n        =\n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n        (\n        \u03b8\n        ;\n        \n          y\n        \n        )\n        =\n        \n          f\n          \n            n\n          \n        \n        (\n        \n          y\n        \n        ;\n        \u03b8\n        )\n        \n        ,\n      \n    \n    {\\displaystyle {\\mathcal {L}}_{n}(\\theta )={\\mathcal {L}}_{n}(\\theta ;\\mathbf {y} )=f_{n}(\\mathbf {y} ;\\theta )\\;,}\n  which is called the likelihood function. For independent and identically distributed random variables, \n  \n    \n      \n        \n          f\n          \n            n\n          \n        \n        (\n        \n          y\n        \n        ;\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f_{n}(\\mathbf {y} ;\\theta )}\n   will be the product of univariate density functions:\n\n  \n    \n      \n        \n          f\n          \n            n\n          \n        \n        (\n        \n          y\n        \n        ;\n        \u03b8\n        )\n        =\n        \n          \u220f\n          \n            k\n            =\n            1\n          \n          \n            n\n          \n        \n        \n        \n          f\n          \n            k\n          \n          \n            \n              u\n              n\n              i\n              v\n              a\n              r\n            \n          \n        \n        (\n        \n          y\n          \n            k\n          \n        \n        ;\n        \u03b8\n        )\n         \n        .\n      \n    \n    {\\displaystyle f_{n}(\\mathbf {y} ;\\theta )=\\prod _{k=1}^{n}\\,f_{k}^{\\mathsf {univar}}(y_{k};\\theta )~.}\n  The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space, that is\n\n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            \n              \u03b8\n              \u2208\n              \u0398\n            \n          \n        \n        \n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n        (\n        \u03b8\n        \n        ;\n        \n          y\n        \n        )\n         \n        .\n      \n    \n    {\\displaystyle {\\hat {\\theta }}={\\underset {\\theta \\in \\Theta }{\\operatorname {arg\\;max} }}\\,{\\mathcal {L}}_{n}(\\theta \\,;\\mathbf {y} )~.}\n  Intuitively, this selects the parameter values that make the observed data most probable. The specific value \n  \n    \n      \n         \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        (\n        \n          y\n        \n        )\n        \u2208\n        \u0398\n         \n      \n    \n    {\\displaystyle ~{\\hat {\\theta }}={\\hat {\\theta }}_{n}(\\mathbf {y} )\\in \\Theta ~}\n   that maximizes the likelihood function \n  \n    \n      \n        \n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n        \n      \n    \n    {\\displaystyle \\,{\\mathcal {L}}_{n}\\,}\n   is called the maximum likelihood estimate. Further, if the function \n  \n    \n      \n        \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        :\n        \n          \n            R\n          \n          \n            n\n          \n        \n        \u2192\n        \u0398\n        \n      \n    \n    {\\displaystyle \\;{\\hat {\\theta }}_{n}:\\mathbb {R} ^{n}\\to \\Theta \\;}\n   so defined is measurable, then it is called the maximum likelihood estimator. It is generally a function defined over the sample space, i.e. taking a given sample as its argument. A sufficient but not necessary condition for its existence is for the likelihood function to be continuous over a parameter space \n  \n    \n      \n        \n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\Theta \\,}\n   that is compact. For an open \n  \n    \n      \n        \n        \u0398\n        \n      \n    \n    {\\displaystyle \\,\\Theta \\,}\n   the likelihood function may increase without ever reaching a supremum value.\nIn practice, it is often convenient to work with the natural logarithm of the likelihood function, called the log-likelihood:\n\n  \n    \n      \n        \u2113\n        (\n        \u03b8\n        \n        ;\n        \n          y\n        \n        )\n        =\n        ln\n        \u2061\n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n        (\n        \u03b8\n        \n        ;\n        \n          y\n        \n        )\n         \n        .\n      \n    \n    {\\displaystyle \\ell (\\theta \\,;\\mathbf {y} )=\\ln {\\mathcal {L}}_{n}(\\theta \\,;\\mathbf {y} )~.}\n  Since the logarithm is a monotonic function, the maximum of \n  \n    \n      \n        \n        \u2113\n        (\n        \u03b8\n        \n        ;\n        \n          y\n        \n        )\n        \n      \n    \n    {\\displaystyle \\;\\ell (\\theta \\,;\\mathbf {y} )\\;}\n   occurs at the same value of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   as does the maximum of \n  \n    \n      \n        \n        \n          \n            \n              L\n            \n          \n          \n            n\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\,{\\mathcal {L}}_{n}~.}\n   If \n  \n    \n      \n        \u2113\n        (\n        \u03b8\n        \n        ;\n        \n          y\n        \n        )\n      \n    \n    {\\displaystyle \\ell (\\theta \\,;\\mathbf {y} )}\n   is differentiable in \n  \n    \n      \n        \n        \u0398\n        \n        ,\n      \n    \n    {\\displaystyle \\,\\Theta \\,,}\n   the necessary conditions for the occurrence of a maximum (or a minimum) are\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              \u2113\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        0\n        ,\n        \n        \n          \n            \n              \u2202\n              \u2113\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        0\n        ,\n        \n        \u2026\n        ,\n        \n        \n          \n            \n              \u2202\n              \u2113\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  k\n                \n              \n            \n          \n        \n        =\n        0\n         \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\partial \\ell }{\\partial \\theta _{1}}}=0,\\quad {\\frac {\\partial \\ell }{\\partial \\theta _{2}}}=0,\\quad \\ldots ,\\quad {\\frac {\\partial \\ell }{\\partial \\theta _{k}}}=0~,}\n  known as the likelihood equations. For some models, these equations can be explicitly solved for \n  \n    \n      \n        \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\,{\\widehat {\\theta \\,}}\\,,}\n   but in general no closed-form solution to the maximization problem is known or available, and an MLE can only be found via numerical optimization. Another problem is that in finite samples, there may exist multiple roots for the likelihood equations. Whether the identified root \n  \n    \n      \n        \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\,{\\widehat {\\theta \\,}}\\,}\n   of the likelihood equations is indeed a (local) maximum depends on whether the matrix of second-order partial and cross-partial derivatives, the so-called Hessian matrix\n\n  \n    \n      \n        \n          H\n        \n        \n          (\n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          )\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                1\n                              \n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                1\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \u2026\n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                1\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                k\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                2\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                1\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                2\n                              \n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \u2026\n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                2\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                k\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n                \n                  \u22ee\n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                k\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                1\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                k\n                              \n                            \n                            \n                            \u2202\n                            \n                              \u03b8\n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n                \n                  \u2026\n                \n                \n                  \n                    \n                      \n                      \n                        \n                          \n                            \n                              \u2202\n                              \n                                2\n                              \n                            \n                            \u2113\n                          \n                          \n                            \u2202\n                            \n                              \u03b8\n                              \n                                k\n                              \n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      |\n                    \n                    \n                      \u03b8\n                      =\n                      \n                        \n                          \n                            \n                              \u03b8\n                              \n                            \n                            ^\n                          \n                        \n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\mathbf {H} \\left({\\widehat {\\theta \\,}}\\right)={\\begin{bmatrix}\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{1}^{2}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{1}\\,\\partial \\theta _{2}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\dots &\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{1}\\,\\partial \\theta _{k}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}\\\\\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{2}\\,\\partial \\theta _{1}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{2}^{2}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\dots &\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{2}\\,\\partial \\theta _{k}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{k}\\,\\partial \\theta _{1}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{k}\\,\\partial \\theta _{2}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}&\\dots &\\left.{\\frac {\\partial ^{2}\\ell }{\\partial \\theta _{k}^{2}}}\\right|_{\\theta ={\\widehat {\\theta \\,}}}\\end{bmatrix}}~,}\n  is negative semi-definite at \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}}\n  , as this indicates local concavity. Conveniently, most common probability distributions \u2013 in particular the exponential family \u2013 are logarithmically concave.\n\n\n=== Restricted parameter space ===\n\nWhile the domain of the likelihood function\u2014the parameter space\u2014is generally a finite-dimensional subset of Euclidean space, additional restrictions sometimes need to be incorporated into the estimation process. The parameter space can be expressed as\n\n  \n    \n      \n        \u0398\n        =\n        \n          {\n          \n            \u03b8\n            :\n            \u03b8\n            \u2208\n            \n              \n                R\n              \n              \n                k\n              \n            \n            ,\n            \n            h\n            (\n            \u03b8\n            )\n            =\n            0\n          \n          }\n        \n         \n        ,\n      \n    \n    {\\displaystyle \\Theta =\\left\\{\\theta :\\theta \\in \\mathbb {R} ^{k},\\;h(\\theta )=0\\right\\}~,}\n  where \n  \n    \n      \n        \n        h\n        (\n        \u03b8\n        )\n        =\n        \n          [\n          \n            \n              h\n              \n                1\n              \n            \n            (\n            \u03b8\n            )\n            ,\n            \n              h\n              \n                2\n              \n            \n            (\n            \u03b8\n            )\n            ,\n            \u2026\n            ,\n            \n              h\n              \n                r\n              \n            \n            (\n            \u03b8\n            )\n          \n          ]\n        \n        \n      \n    \n    {\\displaystyle \\;h(\\theta )=\\left[h_{1}(\\theta ),h_{2}(\\theta ),\\ldots ,h_{r}(\\theta )\\right]\\;}\n   is a vector-valued function mapping \n  \n    \n      \n        \n        \n          \n            R\n          \n          \n            k\n          \n        \n        \n      \n    \n    {\\displaystyle \\,\\mathbb {R} ^{k}\\,}\n   into \n  \n    \n      \n        \n        \n          \n            R\n          \n          \n            r\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\;\\mathbb {R} ^{r}~.}\n   Estimating the true parameter \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   belonging to \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   then, as a practical matter, means to find the maximum of the likelihood function subject to the constraint \n  \n    \n      \n         \n        h\n        (\n        \u03b8\n        )\n        =\n        0\n         \n        .\n      \n    \n    {\\displaystyle ~h(\\theta )=0~.}\n  \nTheoretically, the most natural approach to this constrained optimization problem is the method of substitution, that is \"filling out\" the restrictions \n  \n    \n      \n        \n        \n          h\n          \n            1\n          \n        \n        ,\n        \n          h\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          h\n          \n            r\n          \n        \n        \n      \n    \n    {\\displaystyle \\;h_{1},h_{2},\\ldots ,h_{r}\\;}\n   to a set \n  \n    \n      \n        \n        \n          h\n          \n            1\n          \n        \n        ,\n        \n          h\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          h\n          \n            r\n          \n        \n        ,\n        \n          h\n          \n            r\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          h\n          \n            k\n          \n        \n        \n      \n    \n    {\\displaystyle \\;h_{1},h_{2},\\ldots ,h_{r},h_{r+1},\\ldots ,h_{k}\\;}\n   in such a way that \n  \n    \n      \n        \n        \n          h\n          \n            \u2217\n          \n        \n        =\n        \n          [\n          \n            \n              h\n              \n                1\n              \n            \n            ,\n            \n              h\n              \n                2\n              \n            \n            ,\n            \u2026\n            ,\n            \n              h\n              \n                k\n              \n            \n          \n          ]\n        \n        \n      \n    \n    {\\displaystyle \\;h^{\\ast }=\\left[h_{1},h_{2},\\ldots ,h_{k}\\right]\\;}\n   is a one-to-one function from \n  \n    \n      \n        \n          \n            R\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{k}}\n   to itself, and reparameterize the likelihood function by setting \n  \n    \n      \n        \n        \n          \u03d5\n          \n            i\n          \n        \n        =\n        \n          h\n          \n            i\n          \n        \n        (\n        \n          \u03b8\n          \n            1\n          \n        \n        ,\n        \n          \u03b8\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u03b8\n          \n            k\n          \n        \n        )\n         \n        .\n      \n    \n    {\\displaystyle \\;\\phi _{i}=h_{i}(\\theta _{1},\\theta _{2},\\ldots ,\\theta _{k})~.}\n   Because of the equivariance of the maximum likelihood estimator, the properties of the MLE apply to the restricted estimates also. For instance, in a multivariate normal distribution the covariance matrix \n  \n    \n      \n        \n        \u03a3\n        \n      \n    \n    {\\displaystyle \\,\\Sigma \\,}\n   must be positive-definite; this restriction can be imposed by replacing \n  \n    \n      \n        \n        \u03a3\n        =\n        \n          \u0393\n          \n            \n              T\n            \n          \n        \n        \u0393\n        \n        ,\n      \n    \n    {\\displaystyle \\;\\Sigma =\\Gamma ^{\\mathsf {T}}\\Gamma \\;,}\n   where \n  \n    \n      \n        \u0393\n      \n    \n    {\\displaystyle \\Gamma }\n   is a real upper triangular matrix and \n  \n    \n      \n        \n          \u0393\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\Gamma ^{\\mathsf {T}}}\n   is its transpose.In practice, restrictions are usually imposed using the method of Lagrange which, given the constraints as defined above, leads to the restricted likelihood equations\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              \u2113\n            \n            \n              \u2202\n              \u03b8\n            \n          \n        \n        \u2212\n        \n          \n            \n              \u2202\n              h\n              (\n              \u03b8\n              \n                )\n                \n                  \n                    T\n                  \n                \n              \n            \n            \n              \u2202\n              \u03b8\n            \n          \n        \n        \u03bb\n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial \\ell }{\\partial \\theta }}-{\\frac {\\partial h(\\theta )^{\\mathsf {T}}}{\\partial \\theta }}\\lambda =0}\n   and \n  \n    \n      \n        h\n        (\n        \u03b8\n        )\n        =\n        0\n        \n        ,\n      \n    \n    {\\displaystyle h(\\theta )=0\\;,}\n  where \n  \n    \n      \n         \n        \u03bb\n        =\n        \n          \n            [\n            \n              \n                \u03bb\n                \n                  1\n                \n              \n              ,\n              \n                \u03bb\n                \n                  2\n                \n              \n              ,\n              \u2026\n              ,\n              \n                \u03bb\n                \n                  r\n                \n              \n            \n            ]\n          \n          \n            \n              T\n            \n          \n        \n         \n      \n    \n    {\\displaystyle ~\\lambda =\\left[\\lambda _{1},\\lambda _{2},\\ldots ,\\lambda _{r}\\right]^{\\mathsf {T}}~}\n   is a column-vector of Lagrange multipliers and \n  \n    \n      \n        \n        \n          \n            \n              \u2202\n              h\n              (\n              \u03b8\n              \n                )\n                \n                  \n                    T\n                  \n                \n              \n            \n            \n              \u2202\n              \u03b8\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\;{\\frac {\\partial h(\\theta )^{\\mathsf {T}}}{\\partial \\theta }}\\;}\n   is the k \u00d7 r Jacobian matrix of partial derivatives. Naturally, if the constraints are not binding at the maximum, the Lagrange multipliers should be zero. This in turn allows for a statistical test of the \"validity\" of the constraint, known as the Lagrange multiplier test.\n\n\n== Properties ==\nA maximum likelihood estimator is an extremum estimator obtained by maximizing, as a function of \u03b8, the objective function \n  \n    \n      \n        \n          \n            \n              \n                \u2113\n                \n              \n              ^\n            \n          \n        \n        (\n        \u03b8\n        \n        ;\n        x\n        )\n      \n    \n    {\\displaystyle {\\widehat {\\ell \\,}}(\\theta \\,;x)}\n  . If the data are independent and identically distributed, then we have \n\n  \n    \n      \n        \n          \n            \n              \n                \u2113\n                \n              \n              ^\n            \n          \n        \n        (\n        \u03b8\n        \n        ;\n        x\n        )\n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        ln\n        \u2061\n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \u03b8\n        )\n        ,\n      \n    \n    {\\displaystyle {\\widehat {\\ell \\,}}(\\theta \\,;x)={\\frac {1}{n}}\\sum _{i=1}^{n}\\ln f(x_{i}\\mid \\theta ),}\n  this being the sample analogue of the expected log-likelihood \n  \n    \n      \n        \u2113\n        (\n        \u03b8\n        )\n        =\n        \n          \n            E\n          \n        \n        \u2061\n        [\n        \n        ln\n        \u2061\n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \u03b8\n        )\n        \n        ]\n      \n    \n    {\\displaystyle \\ell (\\theta )=\\operatorname {\\mathbb {E} } [\\,\\ln f(x_{i}\\mid \\theta )\\,]}\n  , where this expectation is taken with respect to the true density.\nMaximum-likelihood estimators have no optimum properties for finite samples, in the sense that (when evaluated on finite samples) other estimators may have greater concentration around the true parameter-value. However, like other estimation methods, maximum likelihood estimation possesses a number of attractive limiting properties: As the sample size increases to infinity, sequences of maximum likelihood estimators have these properties:\n\nConsistency: the sequence of MLEs converges in probability to the value being estimated.\nFunctional equivariance: If \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n   is the maximum likelihood estimator for \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , and if \n  \n    \n      \n        g\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle g(\\theta )}\n   is any transformation of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , then the maximum likelihood estimator for \n  \n    \n      \n        \u03b1\n        =\n        g\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\alpha =g(\\theta )}\n   is \n  \n    \n      \n        \n          \n            \n              \u03b1\n              ^\n            \n          \n        \n        =\n        g\n        (\n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {\\alpha }}=g({\\hat {\\theta }})}\n  .\nEfficiency, i.e. it achieves the Cram\u00e9r\u2013Rao lower bound when the sample size tends to infinity. This means that no consistent estimator has lower asymptotic mean squared error than the MLE (or other estimators attaining this bound), which also means that MLE has asymptotic normality.\nSecond-order efficiency after correction for bias.\n\n\n=== Consistency ===\nUnder the conditions outlined below, the maximum likelihood estimator is consistent. The consistency means that if the data were generated by \n  \n    \n      \n        f\n        (\n        \u22c5\n        \n        ;\n        \n          \u03b8\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f(\\cdot \\,;\\theta _{0})}\n   and we have a sufficiently large number of observations n, then it is possible to find the value of \u03b80 with arbitrary precision. In mathematical terms this means that as n goes to infinity the estimator \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}}\n   converges in probability to its true value:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          \n            \n              m\n              l\n              e\n            \n          \n        \n         \n        \n          \n            \u2192\n            \n              p\n            \n          \n        \n         \n        \n          \u03b8\n          \n            0\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}_{\\mathrm {mle} }\\ {\\xrightarrow {\\text{p}}}\\ \\theta _{0}.}\n  Under slightly stronger conditions, the estimator converges almost surely (or strongly):\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          \n            \n              m\n              l\n              e\n            \n          \n        \n         \n        \n          \n            \u2192\n            \n              a.s.\n            \n          \n        \n         \n        \n          \u03b8\n          \n            0\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}_{\\mathrm {mle} }\\ {\\xrightarrow {\\text{a.s.}}}\\ \\theta _{0}.}\n  In practical applications, data is never generated by \n  \n    \n      \n        f\n        (\n        \u22c5\n        \n        ;\n        \n          \u03b8\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f(\\cdot \\,;\\theta _{0})}\n  . Rather, \n  \n    \n      \n        f\n        (\n        \u22c5\n        \n        ;\n        \n          \u03b8\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f(\\cdot \\,;\\theta _{0})}\n   is a model, often in idealized form, of the process generated by the data. It is a common aphorism in statistics that all models are wrong. Thus, true consistency does not occur in practical applications. Nevertheless, consistency is often considered to be a desirable property for an estimator to have.\nTo establish consistency, the following conditions are sufficient.\n\nThe dominance condition can be employed in the case of i.i.d. observations. In the non-i.i.d. case, the uniform convergence in probability can be checked by showing that the sequence \n  \n    \n      \n        \n          \n            \n              \n                \u2113\n                \n              \n              ^\n            \n          \n        \n        (\n        \u03b8\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle {\\widehat {\\ell \\,}}(\\theta \\mid x)}\n   is stochastically equicontinuous.\n\nIf one wants to demonstrate that the ML estimator \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}}\n   converges to \u03b80 almost surely, then a stronger condition of uniform convergence almost surely has to be imposed:\n\n  \n    \n      \n        \n          sup\n          \n            \u03b8\n            \u2208\n            \u0398\n          \n        \n        \n          \u2016\n          \n            \n            \n              \n                \n                  \n                    \u2113\n                    \n                  \n                  ^\n                \n              \n            \n            (\n            \u03b8\n            \u2223\n            x\n            )\n            \u2212\n            \u2113\n            (\n            \u03b8\n            )\n            \n          \n          \u2016\n        \n         \n        \n          \u2192\n          \n            a.s.\n          \n        \n         \n        0.\n      \n    \n    {\\displaystyle \\sup _{\\theta \\in \\Theta }\\left\\|\\;{\\widehat {\\ell \\,}}(\\theta \\mid x)-\\ell (\\theta )\\;\\right\\|\\ \\xrightarrow {\\text{a.s.}} \\ 0.}\n  \n\nAdditionally, if (as assumed above) the data were generated by \n  \n    \n      \n        f\n        (\n        \u22c5\n        \n        ;\n        \n          \u03b8\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f(\\cdot \\,;\\theta _{0})}\n  , then under certain conditions, it can also be shown that the maximum likelihood estimator converges in distribution to a normal distribution. Specifically,\n  \n    \n      \n        \n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \u03b8\n                      \n                    \n                    ^\n                  \n                \n              \n              \n                \n                  m\n                  l\n                  e\n                \n              \n            \n            \u2212\n            \n              \u03b8\n              \n                0\n              \n            \n          \n          )\n        \n         \n        \n          \u2192\n          \n            d\n          \n        \n         \n        \n          \n            N\n          \n        \n        \n          (\n          \n            0\n            ,\n            \n            \n              I\n              \n                \u2212\n                1\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\sqrt {n}}\\left({\\widehat {\\theta \\,}}_{\\mathrm {mle} }-\\theta _{0}\\right)\\ \\xrightarrow {d} \\ {\\mathcal {N}}\\left(0,\\,I^{-1}\\right)}\n  \nwhere I is the Fisher information matrix.\n\n\n=== Functional invariance ===\nThe maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability (or probability density, in the continuous case). If the parameter consists of a number of components, then we define their separate maximum likelihood estimators, as the corresponding component of the MLE of the complete parameter. Consistent with this, if \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}}\n   is the MLE for \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , and if \n  \n    \n      \n        g\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle g(\\theta )}\n   is any transformation of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , then the MLE for \n  \n    \n      \n        \u03b1\n        =\n        g\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\alpha =g(\\theta )}\n   is by definition\n\n  \n    \n      \n        \n          \n            \n              \u03b1\n              ^\n            \n          \n        \n        =\n        g\n        (\n        \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \n        )\n        .\n        \n      \n    \n    {\\displaystyle {\\widehat {\\alpha }}=g(\\,{\\widehat {\\theta \\,}}\\,).\\,}\n  It maximizes the so-called profile likelihood:\n\n  \n    \n      \n        \n          \n            \n              L\n              \u00af\n            \n          \n        \n        (\n        \u03b1\n        )\n        =\n        \n          sup\n          \n            \u03b8\n            :\n            \u03b1\n            =\n            g\n            (\n            \u03b8\n            )\n          \n        \n        L\n        (\n        \u03b8\n        )\n        .\n        \n      \n    \n    {\\displaystyle {\\bar {L}}(\\alpha )=\\sup _{\\theta :\\alpha =g(\\theta )}L(\\theta ).\\,}\n  The MLE is also equivariant with respect to certain transformations of the data.  If \n  \n    \n      \n        y\n        =\n        g\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y=g(x)}\n   where \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   is one to one and does not depend on the parameters to be estimated, then the density functions satisfy\n\n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n            \n              \n                |\n              \n              \n                g\n                \u2032\n              \n              (\n              x\n              )\n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f_{Y}(y)={\\frac {f_{X}(x)}{|g'(x)|}}}\n  and hence the likelihood functions for \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   differ only by a factor that does not depend on the model parameters.\nFor example, the MLE parameters of the log-normal distribution are the same as those of the normal distribution fitted to the logarithm of the data.\n\n\n=== Efficiency ===\nAs assumed above, if the data were generated by \n  \n    \n      \n         \n        f\n        (\n        \u22c5\n        \n        ;\n        \n          \u03b8\n          \n            0\n          \n        \n        )\n         \n        ,\n      \n    \n    {\\displaystyle ~f(\\cdot \\,;\\theta _{0})~,}\n   then under certain conditions, it can also be shown that the maximum likelihood estimator converges in distribution to a normal distribution. It is \u221an -consistent and asymptotically efficient, meaning that it reaches the Cram\u00e9r\u2013Rao bound. Specifically,\n\n  \n    \n      \n        \n          \n            n\n            \n          \n        \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    \n                      \u03b8\n                      \n                    \n                    ^\n                  \n                \n              \n              \n                mle\n              \n            \n            \u2212\n            \n              \u03b8\n              \n                0\n              \n            \n          \n          )\n        \n         \n         \n        \n          \u2192\n          \n            d\n          \n        \n         \n         \n        \n          \n            N\n          \n        \n        \n          (\n          \n            0\n            ,\n             \n            \n              \n                \n                  I\n                \n              \n              \n                \u2212\n                1\n              \n            \n          \n          )\n        \n         \n        ,\n      \n    \n    {\\displaystyle {\\sqrt {n\\,}}\\,\\left({\\widehat {\\theta \\,}}_{\\text{mle}}-\\theta _{0}\\right)\\ \\ \\xrightarrow {d} \\ \\ {\\mathcal {N}}\\left(0,\\ {\\mathcal {I}}^{-1}\\right)~,}\n  where \n  \n    \n      \n         \n        \n          \n            I\n          \n        \n         \n      \n    \n    {\\displaystyle ~{\\mathcal {I}}~}\n   is the Fisher information matrix:\n\n  \n    \n      \n        \n          \n            \n              I\n            \n          \n          \n            j\n            k\n          \n        \n        =\n        \n          \n            E\n          \n        \n        \n        \n          \n            [\n          \n        \n        \n        \u2212\n        \n          \n            \n              \n                \u2202\n                \n                  2\n                \n              \n              ln\n              \u2061\n              \n                f\n                \n                  \n                    \u03b8\n                    \n                      0\n                    \n                  \n                \n              \n              (\n              \n                X\n                \n                  t\n                \n              \n              )\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  j\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  k\n                \n              \n            \n          \n        \n        \n        \n          \n            ]\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\mathcal {I}}_{jk}=\\operatorname {\\mathbb {E} } \\,{\\biggl [}\\;-{\\frac {\\partial ^{2}\\ln f_{\\theta _{0}}(X_{t})}{\\partial \\theta _{j}\\,\\partial \\theta _{k}}}\\;{\\biggr ]}~.}\n  In particular, it means that the bias of the maximum likelihood estimator is equal to zero up to the order 1/\u221an .\n\n\n=== Second-order efficiency after correction for bias ===\nHowever, when we consider the higher-order terms in the expansion of the distribution of this estimator, it turns out that \u03b8mle has bias of order 1\u2044n. This bias is equal to (componentwise)\n\n  \n    \n      \n        \n          b\n          \n            h\n          \n        \n        \n        \u2261\n        \n        \n          \n            E\n          \n        \n        \u2061\n        \n          \n            [\n          \n        \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \u03b8\n                      ^\n                    \n                  \n                \n                \n                  \n                    m\n                    l\n                    e\n                  \n                \n              \n              \u2212\n              \n                \u03b8\n                \n                  0\n                \n              \n            \n            )\n          \n          \n            h\n          \n        \n        \n        \n          \n            ]\n          \n        \n        \n        =\n        \n        \n          \n            1\n            \n              \n              n\n              \n            \n          \n        \n        \n        \n          \u2211\n          \n            i\n            ,\n            j\n            ,\n            k\n            =\n            1\n          \n          \n            m\n          \n        \n        \n        \n          \n            \n              I\n            \n          \n          \n            h\n            i\n          \n        \n        \n        \n          \n            \n              I\n            \n          \n          \n            j\n            k\n          \n        \n        \n          (\n          \n            \n              \n                1\n                \n                  \n                  2\n                  \n                \n              \n            \n            \n            \n              K\n              \n                i\n                j\n                k\n              \n            \n            \n            +\n            \n            \n              J\n              \n                j\n                ,\n                i\n                k\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle b_{h}\\;\\equiv \\;\\operatorname {\\mathbb {E} } {\\biggl [}\\;\\left({\\widehat {\\theta }}_{\\mathrm {mle} }-\\theta _{0}\\right)_{h}\\;{\\biggr ]}\\;=\\;{\\frac {1}{\\,n\\,}}\\,\\sum _{i,j,k=1}^{m}\\;{\\mathcal {I}}^{hi}\\;{\\mathcal {I}}^{jk}\\left({\\frac {1}{\\,2\\,}}\\,K_{ijk}\\;+\\;J_{j,ik}\\right)}\n  where \n  \n    \n      \n        \n          \n            \n              I\n            \n          \n          \n            j\n            k\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}^{jk}}\n   (with superscripts) denotes the (j,k)-th component of the inverse Fisher information matrix \n  \n    \n      \n        \n          \n            \n              I\n            \n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}^{-1}}\n  , and\n\n  \n    \n      \n        \n          \n            1\n            \n              \n              2\n              \n            \n          \n        \n        \n        \n          K\n          \n            i\n            j\n            k\n          \n        \n        \n        +\n        \n        \n          J\n          \n            j\n            ,\n            i\n            k\n          \n        \n        \n        =\n        \n        \n          \n            E\n          \n        \n        \n        \n          \n            [\n          \n        \n        \n        \n          \n            1\n            2\n          \n        \n        \n          \n            \n              \n                \u2202\n                \n                  3\n                \n              \n              ln\n              \u2061\n              \n                f\n                \n                  \n                    \u03b8\n                    \n                      0\n                    \n                  \n                \n              \n              (\n              \n                X\n                \n                  t\n                \n              \n              )\n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  i\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  j\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  k\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              \n              \u2202\n              ln\n              \u2061\n              \n                f\n                \n                  \n                    \u03b8\n                    \n                      0\n                    \n                  \n                \n              \n              (\n              \n                X\n                \n                  t\n                \n              \n              )\n              \n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  j\n                \n              \n            \n          \n        \n        \n        \n          \n            \n              \n              \n                \u2202\n                \n                  2\n                \n              \n              ln\n              \u2061\n              \n                f\n                \n                  \n                    \u03b8\n                    \n                      0\n                    \n                  \n                \n              \n              (\n              \n                X\n                \n                  t\n                \n              \n              )\n              \n            \n            \n              \u2202\n              \n                \u03b8\n                \n                  i\n                \n              \n              \n              \u2202\n              \n                \u03b8\n                \n                  k\n                \n              \n            \n          \n        \n        \n        \n          \n            ]\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\frac {1}{\\,2\\,}}\\,K_{ijk}\\;+\\;J_{j,ik}\\;=\\;\\operatorname {\\mathbb {E} } \\,{\\biggl [}\\;{\\frac {1}{2}}{\\frac {\\partial ^{3}\\ln f_{\\theta _{0}}(X_{t})}{\\partial \\theta _{i}\\;\\partial \\theta _{j}\\;\\partial \\theta _{k}}}+{\\frac {\\;\\partial \\ln f_{\\theta _{0}}(X_{t})\\;}{\\partial \\theta _{j}}}\\,{\\frac {\\;\\partial ^{2}\\ln f_{\\theta _{0}}(X_{t})\\;}{\\partial \\theta _{i}\\,\\partial \\theta _{k}}}\\;{\\biggr ]}~.}\n  Using these formulae it is possible to estimate the second-order bias of the maximum likelihood estimator, and correct for that bias by subtracting it:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          \n            mle\n          \n          \n            \u2217\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          \n            mle\n          \n        \n        \u2212\n        \n          \n            \n              \n                b\n                \n              \n              ^\n            \n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}_{\\text{mle}}^{*}={\\widehat {\\theta \\,}}_{\\text{mle}}-{\\widehat {b\\,}}~.}\n  This estimator is unbiased up to the terms of order 1/\u2009n\u2009, and is called the bias-corrected maximum likelihood estimator.\nThis bias-corrected estimator is second-order efficient (at least within the curved exponential family), meaning that it has minimal mean squared error among all second-order bias-corrected estimators, up to the terms of the order 1/\u2009n2\u2009 . It is possible to continue this process, that is to derive the third-order bias-correction term, and so on. However, the maximum likelihood estimator is not third-order efficient.\n\n\n=== Relation to Bayesian inference ===\nA maximum likelihood estimator coincides with the most probable Bayesian estimator given a uniform prior distribution on the parameters. Indeed, the maximum a posteriori estimate is the parameter \u03b8 that maximizes the probability of \u03b8 given the data, given by Bayes' theorem:\n\n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \u03b8\n        \u2223\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            \n              f\n              (\n              \n                x\n                \n                  1\n                \n              \n              ,\n              \n                x\n                \n                  2\n                \n              \n              ,\n              \u2026\n              ,\n              \n                x\n                \n                  n\n                \n              \n              \u2223\n              \u03b8\n              )\n              \n                \n                  P\n                \n              \n              \u2061\n              (\n              \u03b8\n              )\n            \n            \n              \n                \n                  P\n                \n              \n              \u2061\n              (\n              \n                x\n                \n                  1\n                \n              \n              ,\n              \n                x\n                \n                  2\n                \n              \n              ,\n              \u2026\n              ,\n              \n                x\n                \n                  n\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (\\theta \\mid x_{1},x_{2},\\ldots ,x_{n})={\\frac {f(x_{1},x_{2},\\ldots ,x_{n}\\mid \\theta )\\operatorname {\\mathbb {P} } (\\theta )}{\\operatorname {\\mathbb {P} } (x_{1},x_{2},\\ldots ,x_{n})}}}\n  where \n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (\\theta )}\n   is the prior distribution for the parameter \u03b8 and where \n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (x_{1},x_{2},\\ldots ,x_{n})}\n   is the probability of the data averaged over all parameters. Since the denominator is independent of \u03b8, the Bayesian estimator is obtained by maximizing \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        \u2223\n        \u03b8\n        )\n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(x_{1},x_{2},\\ldots ,x_{n}\\mid \\theta )\\operatorname {\\mathbb {P} } (\\theta )}\n   with respect to \u03b8. If we further assume that the prior \n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (\\theta )}\n   is a uniform distribution, the Bayesian estimator is obtained by maximizing the likelihood function \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        \u2223\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(x_{1},x_{2},\\ldots ,x_{n}\\mid \\theta )}\n  . Thus the Bayesian estimator coincides with the maximum likelihood estimator for a uniform prior distribution \n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (\\theta )}\n  .\n\n\n==== Application of maximum-likelihood estimation in Bayes decision theory ====\nIn many practical applications in machine learning, maximum-likelihood estimation is used as the model for parameter estimation.\nThe Bayesian Decision theory is about designing a classifier that minimizes total expected risk, especially, when the costs (the loss function) associated with different decisions are equal, the classifier is minimizing the error over the whole distribution.Thus, the Bayes Decision Rule is stated as\n\n\"decide \n  \n    \n      \n        \n        \n          w\n          \n            1\n          \n        \n        \n      \n    \n    {\\displaystyle \\;w_{1}\\;}\n   if \n  \n    \n      \n         \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          w\n          \n            1\n          \n        \n        \n          |\n        \n        x\n        )\n        \n        >\n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          w\n          \n            2\n          \n        \n        \n          |\n        \n        x\n        )\n         \n        ;\n         \n      \n    \n    {\\displaystyle ~\\operatorname {\\mathbb {P} } (w_{1}|x)\\;>\\;\\operatorname {\\mathbb {P} } (w_{2}|x)~;~}\n   otherwise decide \n  \n    \n      \n        \n        \n          w\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle \\;w_{2}\\;}\n  \"where \n  \n    \n      \n        \n        \n          w\n          \n            1\n          \n        \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle \\;w_{1}\\,,w_{2}\\;}\n   are predictions of different classes. From a perspective of minimizing error, it can also be stated as\n\n  \n    \n      \n        w\n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            w\n          \n        \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n           error\n        \n        \u2223\n        x\n        )\n        \n          \n            P\n          \n        \n        \u2061\n        (\n        x\n        )\n        \n        d\n        \u2061\n        x\n         \n      \n    \n    {\\displaystyle w={\\underset {w}{\\operatorname {arg\\;max} }}\\;\\int _{-\\infty }^{\\infty }\\operatorname {\\mathbb {P} } ({\\text{ error}}\\mid x)\\operatorname {\\mathbb {P} } (x)\\,\\operatorname {d} x~}\n  where\n\n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n           error\n        \n        \u2223\n        x\n        )\n        =\n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          w\n          \n            1\n          \n        \n        \u2223\n        x\n        )\n         \n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } ({\\text{ error}}\\mid x)=\\operatorname {\\mathbb {P} } (w_{1}\\mid x)~}\n  if we decide \n  \n    \n      \n        \n        \n          w\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle \\;w_{2}\\;}\n   and \n  \n    \n      \n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n           error\n        \n        \u2223\n        x\n        )\n        =\n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          w\n          \n            2\n          \n        \n        \u2223\n        x\n        )\n        \n      \n    \n    {\\displaystyle \\;\\operatorname {\\mathbb {P} } ({\\text{ error}}\\mid x)=\\operatorname {\\mathbb {P} } (w_{2}\\mid x)\\;}\n   if we decide \n  \n    \n      \n        \n        \n          w\n          \n            1\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\;w_{1}\\;.}\n  \nBy applying Bayes' theorem\n\n  \n    \n      \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        \n          w\n          \n            i\n          \n        \n        \u2223\n        x\n        )\n        =\n        \n          \n            \n              \n                \n                  P\n                \n              \n              \u2061\n              (\n              x\n              \u2223\n              \n                w\n                \n                  i\n                \n              \n              )\n              \n                \n                  P\n                \n              \n              \u2061\n              (\n              \n                w\n                \n                  i\n                \n              \n              )\n            \n            \n              \n                \n                  P\n                \n              \n              \u2061\n              (\n              x\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {P} } (w_{i}\\mid x)={\\frac {\\operatorname {\\mathbb {P} } (x\\mid w_{i})\\operatorname {\\mathbb {P} } (w_{i})}{\\operatorname {\\mathbb {P} } (x)}}}\n  ,and if we further assume the zero-or-one loss function, which is a same loss for all errors, the Bayes Decision rule can be reformulated as:\n\n  \n    \n      \n        \n          h\n          \n            Bayes\n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            w\n          \n        \n        \n        \n          \n            [\n          \n        \n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        x\n        \u2223\n        w\n        )\n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        w\n        )\n        \n        \n          \n            ]\n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle h_{\\text{Bayes}}={\\underset {w}{\\operatorname {arg\\;max} }}\\,{\\bigl [}\\,\\operatorname {\\mathbb {P} } (x\\mid w)\\,\\operatorname {\\mathbb {P} } (w)\\,{\\bigr ]}\\;,}\n  where \n  \n    \n      \n        \n          h\n          \n            Bayes\n          \n        \n      \n    \n    {\\displaystyle h_{\\text{Bayes}}}\n   is the prediction and \n  \n    \n      \n        \n        \n          \n            P\n          \n        \n        \u2061\n        (\n        w\n        )\n        \n      \n    \n    {\\displaystyle \\;\\operatorname {\\mathbb {P} } (w)\\;}\n   is the prior probability.\n\n\n=== Relation to minimizing Kullback\u2013Leibler divergence and cross entropy ===\nFinding \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n   that maximizes the likelihood is asymptotically equivalent to finding the \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n   that defines a probability distribution (\n  \n    \n      \n        \n          Q\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q_{\\hat {\\theta }}}\n  ) that has a minimal distance, in terms of Kullback\u2013Leibler divergence, to the real probability distribution from which our data were generated (i.e., generated by \n  \n    \n      \n        \n          P\n          \n            \n              \u03b8\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\theta _{0}}}\n  ). In an ideal world, P and Q are the same (and the only thing unknown is \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   that defines P), but even if they are not and the model we use is misspecified, still the MLE will give us the \"closest\" distribution (within the restriction of a model Q that depends on \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}}\n  ) to the real distribution \n  \n    \n      \n        \n          P\n          \n            \n              \u03b8\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\theta _{0}}}\n  .\nSince cross entropy is just Shannon's entropy plus KL divergence, and since the entropy of \n  \n    \n      \n        \n          P\n          \n            \n              \u03b8\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\theta _{0}}}\n   is constant, then the MLE is also asymptotically minimizing cross entropy.\n\n\n== Examples ==\n\n\n=== Discrete uniform distribution ===\n\nConsider a case where n tickets numbered from 1 to n are placed in a box and one is selected at random (see uniform distribution); thus, the sample size is 1.  If n is unknown, then the maximum likelihood estimator \n  \n    \n      \n        \n          \n            \n              n\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {n}}}\n   of n is the number m on the drawn ticket. (The likelihood is 0 for n < m, 1\u2044n for n \u2265 m, and this is greatest when n = m. Note that the maximum likelihood estimate of n occurs at the lower extreme of possible values {m, m + 1, ...}, rather than somewhere in the \"middle\" of the range of possible values, which would result in less bias.) The expected value of the number m on the drawn ticket, and therefore the expected value of \n  \n    \n      \n        \n          \n            \n              n\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {n}}}\n  , is (n + 1)/2. As a result, with a sample size of 1, the maximum likelihood estimator for n will systematically underestimate n by (n \u2212 1)/2.\n\n\n=== Discrete distribution, finite parameter space ===\nSuppose one wishes to determine just how biased an unfair coin is.  Call the probability of tossing a \u2018head\u2019 p. The goal then becomes to determine p.\nSuppose the coin is tossed 80 times: i.e. the sample might be something like x1 = H, x2 = T, ..., x80 = T, and the count of the number of heads \"H\" is observed.\nThe probability of tossing tails is 1 \u2212 p (so here p is \u03b8 above). Suppose the outcome is 49 heads and 31 tails, and suppose the coin was taken from a box containing three coins: one which gives heads with probability p = 1\u20443, one which gives heads with probability p = 1\u20442 and another which gives heads with probability p = 2\u20443. The coins have lost their labels, so which one it was is unknown. Using maximum likelihood estimation, the coin that has the largest likelihood can be found, given the data that were observed. By using the probability mass function of the binomial distribution with sample size equal to 80, number successes equal to 49 but for different values of p (the \"probability of success\"), the likelihood function (defined below) takes one of three values:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    P\n                  \n                \n                \u2061\n                \n                  \n                    [\n                  \n                \n                \n                \n                  H\n                \n                =\n                49\n                \u2223\n                p\n                =\n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      (\n                    \n                    \n                      80\n                      49\n                    \n                    \n                      )\n                    \n                  \n                \n                (\n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  )\n                  \n                    49\n                  \n                \n                (\n                1\n                \u2212\n                \n                  \n                    \n                      1\n                      3\n                    \n                  \n                \n                \n                  )\n                  \n                    31\n                  \n                \n                \u2248\n                0.000\n                ,\n              \n            \n            \n              \n                \n                  \n                    P\n                  \n                \n                \u2061\n                \n                  \n                    [\n                  \n                \n                \n                \n                  H\n                \n                =\n                49\n                \u2223\n                p\n                =\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      (\n                    \n                    \n                      80\n                      49\n                    \n                    \n                      )\n                    \n                  \n                \n                (\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \n                  )\n                  \n                    49\n                  \n                \n                (\n                1\n                \u2212\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \n                  )\n                  \n                    31\n                  \n                \n                \u2248\n                0.012\n                ,\n              \n            \n            \n              \n                \n                  \n                    P\n                  \n                \n                \u2061\n                \n                  \n                    [\n                  \n                \n                \n                \n                  H\n                \n                =\n                49\n                \u2223\n                p\n                =\n                \n                  \n                    \n                      2\n                      3\n                    \n                  \n                \n                \n                \n                  \n                    ]\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      (\n                    \n                    \n                      80\n                      49\n                    \n                    \n                      )\n                    \n                  \n                \n                (\n                \n                  \n                    \n                      2\n                      3\n                    \n                  \n                \n                \n                  )\n                  \n                    49\n                  \n                \n                (\n                1\n                \u2212\n                \n                  \n                    \n                      2\n                      3\n                    \n                  \n                \n                \n                  )\n                  \n                    31\n                  \n                \n                \u2248\n                0.054\n                 \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {\\mathbb {P} } {\\bigl [}\\;\\mathrm {H} =49\\mid p={\\tfrac {1}{3}}\\;{\\bigr ]}&={\\binom {80}{49}}({\\tfrac {1}{3}})^{49}(1-{\\tfrac {1}{3}})^{31}\\approx 0.000,\\\\[6pt]\\operatorname {\\mathbb {P} } {\\bigl [}\\;\\mathrm {H} =49\\mid p={\\tfrac {1}{2}}\\;{\\bigr ]}&={\\binom {80}{49}}({\\tfrac {1}{2}})^{49}(1-{\\tfrac {1}{2}})^{31}\\approx 0.012,\\\\[6pt]\\operatorname {\\mathbb {P} } {\\bigl [}\\;\\mathrm {H} =49\\mid p={\\tfrac {2}{3}}\\;{\\bigr ]}&={\\binom {80}{49}}({\\tfrac {2}{3}})^{49}(1-{\\tfrac {2}{3}})^{31}\\approx 0.054~.\\end{aligned}}}\n  The likelihood is maximized when p = 2\u20443, and so this is the maximum likelihood estimate for p.\n\n\n=== Discrete distribution, continuous parameter space ===\nNow suppose that there was only one coin but its p could have been any value  0 \u2264 p \u2264 1 . The likelihood function to be maximised is\n\n  \n    \n      \n        L\n        (\n        p\n        )\n        =\n        \n          f\n          \n            D\n          \n        \n        (\n        \n          H\n        \n        =\n        49\n        \u2223\n        p\n        )\n        =\n        \n          \n            \n              (\n            \n            \n              80\n              49\n            \n            \n              )\n            \n          \n        \n        \n          p\n          \n            49\n          \n        \n        (\n        1\n        \u2212\n        p\n        \n          )\n          \n            31\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle L(p)=f_{D}(\\mathrm {H} =49\\mid p)={\\binom {80}{49}}p^{49}(1-p)^{31}~,}\n  and the maximisation is over all possible values 0 \u2264 p \u2264 1 .\n\nOne way to maximize this function is by differentiating with respect to p and setting to zero:\n\n  \n    \n      \n        \n          \n            \n              \n                0\n              \n              \n                \n                =\n                \n                  \n                    \u2202\n                    \n                      \u2202\n                      p\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          (\n                        \n                        \n                          80\n                          49\n                        \n                        \n                          )\n                        \n                      \n                    \n                    \n                      p\n                      \n                        49\n                      \n                    \n                    (\n                    1\n                    \u2212\n                    p\n                    \n                      )\n                      \n                        31\n                      \n                    \n                  \n                  )\n                \n                 \n                ,\n              \n            \n            \n              \n                0\n              \n              \n                \n                =\n                49\n                \n                  p\n                  \n                    48\n                  \n                \n                (\n                1\n                \u2212\n                p\n                \n                  )\n                  \n                    31\n                  \n                \n                \u2212\n                31\n                \n                  p\n                  \n                    49\n                  \n                \n                (\n                1\n                \u2212\n                p\n                \n                  )\n                  \n                    30\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  p\n                  \n                    48\n                  \n                \n                (\n                1\n                \u2212\n                p\n                \n                  )\n                  \n                    30\n                  \n                \n                \n                  [\n                  \n                    49\n                    (\n                    1\n                    \u2212\n                    p\n                    )\n                    \u2212\n                    31\n                    p\n                  \n                  ]\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  p\n                  \n                    48\n                  \n                \n                (\n                1\n                \u2212\n                p\n                \n                  )\n                  \n                    30\n                  \n                \n                \n                  [\n                  \n                    49\n                    \u2212\n                    80\n                    p\n                  \n                  ]\n                \n                 \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}0&={\\frac {\\partial }{\\partial p}}\\left({\\binom {80}{49}}p^{49}(1-p)^{31}\\right)~,\\\\[8pt]0&=49p^{48}(1-p)^{31}-31p^{49}(1-p)^{30}\\\\[8pt]&=p^{48}(1-p)^{30}\\left[49(1-p)-31p\\right]\\\\[8pt]&=p^{48}(1-p)^{30}\\left[49-80p\\right]~.\\end{aligned}}}\n  This is a product of three terms.  The first term is 0 when p = 0.  The second is 0 when p = 1.  The third is zero when p = 49\u204480. The solution that maximizes the likelihood is clearly p = 49\u204480 (since p = 0 and p = 1 result in a likelihood of 0). Thus the maximum likelihood estimator for p is 49\u204480.\nThis result is easily generalized by substituting a letter such as s in the place of 49 to represent the observed number of 'successes' of our Bernoulli trials, and a letter such as n in the place of 80 to represent the number of Bernoulli trials. Exactly the same calculation yields s\u2044n which is the maximum likelihood estimator for any sequence of n Bernoulli trials resulting in s 'successes'.\n\n\n=== Continuous distribution, continuous parameter space ===\nFor the normal distribution \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(\\mu ,\\sigma ^{2})}\n   which has probability density function\n\n  \n    \n      \n        f\n        (\n        x\n        \u2223\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              \n                \n                  2\n                  \u03c0\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n               \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  (\n                  x\n                  \u2212\n                  \u03bc\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle f(x\\mid \\mu ,\\sigma ^{2})={\\frac {1}{{\\sqrt {2\\pi \\sigma ^{2}}}\\ }}\\exp \\left(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right),}\n  the corresponding probability density function for a sample of n independent identically distributed normal random variables (the likelihood) is\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        \u2223\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        \u2223\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            (\n            \n              \n                1\n                \n                  2\n                  \u03c0\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            n\n            \n              /\n            \n            2\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  \n                    \u2211\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \u2212\n                  \u03bc\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{n}\\mid \\mu ,\\sigma ^{2})=\\prod _{i=1}^{n}f(x_{i}\\mid \\mu ,\\sigma ^{2})=\\left({\\frac {1}{2\\pi \\sigma ^{2}}}\\right)^{n/2}\\exp \\left(-{\\frac {\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}}{2\\sigma ^{2}}}\\right).}\n  This family of distributions has two parameters: \u03b8 = (\u03bc, \u03c3); so we maximize the likelihood, \n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        =\n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        \u2223\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mu ,\\sigma ^{2})=f(x_{1},\\ldots ,x_{n}\\mid \\mu ,\\sigma ^{2})}\n  , over both parameters simultaneously, or if possible, individually.\nSince the logarithm function itself is a continuous strictly increasing function over the range of the likelihood, the values which maximize the likelihood will also maximize its logarithm (the log-likelihood itself is not necessarily strictly increasing). The log-likelihood can be written as follows:\n\n  \n    \n      \n        log\n        \u2061\n        \n          \n            (\n          \n        \n        \n          \n            L\n          \n        \n        (\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \n          \n            )\n          \n        \n        =\n        \u2212\n        \n          \n            \n              \n              n\n              \n            \n            2\n          \n        \n        log\n        \u2061\n        (\n        2\n        \u03c0\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u2212\n        \n          \n            1\n            \n              2\n              \n                \u03c3\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \u03bc\n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\log {\\Bigl (}{\\mathcal {L}}(\\mu ,\\sigma ^{2}){\\Bigr )}=-{\\frac {\\,n\\,}{2}}\\log(2\\pi \\sigma ^{2})-{\\frac {1}{2\\sigma ^{2}}}\\sum _{i=1}^{n}(\\,x_{i}-\\mu \\,)^{2}}\n  (Note: the log-likelihood is closely related to information entropy and Fisher information.)\nWe now compute the derivatives of this log-likelihood as follows.\n\n  \n    \n      \n        \n          \n            \n              \n                0\n              \n              \n                \n                =\n                \n                  \n                    \u2202\n                    \n                      \u2202\n                      \u03bc\n                    \n                  \n                \n                log\n                \u2061\n                \n                  \n                    (\n                  \n                \n                \n                  \n                    L\n                  \n                \n                (\n                \u03bc\n                ,\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                =\n                0\n                \u2212\n                \n                  \n                    \n                      \n                      \u2212\n                      2\n                      n\n                      (\n                      \n                        \n                          \n                            x\n                            \u00af\n                          \n                        \n                      \n                      \u2212\n                      \u03bc\n                      )\n                      \n                    \n                    \n                      2\n                      \n                        \u03c3\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}0&={\\frac {\\partial }{\\partial \\mu }}\\log {\\Bigl (}{\\mathcal {L}}(\\mu ,\\sigma ^{2}){\\Bigr )}=0-{\\frac {\\;-2n({\\bar {x}}-\\mu )\\;}{2\\sigma ^{2}}}.\\end{aligned}}}\n  where \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   is the sample mean. This is solved by\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            \n              \n              \n                x\n                \n                  i\n                \n              \n              \n            \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\mu }}={\\bar {x}}=\\sum _{i=1}^{n}{\\frac {\\,x_{i}\\,}{n}}.}\n  This is indeed the maximum of the function, since it is the only turning point in \u03bc and the second derivative is strictly less than zero. Its expected value is equal to the parameter \u03bc of the given distribution,\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        \u2061\n        \n          \n            [\n          \n        \n        \n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n        \n        \n          \n            ]\n          \n        \n        =\n        \u03bc\n        ,\n        \n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } {\\bigl [}\\;{\\widehat {\\mu }}\\;{\\bigr ]}=\\mu ,\\,}\n  which means that the maximum likelihood estimator \n  \n    \n      \n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\mu }}}\n   is unbiased.\nSimilarly we differentiate the log-likelihood with respect to \u03c3 and equate to zero:\n\n  \n    \n      \n        \n          \n            \n              \n                0\n              \n              \n                \n                =\n                \n                  \n                    \u2202\n                    \n                      \u2202\n                      \u03c3\n                    \n                  \n                \n                log\n                \u2061\n                \n                  \n                    (\n                  \n                \n                \n                  \n                    L\n                  \n                \n                (\n                \u03bc\n                ,\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                )\n                \n                  \n                    )\n                  \n                \n                =\n                \u2212\n                \n                  \n                    \n                      \n                      n\n                      \n                    \n                    \u03c3\n                  \n                \n                +\n                \n                  \n                    1\n                    \n                      \u03c3\n                      \n                        3\n                      \n                    \n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                \n                  x\n                  \n                    i\n                  \n                \n                \u2212\n                \u03bc\n                \n                \n                  )\n                  \n                    2\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}0&={\\frac {\\partial }{\\partial \\sigma }}\\log {\\Bigl (}{\\mathcal {L}}(\\mu ,\\sigma ^{2}){\\Bigr )}=-{\\frac {\\,n\\,}{\\sigma }}+{\\frac {1}{\\sigma ^{3}}}\\sum _{i=1}^{n}(\\,x_{i}-\\mu \\,)^{2}.\\end{aligned}}}\n  which is solved by\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \u03bc\n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}^{2}={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}.}\n  Inserting the estimate \n  \n    \n      \n        \u03bc\n        =\n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mu ={\\widehat {\\mu }}}\n   we obtain\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            i\n          \n          \n            2\n          \n        \n        \u2212\n        \n          \n            1\n            \n              n\n              \n                2\n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            i\n          \n        \n        \n          x\n          \n            j\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}^{2}={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}^{2}-{\\frac {1}{n^{2}}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}x_{i}x_{j}.}\n  To calculate its expected value, it is convenient to rewrite the expression in terms of zero-mean random variables (statistical error)  \n  \n    \n      \n        \n          \u03b4\n          \n            i\n          \n        \n        \u2261\n        \u03bc\n        \u2212\n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\delta _{i}\\equiv \\mu -x_{i}}\n  . Expressing the estimate in these variables yields\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \u03bc\n        \u2212\n        \n          \u03b4\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        \u2212\n        \n          \n            1\n            \n              n\n              \n                2\n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \u03bc\n        \u2212\n        \n          \u03b4\n          \n            i\n          \n        \n        )\n        (\n        \u03bc\n        \u2212\n        \n          \u03b4\n          \n            j\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}^{2}={\\frac {1}{n}}\\sum _{i=1}^{n}(\\mu -\\delta _{i})^{2}-{\\frac {1}{n^{2}}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}(\\mu -\\delta _{i})(\\mu -\\delta _{j}).}\n  Simplifying the expression above, utilizing the facts that \n  \n    \n      \n        \n          \n            E\n          \n        \n        \u2061\n        \n          \n            [\n          \n        \n        \n        \n          \u03b4\n          \n            i\n          \n        \n        \n        \n          \n            ]\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } {\\bigl [}\\;\\delta _{i}\\;{\\bigr ]}=0}\n   and \n  \n    \n      \n        E\n        \u2061\n        \n          \n            [\n          \n        \n        \n        \n          \u03b4\n          \n            i\n          \n          \n            2\n          \n        \n        \n        \n          \n            ]\n          \n        \n        =\n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {E} {\\bigl [}\\;\\delta _{i}^{2}\\;{\\bigr ]}=\\sigma ^{2}}\n  , allows us to obtain\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        \u2061\n        \n          \n            [\n          \n        \n        \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n        \n          \n            ]\n          \n        \n        =\n        \n          \n            \n              \n              n\n              \u2212\n              1\n              \n            \n            n\n          \n        \n        \n          \u03c3\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } {\\bigl [}\\;{\\widehat {\\sigma }}^{2}\\;{\\bigr ]}={\\frac {\\,n-1\\,}{n}}\\sigma ^{2}.}\n  This means that the estimator \n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}^{2}}\n   is biased for \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . It can also be shown that \n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}}\n   is biased for \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  , but that both \n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}^{2}}\n   and \n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}}\n   are consistent.\nFormally we say that the maximum likelihood estimator for \n  \n    \n      \n        \u03b8\n        =\n        (\n        \u03bc\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\theta =(\\mu ,\\sigma ^{2})}\n   is\n\n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        =\n        \n          (\n          \n            \n              \n                \n                  \u03bc\n                  ^\n                \n              \n            \n            ,\n            \n              \n                \n                  \n                    \u03c3\n                    ^\n                  \n                \n              \n              \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\theta \\,}}=\\left({\\widehat {\\mu }},{\\widehat {\\sigma }}^{2}\\right).}\n  In this case the MLEs could be obtained individually.  In general this may not be the case, and the MLEs would have to be obtained simultaneously.\nThe normal log-likelihood at its maximum takes a particularly simple form:\n\n  \n    \n      \n        log\n        \u2061\n        \n          \n            (\n          \n        \n        \n          \n            L\n          \n        \n        (\n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n        ,\n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n        )\n        \n          \n            )\n          \n        \n        =\n        \n          \n            \n              \n              \u2212\n              n\n              \n              \n            \n            2\n          \n        \n        \n          \n            (\n          \n        \n        \n        log\n        \u2061\n        (\n        2\n        \u03c0\n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        )\n        +\n        1\n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle \\log {\\Bigl (}{\\mathcal {L}}({\\widehat {\\mu }},{\\widehat {\\sigma }}){\\Bigr )}={\\frac {\\,-n\\;\\;}{2}}{\\bigl (}\\,\\log(2\\pi {\\widehat {\\sigma }}^{2})+1\\,{\\bigr )}}\n  This maximum log-likelihood can be shown to be the same for more general least squares, even for non-linear least squares.  This is often used in determining likelihood-based approximate confidence intervals and confidence regions, which are generally more accurate than those using the asymptotic normality discussed above.\n\n\n== Non-independent variables ==\nIt may be the case that variables are correlated, that is, not independent. Two random variables \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle y_{1}}\n   and \n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle y_{2}}\n   are independent only if their joint probability density function is the product of the individual probability density functions, i.e.\n\n  \n    \n      \n        f\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        )\n        =\n        f\n        (\n        \n          y\n          \n            1\n          \n        \n        )\n        f\n        (\n        \n          y\n          \n            2\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle f(y_{1},y_{2})=f(y_{1})f(y_{2})\\,}\n  Suppose one constructs an order-n Gaussian vector out of random variables \n  \n    \n      \n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (y_{1},\\ldots ,y_{n})}\n  , where each variable has means given by \n  \n    \n      \n        (\n        \n          \u03bc\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u03bc\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\mu _{1},\\ldots ,\\mu _{n})}\n  . Furthermore, let the covariance matrix be denoted by \n  \n    \n      \n        \n          \n            \u03a3\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {\\Sigma }}}\n  . The joint probability density function of these n random variables then follows a multivariate normal distribution given by:\n\n  \n    \n      \n        f\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              (\n              2\n              \u03c0\n              \n                )\n                \n                  n\n                  \n                    /\n                  \n                  2\n                \n              \n              \n                \n                  det\n                  (\n                  \n                    \n                      \u03a3\n                    \n                  \n                  )\n                \n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              [\n              \n                \n                  y\n                  \n                    1\n                  \n                \n                \u2212\n                \n                  \u03bc\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  y\n                  \n                    n\n                  \n                \n                \u2212\n                \n                  \u03bc\n                  \n                    n\n                  \n                \n              \n              ]\n            \n            \n              \n                \n                  \u03a3\n                \n              \n              \n                \u2212\n                1\n              \n            \n            \n              \n                [\n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                  \u2212\n                  \n                    \u03bc\n                    \n                      1\n                    \n                  \n                  ,\n                  \u2026\n                  ,\n                  \n                    y\n                    \n                      n\n                    \n                  \n                  \u2212\n                  \n                    \u03bc\n                    \n                      n\n                    \n                  \n                \n                ]\n              \n              \n                \n                  T\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f(y_{1},\\ldots ,y_{n})={\\frac {1}{(2\\pi )^{n/2}{\\sqrt {\\det({\\mathit {\\Sigma }})}}}}\\exp \\left(-{\\frac {1}{2}}\\left[y_{1}-\\mu _{1},\\ldots ,y_{n}-\\mu _{n}\\right]{\\mathit {\\Sigma }}^{-1}\\left[y_{1}-\\mu _{1},\\ldots ,y_{n}-\\mu _{n}\\right]^{\\mathrm {T} }\\right)}\n  In the bivariate case, the joint probability density function is given by:\n\n  \n    \n      \n        f\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n              \n                \u03c3\n                \n                  1\n                \n              \n              \n                \u03c3\n                \n                  2\n                \n              \n              \n                \n                  1\n                  \u2212\n                  \n                    \u03c1\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        exp\n        \u2061\n        \n          [\n          \n            \u2212\n            \n              \n                1\n                \n                  2\n                  (\n                  1\n                  \u2212\n                  \n                    \u03c1\n                    \n                      2\n                    \n                  \n                  )\n                \n              \n            \n            \n              (\n              \n                \n                  \n                    \n                      (\n                      \n                        y\n                        \n                          1\n                        \n                      \n                      \u2212\n                      \n                        \u03bc\n                        \n                          1\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                    \n                    \n                      \u03c3\n                      \n                        1\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n                \u2212\n                \n                  \n                    \n                      2\n                      \u03c1\n                      (\n                      \n                        y\n                        \n                          1\n                        \n                      \n                      \u2212\n                      \n                        \u03bc\n                        \n                          1\n                        \n                      \n                      )\n                      (\n                      \n                        y\n                        \n                          2\n                        \n                      \n                      \u2212\n                      \n                        \u03bc\n                        \n                          2\n                        \n                      \n                      )\n                    \n                    \n                      \n                        \u03c3\n                        \n                          1\n                        \n                      \n                      \n                        \u03c3\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                +\n                \n                  \n                    \n                      (\n                      \n                        y\n                        \n                          2\n                        \n                      \n                      \u2212\n                      \n                        \u03bc\n                        \n                          2\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                    \n                    \n                      \u03c3\n                      \n                        2\n                      \n                      \n                        2\n                      \n                    \n                  \n                \n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle f(y_{1},y_{2})={\\frac {1}{2\\pi \\sigma _{1}\\sigma _{2}{\\sqrt {1-\\rho ^{2}}}}}\\exp \\left[-{\\frac {1}{2(1-\\rho ^{2})}}\\left({\\frac {(y_{1}-\\mu _{1})^{2}}{\\sigma _{1}^{2}}}-{\\frac {2\\rho (y_{1}-\\mu _{1})(y_{2}-\\mu _{2})}{\\sigma _{1}\\sigma _{2}}}+{\\frac {(y_{2}-\\mu _{2})^{2}}{\\sigma _{2}^{2}}}\\right)\\right]}\n  In this and other cases where a joint density function exists, the likelihood function is defined as above, in the section \"principles,\" using this density.\n\n\n=== Example ===\n\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n         \n        \n          X\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n         \n        \n          X\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\ X_{2},\\ldots ,\\ X_{m}}\n   are counts in cells / boxes 1 up to m; each box has a different probability (think of the boxes being bigger or smaller) and we fix the number of balls that fall to be \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  :\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          x\n          \n            m\n          \n        \n        =\n        n\n      \n    \n    {\\displaystyle x_{1}+x_{2}+\\cdots +x_{m}=n}\n  . The probability of each box is \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  , with a constraint: \n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        +\n        \n          p\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          p\n          \n            m\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle p_{1}+p_{2}+\\cdots +p_{m}=1}\n  . This is a case in which the \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n   s are not independent, the joint probability of a vector \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n         \n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ x_{2},\\ldots ,x_{m}}\n   is called the multinomial and has the form:\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            m\n          \n        \n        \u2223\n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            m\n          \n        \n        )\n        =\n        \n          \n            \n              n\n              !\n            \n            \n              \u220f\n              \n                x\n                \n                  i\n                \n              \n              !\n            \n          \n        \n        \u220f\n        \n          p\n          \n            i\n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n        =\n        \n          \n            \n              (\n            \n            \n              n\n              \n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \n                  x\n                  \n                    2\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  x\n                  \n                    m\n                  \n                \n              \n            \n            \n              )\n            \n          \n        \n        \n          p\n          \n            1\n          \n          \n            \n              x\n              \n                1\n              \n            \n          \n        \n        \n          p\n          \n            2\n          \n          \n            \n              x\n              \n                2\n              \n            \n          \n        \n        \u22ef\n        \n          p\n          \n            m\n          \n          \n            \n              x\n              \n                m\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x_{1},x_{2},\\ldots ,x_{m}\\mid p_{1},p_{2},\\ldots ,p_{m})={\\frac {n!}{\\prod x_{i}!}}\\prod p_{i}^{x_{i}}={\\binom {n}{x_{1},x_{2},\\ldots ,x_{m}}}p_{1}^{x_{1}}p_{2}^{x_{2}}\\cdots p_{m}^{x_{m}}}\n  Each box taken separately against all the other boxes is a binomial and this is an extension thereof.\nThe log-likelihood of this is:\n\n  \n    \n      \n        \u2113\n        (\n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            m\n          \n        \n        )\n        =\n        log\n        \u2061\n        n\n        !\n        \u2212\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        log\n        \u2061\n        \n          x\n          \n            i\n          \n        \n        !\n        +\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          x\n          \n            i\n          \n        \n        log\n        \u2061\n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\ell (p_{1},p_{2},\\ldots ,p_{m})=\\log n!-\\sum _{i=1}^{m}\\log x_{i}!+\\sum _{i=1}^{m}x_{i}\\log p_{i}}\n  The constraint has to be taken into account and use the Lagrange multipliers:\n\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            m\n          \n        \n        ,\n        \u03bb\n        )\n        =\n        \u2113\n        (\n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          p\n          \n            m\n          \n        \n        )\n        +\n        \u03bb\n        \n          (\n          \n            1\n            \u2212\n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                m\n              \n            \n            \n              p\n              \n                i\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle L(p_{1},p_{2},\\ldots ,p_{m},\\lambda )=\\ell (p_{1},p_{2},\\ldots ,p_{m})+\\lambda \\left(1-\\sum _{i=1}^{m}p_{i}\\right)}\n  By posing all the derivatives to be 0, the most natural estimate is derived\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              x\n              \n                i\n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}_{i}={\\frac {x_{i}}{n}}}\n  Maximizing log likelihood, with and without constraints, can be an unsolvable problem in closed form, then we have to use iterative procedures.\n\n\n== Iterative procedures ==\nExcept for special cases, the likelihood equations\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              \u2113\n              (\n              \u03b8\n              ;\n              \n                y\n              \n              )\n            \n            \n              \u2202\n              \u03b8\n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial \\ell (\\theta ;\\mathbf {y} )}{\\partial \\theta }}=0}\n  cannot be solved explicitly for an estimator \n  \n    \n      \n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        (\n        \n          y\n        \n        )\n      \n    \n    {\\displaystyle {\\widehat {\\theta }}={\\widehat {\\theta }}(\\mathbf {y} )}\n  . Instead, they need to be solved iteratively: starting from an initial guess of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   (say \n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta }}_{1}}\n  ), one seeks to obtain a convergent sequence \n  \n    \n      \n        \n          {\n          \n            \n              \n                \n                  \u03b8\n                  ^\n                \n              \n            \n            \n              r\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{{\\widehat {\\theta }}_{r}\\right\\}}\n  . Many methods for this kind of optimization problem are available, but the most commonly used ones are algorithms based on an updating formula of the form\n\n  \n    \n      \n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            r\n            +\n            1\n          \n        \n        =\n        \n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          \n            r\n          \n        \n        +\n        \n          \u03b7\n          \n            r\n          \n        \n        \n          \n            d\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\widehat {\\theta }}_{r+1}={\\widehat {\\theta }}_{r}+\\eta _{r}\\mathbf {d} _{r}\\left({\\widehat {\\theta }}\\right)}\n  where the vector \n  \n    \n      \n        \n          \n            d\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {d} _{r}\\left({\\widehat {\\theta }}\\right)}\n   indicates the descent direction of the rth \"step,\" and the scalar \n  \n    \n      \n        \n          \u03b7\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle \\eta _{r}}\n   captures the \"step length,\" also known as the learning rate.\n\n\n=== Gradient descent method ===\n(Note: here it is a maximization problem, so the sign before gradient is flipped)\n\n  \n    \n      \n        \n          \u03b7\n          \n            r\n          \n        \n        \u2208\n        \n          \n            R\n          \n          \n            +\n          \n        \n      \n    \n    {\\displaystyle \\eta _{r}\\in \\mathbb {R} ^{+}}\n    that is small enough for convergence and \n  \n    \n      \n        \n          \n            d\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n        =\n        \u2207\n        \u2113\n        \n          (\n          \n            \n              \n                \n                  \n                    \u03b8\n                    ^\n                  \n                \n              \n              \n                r\n              \n            \n            ;\n            \n              y\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {d} _{r}\\left({\\widehat {\\theta }}\\right)=\\nabla \\ell \\left({\\widehat {\\theta }}_{r};\\mathbf {y} \\right)}\n  Gradient descent method requires to calculate the gradient at the rth iteration, but no need to calculate the inverse of second-order derivative, i.e., the Hessian matrix. Therefore, it is computationally faster than Newton-Raphson method.\n\n\n=== Newton\u2013Raphson method ===\n\n  \n    \n      \n        \n          \u03b7\n          \n            r\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\eta _{r}=1}\n   and \n  \n    \n      \n        \n          \n            d\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n        =\n        \u2212\n        \n          \n            H\n          \n          \n            r\n          \n          \n            \u2212\n            1\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n        \n          \n            s\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {d} _{r}\\left({\\widehat {\\theta }}\\right)=-\\mathbf {H} _{r}^{-1}\\left({\\widehat {\\theta }}\\right)\\mathbf {s} _{r}\\left({\\widehat {\\theta }}\\right)}\n  where \n  \n    \n      \n        \n          \n            s\n          \n          \n            r\n          \n        \n        (\n        \n          \n            \n              \u03b8\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {s} _{r}({\\widehat {\\theta }})}\n   is the score and \n  \n    \n      \n        \n          \n            H\n          \n          \n            r\n          \n          \n            \u2212\n            1\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {H} _{r}^{-1}\\left({\\widehat {\\theta }}\\right)}\n   is the inverse of the Hessian matrix of the log-likelihood function, both evaluated the rth iteration. But because the calculation of the Hessian matrix is computationally costly, numerous alternatives have been proposed. The popular Berndt\u2013Hall\u2013Hall\u2013Hausman algorithm approximates the Hessian with the outer product of the expected gradient, such that\n\n  \n    \n      \n        \n          \n            d\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n        =\n        \u2212\n        \n          \n            [\n            \n              \n                \n                  1\n                  n\n                \n              \n              \n                \u2211\n                \n                  t\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                \n                  \n                    \u2202\n                    \u2113\n                    (\n                    \u03b8\n                    ;\n                    \n                      y\n                    \n                    )\n                  \n                  \n                    \u2202\n                    \u03b8\n                  \n                \n              \n              \n                \n                  (\n                  \n                    \n                      \n                        \u2202\n                        \u2113\n                        (\n                        \u03b8\n                        ;\n                        \n                          y\n                        \n                        )\n                      \n                      \n                        \u2202\n                        \u03b8\n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    T\n                  \n                \n              \n            \n            ]\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            s\n          \n          \n            r\n          \n        \n        \n          (\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {d} _{r}\\left({\\widehat {\\theta }}\\right)=-\\left[{\\frac {1}{n}}\\sum _{t=1}^{n}{\\frac {\\partial \\ell (\\theta ;\\mathbf {y} )}{\\partial \\theta }}\\left({\\frac {\\partial \\ell (\\theta ;\\mathbf {y} )}{\\partial \\theta }}\\right)^{\\mathsf {T}}\\right]^{-1}\\mathbf {s} _{r}\\left({\\widehat {\\theta }}\\right)}\n  \n\n\n=== Quasi-Newton methods ===\nOther quasi-Newton methods use more elaborate secant updates to give approximation of Hessian matrix.\n\n\n==== Davidon\u2013Fletcher\u2013Powell formula ====\nDFP formula finds a solution that is symmetric, positive-definite and closest to the current approximate value of second-order derivative:\n\n  \n    \n      \n        \n          \n            H\n          \n          \n            k\n            +\n            1\n          \n        \n        =\n        \n          (\n          \n            I\n            \u2212\n            \n              \u03b3\n              \n                k\n              \n            \n            \n              y\n              \n                k\n              \n            \n            \n              s\n              \n                k\n              \n              \n                \n                  T\n                \n              \n            \n          \n          )\n        \n        \n          \n            H\n          \n          \n            k\n          \n        \n        \n          (\n          \n            I\n            \u2212\n            \n              \u03b3\n              \n                k\n              \n            \n            \n              s\n              \n                k\n              \n            \n            \n              y\n              \n                k\n              \n              \n                \n                  T\n                \n              \n            \n          \n          )\n        \n        +\n        \n          \u03b3\n          \n            k\n          \n        \n        \n          y\n          \n            k\n          \n        \n        \n          y\n          \n            k\n          \n          \n            \n              T\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {H} _{k+1}=\\left(I-\\gamma _{k}y_{k}s_{k}^{\\mathsf {T}}\\right)\\mathbf {H} _{k}\\left(I-\\gamma _{k}s_{k}y_{k}^{\\mathsf {T}}\\right)+\\gamma _{k}y_{k}y_{k}^{\\mathsf {T}},}\n  where\n\n  \n    \n      \n        \n          y\n          \n            k\n          \n        \n        =\n        \u2207\n        \u2113\n        (\n        \n          x\n          \n            k\n          \n        \n        +\n        \n          s\n          \n            k\n          \n        \n        )\n        \u2212\n        \u2207\n        \u2113\n        (\n        \n          x\n          \n            k\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle y_{k}=\\nabla \\ell (x_{k}+s_{k})-\\nabla \\ell (x_{k}),}\n  \n\n  \n    \n      \n        \n          \u03b3\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            \n              \n                y\n                \n                  k\n                \n                \n                  T\n                \n              \n              \n                s\n                \n                  k\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\gamma _{k}={\\frac {1}{y_{k}^{T}s_{k}}},}\n  \n\n  \n    \n      \n        \n          s\n          \n            k\n          \n        \n        =\n        \n          x\n          \n            k\n            +\n            1\n          \n        \n        \u2212\n        \n          x\n          \n            k\n          \n        \n        .\n      \n    \n    {\\displaystyle s_{k}=x_{k+1}-x_{k}.}\n  \n\n\n==== Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm ====\nBFGS also gives a solution that is symmetric and positive-definite:\n\n  \n    \n      \n        \n          B\n          \n            k\n            +\n            1\n          \n        \n        =\n        \n          B\n          \n            k\n          \n        \n        +\n        \n          \n            \n              \n                y\n                \n                  k\n                \n              \n              \n                y\n                \n                  k\n                \n                \n                  \n                    T\n                  \n                \n              \n            \n            \n              \n                y\n                \n                  k\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                s\n                \n                  k\n                \n              \n            \n          \n        \n        \u2212\n        \n          \n            \n              \n                B\n                \n                  k\n                \n              \n              \n                s\n                \n                  k\n                \n              \n              \n                s\n                \n                  k\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                B\n                \n                  k\n                \n                \n                  \n                    T\n                  \n                \n              \n            \n            \n              \n                s\n                \n                  k\n                \n                \n                  \n                    T\n                  \n                \n              \n              \n                B\n                \n                  k\n                \n              \n              \n                s\n                \n                  k\n                \n              \n            \n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle B_{k+1}=B_{k}+{\\frac {y_{k}y_{k}^{\\mathsf {T}}}{y_{k}^{\\mathsf {T}}s_{k}}}-{\\frac {B_{k}s_{k}s_{k}^{\\mathsf {T}}B_{k}^{\\mathsf {T}}}{s_{k}^{\\mathsf {T}}B_{k}s_{k}}}\\ ,}\n  where\n\n  \n    \n      \n        \n          y\n          \n            k\n          \n        \n        =\n        \u2207\n        \u2113\n        (\n        \n          x\n          \n            k\n          \n        \n        +\n        \n          s\n          \n            k\n          \n        \n        )\n        \u2212\n        \u2207\n        \u2113\n        (\n        \n          x\n          \n            k\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle y_{k}=\\nabla \\ell (x_{k}+s_{k})-\\nabla \\ell (x_{k}),}\n  \n\n  \n    \n      \n        \n          s\n          \n            k\n          \n        \n        =\n        \n          x\n          \n            k\n            +\n            1\n          \n        \n        \u2212\n        \n          x\n          \n            k\n          \n        \n        .\n      \n    \n    {\\displaystyle s_{k}=x_{k+1}-x_{k}.}\n  BFGS method is not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum. However, BFGS can have acceptable performance even for non-smooth optimization instances\n\n\n==== Fisher's scoring ====\nAnother popular method is to replace the Hessian with the Fisher information matrix, \n  \n    \n      \n        \n          \n            I\n          \n        \n        (\n        \u03b8\n        )\n        =\n        \n          \n            E\n          \n        \n        \u2061\n        \n          [\n          \n            \n              \n                H\n              \n              \n                r\n              \n            \n            \n              (\n              \n                \n                  \n                    \u03b8\n                    ^\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}(\\theta )=\\operatorname {\\mathbb {E} } \\left[\\mathbf {H} _{r}\\left({\\widehat {\\theta }}\\right)\\right]}\n  , giving us the Fisher scoring algorithm. This procedure is standard in the estimation of many methods, such as generalized linear models.\nAlthough popular, quasi-Newton methods may converge to a stationary point that is not necessarily a local or global maximum, but rather a local minimum or a saddle point. Therefore, it is important to assess the validity of the obtained solution to the likelihood equations, by verifying that the Hessian, evaluated at the solution, is both negative definite and well-conditioned.\n\n\n== History ==\n\nEarly users of maximum likelihood were Carl Friedrich Gauss, Pierre-Simon Laplace, Thorvald N. Thiele, and Francis Ysidro Edgeworth. However, its widespread use rose between 1912 and 1922 when Ronald Fisher recommended, widely popularized, and carefully analyzed maximum-likelihood estimation (with fruitless attempts at proofs).Maximum-likelihood estimation finally transcended heuristic justification in a proof published by Samuel S. Wilks in 1938, now called Wilks' theorem. The theorem shows that the error in the logarithm of likelihood values for estimates from multiple independent observations is asymptotically \u03c7\u20092-distributed, which enables convenient determination of a confidence region around any estimate of the parameters. The only difficult part of Wilks\u2019 proof depends on the expected value of the Fisher information matrix, which is provided by a theorem proven by Fisher. Wilks continued to improve on the generality of the theorem throughout his life, with his most general proof published in 1962.Reviews of the development of maximum likelihood estimation have been provided by a number of authors.\n\n\n== See also ==\n\n\n=== Related concepts ===\nAkaike information criterion: a criterion to compare statistical models, based on MLE\nExtremum estimator: a more general class of estimators to which MLE belongs\nFisher information: information matrix, its relationship to covariance matrix of ML estimates\nMean squared error: a measure of how 'good' an estimator of a distributional parameter is (be it the maximum likelihood estimator or some other estimator)\nRANSAC: a method to estimate parameters of a mathematical model given data that contains outliers\nRao\u2013Blackwell theorem: yields a process for finding the best possible unbiased estimator (in the sense of having minimal mean squared error); the MLE is often a good starting place for the process\nWilks\u2019 theorem: provides a means of estimating the size and shape of the region of roughly equally-probable estimates for the population's parameter values, using the information from a single sample, using a chi-squared distribution\n\n\n=== Other estimation methods ===\nGeneralized method of moments: methods related to the likelihood equation in maximum likelihood estimation\nM-estimator: an approach used in robust statistics\nMaximum a posteriori (MAP) estimator: for a contrast in the way to calculate estimators when prior knowledge is postulated\nMaximum spacing estimation: a related method that is more robust in many situations\nMaximum entropy estimation\nMethod of moments (statistics): another popular method for finding parameters of distributions\nMethod of support, a variation of the maximum likelihood technique\nMinimum-distance estimation\nPartial likelihood methods for panel data\nQuasi-maximum likelihood estimator: an MLE estimator that is misspecified, but still consistent\nRestricted maximum likelihood: a variation using a likelihood function calculated from a transformed set of data\n\n\n== References ==\n\n\n== Further reading ==\nCramer, J.S. (1986). Econometric Applications of Maximum Likelihood Methods. New York, NY: Cambridge University Press. ISBN 0-521-25317-9.\nEliason, Scott R. (1993). Maximum Likelihood Estimation : Logic and Practice. Newbury Park: Sage. ISBN 0-8039-4107-2.\nKing, Gary (1989). Unifying Political Methodology : the Likehood Theory of Statistical Inference. Cambridge University Press. ISBN 0-521-36697-6.\nLe Cam, Lucien (1990). \"Maximum likelihood : An Introduction\". ISI Review. 58 (2): 153\u2013171. doi:10.2307/1403464. JSTOR 1403464.\nMagnus, Jan R. (2017). \"Maximum Likelihood\". Introduction to the Theory of Econometrics. Amsterdam, NL: VU University Press. pp. 53\u201368. ISBN 978-90-8659-766-6.\nMillar, Russell B. (2011). Maximum Likelihood Estimation and Inference. Hoboken, NJ: Wiley. ISBN 978-0-470-09482-2.\nPickles, Andrew (1986). An Introduction to Likelihood Analysis. Norwich: W. H. Hutchins & Sons. ISBN 0-86094-190-6.\nSeverini, Thomas A. (2000). Likelihood Methods in Statistics. New York, NY: Oxford University Press. ISBN 0-19-850650-3.\nWard, Michael D.; Ahlquist, John S. (2018). Maximum Likelihood for Social Science : Strategies for Analysis. Cambridge University Press. ISBN 978-1-316-63682-4.\n\n\n== External links ==\nTilevik, Andreas (2022). Maximum likelihood vs least squares in linear regression (video)\n\"Maximum-likelihood method\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nPurcell, S. \"Maximum Likelihood Estimation\".\nSargent, Thomas; Stachurski, John. \"Maximum Likelihood Estimation\". Quantitative Economics with Python.\nToomet, Ott; Henningsen, Arne (2019-05-19). \"maxLik: A package for maximum likelihood estimation in R\".\nLesser, Lawrence M. (2007). \"'MLE' song lyrics\". Mathematical Sciences / College of Science. math.utep.edu. El Paso, TX: University of Texas. Retrieved 2021-03-06.{{cite web}}:  CS1 maint: url-status (link)", "Artificial neural network": "Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets, are computing systems inspired by the biological neural networks that constitute animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.\n\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n\n\n== Training ==\nNeural networks learn (or are trained) by processing examples, each of which contains a known \"input\" and \"result,\" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This difference is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output that is increasingly similar to the target output. After a sufficient number of these adjustments, the training can be terminated based on certain criteria. This is a form of supervised learning.\nSuch systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers, and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.\n\n\n== History ==\n\nThe simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network, funded by the United States Office of Naval Research.Some say that research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However, by the time this book came out, methods for training multilayer perceptrons (MLPs) were already known. \nThe first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling. \nThe first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.\nIn computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision.\nThe backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as \nthe reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not have an implementation of this procedure, although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960-61 in the context of control theory. \nIn 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. \nIn 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart, Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989, Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. \nIn 1992, max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. \nLeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In the 1980s, backpropagation did not work well for deep FNNs and RNNs. To overcome this problem, Juergen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.In 1992, Juergen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.  \nThe modern Transformer was introduced by Ashish Vaswani et. al. in their 2017 paper \"Attention Is All You Need.\" \nIt combines this with a softmax operator and a projection matrix.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.In 1991, Juergen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\" \nIn 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.\nExcellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\nSepp Hochreiter's diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Juergen Schmidhuber. Hochreiter identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. This led to the deep learning method called long short-term memory (LSTM), published in Neural Computation (1997). LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used the LSTM principle to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.The development of metal\u2013oxide\u2013semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).\n\n\n== Models ==\n\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\n\n=== Artificial neurons ===\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\n\n=== Organization ===\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\n\n=== Hyperparameter ===\n\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\n\n=== Learning ===\n\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\n\n==== Learning rate ====\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\n\n==== Cost function ====\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).\n\n\n==== Backpropagation ====\n\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\n\n=== Learning paradigms ===\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\n\n==== Supervised learning ====\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\n\n==== Unsupervised learning ====\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n   and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n          =\n          a\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)=a}\n   where \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n   is a constant and the cost \n  \n    \n      \n        \n          C\n          =\n          E\n          [\n          (\n          x\n          \u2212\n          f\n          (\n          x\n          )\n          \n            )\n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n  . Minimizing this cost produces a value of \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n   that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n   and \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)}\n  , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\n\n==== Reinforcement learning ====\n\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          \n            \n              s\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              s\n              \n                n\n              \n            \n          \n          \u2208\n          S\n        \n      \n    \n    {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n   and actions \n  \n    \n      \n        \n          \n            \n              a\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                m\n              \n            \n          \n          \u2208\n          A\n        \n      \n    \n    {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n  . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      \n        \n          P\n          (\n          \n            c\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(c_{t}|s_{t})}\n  , the observation distribution \n  \n    \n      \n        \n          P\n          (\n          \n            x\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(x_{t}|s_{t})}\n   and the transition distribution \n  \n    \n      \n        \n          P\n          (\n          \n            s\n            \n              t\n              +\n              1\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          ,\n          \n            a\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n  , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\n\n==== Self-learning ====\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n  In situation s perform action a;\n  Receive consequence situation s';\n  Compute emotion of being in consequence situation v(s');\n  Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\n\n==== Neuroevolution ====\n\nNeuroevolution can create neural network topologies and weights using evolutionary computation. With modern enhancements, neuroevolution is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n=== Stochastic neural network ===\nStochastic neural networks originating from  Sherrington\u2013Kirkpatrick models  are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\n\n=== Other ===\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\n\n==== Modes ====\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\n\n== Types ==\n\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; long short-term memory avoid the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\n\n== Network design ==\n\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras.Design issues include deciding the number, type and connectedness of network layers, as well as the size of each and the connection type (full, pooling, ...).\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.\n\n\n== Use ==\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Overly complex models are slow learning.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.ANN capabilities fall within the following broad categories:\nFunction approximation, or regression analysis, including time series prediction, fitness approximation and modeling.\nClassification, including pattern and sequence recognition, novelty detection and sequential decision making.\nData processing, including filtering, clustering, blind source separation and compression.\nRobotics, including directing manipulators and prostheses.\n\n\n== Applications ==\nBecause of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. ex-ante models for specific financial long-run forecasts and artificial financial markets), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\n\n\n== Theoretical properties ==\n\n\n=== Computational power ===\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\n\n=== Capacity ===\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\n\n=== Convergence ===\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\n\n=== Generalization and statistics ===\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\nThe second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            \n              e\n              \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                \u2211\n                \n                  j\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                e\n                \n                  \n                    x\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\n  \n\n\n== Criticism ==\n\n\n=== Training ===\nA common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\n\n\n=== Theory ===\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney commented that, as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\". One response to Dewdney is that neural networks handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\n\n=== Hardware ===\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons \u2013  which require enormous CPU power and time.\nSchmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\n\n=== Practical counterexamples ===\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\n\n=== Hybrid approaches ===\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\n\n== Gallery ==\n\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==", "Type I and type II errors": "In statistical hypothesis testing, a type I error is the mistaken rejection of an actually true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\"), while a type II error is the failure to reject a null hypothesis that is actually false (also known as a \"false negative\" finding or conclusion; example: \"a guilty person is not convicted\"). Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is a statistical impossibility if the outcome is not determined by a known, observable causal process.\nBy selecting a low threshold (cut-off) value and modifying the alpha (\u03b1) level, the quality of the hypothesis test can be increased. The knowledge of type I errors and type II errors is widely used in medical science, biometrics and computer science.Intuitively, type I errors can be thought of as errors of commission (i.e., the researcher unluckily concludes that something is the fact). For instance, consider a study where researchers compare a drug with a placebo. If the patients who are given the drug get better than the patients given the placebo by chance, it may appear that the drug is effective, but in fact the conclusion is incorrect.\nIn reverse, type II errors are errors of omission.  In the example above, if the patients who got the drug did not get better at a higher rate than the ones who got the placebo, but this was a random fluke, that would be a type II error. The consequence of a type II error depends on the size and direction of the missed determination and the circumstances. An expensive cure for one in a million patients may be inconsequential even if it truly is a cure.\n\n\n== Definition ==\n\n\n=== Statistical background ===\nIn statistical test theory, the notion of a statistical error is an integral part of hypothesis testing. The test goes about choosing about two competing propositions called null hypothesis, denoted by H0 and alternative hypothesis, denoted by H1. This is conceptually similar to the judgement in a court trial. The null hypothesis corresponds to the position of the defendant: just as he is presumed to be innocent until proven guilty, so is the null hypothesis presumed to be true until the data provide convincing evidence against it. The alternative hypothesis corresponds to the position against the defendant. Specifically, the null hypothesis also involves the absence of a difference or the absence of an association. Thus, the null hypothesis can never be that there is a difference or an association.\nIf the result of the test corresponds with reality, then a correct decision has been made. However, if the result of the test does not correspond with reality, then an error has occurred. There are two situations in which the decision is wrong. The null hypothesis may be true, whereas we reject H0. On the other hand, the alternative hypothesis H1 may be true, whereas we do not reject H0. Two types of error are distinguished: type I error and type II error.\n\n\n=== Type I error ===\nThe first kind of error is the mistaken rejection of a null hypothesis as the result of a test procedure. This kind of error is called a type I error (false positive) and is sometimes called an error of the first kind. In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant.\n\n\n=== Type II error ===\nThe second kind of error is the mistaken failure to reject the null hypothesis as the result of a test procedure. This sort of error is called a type II error (false negative) and is also referred to as an error of the second kind. In terms of the courtroom example, a type II error corresponds to acquitting a criminal.\n\n\n=== Crossover error rate ===\nThe crossover error rate (CER) is the point at which type I errors and type II errors are equal. A system with a lower CER value provides more accuracy than a system with a higher CER value.\n\n\n=== False positive and false negative ===\n\nIn terms of false positives and false negatives, a positive result corresponds to rejecting the null hypothesis, while a negative result corresponds to failing to reject the null hypothesis; \"false\" means the conclusion drawn is incorrect. Thus, a type I error is equivalent to a false positive, and a type II error is equivalent to a false negative.\n\n\n=== Table of error types ===\nTabularised relations between truth/falseness of the null hypothesis and outcomes of the test:\n\n\n== Error rate ==\n\nA perfect test would have zero false positives and zero false negatives. However, statistical methods are probabilistic, and it cannot be known for certain whether statistical conclusions are correct. Whenever there is uncertainty, there is the possibility of making an error. Considering this nature of statistics science, all statistical hypothesis tests have a probability of making type I and type II errors.\nThe type I error rate  is the probability of rejecting the null hypothesis given that it is true. The test is designed to keep the type I error rate below a prespecified bound called the significance level, usually denoted by the Greek letter \u03b1 (alpha) and is also called the alpha level.  Usually, the significance level is set to 0.05 (5%), implying that it is acceptable to have a 5% probability of incorrectly rejecting the true null hypothesis.\nThe rate of the type II error is denoted by the Greek letter \u03b2 (beta) and related to the power of a test, which equals 1\u2212\u03b2.These two types of error rates are traded off against each other: for any given sample set, the effort to reduce one type of error generally results in increasing the other type of error.\n\n\n=== The quality of hypothesis test ===\nThe same idea can be expressed in terms of the rate of correct results and therefore used to minimize error rates and improve the quality of hypothesis test. To reduce the probability of committing a type I error, making the alpha value more stringent is quite simple and efficient. To decrease the probability of committing a type II error, which is closely associated with analyses' power, either increasing the test's sample size or relaxing the alpha level could increase the analyses' power. A test statistic is robust if the type I error rate is controlled.\nVarying different threshold (cut-off) value could also be used to make the test either more specific or more sensitive, which in turn elevates the test quality. For example, imagine a medical test, in which an experimenter might measure the concentration of a certain protein in the blood sample. The experimenter could adjust the threshold (black vertical line in the figure) and people would be diagnosed as having diseases if any number is detected above this certain threshold. According to the image, changing the threshold would result in changes in false positives and false negatives, corresponding to movement on the curve.\n\n\n== Example ==\nSince in a real experiment it is impossible to avoid all type I and type II errors, it is important to consider the amount of risk one is willing to take to falsely reject H0 or accept H0. The solution to this question would be to report the p-value or significance level \u03b1 of the statistic. For example, if the p-value of a test statistic result is estimated at 0.0596, then there is a probability of 5.96% that we falsely reject H0. Or, if we say, the statistic is performed at level \u03b1, like 0.05, then we allow to falsely reject H0 at 5%. A significance level \u03b1 of 0.05 is relatively common, but there is no general rule that fits all scenarios.\n\n\n=== Vehicle speed measuring ===\nThe speed limit of a freeway in the United States is 120 kilometers per hour (75 mph). A device is set to measure the speed of passing vehicles. Suppose that the device will conduct three measurements of the speed of a passing vehicle, recording as a random sample X1, X2, X3. The traffic police will or will not fine the drivers depending on the average speed \n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n  . That is to say, the test statistic\n\n  \n    \n      \n        T\n        =\n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              +\n              \n                X\n                \n                  2\n                \n              \n              +\n              \n                X\n                \n                  3\n                \n              \n            \n            3\n          \n        \n        =\n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle T={\\frac {X_{1}+X_{2}+X_{3}}{3}}={\\bar {X}}}\n  \nIn addition, we suppose that the measurements X1, X2, X3 are modeled as normal distribution N(\u03bc,4). Then, T should follow N(\u03bc,4/3) and the parameter \u03bc represents the true speed of passing vehicle. In this experiment, the null hypothesis H0 and the alternative hypothesis H1 should be\nH0: \u03bc=120     against      H1: \u03bc>120.\nIf we perform the statistic level at \u03b1=0.05, then a critical value c should be calculated to solve\n\n  \n    \n      \n        P\n        \n          (\n          \n            Z\n            \u2a7e\n            \n              \n                \n                  c\n                  \u2212\n                  120\n                \n                \n                  2\n                  \n                    3\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        0.05\n      \n    \n    {\\displaystyle P\\left(Z\\geqslant {\\frac {c-120}{\\frac {2}{\\sqrt {3}}}}\\right)=0.05}\n  \nAccording to change-of-units rule for the normal distribution. Referring to Z-table, we can get\n\n  \n    \n      \n        \n          \n            \n              c\n              \u2212\n              120\n            \n            \n              2\n              \n                3\n              \n            \n          \n        \n        =\n        1.645\n        \u21d2\n        c\n        =\n        121.9\n      \n    \n    {\\displaystyle {\\frac {c-120}{\\frac {2}{\\sqrt {3}}}}=1.645\\Rightarrow c=121.9}\n  \nHere, the critical region. That is to say, if the recorded speed of a vehicle is greater than critical value 121.9, the driver will be fined. However, there are still 5% of the drivers are falsely fined since the recorded average speed is greater than 121.9 but the true speed does not pass 120, which we say, a type I error.\nThe type II error corresponds to the case that the true speed of a vehicle is over 120 kilometers per hour but the driver is not fined. For example, if the true speed of a vehicle \u03bc=125, the probability that the driver is not fined can be calculated as\n\n  \n    \n      \n        P\n        =\n        (\n        T\n        <\n        121.9\n        \n          |\n        \n        \u03bc\n        =\n        125\n        )\n        =\n        P\n        \n          (\n          \n            \n              \n                \n                  T\n                  \u2212\n                  125\n                \n                \n                  2\n                  \n                    3\n                  \n                \n              \n            \n            <\n            \n              \n                \n                  121.9\n                  \u2212\n                  125\n                \n                \n                  2\n                  \n                    3\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        \u03d5\n        (\n        \u2212\n        2.68\n        )\n        =\n        0.0036\n      \n    \n    {\\displaystyle P=(T<121.9|\\mu =125)=P\\left({\\frac {T-125}{\\frac {2}{\\sqrt {3}}}}<{\\frac {121.9-125}{\\frac {2}{\\sqrt {3}}}}\\right)=\\phi (-2.68)=0.0036}\n  \nwhich means, if the true speed of a vehicle is 125, the driver has the probability of 0.36% to avoid the fine when the statistic is performed at level 125 since the recorded average speed is lower than 121.9. If the true speed is closer to 121.9 than 125, then the probability of avoiding the fine will also be higher.\nThe tradeoffs between type I error and type II error should also be considered. That is, in this case, if the traffic police do not want to falsely fine innocent drivers, the level \u03b1 can be set to a smaller value, like 0.01. However, if that is the case, more drivers whose true speed is over 120 kilometers per hour, like 125, would be more likely to avoid the fine.\n\n\n== Etymology ==\nIn 1928, Jerzy Neyman (1894\u20131981) and Egon Pearson (1895\u20131980), both eminent statisticians, discussed the problems associated with \"deciding whether or not a particular sample may be judged as likely to have been randomly drawn from a certain population\": and, as Florence Nightingale David remarked, \"it is necessary to remember the adjective 'random' [in the term 'random sample'] should apply to the method of drawing the sample and not to the sample itself\".They identified \"two sources of error\", namely:\n\n(a) the error of rejecting a hypothesis that should have not been rejected, and(b) the error of failing to reject a hypothesis that should have been rejected.In 1930, they elaborated on these two sources of error, remarking that:\n\n...in testing hypotheses two considerations must be kept in view, we must be able to reduce the chance of rejecting a true hypothesis to as low a value as desired; the test must be so devised that it will reject the hypothesis tested when it is likely to be false.\nIn 1933, they observed that these \"problems are rarely presented in such a form that we can discriminate with certainty between the true and false hypothesis\" . They also noted that, in deciding whether to fail to reject, or reject a particular hypothesis amongst a \"set of alternative hypotheses\", H1, H2..., it was easy to make an error:\n\n...[and] these errors will be of two kinds:(I) we reject H0 [i.e., the hypothesis to be tested] when it is true,\n(II) we fail to reject H0 when some alternative hypothesis HA or H1 is true.  (There are various notations for the alternative).\nIn all of the papers co-written by Neyman and Pearson the expression H0 always signifies \"the hypothesis to be tested\".\nIn the same paper they call these two sources of error, errors of type I and errors of type II respectively.\n\n\n== Related terms ==\n\n\n=== Null hypothesis ===\n\nIt is standard practice for statisticians to conduct tests in order to determine whether or not a \"speculative hypothesis\" concerning the observed phenomena of the world (or its inhabitants) can be supported. The results of such testing determine whether a particular set of results agrees reasonably (or does not agree) with the speculated hypothesis.\nOn the basis that it is always assumed, by statistical convention, that the speculated hypothesis is wrong, and the so-called \"null hypothesis\" that the observed phenomena simply occur by chance (and that, as a consequence, the speculated agent has no effect) \u2013 the test will determine whether this hypothesis is right or wrong. This is why the hypothesis under test is often called the null hypothesis (most likely, coined by Fisher (1935, p. 19)), because it is this hypothesis that is to be either nullified or not nullified by the test. When the null hypothesis is nullified, it is possible to conclude that data support the \"alternative hypothesis\" (which is the original speculated one).\nThe consistent application by statisticians of Neyman and Pearson's convention of representing \"the hypothesis to be tested\" (or \"the hypothesis to be nullified\") with the expression H0 has led to circumstances where many understand the term \"the null hypothesis\" as meaning \"the nil hypothesis\" \u2013 a statement that the results in question have arisen through chance. This is not necessarily the case \u2013 the key restriction, as per Fisher (1966), is that \"the null hypothesis must be exact, that is free from vagueness and ambiguity, because it must supply the basis of the 'problem of distribution,' of which the test of significance is the solution.\" As a consequence of this, in experimental science the null hypothesis is generally a statement that a particular treatment has no effect; in observational science, it is that there is no difference between the value of a particular measured variable, and that of an experimental prediction.\n\n\n=== Statistical significance ===\nIf the probability of obtaining a result as extreme as the one obtained, supposing that the null hypothesis were true, is lower than a pre-specified cut-off probability (for example, 5%), then the result is said to be statistically significant and the null hypothesis is rejected.\nBritish statistician Sir Ronald Aylmer Fisher (1890\u20131962) stressed that the \"null hypothesis\":\n\n... is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis.\n\n\n== Application domains ==\n\n\n=== Medicine ===\nIn the practice of medicine, the differences between the applications of screening and testing are considerable.\n\n\n==== Medical screening ====\nScreening involves relatively cheap tests that are given to large populations, none of whom manifest any clinical indication of disease (e.g., Pap smears).\nTesting involves far more expensive, often invasive, procedures that are given only to those who manifest some clinical indication of disease, and are most often applied to confirm a suspected diagnosis.\nFor example, most states in the USA require newborns to be screened for phenylketonuria and hypothyroidism, among other congenital disorders.\nHypothesis: \"The newborns have phenylketonuria and hypothyroidism\"\nNull Hypothesis (H0): \"The newborns do not have phenylketonuria and hypothyroidism\",\nType I error (false positive): The true fact is that the newborns do not have phenylketonuria and hypothyroidism but we consider they have the disorders according to the data.\nType II error (false negative): The true fact is that the newborns have phenylketonuria and hypothyroidism but we consider they do not have the disorders according to the data.\nAlthough they display a high rate of false positives, the screening tests are considered valuable because they greatly increase the likelihood of detecting these disorders at a far earlier stage.\nThe simple blood tests used to screen possible blood donors for HIV and hepatitis have a significant rate of false positives; however, physicians use much more expensive and far more precise tests to determine whether a person is actually infected with either of these viruses.\nPerhaps the most widely discussed false positives in medical screening come from the breast cancer screening procedure mammography. The US rate of false positive mammograms is up to 15%, the highest in world. One consequence of the high false positive rate in the US is that, in any 10-year period, half of the American women screened receive a false positive mammogram. False positive mammograms are costly, with over $100 million spent annually in the U.S. on follow-up testing and treatment. They also cause women unneeded anxiety. As a result of the high false positive rate in the US, as many as 90\u201395% of women who get a positive mammogram do not have the condition. The lowest rate in the world is in the Netherlands, 1%. The lowest rates are generally in Northern Europe where mammography films are read twice and a high threshold for additional testing is set (the high threshold decreases the power of the test).\nThe ideal population screening test would be cheap, easy to administer, and produce zero false-negatives, if possible. Such tests usually produce more false-positives, which can subsequently be sorted out by more sophisticated (and expensive) testing.\n\n\n==== Medical testing ====\nFalse negatives and false positives are significant issues in medical testing.\nHypothesis: \"The patients have the specific disease\".\nNull hypothesis (H0): \"The patients do not have the specific disease\".\nType I error (false positive): \"The true fact is that the patients do not have a specific disease but the physicians judges the patients was ill according to the test reports\".\nFalse positives can also produce serious and counter-intuitive problems when the condition being searched for is rare, as in screening. If a test has a false positive rate of one in ten thousand, but only one in a million samples (or people) is a true positive, most of the positives detected by that test will be false. The probability that an observed positive result is a false positive may be calculated using Bayes' theorem.\nType II error (false negative): \"The true fact is that the disease is actually present but the test reports provide a falsely reassuring message to patients and physicians that the disease is absent\".\nFalse negatives produce serious and counter-intuitive problems, especially when the condition being searched for is common. If a test with a false negative rate of only 10% is used to test a population with a true occurrence rate of 70%, many of the negatives detected by the test will be false.\nThis sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. A common example is relying on cardiac stress tests to detect coronary atherosclerosis, even though cardiac stress tests are known to only detect limitations of coronary artery blood flow due to advanced stenosis.\n\n\n=== Biometrics ===\nBiometric matching, such as for fingerprint recognition, facial recognition or iris recognition, is susceptible to type I and type II errors.\nHypothesis: \"The input does not identify someone in the searched list of people\"\nNull hypothesis: \"The input does identify someone in the searched list of people\"\nType I error (false reject rate): \"The true fact is that the person is someone in the searched list but the system concludes that the person is not according to the data\".\nType II error (false match rate): \"The true fact is that the person is not someone in the searched list but the system concludes that the person is someone whom we are looking for according to the data\".\nThe probability of type I errors is called the \"false reject rate\" (FRR) or false non-match rate (FNMR), while the probability of type II errors is called the \"false accept rate\" (FAR) or false match rate (FMR).\nIf the system is designed to rarely match suspects then the probability of type II errors can be called the \"false alarm rate\". On the other hand, if the system is used for validation (and acceptance is the norm) then the FAR is a measure of system security, while the FRR measures user inconvenience level.\n\n\n=== Security screening ===\n\nFalse positives are routinely found every day in airport security screening, which are ultimately visual inspection systems. The installed security alarms are intended to prevent weapons being brought onto aircraft; yet they are often set to such high sensitivity that they alarm many times a day for minor items, such as keys, belt buckles, loose change, mobile phones, and tacks in shoes.\nHere, the null hypothesis is that the item is not a weapon, while the alternative hypothesis is that the item is a weapon.\nA type I error (false positive): \"The true fact is that the item is not a weapon but the system still alarms\".\nType II error (false negative) \"The true fact is that the item is a weapon but the system keeps silent at this time\".\nThe ratio of false positives (identifying an innocent traveler as a terrorist) to true positives (detecting a would-be terrorist) is, therefore, very high; and because almost every alarm is a false positive, the positive predictive value of these screening tests is very low.\nThe relative cost of false results determines the likelihood that test creators allow these events to occur. As the cost of a false negative in this scenario is extremely high (not detecting a bomb being brought onto a plane could result in hundreds of deaths) whilst the cost of a false positive is relatively low (a reasonably simple further inspection) the most appropriate test is one with a low statistical specificity but high statistical sensitivity (one that allows a high rate of false positives in return for minimal false negatives).\n\n\n=== Computers ===\nThe notions of false positives and false negatives have a wide currency in the realm of computers and computer applications, including computer security, spam filtering, Malware, Optical character recognition and many others.\nFor example, in the case of spam filtering the hypothesis here is that the message is a spam.\nThus, null hypothesis: \"The message is not a spam\".\nType I error (false positive): \"Spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery\".\nWhile most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task.\nType II error (false negative): \"Spam email is not detected as spam, but is classified as non-spam\". A low number of false negatives is an indicator of the efficiency of spam filtering.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\nBias and Confounding \u2013 presentation by Nigel Paneth, Graduate School of Public Health, University of Pittsburgh", "Natural selection": "Natural selection is the differential survival and reproduction of individuals due to differences in phenotype. It is a key mechanism of evolution, the change in the heritable traits characteristic of a population over generations. Charles Darwin popularised the term \"natural selection\", contrasting it with artificial selection, which is intentional, whereas natural selection is not.\nVariation exists within all populations of organisms. This occurs partly because random mutations arise in the genome of an individual organism, and their offspring can inherit such mutations. Throughout the lives of the individuals, their genomes interact with their environments to cause variations in traits. The environment of a genome includes the molecular biology in the cell, other cells, other individuals, populations, species, as well as the abiotic environment. Because individuals with certain variants of the trait tend to survive and reproduce more than individuals with other less successful variants, the population evolves. Other factors affecting reproductive success include sexual selection (now often included in natural selection) and fecundity selection.\nNatural selection acts on the phenotype, the characteristics of the organism which actually interact with the environment, but the genetic\n(heritable) basis of any phenotype that gives that phenotype a reproductive advantage may become more common in a population. Over time this process can result in populations that specialise for particular ecological niches (microevolution) and may eventually result in speciation  (the emergence of new species, macroevolution). In other words, natural selection is a key process in the evolution of a population.\nNatural selection is a cornerstone of modern biology. The concept, published by Darwin and Alfred Russel Wallace in a joint presentation of papers in 1858, was elaborated in Darwin's influential 1859 book On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. He described natural selection as analogous to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favoured for reproduction. The concept of natural selection originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, science had yet to develop modern theories of genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical genetics formed the modern synthesis of the mid-20th century. The addition of molecular genetics has led to evolutionary developmental biology, which explains evolution at the molecular level. While genotypes can slowly change by random genetic drift, natural selection remains the primary explanation for adaptive evolution.\n\n\n== Historical development ==\n\n\n=== Pre-Darwinian theories ===\n\nSeveral philosophers of the classical era, including Empedocles and his intellectual successor, the Roman poet Lucretius, expressed the idea that nature produces a huge variety of creatures, randomly, and that only those creatures that manage to provide for themselves and reproduce successfully persist. Empedocles' idea that organisms arose entirely by the incidental workings of causes such as heat and cold was criticised by Aristotle in Book II of Physics. He posited natural teleology in its place, and believed that form was achieved for a purpose, citing the regularity of heredity in species as proof. Nevertheless, he accepted in his biology that new types of animals, monstrosities (\u03c4\u03b5\u03c1\u03b1\u03c2), can occur in very rare instances (Generation of Animals, Book IV). As quoted in Darwin's 1872 edition of The Origin of Species, Aristotle considered whether different forms (e.g., of teeth) might have appeared accidentally, but only the useful forms survived:\n\nSo what hinders the different parts [of the body] from having this merely accidental relation in nature? as the teeth, for example, grow by necessity, the front ones sharp, adapted for dividing, and the grinders flat, and serviceable for masticating the food; since they were not made for the sake of this, but it was the result of accident. And in like manner as to the other parts in which there appears to exist an adaptation to an end. Wheresoever, therefore, all things together (that is all the parts of one whole) happened like as if they were made for the sake of something, these were preserved, having been appropriately constituted by an internal spontaneity, and whatsoever things were not thus constituted, perished, and still perish.\nBut Aristotle rejected this possibility in the next paragraph, making clear that he is talking about  the development of animals as embryos with the phrase \"either invariably or normally come about\", not the origin of species:\n\n... Yet it is impossible that this should be the true view. For teeth and all other natural things either invariably or normally come about in a given way; but of not one of the results of chance or spontaneity is this true. We do not ascribe to chance or mere coincidence the frequency of rain in winter, but frequent rain in summer we do; nor heat in the dog-days, but only if we have it in winter. If then, it is agreed that things are either the result of coincidence or for an end, and these cannot be the result of coincidence or spontaneity, it follows that they must be for an end; and that such things are all due to nature even the champions of the theory which is before us would agree. Therefore action for an end is present in things which come to be and are by nature.\nThe struggle for existence was later described by the Islamic writer Al-Jahiz in the 9th century.At the turn of the 16th century Leonardo da Vinci collected a set of fossils of ammonites as well as other biological material. He extensively reasoned in his writings that the shapes of animals are not given once and forever by the \"upper power\" but instead are generated in different forms naturally and then selected for reproduction by their compatibility with environment The more recent classical arguments were reintroduced in the 18th century by Pierre Louis Maupertuis and others, including Darwin's grandfather, Erasmus Darwin.\nUntil the early 19th century, the prevailing view in Western societies was that differences between individuals of a species were uninteresting departures from their Platonic ideals (or typus) of created kinds. However, the theory of uniformitarianism in geology promoted the idea that simple, weak forces could act continuously over long periods of time to produce radical changes in the Earth's landscape. The success of this theory raised awareness of the vast scale of geological time and made plausible the idea that tiny, virtually imperceptible changes in successive generations could produce consequences on the scale of differences between species.The early 19th-century zoologist Jean-Baptiste Lamarck suggested the inheritance of acquired characteristics as a mechanism for evolutionary change; adaptive traits acquired by an organism during its lifetime could be inherited by that organism's progeny, eventually causing transmutation of species. This theory, Lamarckism, was an influence on the Soviet biologist Trofim Lysenko's ill-fated antagonism to mainstream genetic theory as late as the mid 20th century.Between 1835 and 1837, the zoologist Edward Blyth worked on the area of variation, artificial selection, and how a similar process occurs in nature. Darwin acknowledged Blyth's ideas in the first chapter on variation of On the Origin of Species.\n\n\n=== Darwin's theory ===\n\nIn 1859, Charles Darwin set out his theory of evolution by natural selection as an explanation for adaptation and speciation. He defined natural selection as the \"principle by which each slight variation [of a trait], if useful, is preserved\". The concept was simple but powerful: individuals best adapted to their environments are more likely to survive and reproduce. As long as there is some variation between them and that variation is heritable, there will be an inevitable selection of individuals with the most advantageous variations. If the variations are heritable, then differential reproductive success leads to the evolution of particular populations of a species, and populations that evolve to be sufficiently different eventually become different species.\n\nDarwin's ideas were inspired by the observations that he had made on the second voyage of HMS Beagle (1831\u20131836), and by the work of a political economist, Thomas Robert Malthus, who, in An Essay on the Principle of Population (1798), noted that population (if unchecked) increases exponentially, whereas the food supply grows only arithmetically; thus, inevitable limitations of resources would have demographic implications, leading to a \"struggle for existence\". When Darwin read Malthus in 1838 he was already primed by his work as a naturalist to appreciate the \"struggle for existence\" in nature. It struck him that as population outgrew resources, \"favourable variations would tend to be preserved, and unfavourable ones to be destroyed. The result of this would be the formation of new species.\" Darwin wrote:\n\nIf during the long course of ages and under varying conditions of life, organic beings vary at all in the several parts of their organisation, and I think this cannot be disputed; if there be, owing to the high geometrical powers of increase of each species, at some age, season, or year, a severe struggle for life, and this certainly cannot be disputed; then, considering the infinite complexity of the relations of all organic beings to each other and to their conditions of existence, causing an infinite diversity in structure, constitution, and habits, to be advantageous to them, I think it would be a most extraordinary fact if no variation ever had occurred useful to each being's own welfare, in the same way as so many variations have occurred useful to man. But if variations useful to any organic being do occur, assuredly individuals thus characterised will have the best chance of being preserved in the struggle for life; and from the strong principle of inheritance they will tend to produce offspring similarly characterised. This principle of preservation, I have called, for the sake of brevity, Natural Selection.\nOnce he had his theory, Darwin was meticulous about gathering and refining evidence before making his idea public. He was in the process of writing his \"big book\" to present his research when the naturalist Alfred Russel Wallace independently conceived of the principle and described it in an essay he sent to Darwin to forward to Charles Lyell. Lyell and Joseph Dalton Hooker decided to present his essay together with unpublished writings that Darwin had sent to fellow naturalists, and On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection was read to the Linnean Society of London announcing co-discovery of the principle in July 1858. Darwin published a detailed account of his evidence and conclusions in On the Origin of Species in 1859. In the 3rd edition of 1861 Darwin acknowledged that others\u2014like William Charles Wells in 1813, and Patrick Matthew in 1831\u2014had proposed similar ideas, but had neither developed them nor presented them in notable scientific publications.\n\nDarwin thought of natural selection by analogy to how farmers select crops or livestock for breeding, which he called \"artificial selection\"; in his early manuscripts he referred to a \"Nature\" which would do the selection. At the time, other mechanisms of evolution such as evolution by genetic drift were not yet explicitly formulated, and Darwin believed that selection was likely only part of the story: \"I am convinced that Natural Selection has been the main but not exclusive means of modification.\" In a letter to Charles Lyell in September 1860, Darwin regretted the use of the term \"Natural Selection\", preferring the term \"Natural Preservation\".For Darwin and his contemporaries, natural selection was in essence synonymous with evolution by natural selection. After the publication of On the Origin of Species, educated people generally accepted that evolution had occurred in some form. However, natural selection remained controversial as a mechanism, partly because it was perceived to be too weak to explain the range of observed characteristics of living organisms, and partly because even supporters of evolution balked at its \"unguided\" and non-progressive nature, a response that has been characterised as the single most significant impediment to the idea's acceptance. However, some thinkers enthusiastically embraced natural selection; after reading Darwin, Herbert Spencer introduced the phrase survival of the fittest, which became a popular summary of the theory. The fifth edition of On the Origin of Species published in 1869 included Spencer's phrase as an alternative to natural selection, with credit given: \"But the expression often used by Mr. Herbert Spencer of the Survival of the Fittest is more accurate, and is sometimes equally convenient.\" Although the phrase is still often used by non-biologists, modern biologists avoid it because it is tautological if \"fittest\" is read to mean \"functionally superior\" and is applied to individuals rather than considered as an averaged quantity over populations.\n\n\n=== The modern synthesis ===\n\nNatural selection relies crucially on the idea of heredity, but developed before the basic concepts of genetics. Although the Moravian monk Gregor Mendel, the father of modern genetics, was a contemporary of Darwin's, his work lay in obscurity, only being rediscovered in 1900. With the early 20th century integration of evolution with Mendel's laws of inheritance, the so-called modern synthesis, scientists generally came to accept natural selection. The synthesis grew from advances in different fields. Ronald Fisher developed the required mathematical language and wrote The Genetical Theory of Natural Selection (1930). J. B. S. Haldane introduced the concept of the \"cost\" of natural selection.Sewall Wright elucidated the nature of selection and adaptation.\nIn his book Genetics and the Origin of Species (1937), Theodosius Dobzhansky established the idea that mutation, once seen as a rival to selection, actually supplied the raw material for natural selection by creating genetic diversity.\n\n\n=== A second synthesis ===\n\nErnst Mayr recognised the key importance of reproductive isolation for speciation in his Systematics and the Origin of Species (1942).W. D. Hamilton conceived of kin selection in 1964. This synthesis cemented natural selection as the foundation of evolutionary theory, where it remains today. A second synthesis was brought about at the end of the 20th century by advances in molecular genetics, creating the field of evolutionary developmental biology (\"evo-devo\"), which seeks to explain the evolution of form in terms of the genetic regulatory programs which control the development of the embryo at molecular level. Natural selection is here understood to act on embryonic development to change the morphology of the adult body.\n\n\n== Terminology ==\nThe term natural selection is most often defined to operate on heritable traits, because these directly participate in evolution. However, natural selection is \"blind\" in the sense that changes in phenotype can give a reproductive advantage regardless of whether or not the trait is heritable. Following Darwin's primary usage, the term is used to refer both to the evolutionary consequence of blind selection and to its mechanisms. It is sometimes helpful to explicitly distinguish between selection's mechanisms and its effects; when this distinction is important, scientists define \"(phenotypic) natural selection\" specifically as \"those mechanisms that contribute to the selection of individuals that reproduce\", without regard to whether the basis of the selection is heritable. Traits that cause greater reproductive success of an organism are said to be selected for, while those that reduce success are selected against.\n\n\n== Mechanism ==\n\n\n=== Heritable variation, differential reproduction ===\n\nNatural variation occurs among the individuals of any population of organisms. Some differences may improve an individual's chances of surviving and reproducing such that its lifetime reproductive rate is increased, which means that it leaves more offspring. If the traits that give these individuals a reproductive advantage are also heritable, that is, passed from parent to offspring, then there will be differential reproduction, that is, a slightly higher proportion of fast rabbits or efficient algae in the next generation. Even if the reproductive advantage is very slight, over many generations any advantageous heritable trait becomes dominant in the population. In this way the natural environment of an organism \"selects for\" traits that confer a reproductive advantage, causing evolutionary change, as Darwin described. This gives the appearance of purpose, but in natural selection there is no intentional choice. Artificial selection is purposive where natural selection is not, though biologists often use teleological language to describe it.The peppered moth exists in both light and dark colours in Great Britain, but during the industrial revolution, many of the trees on which the moths rested became blackened by soot, giving the dark-coloured moths an advantage in hiding from predators. This gave dark-coloured moths a better chance of surviving to produce dark-coloured offspring, and in just fifty years from the first dark moth being caught, nearly all of the moths in industrial Manchester were dark. The balance was reversed by the effect of the Clean Air Act 1956, and the dark moths became rare again, demonstrating the influence of natural selection on peppered moth evolution. A recent study, using image analysis and avian vision models, shows that pale individuals more closely match lichen backgrounds than dark morphs and for the first time quantifies the camouflage of moths to predation risk.\n\n\n=== Fitness ===\n\nThe concept of fitness is central to natural selection. In broad terms, individuals that are more \"fit\" have better potential for survival, as in the well-known phrase \"survival of the fittest\", but the precise meaning of the term is much more subtle. Modern evolutionary theory defines fitness not by how long an organism lives, but by how successful it is at reproducing. If an organism lives half as long as others of its species, but has twice as many offspring surviving to adulthood, its genes become more common in the adult population of the next generation. Though natural selection acts on individuals, the effects of chance mean that fitness can only really be defined \"on average\" for the individuals within a population. The fitness of a particular genotype corresponds to the average effect on all individuals with that genotype.\nA distinction must be made between the concept of \"survival of the fittest\" and \"improvement in fitness\". \"Survival of the fittest\" does not give an \"improvement in fitness\", it only represents the removal of the less fit variants from a population. A mathematical example of \"survival of the fittest\" is given by Haldane in his paper \"The Cost of Natural Selection\". Haldane called this process \"substitution\" or more commonly in biology, this is called \"fixation\". This is correctly described by the differential survival and reproduction of individuals due to differences in phenotype. On the other hand, \"improvement in fitness\" is not dependent on the differential survival and reproduction of individuals due to differences in phenotype, it is dependent on the absolute survival of the particular variant. The probability of a beneficial mutation occurring on some member of a population depends on the total number of replications of that variant. The mathematics of \"improvement in fitness was described by Kleinman. An empirical example of \"improvement in fitness\" is given by the Kishony Mega-plate experiment. In this experiment, \"improvement in fitness\" depends on the number of replications of the particular variant for a new variant to appear that is capable of growing in the next higher drug concentration region. Fixation or substitution is not required for this \"improvement in fitness\". On the other hand, \"improvement in fitness\" can occur in an environment where \"survival of the fittest\" is also acting. Richard Lenski's classic E. coli long-term evolution experiment is an example of adaptation in a competitive environment, (\"improvement in fitness\" during \"survival of the fittest\"). The probability of a beneficial mutation occurring on some member of the lineage to give improved fitness is slowed by the competition. The variant which is a candidate for a beneficial mutation in this limited carrying capacity environment must first out-compete the \"less fit\" variants in order to accumulate the requisite number of replications for there to be a reasonable probability of that beneficial mutation occurring.\n\n\n=== Competition ===\n\nIn biology, competition is an interaction between organisms in which the fitness of one is lowered by the presence of another. This may be because both rely on a limited supply of a resource such as food, water, or territory. Competition may be within or between species, and may be direct or indirect. Species less suited to compete should in theory either adapt or die out, since competition plays a powerful role in natural selection, but according to the \"room to roam\" theory it may be less important than expansion among larger clades.Competition is modelled by r/K selection theory, which is based on Robert MacArthur and E. O. Wilson's work on island biogeography. In this theory, selective pressures drive evolution in one of two stereotyped directions: r- or K-selection. These terms, r and K, can be illustrated in a logistic model of population dynamics:\n\n  \n    \n      \n        \n          \n            \n              d\n              N\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        r\n        N\n        \n          (\n          \n            1\n            \u2212\n            \n              \n                N\n                K\n              \n            \n          \n          )\n        \n        \n        \n      \n    \n    {\\displaystyle {\\frac {dN}{dt}}=rN\\left(1-{\\frac {N}{K}}\\right)\\qquad \\!}\n  where r is the growth rate of the population (N), and K is the carrying capacity of its local environmental setting. Typically, r-selected species exploit empty niches, and produce many offspring, each with a relatively low probability of surviving to adulthood. In contrast, K-selected species are strong competitors in crowded niches, and invest more heavily in much fewer offspring, each with a relatively high probability of surviving to adulthood.\n\n\n== Classification ==\n\nNatural selection can act on any heritable phenotypic trait, and selective pressure can be produced by any aspect of the environment, including sexual selection and competition with members of the same or other species. However, this does not imply that natural selection is always directional and results in adaptive evolution; natural selection often results in the maintenance of the status quo by eliminating less fit variants.Selection can be classified in several different ways, such as by its effect on a trait, on genetic diversity, by the life cycle stage where it acts, by the unit of selection, or by the resource being competed for.\n\n\n=== By effect on a trait ===\nSelection has different effects on traits. Stabilizing selection acts to hold a trait at a stable optimum, and in the simplest case all deviations from this optimum are selectively disadvantageous. Directional selection favours extreme values of a trait. The uncommon disruptive selection also acts during transition periods when the current mode is sub-optimal, but alters the trait in more than one direction. In particular, if the trait is quantitative and univariate then both higher and lower trait levels are favoured. Disruptive selection can be a precursor to speciation.\n\n\n=== By effect on genetic diversity ===\nAlternatively, selection can be divided according to its effect on genetic diversity. Purifying or negative selection acts to remove genetic variation from the population (and is opposed by de novo mutation, which introduces new variation. In contrast, balancing selection acts to maintain genetic variation in a population, even in the absence of de novo mutation, by negative frequency-dependent selection. One mechanism for this is heterozygote advantage, where individuals with two different alleles have a selective advantage over individuals with just one allele. The polymorphism at the human ABO blood group locus has been explained in this way.\n\n\n=== By life cycle stage ===\nAnother option is to classify selection by the life cycle stage at which it acts. Some biologists recognise just two types: viability (or survival) selection, which acts to increase an organism's probability of survival, and fecundity (or fertility or reproductive) selection, which acts to increase the rate of reproduction, given survival. Others split the life cycle into further components of selection. Thus viability and survival selection may be defined separately and respectively as acting to improve the probability of survival before and after reproductive age is reached, while fecundity selection may be split into additional sub-components including sexual selection, gametic selection, acting on gamete survival, and compatibility selection, acting on zygote formation.\n\n\n=== By unit of selection ===\nSelection can also be classified by the level or unit of selection. Individual selection acts on the individual, in the sense that adaptations are \"for\" the benefit of the individual, and result from selection among individuals. Gene selection acts directly at the level of the gene. In kin selection and intragenomic conflict, gene-level selection provides a more apt explanation of the underlying process. Group selection, if it occurs, acts on groups of organisms, on the assumption that groups replicate and mutate in an analogous way to genes and individuals. There is an ongoing debate over the degree to which group selection occurs in nature.\n\n\n=== By resource being competed for ===\n\nFinally, selection can be classified according to the resource being competed for. Sexual selection results from competition for mates. Sexual selection typically proceeds via fecundity selection, sometimes at the expense of viability. Ecological selection is natural selection via any means other than sexual selection, such as kin selection, competition, and infanticide. Following Darwin, natural selection is sometimes defined as ecological selection, in which case sexual selection is considered a separate mechanism.Sexual selection as first articulated by Darwin (using the example of the peacock's tail) refers specifically to competition for mates, which can be intrasexual, between individuals of the same sex, that is male\u2013male competition, or intersexual, where one gender chooses mates, most often with males displaying and females choosing. However, in some species, mate choice is primarily by males, as in some fishes of the family Syngnathidae.Phenotypic traits can be displayed in one sex and desired in the other sex, causing a positive feedback loop called a Fisherian runaway, for example, the extravagant plumage of some male birds such as the peacock. An alternate theory proposed by the same Ronald Fisher in 1930 is the sexy son hypothesis, that mothers want promiscuous sons to give them large numbers of grandchildren and so choose promiscuous fathers for their children. Aggression between members of the same sex is sometimes associated with very distinctive features, such as the antlers of stags, which are used in combat with other stags. More generally, intrasexual selection is often associated with sexual dimorphism, including differences in body size between males and females of a species.\n\n\n== Arms races ==\n\nNatural selection is seen in action in the development of antibiotic resistance in microorganisms. Since the discovery of penicillin in 1928, antibiotics have been used to fight bacterial diseases. The widespread misuse of antibiotics has selected for microbial resistance to antibiotics in clinical use, to the point that the methicillin-resistant Staphylococcus aureus (MRSA) has been described as a \"superbug\" because of the threat it poses to health and its relative invulnerability to existing drugs. Response strategies typically include the use of different, stronger antibiotics; however, new strains of MRSA have recently emerged that are resistant even to these drugs. This is an evolutionary arms race, in which bacteria develop strains less susceptible to antibiotics, while medical researchers attempt to develop new antibiotics that can kill them. A similar situation occurs with pesticide resistance in plants and insects. Arms races are not necessarily induced by man; a well-documented example involves the spread of a gene in the butterfly Hypolimnas bolina suppressing male-killing activity by Wolbachia bacteria parasites on the island of Samoa, where the spread of the gene is known to have occurred over a period of just five years\n\n\n== Evolution by means of natural selection ==\n\nA prerequisite for natural selection to result in adaptive evolution, novel traits and speciation is the presence of heritable genetic variation that results in fitness differences. Genetic variation is the result of mutations, genetic recombinations and alterations in the karyotype (the number, shape, size and internal arrangement of the chromosomes). Any of these changes might have an effect that is highly advantageous or highly disadvantageous, but large effects are rare. In the past, most changes in the genetic material were considered neutral or close to neutral because they occurred in noncoding DNA or resulted in a synonymous substitution. However, many mutations in non-coding DNA have deleterious effects. Although both mutation rates and average fitness effects of mutations are dependent on the organism, a majority of mutations in humans are slightly deleterious.Some mutations occur in \"toolkit\" or regulatory genes. Changes in these often have large effects on the phenotype of the individual because they regulate the function of many other genes. Most, but not all, mutations in regulatory genes result in non-viable embryos. Some nonlethal regulatory mutations occur in HOX genes in humans, which can result in a cervical rib or polydactyly, an increase in the number of fingers or toes. When such mutations result in a higher fitness, natural selection favours these phenotypes and the novel trait spreads in the population.\nEstablished traits are not immutable; traits that have high fitness in one environmental context may be much less fit if environmental conditions change. In the absence of natural selection to preserve such a trait, it becomes more variable and deteriorate over time, possibly resulting in a vestigial manifestation of the trait, also called evolutionary baggage. In many circumstances, the apparently vestigial structure may retain a limited functionality, or may be co-opted for other advantageous traits in a phenomenon known as preadaptation. A famous example of a vestigial structure, the eye of the blind mole-rat, is believed to retain function in photoperiod perception.\n\n\n=== Speciation ===\n\nSpeciation requires a degree of reproductive isolation\u2014that is, a reduction in gene flow. However, it is intrinsic to the concept of a species that hybrids are selected against, opposing the evolution of reproductive isolation, a problem that was recognised by Darwin. The problem does not occur in allopatric speciation with geographically separated populations, which can diverge with different sets of mutations. E. B. Poulton realized in 1903 that reproductive isolation could evolve through divergence, if each lineage acquired a different, incompatible allele of the same gene. Selection against the heterozygote would then directly create reproductive isolation, leading to the Bateson\u2013Dobzhansky\u2013Muller model, further elaborated by H. Allen Orr and Sergey Gavrilets. With reinforcement, however, natural selection can favor an increase in pre-zygotic isolation, influencing the process of speciation directly.\n\n\n== Genetic basis ==\n\n\n=== Genotype and phenotype ===\n\nNatural selection acts on an organism's phenotype, or physical characteristics. Phenotype is determined by an organism's genetic make-up (genotype) and the environment in which the organism lives. When different organisms in a population possess different versions of a gene for a certain trait, each of these versions is known as an allele. It is this genetic variation that underlies differences in phenotype. An example is the ABO blood type antigens in humans, where three alleles govern the phenotype.Some traits are governed by only a single gene, but most traits are influenced by the interactions of many genes. A variation in one of the many genes that contributes to a trait may have only a small effect on the phenotype; together, these genes can produce a continuum of possible phenotypic values.\n\n\n=== Directionality of selection ===\n\nWhen some component of a trait is heritable, selection alters the frequencies of the different alleles, or variants of the gene that produces the variants of the trait. Selection can be divided into three classes, on the basis of its effect on allele frequencies: directional, stabilizing, and disruptive selection. Directional selection occurs when an allele has a greater fitness than others, so that it increases in frequency, gaining an increasing share in the population. This process can continue until the allele is fixed and the entire population shares the fitter phenotype. Far more common is stabilizing selection, which lowers the frequency of alleles that have a deleterious effect on the phenotype\u2014that is, produce organisms of lower fitness. This process can continue until the allele is eliminated from the population. Stabilizing selection conserves functional genetic features, such as protein-coding genes or regulatory sequences, over time by selective pressure against deleterious variants. Disruptive (or diversifying) selection is selection favoring extreme trait values over intermediate trait values. Disruptive selection may cause sympatric speciation through niche partitioning.\nSome forms of balancing selection do not result in fixation, but maintain an allele at intermediate frequencies in a population. This can occur in diploid species (with pairs of chromosomes) when heterozygous individuals (with just one copy of the allele) have a higher fitness than homozygous individuals (with two copies). This is called heterozygote advantage or over-dominance, of which the best-known example is the resistance to malaria in humans heterozygous for sickle-cell anaemia. Maintenance of allelic variation can also occur through disruptive or diversifying selection, which favours genotypes that depart from the average in either direction (that is, the opposite of over-dominance), and can result in a bimodal distribution of trait values. Finally, balancing selection can occur through frequency-dependent selection, where the fitness of one particular phenotype depends on the distribution of other phenotypes in the population. The principles of game theory have been applied to understand the fitness distributions in these situations, particularly in the study of kin selection and the evolution of reciprocal altruism.\n\n\n=== Selection, genetic variation, and drift ===\n\nA portion of all genetic variation is functionally neutral, producing no phenotypic effect or significant difference in fitness. Motoo Kimura's neutral theory of molecular evolution by genetic drift proposes that this variation accounts for a large fraction of observed genetic diversity. Neutral events can radically reduce genetic variation through population bottlenecks. which among other things can cause the founder effect in initially small new populations. When genetic variation does not result in differences in fitness, selection cannot directly affect the frequency of such variation. As a result, the genetic variation at those sites is higher than at sites where variation does influence fitness. However, after a period with no new mutations, the genetic variation at these sites is eliminated due to genetic drift. Natural selection reduces genetic variation by eliminating maladapted individuals, and consequently the mutations that caused the maladaptation. At the same time, new mutations occur, resulting in a mutation\u2013selection balance. The exact outcome of the two processes depends both on the rate at which new mutations occur and on the strength of the natural selection, which is a function of how unfavourable the mutation proves to be.Genetic linkage occurs when the loci of two alleles are close on a chromosome. During the formation of gametes, recombination reshuffles the alleles. The chance that such a reshuffle occurs between two alleles is inversely related to the distance between them. Selective sweeps occur when an allele becomes more common in a population as a result of positive selection. As the prevalence of one allele increases, closely linked alleles can also become more common by \"genetic hitchhiking\", whether they are neutral or even slightly deleterious. A strong selective sweep results in a region of the genome where the positively selected haplotype (the allele and its neighbours) are in essence the only ones that exist in the population. Selective sweeps can be detected by measuring linkage disequilibrium, or whether a given haplotype is overrepresented in the population. Since a selective sweep also results in selection of neighbouring alleles, the presence of a block of strong linkage disequilibrium might indicate a 'recent' selective sweep near the centre of the block.Background selection is the opposite of a selective sweep. If a specific site experiences strong and persistent purifying selection, linked variation tends to be weeded out along with it, producing a region in the genome of low overall variability. Because background selection is a result of deleterious new mutations, which can occur randomly in any haplotype, it does not produce clear blocks of linkage disequilibrium, although with low recombination it can still lead to slightly negative linkage disequilibrium overall.\n\n\n== Impact ==\n\nDarwin's ideas, along with those of Adam Smith and Karl Marx, had a profound influence on 19th century thought, including his radical claim that \"elaborately constructed forms, so different from each other, and dependent on each other in so complex a manner\" evolved from the simplest forms of life by a few simple principles. This inspired some of Darwin's most ardent supporters\u2014and provoked the strongest opposition. Natural selection had the power, according to Stephen Jay Gould, to \"dethrone some of the deepest and most traditional comforts of Western thought\", such as the belief that humans have a special place in the world.In the words of the philosopher Daniel Dennett, \"Darwin's dangerous idea\" of evolution by natural selection is a \"universal acid,\" which cannot be kept restricted to any vessel or container, as it soon leaks out, working its way into ever-wider surroundings. Thus, in the last decades, the concept of natural selection has spread from evolutionary biology to other disciplines, including evolutionary computation, quantum Darwinism, evolutionary economics, evolutionary epistemology, evolutionary psychology, and cosmological natural selection. This unlimited applicability has been called universal Darwinism.\n\n\n=== Origin of life ===\n\nHow life originated from inorganic matter remains an unresolved problem in biology. One prominent hypothesis is that life first appeared in the form of short self-replicating RNA polymers. On this view, life may have come into existence when RNA chains first experienced the basic conditions, as conceived by Charles Darwin, for natural selection to operate. These conditions are: heritability, variation of type, and competition for limited resources. The fitness of an early RNA replicator would likely have been a function of adaptive capacities that were intrinsic (i.e., determined by the nucleotide sequence) and the availability of resources. The three primary adaptive capacities could logically have been: (1) the capacity to replicate with moderate fidelity (giving rise to both heritability and variation of type), (2) the capacity to avoid decay, and (3) the capacity to acquire and process resources. These capacities would have been determined initially by the folded configurations (including those configurations with ribozyme activity) of the RNA replicators that, in turn, would have been encoded in their individual nucleotide sequences.\n\n\n=== Cell and molecular biology ===\nIn 1881, the embryologist Wilhelm Roux published Der Kampf der Theile im Organismus (The Struggle of Parts in the Organism) in which he suggested that the development of an organism results from a Darwinian competition between the parts of the embryo, occurring at all levels, from molecules to organs. In recent years, a modern version of this theory has been proposed by Jean-Jacques Kupiec. According to this cellular Darwinism, random variation at the molecular level generates diversity in cell types whereas cell interactions impose a characteristic order on the developing embryo.\n\n\n=== Social and psychological theory ===\n\nThe social implications of the theory of evolution by natural selection also became the source of continuing controversy. Friedrich Engels, a German political philosopher and co-originator of the ideology of communism, wrote in 1872 that \"Darwin did not know what a bitter satire he wrote on mankind, and especially on his countrymen, when he showed that free competition, the struggle for existence, which the economists celebrate as the highest historical achievement, is the normal state of the animal kingdom.\" Herbert Spencer and the eugenics advocate Francis Galton's interpretation of natural selection as necessarily progressive, leading to supposed advances in intelligence and civilisation, became a justification for colonialism, eugenics, and social Darwinism. For example, in 1940, Konrad Lorenz, in writings that he subsequently disowned, used the theory as a justification for policies of the Nazi state. He wrote \"... selection for toughness, heroism, and social utility ... must be accomplished by some human institution, if mankind, in default of selective factors, is not to be ruined by domestication-induced degeneracy. The racial idea as the basis of our state has already accomplished much in this respect.\" Others have developed ideas that human societies and culture evolve by mechanisms analogous to those that apply to evolution of species.More recently, work among anthropologists and psychologists has led to the development of sociobiology and later of evolutionary psychology, a field that attempts to explain features of human psychology in terms of adaptation to the ancestral environment. The most prominent example of evolutionary psychology, notably advanced in the early work of Noam Chomsky and later by Steven Pinker, is the hypothesis that the human brain has adapted to acquire the grammatical rules of natural language. Other aspects of human behaviour and social structures, from specific cultural norms such as incest avoidance to broader patterns such as gender roles, have been hypothesised to have similar origins as adaptations to the early environment in which modern humans evolved. By analogy to the action of natural selection on genes, the concept of memes\u2014\"units of cultural transmission,\" or culture's equivalents of genes undergoing selection and recombination\u2014has arisen, first described in this form by Richard Dawkins in 1976 and subsequently expanded upon by philosophers such as Daniel Dennett as explanations for complex cultural activities, including human consciousness.\n\n\n=== Information and systems theory ===\nIn 1922, Alfred J. Lotka proposed that natural selection might be understood as a physical principle that could be described in terms of the use of energy by a system, a concept later developed by Howard T. Odum as the maximum power principle in thermodynamics, whereby evolutionary systems with selective advantage maximise the rate of useful energy transformation.The principles of natural selection have inspired a variety of computational techniques, such as \"soft\" artificial life, that simulate selective processes and can be highly efficient in 'adapting' entities to an environment defined by a specified fitness function. For example, a class of heuristic optimisation algorithms known as genetic algorithms, pioneered by John Henry Holland in the 1970s and expanded upon by David E. Goldberg, identify optimal solutions by simulated reproduction and mutation of a population of solutions defined by an initial probability distribution. Such algorithms are particularly useful when applied to problems whose energy landscape is very rough or has many local minima.\n\n\n=== In fiction ===\n\nDarwinian evolution by natural selection is pervasive in literature, whether taken optimistically in terms of how humanity may evolve towards perfection, or pessimistically in terms of the dire consequences of the interaction of human nature and the struggle for survival. Among major responses is Samuel Butler's 1872 pessimistic Erewhon (\"nowhere\", written mostly backwards). In 1893 H. G. Wells imagined \"The Man of the Year Million\", transformed by natural selection into a being with a huge head and eyes, and shrunken body.\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== Further reading ==\nFor technical audiences\nBell, Graham (2008). Selection: The Mechanism of Evolution (2nd ed.). Oxford; New York: Oxford University Press. ISBN 978-0-19-856972-5. LCCN 2007039692. OCLC 170034792.\nJohnson, Clifford (1976). Introduction to Natural Selection. Baltimore, MD: University Park Press. ISBN 978-0-8391-0936-5. LCCN 76008175. OCLC 2091640.\nGould, Stephen Jay (2002). The Structure of Evolutionary Theory. Cambridge, MA: Belknap Press of Harvard University Press. ISBN 978-0-674-00613-3. LCCN 2001043556. OCLC 47869352.\nMaynard Smith, John (1993) [Originally published 1958; Harmondsworth, England: Penguin Books]. The Theory of Evolution (Canto ed.). Cambridge, New York: Cambridge University Press. ISBN 978-0-521-45128-4. LCCN 93020358. OCLC 27676642.\nPopper, Karl (December 1978). \"Natural Selection and the Emergence of Mind\". Dialectica. 32 (3\u20134): 339\u2013355. doi:10.1111/j.1746-8361.1978.tb01321.x. ISSN 0012-2017.\nSammut-Bonnici, Tanya; Wensley, Robin (September 2002). \"Darwinism, probability and complexity: Market-based organizational transformation and change explained through the theories of evolution\" (PDF). International Journal of Management Reviews. 4 (3): 291\u2013315. doi:10.1111/1468-2370.00088. ISSN 1460-8545.\nSober, Elliott, ed. (1994). Conceptual Issues in Evolutionary Biology (2nd ed.). Cambridge, MA: MIT Press. ISBN 978-0-262-69162-8. LCCN 93008199. OCLC 28150417.\nWilliams, George C. (1992). Natural Selection: Domains, Levels, and Challenges. Oxford Series in Ecology and Evolution. New York: Oxford University Press. ISBN 978-0-19-506933-4. LCCN 91038938. OCLC 228136567.\nFor general audiences\nDawkins, Richard (1996). Climbing Mount Improbable (1st American ed.). New York: W.W. Norton & Company. ISBN 978-0-393-03930-6. LCCN 34633422. OCLC 34633422.\nGould, Stephen Jay (1977). Ever Since Darwin: Reflections in Natural History (1st ed.). New York: W.W. Norton & Company. ISBN 978-0-393-06425-4. LCCN 77022504. OCLC 3090189.\nJones, Steve (2000). Darwin's Ghost: The Origin of Species Updated (1st ed.). New York: Random House. ISBN 978-0-375-50103-6. LCCN 99053246. OCLC 42690131.\nLewontin, Richard C. (September 1978). \"Adaptation\". Scientific American. 239 (3): 212\u2013230. Bibcode:1978SciAm.239c.212L. doi:10.1038/scientificamerican0978-212. ISSN 0036-8733. PMID 705323.\nMayr, Ernst (2002) [Originally published 2001; New York: Basic Books]. What Evolution Is. Science Masters. London: Weidenfeld & Nicolson. ISBN 978-0-297-60741-0. LCCN 2001036562. OCLC 248107061.\nWeiner, Jonathan (1994). The Beak of the Finch: A Story of Evolution in Our Time (1st ed.). New York: Knopf. ISBN 978-0-679-40003-5. LCCN 93036755. OCLC 29029572.\nHistorical\nKohn, Marek (2004). A Reason for Everything: Natural Selection and the English Imagination. London: Faber and Faber. ISBN 978-0-571-22392-3. LCCN 2005360890. OCLC 57200626.\nZirkle, Conway (25 April 1941). \"Natural Selection before the 'Origin of Species'\". Proceedings of the American Philosophical Society. 84 (1): 71\u2013123. ISSN 0003-049X. JSTOR 984852.\n\n\n== External links ==\n\nDarwin, Charles. \"On the Origin of Species\". Archived from the original on 25 February 2001. \u2013 Chapter 4, Natural Selection", "Apriori algorithm": "Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.\n\n\n== Overview ==\nThe Apriori algorithm was proposed by Agrawal and Srikant in 1994. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation or IP addresses). Other algorithms are designed for finding association rules in data having no transactions (Winepi and Minepi), or having no timestamps (DNA sequencing). Each transaction is seen as a set of items (an itemset). Given a threshold \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , the Apriori algorithm identifies the item sets which are subsets of at least \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   transactions in the database.\nApriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data.  The algorithm terminates when no further successful extensions are found.\nApriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   from item sets of length \n  \n    \n      \n        k\n        \u2212\n        1\n      \n    \n    {\\displaystyle k-1}\n  .  Then it prunes the candidates which have an infrequent sub pattern.  According to the downward closure lemma, the candidate set contains all frequent \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\nThe pseudo code for the algorithm is given below for a transaction database \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  , and a support threshold of \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  . Usual set theoretic notation is employed, though note that \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is a multiset. \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C_{k}}\n   is the candidate set for level \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  . At each step, the algorithm is assumed to generate the candidate sets from the large item sets of the preceding level, heeding the downward closure lemma. \n  \n    \n      \n        \n          c\n          o\n          u\n          n\n          t\n        \n        [\n        c\n        ]\n      \n    \n    {\\displaystyle \\mathrm {count} [c]}\n   accesses a field of the data structure that represents candidate set \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  , which is initially assumed to be zero. Many details are omitted below, usually the most important part of the implementation is the data structure used for storing the candidate sets, and counting their frequencies.\n\nApriori(T, \u03b5)\n    L1 \u2190 {large 1 - itemsets}\n    k \u2190 2\n    while Lk\u22121 is not empty\n        Ck \u2190 Apriori_gen(Lk\u22121, k)\n        for transactions t in T\n            Dt \u2190 {c in Ck : c \u2286 t}\n            for candidates c in Dt\n                count[c] \u2190 count[c] + 1\n\n        Lk \u2190 {c in Ck : count[c] \u2265 \u03b5}\n        k \u2190 k + 1\n\n    return Union(Lk)\n    \nApriori_gen(L, k)\n     result \u2190 list()\n     for all p \u2208 L, q \u2208 L where p1 = q1, p2 = q2, ..., pk-2 = qk-2 and pk-1 < qk-1\n         c = p \u222a {qk-1}\n         if u \u2208 L for all u \u2286 c where |u| = k-1\n             result.add(c)\n      return result\n\n\n== Examples ==\n\n\n=== Example 1 ===\nConsider the following database, where each row is a transaction and each cell is an individual item of the transaction:\n\nThe association rules that can be determined from this database are the following: \n\n100% of sets with alpha also contain beta\n50% of sets with alpha, beta also have epsilon\n50% of sets with alpha, beta also have thetawe can also illustrate this through a variety of examples.\n\n\n=== Example 2 ===\nAssume that a large supermarket tracks sales data by stock-keeping unit (SKU) for each item: each item, such as \"butter\" or \"bread\", is identified by a numerical SKU. The supermarket has a database of transactions where each transaction is a set of SKUs that were bought together.\nLet the database of transactions consist of following itemsets:\n\nWe will use Apriori to determine the frequent item sets of this database. To do this, we will say that an item set is frequent if it appears in at least 3 transactions of the database: the value 3 is the support threshold.\nThe first step of Apriori is to count up the number of occurrences, called the support, of each member item separately. By scanning the database for the first time, we obtain the following result\n\nAll the itemsets of size 1 have a support of at least 3, so they are all frequent.\nThe next step is to generate a list of all pairs of the frequent items.\nFor example, regarding the pair {1,2}: the first table of Example 2 shows items 1 and 2 appearing together in three of the itemsets; therefore, we say item {1,2} has support of three.\n\nThe pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are frequent. The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not frequent, any larger set which contains {1,3} or {1,4} cannot be frequent. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs:\n\nin the example, there are no frequent triplets. {2,3,4} is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold.\nWe have thus determined the frequent sets of items in the database, and illustrated how some items were not counted because one of their subsets was already known to be below the threshold.\n\n\n== Limitations ==\nApriori, while historically significant, suffers from a number of inefficiencies or trade-offs, which have spawned other algorithms.  Candidate generation generates large numbers of subsets (The algorithm attempts to load up the candidate set, with as many as possible subsets before each scan of the database).  Bottom-up subset exploration (essentially a breadth-first traversal of the subset lattice)  finds any maximal subset S only after all \n  \n    \n      \n        \n          2\n          \n            \n              |\n            \n            S\n            \n              |\n            \n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle 2^{|S|}-1}\n   of its proper subsets.\nThe algorithm scans the database too many times,  which reduces the overall performance. Due to this, the algorithm assumes that the database is permanently in the memory. \nAlso, both the time and space complexity of this algorithm are very high: \n  \n    \n      \n        O\n        \n          (\n          \n            2\n            \n              \n                |\n              \n              D\n              \n                |\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle O\\left(2^{|D|}\\right)}\n  , thus exponential, where \n  \n    \n      \n        \n          |\n        \n        D\n        \n          |\n        \n      \n    \n    {\\displaystyle |D|}\n   is the horizontal width (the total number of items) present in the database.\nLater algorithms such as Max-Miner try to identify the maximal frequent item sets without enumerating their subsets, and perform \"jumps\" in the search space rather than a purely bottom-up approach.\n\n\n== References ==\n\n\n== External links ==\nARtool, GPL Java association rule mining application with GUI, offering implementations of multiple algorithms for discovery of frequent patterns and extraction of association rules (includes Apriori)\nSPMF offers Java open-source implementations of Apriori and several variations such as AprioriClose, UApriori, AprioriInverse, AprioriRare, MSApriori, AprioriTID, and other more efficient algorithms such as FPGrowth and LCM.\nChristian Borgelt provides C implementations for Apriori and many other frequent pattern mining algorithms (Eclat, FPGrowth, etc.). The code is distributed as free software under the MIT license.\nThe R package arules contains Apriori and Eclat and infrastructure for representing, manipulating and analyzing transaction data and patterns.\nEfficient-Apriori is a Python package with an implementation of the algorithm as presented in the original paper.", "Hyperplane separation theorem": "In geometry, the hyperplane separation theorem is a theorem about disjoint convex sets in n-dimensional Euclidean space. There are several rather similar versions. In one version of the theorem, if both these sets are closed and at least one of them is compact, then there is a hyperplane in between them and even two parallel hyperplanes in between them separated by a gap. In another version, if both disjoint convex sets are open, then there is a hyperplane in between them, but not necessarily any gap.  An axis which is orthogonal to a separating hyperplane is a separating axis, because the orthogonal projections of the convex bodies onto the axis are disjoint.\nThe hyperplane separation theorem is due to Hermann Minkowski. The Hahn\u2013Banach separation theorem generalizes the result to topological vector spaces.\nA related result is the supporting hyperplane theorem.\nIn the context of support-vector machines, the optimally separating hyperplane or maximum-margin hyperplane is a hyperplane which separates two convex hulls of points and is equidistant from the two.\n\n\n== Statements and proof ==\nIn all cases, assume \n  \n    \n      \n        A\n        ,\n        B\n      \n    \n    {\\displaystyle A,B}\n   to be disjoint, nonempty, and convex subsets of \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  . The summary of the results are as follows:\n\nThe number of dimensions must be finite. In infinite-dimensional spaces there are examples of two closed, convex, disjoint sets which cannot be separated by a closed hyperplane (a hyperplane where a continuous linear functional equals some constant) even in the weak sense where the inequalities are not strict.Here, the compactness in the hypothesis cannot be relaxed; see an example in the section Counterexamples and uniqueness. This version of the separation theorem does generalize to infinite-dimension; the generalization is more commonly known as the Hahn\u2013Banach separation theorem.\nThe proof is based on the following lemma:\n\nSince a separating hyperplane cannot intersect the interiors of open convex sets, we have a corollary:\n\n\n== Case with possible intersections ==\nIf the sets \n  \n    \n      \n        A\n        ,\n        B\n      \n    \n    {\\displaystyle A,B}\n   have possible intersections, but their relative interiors are disjoint, then the proof of the first case still applies with no change, thus yielding:\n\nin particular, we have the supporting hyperplane theorem.\n\n\n== Converse of theorem ==\nNote that the existence of a hyperplane that only \"separates\" two convex sets in the weak sense of both inequalities being non-strict obviously does not imply that the two sets are disjoint. Both sets could have points located on the hyperplane.\n\n\n== Counterexamples and uniqueness ==\n\nIf one of A or B is not convex, then there are many possible counterexamples.  For example, A and B could be concentric circles.  A more subtle counterexample is one in which A and B are both closed but neither one is compact.  For example, if A is a closed half plane and B is bounded by one arm of a hyperbola, then there is no strictly separating hyperplane:\n\n  \n    \n      \n        A\n        =\n        {\n        (\n        x\n        ,\n        y\n        )\n        :\n        x\n        \u2264\n        0\n        }\n      \n    \n    {\\displaystyle A=\\{(x,y):x\\leq 0\\}}\n  \n\n  \n    \n      \n        B\n        =\n        {\n        (\n        x\n        ,\n        y\n        )\n        :\n        x\n        >\n        0\n        ,\n        y\n        \u2265\n        1\n        \n          /\n        \n        x\n        }\n        .\n         \n      \n    \n    {\\displaystyle B=\\{(x,y):x>0,y\\geq 1/x\\}.\\ }\n  (Although, by an instance of the second theorem, there is a hyperplane that separates their interiors.)  Another type of counterexample has A compact and B open.  For example, A can be a closed square and B can be an open square that touches A.\nIn the first version of the theorem, evidently the separating hyperplane is never unique.  In the second version, it may or may not be unique.  Technically a separating axis is never unique because it can be translated; in the second version of the theorem, a separating axis can be unique up to translation.\nThe horn angle provides a good counterexample to many hyperplane separations. For example, in \n  \n    \n      \n        \n          \n            R\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{2}}\n  , the unit disk is disjoint from the open interval \n  \n    \n      \n        (\n        (\n        1\n        ,\n        0\n        )\n        ,\n        (\n        1\n        ,\n        1\n        )\n        )\n      \n    \n    {\\displaystyle ((1,0),(1,1))}\n  , but the only line separating them contains the entirety of \n  \n    \n      \n        (\n        (\n        1\n        ,\n        0\n        )\n        ,\n        (\n        1\n        ,\n        1\n        )\n        )\n      \n    \n    {\\displaystyle ((1,0),(1,1))}\n  . This shows that if \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is closed and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is relatively open, then there does not necessarily exist a separation that is strict for \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  . However, if \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is closed polytope then such a separation exists.\n\n\n== More variants ==\nFarkas' lemma and related results can be understood as hyperplane separation theorems when the convex bodies are defined by finitely many linear inequalities.\nMore results may be found.\n\n\n== Use in collision detection ==\nIn collision detection, the hyperplane separation theorem is usually used in the following form:\n\nRegardless of dimensionality, the separating axis is always a line.\nFor example, in 3D, the space is separated by planes, but the separating axis is perpendicular to the separating plane.\nThe separating axis theorem can be applied for fast collision detection between polygon meshes. Each face's normal or other feature direction is used as a separating axis. Note that this yields possible separating axes, not separating lines/planes.\nIn 3D, using face normals alone will fail to separate some edge-on-edge non-colliding cases. Additional axes, consisting of the cross-products of pairs of edges, one taken from each object, are required.For increased efficiency, parallel axes may be calculated as a single axis.\n\n\n== See also ==\nDual cone\nFarkas's lemma\nKirchberger's theorem\nOptimal control\n\n\n== Notes ==\n\n\n== References ==\nBoyd, Stephen P.; Vandenberghe, Lieven (2004). Convex Optimization (PDF). Cambridge University Press. ISBN 978-0-521-83378-3.\nGolshtein, E. G.; Tretyakov, N.V. (1996). Modified Lagrangians and monotone maps in optimization. New York: Wiley. p. 6. ISBN 0-471-54821-9.\nShimizu, Kiyotaka; Ishizuka, Yo; Bard, Jonathan F. (1997). Nondifferentiable and two-level mathematical programming. Boston: Kluwer Academic Publishers. p. 19. ISBN 0-7923-9821-1.Soltan, V. (2021). Support and separation properties of convex sets in finite dimension. Extracta Math. Vol. 36, no. 2, 241-278.\n\n\n== External links ==\nCollision detection and response", "Bayes' theorem": "In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately by conditioning it relative to their age, rather than simply assuming that the individual is typical of the population as a whole.\nOne of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in the theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics, being considered by one authority as; \"to the theory of probability what Pythagoras's theorem is to geometry.\"\n\n\n== History ==\nBayes' theorem is named after the Reverend Thomas Bayes (), also a statistician and philosopher. Bayes used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter. His work was published in 1763 as An Essay towards solving a Problem in the Doctrine of Chances. Bayes studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). On Bayes's death his family transferred his papers to a friend, the minister, philosopher, and mathematician Richard Price.\nOver two years, Richard Price significantly edited the unpublished manuscript, before sending it to a friend who read it aloud at the Royal Society on 23 December 1763. Price edited Bayes's major work \"An Essay towards solving a Problem in the Doctrine of Chances\" (1763), which appeared in Philosophical Transactions, and contains Bayes' theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics and chose one of the two solutions offered by Bayes. In 1765, Price was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.  On 27 April a letter sent to his friend Benjamin Franklin was read out at the Royal Society, and later published, where Price applies this work to population and computing 'life-annuities'.Independently of Bayes, Pierre-Simon Laplace in 1774, and later in his 1812 Th\u00e9orie analytique des probabilit\u00e9s, used conditional probability to formulate the relation of an updated posterior probability from a prior probability, given evidence. He reproduced and extended Bayes's results in 1774, apparently unaware of Bayes's work. The Bayesian interpretation of probability was developed mainly by Laplace.About 200 years later, Sir Harold Jeffreys put Bayes's algorithm and Laplace's formulation on an axiomatic basis, writing in a 1973 book that Bayes' theorem \"is to the theory of probability what the Pythagorean theorem is to geometry\".Stephen Stigler used a Bayesian argument to conclude that Bayes' theorem was discovered by Nicholas Saunderson, a blind English mathematician, some time before Bayes; that interpretation, however, has been disputed.\nMartyn Hooper and Sharon McGrayne have argued that Richard Price's contribution was substantial:\n\nBy modern standards, we should refer to the Bayes\u2013Price rule. Price discovered Bayes's work, recognized its importance, corrected it, contributed to the article, and found a use for it. The modern convention of employing Bayes's name alone is unfair but so entrenched that anything else makes little sense.\n\n\n== Statement of theorem ==\nBayes' theorem is stated mathematically as the following equation:\n\nwhere \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are events and \n  \n    \n      \n        P\n        (\n        B\n        )\n        \u2260\n        0\n      \n    \n    {\\displaystyle P(B)\\neq 0}\n  .\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   is a conditional probability: the probability of event \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   occurring given that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is true. It is also called the posterior probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  .\n\n  \n    \n      \n        P\n        (\n        B\n        \u2223\n        A\n        )\n      \n    \n    {\\displaystyle P(B\\mid A)}\n   is also a conditional probability: the probability of event \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   occurring given that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is true. It can also be interpreted as the likelihood of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given a fixed \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   because \n  \n    \n      \n        P\n        (\n        B\n        \u2223\n        A\n        )\n        =\n        L\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(B\\mid A)=L(A\\mid B)}\n  .\n\n  \n    \n      \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A)}\n   and \n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n   are the probabilities of observing \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   respectively without any given conditions; they are known as the prior probability and marginal probability.\n\n\n=== Proof ===\n\n\n==== For events ====\nBayes' theorem may be derived from the definition of conditional probability:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        ,\n        \n           if \n        \n        P\n        (\n        B\n        )\n        \u2260\n        0\n        ,\n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}},{\\text{ if }}P(B)\\neq 0,}\n  where \n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n   is the probability of both A and B being true. Similarly,\n\n  \n    \n      \n        P\n        (\n        B\n        \u2223\n        A\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              A\n              )\n            \n          \n        \n        ,\n        \n           if \n        \n        P\n        (\n        A\n        )\n        \u2260\n        0\n        ,\n      \n    \n    {\\displaystyle P(B\\mid A)={\\frac {P(A\\cap B)}{P(A)}},{\\text{ if }}P(A)\\neq 0,}\n  Solving for \n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n   and substituting into the above expression for \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   yields Bayes' theorem:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2223\n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        ,\n        \n           if \n        \n        P\n        (\n        B\n        )\n        \u2260\n        0.\n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}},{\\text{ if }}P(B)\\neq 0.}\n  \n\n\n==== For continuous random variables ====\nFor two continuous random variables X and Y, Bayes' theorem may be analogously derived from the definition of conditional density:\n\n  \n    \n      \n        \n          f\n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  X\n                  ,\n                  Y\n                \n              \n              (\n              x\n              ,\n              y\n              )\n            \n            \n              \n                f\n                \n                  Y\n                \n              \n              (\n              y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)={\\frac {f_{X,Y}(x,y)}{f_{Y}(y)}}}\n  \n\n  \n    \n      \n        \n          f\n          \n            Y\n            \u2223\n            X\n            =\n            x\n          \n        \n        (\n        y\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  X\n                  ,\n                  Y\n                \n              \n              (\n              x\n              ,\n              y\n              )\n            \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle f_{Y\\mid X=x}(y)={\\frac {f_{X,Y}(x,y)}{f_{X}(x)}}}\n  Therefore,\n\n  \n    \n      \n        \n          f\n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  Y\n                  \u2223\n                  X\n                  =\n                  x\n                \n              \n              (\n              y\n              )\n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n            \n              \n                f\n                \n                  Y\n                \n              \n              (\n              y\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)={\\frac {f_{Y\\mid X=x}(y)f_{X}(x)}{f_{Y}(y)}}.}\n  \n\n\n==== General case ====\nLet \n  \n    \n      \n        \n          P\n          \n            Y\n          \n          \n            x\n          \n        \n      \n    \n    {\\displaystyle P_{Y}^{x}}\n   be the conditional distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n   and let \n  \n    \n      \n        \n          P\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle P_{X}}\n   be the distribution of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . The joint distribution is then \n  \n    \n      \n        \n          P\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        d\n        x\n        ,\n        d\n        y\n        )\n        =\n        \n          P\n          \n            Y\n          \n          \n            x\n          \n        \n        (\n        d\n        y\n        )\n        \n          P\n          \n            X\n          \n        \n        (\n        d\n        x\n        )\n      \n    \n    {\\displaystyle P_{X,Y}(dx,dy)=P_{Y}^{x}(dy)P_{X}(dx)}\n  . The conditional distribution \n  \n    \n      \n        \n          P\n          \n            X\n          \n          \n            y\n          \n        \n      \n    \n    {\\displaystyle P_{X}^{y}}\n   of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n    given \n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n   is then determined by\n\nExistence and uniqueness of the needed conditional expectation is a consequence of the Radon-Nikodym theorem. This was formulated by Kolmogorov in  his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing 'I wish to call attention to  ... and especially the theory of conditional probabilities and conditional expectations ...' in the Preface. The Bayes theorem determines the posterior distribution from the prior distribution. Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line. Modern Markov chain Monte Carlo methods  have boosted the importance of Bayes' theorem including cases with  improper priors.\n\n\n== Examples ==\n\n\n=== Recreational mathematics ===\nBayes' rule and computing conditional probabilities provide a solution method for a number of popular puzzles, such as the Three Prisoners problem, the Monty Hall problem, the Two Child problem and the Two Envelopes problem.\n\n\n=== Drug testing ===\n\nSuppose, a particular test for whether someone has been using cannabis is 90% sensitive, meaning the true positive rate (TPR) = 0.90.  Therefore, it leads to 90% true positive results (correct identification of drug use) for cannabis users.\nThe test is also 80% specific, meaning true negative rate (TNR) = 0.80.  Therefore, the test correctly identifies 80% of non-use for non-users, but also generates 20% false positives, or false positive rate (FPR) = 0.20, for non-users.\nAssuming 0.05 prevalence, meaning 5% of people use cannabis, what is the probability that a random person who tests positive is really a cannabis user?\nThe Positive predictive value (PPV) of a test is the proportion of persons who are actually positive out of all those testing positive, and can be calculated from a sample as:\n\nPPV = True positive / Tested positiveIf sensitivity, specificity, and prevalence are known, PPV can be calculated using Bayes theorem.  Let \n  \n    \n      \n        P\n        (\n        \n          User\n        \n        \u2223\n        \n          Positive\n        \n        )\n      \n    \n    {\\displaystyle P({\\text{User}}\\mid {\\text{Positive}})}\n   mean \"the probability that someone is a cannabis user given that they test positive,\" which is what is meant by PPV.  We can write:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                \n                  User\n                \n                \u2223\n                \n                  Positive\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Positive\n                      \n                      \u2223\n                      \n                        User\n                      \n                      )\n                      P\n                      (\n                      \n                        User\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Positive\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Positive\n                      \n                      \u2223\n                      \n                        User\n                      \n                      )\n                      P\n                      (\n                      \n                        User\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Positive\n                      \n                      \u2223\n                      \n                        User\n                      \n                      )\n                      P\n                      (\n                      \n                        User\n                      \n                      )\n                      +\n                      P\n                      (\n                      \n                        Positive\n                      \n                      \u2223\n                      \n                        Non-user\n                      \n                      )\n                      P\n                      (\n                      \n                        Non-user\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.90\n                      \u00d7\n                      0.05\n                    \n                    \n                      0.90\n                      \u00d7\n                      0.05\n                      +\n                      0.20\n                      \u00d7\n                      0.95\n                    \n                  \n                \n                =\n                \n                  \n                    0.045\n                    \n                      0.045\n                      +\n                      0.19\n                    \n                  \n                \n                \u2248\n                19\n                %\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P({\\text{User}}\\mid {\\text{Positive}})&={\\frac {P({\\text{Positive}}\\mid {\\text{User}})P({\\text{User}})}{P({\\text{Positive}})}}\\\\&={\\frac {P({\\text{Positive}}\\mid {\\text{User}})P({\\text{User}})}{P({\\text{Positive}}\\mid {\\text{User}})P({\\text{User}})+P({\\text{Positive}}\\mid {\\text{Non-user}})P({\\text{Non-user}})}}\\\\[8pt]&={\\frac {0.90\\times 0.05}{0.90\\times 0.05+0.20\\times 0.95}}={\\frac {0.045}{0.045+0.19}}\\approx 19\\%\\end{aligned}}}\n  The fact that \n  \n    \n      \n        P\n        (\n        \n          Positive\n        \n        )\n        =\n        P\n        (\n        \n          Positive\n        \n        \u2223\n        \n          User\n        \n        )\n        P\n        (\n        \n          User\n        \n        )\n        +\n        P\n        (\n        \n          Positive\n        \n        \u2223\n        \n          Non-user\n        \n        )\n        P\n        (\n        \n          Non-user\n        \n        )\n      \n    \n    {\\displaystyle P({\\text{Positive}})=P({\\text{Positive}}\\mid {\\text{User}})P({\\text{User}})+P({\\text{Positive}}\\mid {\\text{Non-user}})P({\\text{Non-user}})}\n   \nis a direct application of the Law of Total Probability. In this case, it says that the probability that someone tests positive is the probability that a user tests positive, times the probability of being a user, plus the probability that a non-user tests positive, times the probability of being a non-user. This is true because the classifications user and non-user form a partition of a set, namely the set of people who take the drug test. This combined with the definition of conditional probability results in the above statement.\nIn other words, even if someone tests positive, the probability that they are a cannabis user is only 19% \u2014 this is because in this group, only 5% of people are users, and most positives are false positives coming from the remaining 95%.\nIf 1,000 people were tested:\n\n950 are non-users and 190 of them give false positive (0.20 \u00d7 950)\n50 of them are users and 45 of them give true positive (0.90 \u00d7 50)The 1,000 people thus yields 235 positive tests, of which only 45 are genuine drug users, about 19%. See Figure 1 for an illustration using a frequency box, and note how small the pink area of true positives is compared to the blue area of false positives.\n\n\n==== Sensitivity or specificity ====\nThe importance of specificity can be seen by showing that even if sensitivity is raised to 100% and specificity remains at 80%, the probability of someone testing positive really being a cannabis user only rises from 19% to 21%, but if the sensitivity is held at 90% and the specificity is increased to 95%, the probability rises to 49%.\n\n\n=== Cancer rate ===\nEven if 100% of patients with pancreatic cancer have a certain symptom, when someone has the same symptom, it does not mean that this person has a 100% chance of getting pancreatic cancer. Assuming the incidence rate of pancreatic cancer is 1/100000, while 10/99999 healthy individuals have the same symptoms worldwide, the probability of having pancreatic cancer given the symptoms is only 9.1%, and the other 90.9% could be \"false positives\" (that is, falsely said to have cancer; \"positive\" is a confusing term when, as here, the test gives bad news).\nBased on incidence rate, the following table presents the corresponding numbers per 100,000 people.\n\nWhich can then be used to calculate the probability of having cancer when you have the symptoms: \n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                \n                  Cancer\n                \n                \n                  |\n                \n                \n                  Symptoms\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Symptoms\n                      \n                      \n                        |\n                      \n                      \n                        Cancer\n                      \n                      )\n                      P\n                      (\n                      \n                        Cancer\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Symptoms\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Symptoms\n                      \n                      \n                        |\n                      \n                      \n                        Cancer\n                      \n                      )\n                      P\n                      (\n                      \n                        Cancer\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Symptoms\n                      \n                      \n                        |\n                      \n                      \n                        Cancer\n                      \n                      )\n                      P\n                      (\n                      \n                        Cancer\n                      \n                      )\n                      +\n                      P\n                      (\n                      \n                        Symptoms\n                      \n                      \n                        |\n                      \n                      \n                        Non-Cancer\n                      \n                      )\n                      P\n                      (\n                      \n                        Non-Cancer\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      1\n                      \u00d7\n                      0.00001\n                    \n                    \n                      1\n                      \u00d7\n                      0.00001\n                      +\n                      (\n                      10\n                      \n                        /\n                      \n                      99999\n                      )\n                      \u00d7\n                      0.99999\n                    \n                  \n                \n                =\n                \n                  \n                    1\n                    11\n                  \n                \n                \u2248\n                9.1\n                %\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P({\\text{Cancer}}|{\\text{Symptoms}})&={\\frac {P({\\text{Symptoms}}|{\\text{Cancer}})P({\\text{Cancer}})}{P({\\text{Symptoms}})}}\\\\&={\\frac {P({\\text{Symptoms}}|{\\text{Cancer}})P({\\text{Cancer}})}{P({\\text{Symptoms}}|{\\text{Cancer}})P({\\text{Cancer}})+P({\\text{Symptoms}}|{\\text{Non-Cancer}})P({\\text{Non-Cancer}})}}\\\\[8pt]&={\\frac {1\\times 0.00001}{1\\times 0.00001+(10/99999)\\times 0.99999}}={\\frac {1}{11}}\\approx 9.1\\%\\end{aligned}}}\n  \n\n\n=== Defective item rate ===\nA factory produces items using three machines\u2014A, B, and C\u2014which account for 20%, 30%, and 50% of its output respectively. Of the items produced by machine A, 5% are defective; similarly, 3% of machine B's items and 1% of machine C's are defective. If a randomly selected item is defective, what is the probability it was produced by machine C?\nOnce again, the answer can be reached without using the formula by applying the conditions to a hypothetical number of cases. For example, if the factory produces 1,000 items, 200 will be produced by Machine A, 300 by Machine B, and 500 by Machine C. Machine A will produce 5% \u00d7 200 = 10 defective items, Machine B 3% \u00d7 300 = 9, and Machine C 1% \u00d7 500 = 5, for a total of 24. Thus, the likelihood that a randomly selected defective item was produced by machine C is 5/24 (~20.83%).\nThis problem can also be solved using Bayes' theorem: Let Xi denote the event that a randomly chosen item was made by the i th machine (for i = A,B,C). Let Y denote the event that a randomly chosen item is defective. Then, we are given the following information:\n\n  \n    \n      \n        P\n        (\n        \n          X\n          \n            A\n          \n        \n        )\n        =\n        0.2\n        ,\n        \n        P\n        (\n        \n          X\n          \n            B\n          \n        \n        )\n        =\n        0.3\n        ,\n        \n        P\n        (\n        \n          X\n          \n            C\n          \n        \n        )\n        =\n        0.5.\n      \n    \n    {\\displaystyle P(X_{A})=0.2,\\quad P(X_{B})=0.3,\\quad P(X_{C})=0.5.}\n  If the item was made by the first machine, then the probability that it is defective is 0.05; that is, P(Y\u2009|\u2009XA) = 0.05. Overall, we have\n\n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        \n          X\n          \n            A\n          \n        \n        )\n        =\n        0.05\n        ,\n        \n        P\n        (\n        Y\n        \n          |\n        \n        \n          X\n          \n            B\n          \n        \n        )\n        =\n        0.03\n        ,\n        \n        P\n        (\n        Y\n        \n          |\n        \n        \n          X\n          \n            C\n          \n        \n        )\n        =\n        0.01.\n      \n    \n    {\\displaystyle P(Y|X_{A})=0.05,\\quad P(Y|X_{B})=0.03,\\quad P(Y|X_{C})=0.01.}\n  To answer the original question, we first find P(Y). That can be done in the following way:\n\n  \n    \n      \n        P\n        (\n        Y\n        )\n        =\n        \n          \u2211\n          \n            i\n          \n        \n        P\n        (\n        Y\n        \n          |\n        \n        \n          X\n          \n            i\n          \n        \n        )\n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        =\n        (\n        0.05\n        )\n        (\n        0.2\n        )\n        +\n        (\n        0.03\n        )\n        (\n        0.3\n        )\n        +\n        (\n        0.01\n        )\n        (\n        0.5\n        )\n        =\n        0.024.\n      \n    \n    {\\displaystyle P(Y)=\\sum _{i}P(Y|X_{i})P(X_{i})=(0.05)(0.2)+(0.03)(0.3)+(0.01)(0.5)=0.024.}\n  Hence, 2.4% of the total output is defective.\nWe are given that Y has occurred, and we want to calculate the conditional\nprobability of XC. By Bayes' theorem,\n\n  \n    \n      \n        P\n        (\n        \n          X\n          \n            C\n          \n        \n        \n          |\n        \n        Y\n        )\n        =\n        \n          \n            \n              P\n              (\n              Y\n              \n                |\n              \n              \n                X\n                \n                  C\n                \n              \n              )\n              P\n              (\n              \n                X\n                \n                  C\n                \n              \n              )\n            \n            \n              P\n              (\n              Y\n              )\n            \n          \n        \n        =\n        \n          \n            \n              0.01\n              \u22c5\n              0.50\n            \n            0.024\n          \n        \n        =\n        \n          \n            5\n            24\n          \n        \n      \n    \n    {\\displaystyle P(X_{C}|Y)={\\frac {P(Y|X_{C})P(X_{C})}{P(Y)}}={\\frac {0.01\\cdot 0.50}{0.024}}={\\frac {5}{24}}}\n  Given that the item is defective, the probability that it was made by machine C is 5/24. Although machine C produces half of the total output, it produces a much smaller fraction of the defective items. Hence the knowledge that the item selected was defective enables us to replace the prior probability P(XC) = 1/2 by the smaller posterior probability P(XC\u2009|\u2009Y) = 5/24.\n\n\n== Interpretations ==\n\nThe interpretation of Bayes' rule depends on the interpretation of probability ascribed to the terms. The two predominant interpretations are described below. Figure 2 shows a geometric visualization. \n\n\n=== Bayesian interpretation ===\nIn the Bayesian (or epistemological) interpretation, probability measures a \"degree of belief\". Bayes' theorem links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief will probably rise or fall, but might even remain the same, depending on the results.  For proposition A and evidence B,\n\nP\u2009(A), the prior, is the initial degree of belief in A.\nP\u2009(A\u2009|\u2009B), the posterior, is the degree of belief after incorporating news that B is true.\nthe quotient P(B\u2009|\u2009A)/P(B) represents the support B provides for A.For more on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference.\n\n\n=== Frequentist interpretation ===\n\nIn the frequentist interpretation, probability measures a \"proportion of outcomes\". For example, suppose an experiment is performed many times. P(A) is the proportion of outcomes with property A (the prior)  and P(B) is the proportion with property B. P(B\u2009|\u2009A) is the proportion of outcomes with property B out of outcomes with property A, and P(A\u2009|\u2009B) is the proportion of those with A out of those with B (the posterior).\nThe role of Bayes' theorem is best visualized with tree diagrams such as Figure 3. The two diagrams partition the same outcomes by A and B in opposite orders, to obtain the inverse probabilities. Bayes' theorem links the different partitionings.\n\n\n==== Example ====\n\nAn entomologist spots what might, due to the pattern on its back, be a rare subspecies of beetle. A full 98% of the members of the rare subspecies have the pattern, so  P(Pattern | Rare) = 98%.  Only 5% of members of the common subspecies have the pattern. The rare subspecies is 0.1% of the total population. How likely is the beetle having the pattern to be rare: what is P(Rare | Pattern)?\nFrom the extended form of Bayes' theorem (since any beetle is either rare or common),\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                \n                  Rare\n                \n                \u2223\n                \n                  Pattern\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Pattern\n                      \n                      \u2223\n                      \n                        Rare\n                      \n                      )\n                      P\n                      (\n                      \n                        Rare\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Pattern\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \n                        Pattern\n                      \n                      \u2223\n                      \n                        Rare\n                      \n                      )\n                      P\n                      (\n                      \n                        Rare\n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      \n                        Pattern\n                      \n                      \u2223\n                      \n                        Rare\n                      \n                      )\n                      P\n                      (\n                      \n                        Rare\n                      \n                      )\n                      +\n                      P\n                      (\n                      \n                        Pattern\n                      \n                      \u2223\n                      \n                        Common\n                      \n                      )\n                      P\n                      (\n                      \n                        Common\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.98\n                      \u00d7\n                      0.001\n                    \n                    \n                      0.98\n                      \u00d7\n                      0.001\n                      +\n                      0.05\n                      \u00d7\n                      0.999\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                \u2248\n                1.9\n                %\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P({\\text{Rare}}\\mid {\\text{Pattern}})&={\\frac {P({\\text{Pattern}}\\mid {\\text{Rare}})P({\\text{Rare}})}{P({\\text{Pattern}})}}\\\\[8pt]&={\\frac {P({\\text{Pattern}}\\mid {\\text{Rare}})P({\\text{Rare}})}{P({\\text{Pattern}}\\mid {\\text{Rare}})P({\\text{Rare}})+P({\\text{Pattern}}\\mid {\\text{Common}})P({\\text{Common}})}}\\\\[8pt]&={\\frac {0.98\\times 0.001}{0.98\\times 0.001+0.05\\times 0.999}}\\\\[8pt]&\\approx 1.9\\%\\end{aligned}}}\n  \n\n\n== Forms ==\n\n\n=== Events ===\n\n\n==== Simple form ====\nFor events A and B, provided that P(B) \u2260 0,\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)P(A)}{P(B)}}.}\n  In many applications, for instance in Bayesian inference, the event B is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events A. In such a situation the denominator of the last expression, the probability of the given evidence B, is fixed; what we want to vary is A. Bayes' theorem then shows that the posterior probabilities are proportional to the numerator, so the last equation becomes:\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        \u221d\n        P\n        (\n        A\n        )\n        \u22c5\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        .\n      \n    \n    {\\displaystyle P(A|B)\\propto P(A)\\cdot P(B|A).}\n  In words, the posterior is proportional to the prior times the likelihood.If events A1, A2, ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together,    we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event A, the event A itself and its complement \u00acA are exclusive and exhaustive. Denoting the constant of proportionality by c we have\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        c\n        \u22c5\n        P\n        (\n        A\n        )\n        \u22c5\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        \n           and \n        \n        P\n        (\n        \u00ac\n        A\n        \n          |\n        \n        B\n        )\n        =\n        c\n        \u22c5\n        P\n        (\n        \u00ac\n        A\n        )\n        \u22c5\n        P\n        (\n        B\n        \n          |\n        \n        \u00ac\n        A\n        )\n        .\n      \n    \n    {\\displaystyle P(A|B)=c\\cdot P(A)\\cdot P(B|A){\\text{ and }}P(\\neg A|B)=c\\cdot P(\\neg A)\\cdot P(B|\\neg A).}\n  Adding these two formulas we deduce that\n\n  \n    \n      \n        1\n        =\n        c\n        \u22c5\n        (\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        \u22c5\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        \n          |\n        \n        \u00ac\n        A\n        )\n        \u22c5\n        P\n        (\n        \u00ac\n        A\n        )\n        )\n        ,\n      \n    \n    {\\displaystyle 1=c\\cdot (P(B|A)\\cdot P(A)+P(B|\\neg A)\\cdot P(\\neg A)),}\n  or\n\n  \n    \n      \n        c\n        =\n        \n          \n            1\n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              \u22c5\n              P\n              (\n              A\n              )\n              +\n              P\n              (\n              B\n              \n                |\n              \n              \u00ac\n              A\n              )\n              \u22c5\n              P\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n        =\n        \n          \n            1\n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle c={\\frac {1}{P(B|A)\\cdot P(A)+P(B|\\neg A)\\cdot P(\\neg A)}}={\\frac {1}{P(B)}}.}\n  \n\n\n==== Alternative form ====\nAnother form of Bayes' theorem for two competing statements or hypotheses is:\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n              +\n              P\n              (\n              B\n              \n                |\n              \n              \u00ac\n              A\n              )\n              P\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)}}.}\n  For an epistemological interpretation:\nFor proposition A and evidence or background B,\n\n  \n    \n      \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A)}\n   is the prior probability,   the initial degree of belief in A.\n\n  \n    \n      \n        P\n        (\n        \u00ac\n        A\n        )\n      \n    \n    {\\displaystyle P(\\neg A)}\n   is the corresponding initial degree of belief in  not-A, that  A is false,  where \n  \n    \n      \n        P\n        (\n        \u00ac\n        A\n        )\n        =\n        1\n        \u2212\n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(\\neg A)=1-P(A)}\n  \n\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n      \n    \n    {\\displaystyle P(B|A)}\n   is the conditional probability or likelihood,  the degree of belief in B  given that proposition A is true.\n\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        \u00ac\n        A\n        )\n      \n    \n    {\\displaystyle P(B|\\neg A)}\n   is the conditional probability or likelihood,   the degree of belief in B  given that proposition A is false.\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n      \n    \n    {\\displaystyle P(A|B)}\n   is the posterior probability,   the probability of A after taking into account B.\n\n\n==== Extended form ====\nOften, for some partition {Aj} of the sample space, the event space is given in terms of P(Aj) and P(B | Aj). It is then useful to compute P(B) using the law of total probability:\n\n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        \n          \n            \u2211\n            \n              j\n            \n          \n          P\n          (\n          B\n          \n            |\n          \n          \n            A\n            \n              j\n            \n          \n          )\n          P\n          (\n          \n            A\n            \n              j\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle P(B)={\\sum _{j}P(B|A_{j})P(A_{j})},}\n  \n\n  \n    \n      \n        \u21d2\n        P\n        (\n        \n          A\n          \n            i\n          \n        \n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  i\n                \n              \n              )\n              P\n              (\n              \n                A\n                \n                  i\n                \n              \n              )\n            \n            \n              \n                \u2211\n                \n                  j\n                \n              \n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  j\n                \n              \n              )\n              P\n              (\n              \n                A\n                \n                  j\n                \n              \n              )\n            \n          \n        \n        \u22c5\n      \n    \n    {\\displaystyle \\Rightarrow P(A_{i}|B)={\\frac {P(B|A_{i})P(A_{i})}{\\sum \\limits _{j}P(B|A_{j})P(A_{j})}}\\cdot }\n  In the special case where A is a binary variable:\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n              +\n              P\n              (\n              B\n              \n                |\n              \n              \u00ac\n              A\n              )\n              P\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n        \u22c5\n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)}}\\cdot }\n  \n\n\n=== Random variables ===\n\nConsider a sample space \u03a9 generated by two random variables X and Y. In principle, Bayes' theorem applies to the events A = {X = x} and B = {Y = y}.\n\n  \n    \n      \n        P\n        (\n        X\n        \n          =\n        \n        x\n        \n          |\n        \n        Y\n        \n          =\n        \n        y\n        )\n        =\n        \n          \n            \n              P\n              (\n              Y\n              \n                =\n              \n              y\n              \n                |\n              \n              X\n              \n                =\n              \n              x\n              )\n              P\n              (\n              X\n              \n                =\n              \n              x\n              )\n            \n            \n              P\n              (\n              Y\n              \n                =\n              \n              y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(X{=}x|Y{=}y)={\\frac {P(Y{=}y|X{=}x)P(X{=}x)}{P(Y{=}y)}}}\n  However, terms become 0 at points where either variable has finite probability density. To remain useful, Bayes' theorem must be formulated in terms of the relevant densities (see Derivation).\n\n\n==== Simple form ====\nIf X is continuous and Y is discrete,\n\n  \n    \n      \n        \n          f\n          \n            X\n            \n              |\n            \n            Y\n            \n              =\n            \n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              P\n              (\n              Y\n              \n                =\n              \n              y\n              \n                |\n              \n              X\n              \n                =\n              \n              x\n              )\n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n            \n              P\n              (\n              Y\n              \n                =\n              \n              y\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle f_{X|Y{=}y}(x)={\\frac {P(Y{=}y|X{=}x)f_{X}(x)}{P(Y{=}y)}}}\n  where each \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is a density function.\nIf X is discrete and Y is continuous,\n\n  \n    \n      \n        P\n        (\n        X\n        \n          =\n        \n        x\n        \n          |\n        \n        Y\n        \n          =\n        \n        y\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  Y\n                  \n                    |\n                  \n                  X\n                  \n                    =\n                  \n                  x\n                \n              \n              (\n              y\n              )\n              P\n              (\n              X\n              \n                =\n              \n              x\n              )\n            \n            \n              \n                f\n                \n                  Y\n                \n              \n              (\n              y\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(X{=}x|Y{=}y)={\\frac {f_{Y|X{=}x}(y)P(X{=}x)}{f_{Y}(y)}}.}\n  If both X and Y are continuous,\n\n  \n    \n      \n        \n          f\n          \n            X\n            \n              |\n            \n            Y\n            \n              =\n            \n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  Y\n                  \n                    |\n                  \n                  X\n                  \n                    =\n                  \n                  x\n                \n              \n              (\n              y\n              )\n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n            \n              \n                f\n                \n                  Y\n                \n              \n              (\n              y\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle f_{X|Y{=}y}(x)={\\frac {f_{Y|X{=}x}(y)f_{X}(x)}{f_{Y}(y)}}.}\n  \n\n\n==== Extended form ====\n\nA continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For fY(y), this becomes an integral:\n\n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n        =\n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          f\n          \n            Y\n            \n              |\n            \n            X\n            =\n            \u03be\n          \n        \n        (\n        y\n        )\n        \n          f\n          \n            X\n          \n        \n        (\n        \u03be\n        )\n        \n        d\n        \u03be\n        .\n      \n    \n    {\\displaystyle f_{Y}(y)=\\int _{-\\infty }^{\\infty }f_{Y|X=\\xi }(y)f_{X}(\\xi )\\,d\\xi .}\n  \n\n\n=== Bayes' rule in odds form ===\nBayes' theorem in odds form is:\n\n  \n    \n      \n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n        =\n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        )\n        \u22c5\n        \u039b\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle O(A_{1}:A_{2}\\mid B)=O(A_{1}:A_{2})\\cdot \\Lambda (A_{1}:A_{2}\\mid B)}\n  where\n\n  \n    \n      \n        \u039b\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2223\n              \n                A\n                \n                  1\n                \n              \n              )\n            \n            \n              P\n              (\n              B\n              \u2223\n              \n                A\n                \n                  2\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\Lambda (A_{1}:A_{2}\\mid B)={\\frac {P(B\\mid A_{1})}{P(B\\mid A_{2})}}}\n  is called the Bayes factor or likelihood ratio.  The odds between two events is simply the ratio of the probabilities of the two events. Thus\n\n  \n    \n      \n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            \n              P\n              (\n              \n                A\n                \n                  1\n                \n              \n              )\n            \n            \n              P\n              (\n              \n                A\n                \n                  2\n                \n              \n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle O(A_{1}:A_{2})={\\frac {P(A_{1})}{P(A_{2})}},}\n  \n  \n    \n      \n        O\n        (\n        \n          A\n          \n            1\n          \n        \n        :\n        \n          A\n          \n            2\n          \n        \n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              \n                A\n                \n                  1\n                \n              \n              \u2223\n              B\n              )\n            \n            \n              P\n              (\n              \n                A\n                \n                  2\n                \n              \n              \u2223\n              B\n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle O(A_{1}:A_{2}\\mid B)={\\frac {P(A_{1}\\mid B)}{P(A_{2}\\mid B)}},}\n  Thus,  the rule says that the posterior odds are the prior odds times the Bayes factor, or in other words,  the posterior is proportional to the prior times the likelihood.\nIn the special case that \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        =\n        A\n      \n    \n    {\\displaystyle A_{1}=A}\n   and \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n        =\n        \u00ac\n        A\n      \n    \n    {\\displaystyle A_{2}=\\neg A}\n  , one writes \n  \n    \n      \n        O\n        (\n        A\n        )\n        =\n        O\n        (\n        A\n        :\n        \u00ac\n        A\n        )\n        =\n        P\n        (\n        A\n        )\n        \n          /\n        \n        (\n        1\n        \u2212\n        P\n        (\n        A\n        )\n        )\n      \n    \n    {\\displaystyle O(A)=O(A:\\neg A)=P(A)/(1-P(A))}\n  , and uses a similar abbreviation for the Bayes factor and for the conditional odds. The odds on \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is by definition the odds for and against \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . Bayes' rule can then be written in the abbreviated form\n\n  \n    \n      \n        O\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        O\n        (\n        A\n        )\n        \u22c5\n        \u039b\n        (\n        A\n        \u2223\n        B\n        )\n        ,\n      \n    \n    {\\displaystyle O(A\\mid B)=O(A)\\cdot \\Lambda (A\\mid B),}\n  or, in words, the posterior odds on \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   equals the prior odds on \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   times the likelihood ratio for \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given information \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  . In short,  posterior odds equals prior odds times likelihood ratio.\nFor example, if a medical test has a sensitivity of 90% and a specificity of 91%, then the positive Bayes factor is \n  \n    \n      \n        \n          \u039b\n          \n            +\n          \n        \n        =\n        P\n        (\n        \n          True Positive\n        \n        )\n        \n          /\n        \n        P\n        (\n        \n          False Positive\n        \n        )\n        =\n        90\n        %\n        \n          /\n        \n        (\n        100\n        %\n        \u2212\n        91\n        %\n        )\n        =\n        10\n      \n    \n    {\\displaystyle \\Lambda _{+}=P({\\text{True Positive}})/P({\\text{False Positive}})=90\\%/(100\\%-91\\%)=10}\n  . Now, if the prevalence of this disease is 9.09%, and if we take that as the prior probability, then the prior odds is about 1:10. So after receiving a positive test result, the posterior odds of actually having the disease becomes 1:1; In other words, the posterior probability of actually having the disease is 50%. If a second test is performed in serial testing, and that also turns out to be positive, then the posterior odds of actually having the disease becomes 10:1, which means a posterior probability of about 90.91%. The negative Bayes factor can be calculated to be 91%/(100%-90%)=9.1, so if the second test turns out to be negative, then the posterior odds of actually having the disease is 1:9.1, which means a posterior probability of about 9.9%.\nThe example above can also be understood with more solid numbers: Assume the patient taking the test is from a group of 1000 people, where 91 of them actually have the disease (prevalence of 9.1%). If all these 1000 people take the medical test, 82 of those with the disease will get a true positive result (sensitivity of 90.1%), 9 of those with the disease will get a false negative result (false negative rate of 9.9%), 827 of those without the disease will get a true negative result (specificity of 91.0%), and 82 of those without the disease will get a false positive result (false positive rate of 9.0%). Before taking any test, the patient's odds for having the disease is 91:909. After receiving a positive result, the patient's odds for having the disease is\n\n  \n    \n      \n        \n          \n            91\n            909\n          \n        \n        \u00d7\n        \n          \n            \n              90.1\n              %\n            \n            \n              9.0\n              %\n            \n          \n        \n        =\n        \n          \n            \n              91\n              \u00d7\n              90.1\n              %\n            \n            \n              909\n              \u00d7\n              9.0\n              %\n            \n          \n        \n        =\n        1\n        :\n        1\n      \n    \n    {\\displaystyle {\\frac {91}{909}}\\times {\\frac {90.1\\%}{9.0\\%}}={\\frac {91\\times 90.1\\%}{909\\times 9.0\\%}}=1:1}\n  which is consistent with the fact that there are 82 true positives and 82 false positives in the group of 1000 people.\n\n\n== Correspondence to other mathematical frameworks ==\n\n\n=== Propositional logic ===\nUsing \n  \n    \n      \n        P\n        (\n        \u00ac\n        B\n        \u2223\n        A\n        )\n        =\n        1\n        \u2212\n        P\n        (\n        B\n        \u2223\n        A\n        )\n      \n    \n    {\\displaystyle P(\\neg B\\mid A)=1-P(B\\mid A)}\n   twice, one may use Bayes' theorem to also express \n  \n    \n      \n        P\n        (\n        \u00ac\n        B\n        \u2223\n        \u00ac\n        A\n        )\n      \n    \n    {\\displaystyle P(\\neg B\\mid \\neg A)}\n   in terms of \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   and without negations:\n\n  \n    \n      \n        P\n        (\n        \u00ac\n        B\n        \u2223\n        \u00ac\n        A\n        )\n        =\n        1\n        \u2212\n        \n          (\n          \n            1\n            \u2212\n            P\n            (\n            A\n            \u2223\n            B\n            )\n          \n          )\n        \n        \n          \n            \n              P\n              (\n              B\n              )\n            \n            \n              P\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(\\neg B\\mid \\neg A)=1-\\left(1-P(A\\mid B)\\right){\\frac {P(B)}{P(\\neg A)}}}\n  ,when \n  \n    \n      \n        P\n        (\n        \u00ac\n        A\n        )\n        =\n        1\n        \u2212\n        P\n        (\n        A\n        )\n        \u2260\n        0\n      \n    \n    {\\displaystyle P(\\neg A)=1-P(A)\\neq 0}\n  . From this we can read off the inference\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        1\n        \n        \u27f9\n        \n        P\n        (\n        \u00ac\n        B\n        \u2223\n        \u00ac\n        A\n        )\n        =\n        1\n      \n    \n    {\\displaystyle P(A\\mid B)=1\\implies P(\\neg B\\mid \\neg A)=1}\n  .In words: If certainly \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   implies \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , we infer that certainly \n  \n    \n      \n        \u00ac\n        A\n      \n    \n    {\\displaystyle \\neg A}\n   implies \n  \n    \n      \n        \u00ac\n        B\n      \n    \n    {\\displaystyle \\neg B}\n  . Where \n  \n    \n      \n        P\n        (\n        B\n        )\n        \u2260\n        0\n      \n    \n    {\\displaystyle P(B)\\neq 0}\n  , the two implications being certain are equivalent statements.\nIn the probability formulas, the conditional probability \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   generalizes the logical implication \n  \n    \n      \n        B\n        \n        \u27f9\n        \n        A\n      \n    \n    {\\displaystyle B\\implies A}\n  , where now beyond assigning true or false, we assign probability values to statements. The assertion of \n  \n    \n      \n        B\n        \n        \u27f9\n        \n        A\n      \n    \n    {\\displaystyle B\\implies A}\n   is captured by certainty of the conditional, the assertion of \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        1\n      \n    \n    {\\displaystyle P(A\\mid B)=1}\n  . Relating the directions of implication, Bayes' theorem represents a generalization of the contraposition law, which in classical propositional logic can be expressed as:\n\n  \n    \n      \n        (\n        B\n        \n        \u27f9\n        \n        A\n        )\n        \n        \u27fa\n        \n        (\n        \u00ac\n        A\n        \n        \u27f9\n        \n        \u00ac\n        B\n        )\n      \n    \n    {\\displaystyle (B\\implies A)\\iff (\\neg A\\implies \\neg B)}\n  .Note that in this relation between implications, the positions of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   resp. \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   get flipped.\nThe corresponding formula in terms of probability calculus is Bayes' theorem, which in its expanded form involving the prior probability/base rate \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   of only \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , is expressed as:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        P\n        (\n        B\n        \u2223\n        A\n        )\n        \n          \n            \n              a\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              \u2223\n              A\n              )\n              \n              a\n              (\n              A\n              )\n              +\n              P\n              (\n              B\n              \u2223\n              \u00ac\n              A\n              )\n              \n              a\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)=P(B\\mid A){\\frac {a(A)}{P(B\\mid A)\\,a(A)+P(B\\mid \\neg A)\\,a(\\neg A)}}}\n  .\n\n\n=== Subjective logic ===\nBayes' theorem represents a special case of deriving inverted conditional opinions in subjective logic expressed as:\n\n  \n    \n      \n        (\n        \n          \u03c9\n          \n            A\n            \n              \n                \n                  \n                    |\n                  \n                  ~\n                \n              \n            \n            B\n          \n          \n            S\n          \n        \n        ,\n        \n          \u03c9\n          \n            A\n            \n              \n                \n                  \n                    |\n                  \n                  ~\n                \n              \n            \n            \u00ac\n            B\n          \n          \n            S\n          \n        \n        )\n        =\n        (\n        \n          \u03c9\n          \n            B\n            \u2223\n            A\n          \n          \n            S\n          \n        \n        ,\n        \n          \u03c9\n          \n            B\n            \u2223\n            \u00ac\n            A\n          \n          \n            S\n          \n        \n        )\n        \n          \n            \n              \u03d5\n              ~\n            \n          \n        \n        \n          a\n          \n            A\n          \n        \n        ,\n      \n    \n    {\\displaystyle (\\omega _{A{\\tilde {|}}B}^{S},\\omega _{A{\\tilde {|}}\\lnot B}^{S})=(\\omega _{B\\mid A}^{S},\\omega _{B\\mid \\lnot A}^{S}){\\widetilde {\\phi }}a_{A},}\n  where \n  \n    \n      \n        \n          \n            \n              \u03d5\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widetilde {\\phi }}}\n   denotes the operator for inverting conditional opinions. The argument \n  \n    \n      \n        (\n        \n          \u03c9\n          \n            B\n            \u2223\n            A\n          \n          \n            S\n          \n        \n        ,\n        \n          \u03c9\n          \n            B\n            \u2223\n            \u00ac\n            A\n          \n          \n            S\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\omega _{B\\mid A}^{S},\\omega _{B\\mid \\lnot A}^{S})}\n   denotes a pair of binomial conditional opinions given by source \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  , and the argument \n  \n    \n      \n        \n          a\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle a_{A}}\n   denotes the prior probability (aka. the base rate) of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . The pair of derivative inverted conditional opinions is denoted \n  \n    \n      \n        (\n        \n          \u03c9\n          \n            A\n            \n              \n                \n                  \n                    |\n                  \n                  ~\n                \n              \n            \n            B\n          \n          \n            S\n          \n        \n        ,\n        \n          \u03c9\n          \n            A\n            \n              \n                \n                  \n                    |\n                  \n                  ~\n                \n              \n            \n            \u00ac\n            B\n          \n          \n            S\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\omega _{A{\\tilde {|}}B}^{S},\\omega _{A{\\tilde {|}}\\lnot B}^{S})}\n  . The conditional opinion \n  \n    \n      \n        \n          \u03c9\n          \n            A\n            \u2223\n            B\n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle \\omega _{A\\mid B}^{S}}\n   generalizes the probabilistic conditional \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  , i.e. in addition to assigning a probability the source \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   can assign any subjective opinion to the conditional statement \n  \n    \n      \n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle (A\\mid B)}\n  . A binomial subjective opinion \n  \n    \n      \n        \n          \u03c9\n          \n            A\n          \n          \n            S\n          \n        \n      \n    \n    {\\displaystyle \\omega _{A}^{S}}\n   is the belief in the truth of statement \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   with degrees of epistemic uncertainty, as expressed by source \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  . Every subjective opinion has a corresponding projected probability \n  \n    \n      \n        P\n        (\n        \n          \u03c9\n          \n            A\n          \n          \n            S\n          \n        \n        )\n      \n    \n    {\\displaystyle P(\\omega _{A}^{S})}\n  . The application of Bayes' theorem to projected probabilities of opinions is a homomorphism, meaning that Bayes' theorem can be expressed in terms of projected probabilities of opinions:\n\n  \n    \n      \n        P\n        (\n        \n          \u03c9\n          \n            A\n            \n              \n                \n                  \n                    |\n                  \n                  ~\n                \n              \n            \n            B\n          \n          \n            S\n          \n        \n        )\n        =\n        \n          \n            \n              P\n              (\n              \n                \u03c9\n                \n                  B\n                  \u2223\n                  A\n                \n                \n                  S\n                \n              \n              )\n              a\n              (\n              A\n              )\n            \n            \n              P\n              (\n              \n                \u03c9\n                \n                  B\n                  \u2223\n                  A\n                \n                \n                  S\n                \n              \n              )\n              a\n              (\n              A\n              )\n              +\n              P\n              (\n              \n                \u03c9\n                \n                  B\n                  \u2223\n                  \u00ac\n                  A\n                \n                \n                  S\n                \n              \n              )\n              a\n              (\n              \u00ac\n              A\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(\\omega _{A{\\tilde {|}}B}^{S})={\\frac {P(\\omega _{B\\mid A}^{S})a(A)}{P(\\omega _{B\\mid A}^{S})a(A)+P(\\omega _{B\\mid \\lnot A}^{S})a(\\lnot A)}}.}\n  Hence, the subjective Bayes' theorem represents a generalization of Bayes' theorem.\n\n\n== Generalizations ==\n\n\n=== Conditioned version ===\nA conditioned version of the Bayes' theorem results from the addition of a third event \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   on which all probabilities are conditioned:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        \u2229\n        C\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2223\n              A\n              \u2229\n              C\n              )\n              \n              P\n              (\n              A\n              \u2223\n              C\n              )\n            \n            \n              P\n              (\n              B\n              \u2223\n              C\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B\\cap C)={\\frac {P(B\\mid A\\cap C)\\,P(A\\mid C)}{P(B\\mid C)}}}\n  \n\n\n==== Derivation ====\nUsing the chain rule\n\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        \u2229\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        B\n        \u2229\n        C\n        )\n        \n        P\n        (\n        B\n        \u2223\n        C\n        )\n        \n        P\n        (\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\cap B\\cap C)=P(A\\mid B\\cap C)\\,P(B\\mid C)\\,P(C)}\n  And, on the other hand\n\n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        \u2229\n        C\n        )\n        =\n        P\n        (\n        B\n        \u2229\n        A\n        \u2229\n        C\n        )\n        =\n        P\n        (\n        B\n        \u2223\n        A\n        \u2229\n        C\n        )\n        \n        P\n        (\n        A\n        \u2223\n        C\n        )\n        \n        P\n        (\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\cap B\\cap C)=P(B\\cap A\\cap C)=P(B\\mid A\\cap C)\\,P(A\\mid C)\\,P(C)}\n  The desired result is obtained by identifying both expressions and solving for \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        \u2229\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B\\cap C)}\n  .\n\n\n=== Bayes' rule with 3 events ===\nIn the case of 3 events \u2014 A, B, and C \u2014 it can be shown that:\n\n\n== Use in genetics ==\nIn genetics, Bayes' theorem can be used to calculate the probability of an individual having a specific genotype. Many people seek to approximate their chances of being affected by a genetic disease or their likelihood of being a carrier for a recessive gene of interest. A Bayesian analysis can be done based on family history or genetic testing, in order to predict whether an individual will develop a disease or pass one on to their children. Genetic testing and prediction is a common practice among couples who plan to have children but are concerned that they may both be carriers for a disease, especially within communities with low genetic variance.The first step in Bayesian analysis for genetics is to propose mutually exclusive hypotheses: for a specific allele, an individual either is or is not a carrier. Next, four probabilities are calculated: Prior Probability (the likelihood of each hypothesis considering information such as family history or predictions based on Mendelian Inheritance), Conditional Probability (of a certain outcome), Joint Probability (product of the first two), and Posterior Probability (a weighted product calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities). This type of analysis can be done based purely on family history of a condition or in concert with genetic testing.\n\n\n=== Using pedigree to calculate probabilities ===\nExample of a Bayesian analysis table for a female individual's risk for a disease based on the knowledge that the disease is present in her siblings but not in her parents or any of her four children. Based solely on the status of the subject's siblings and parents, she is equally likely to be a carrier as to be a non-carrier (this likelihood is denoted by the Prior Hypothesis). However, the probability that the subject's four sons would all be unaffected is 1/16 (1\u20442\u00b71\u20442\u00b71\u20442\u00b71\u20442) if she is a carrier, about 1 if she is a non-carrier (this is the Conditional Probability). The Joint Probability reconciles these two predictions by multiplying them together. The last line (the Posterior Probability) is calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities.\n\n\n=== Using genetic test results ===\nParental genetic testing can detect around 90% of known disease alleles in parents that can lead to carrier or affected status in their child. Cystic fibrosis is a heritable disease caused by an autosomal recessive mutation on the CFTR gene, located on the q arm of chromosome 7.Bayesian analysis of a female patient with a family history of cystic fibrosis (CF), who has tested negative for CF, demonstrating how this method was used to determine her risk of having a child born with CF:\nBecause the patient is unaffected, she is either homozygous for the wild-type allele, or heterozygous. To establish prior probabilities, a Punnett square is used, based on the knowledge that neither parent was affected by the disease but both could have been carriers:\n\nGiven that the patient is unaffected, there are only three possibilities. Within these three, there are two scenarios in which the patient carries the mutant allele. Thus the prior probabilities are 2\u20443 and 1\u20443.\nNext, the patient undergoes genetic testing and tests negative for cystic fibrosis. This test has a 90% detection rate, so the conditional probabilities of a negative test are 1/10 and 1.  Finally, the joint and posterior probabilities are calculated as before.\n\nAfter carrying out the same analysis on the patient's male partner (with a negative test result), the chances of their child being affected is equal to the product of the parents' respective posterior probabilities for being carriers times the chances that two carriers will produce an affected offspring (1\u20444).\n\n\n=== Genetic testing done in parallel with other risk factor identification. ===\nBayesian analysis can be done using phenotypic information associated with a genetic condition, and when combined with genetic testing this analysis becomes much more complicated. Cystic Fibrosis, for example, can be identified in a fetus through an ultrasound looking for an echogenic bowel, meaning one that appears brighter than normal on a scan2. This is not a foolproof test, as an echogenic bowel can be present in a  perfectly healthy fetus. Parental genetic testing is very influential in this case, where a phenotypic facet can be overly influential in probability calculation. In the case of a fetus with an echogenic bowel, with a mother who has been tested and is known to be a CF carrier, the posterior probability that the fetus actually has the disease is very high (0.64). However, once the father has tested negative for CF, the posterior probability drops significantly (to 0.16).Risk factor calculation is a powerful tool in genetic counseling and reproductive planning, but it cannot be treated as the only important factor to consider. As above, incomplete testing can yield falsely high probability of carrier status, and testing can be financially inaccessible or unfeasible when a parent is not present.\n\n\n== See also ==\nBayesian epistemology\nInductive probability\nQuantum Bayesianism\nWhy Most Published Research Findings Are False, a 2005 essay in metascience by John Ioannidis\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\nThis article incorporates text from a publication now in the public domain: Mitchell, John Malcolm (1911). \"Price, Richard\".  In Chisholm, Hugh (ed.). Encyclop\u00e6dia Britannica. Vol. 22 (11th ed.). Cambridge University Press. pp. 314\u2013315.\n\n\n== Further reading ==\nGrunau, Hans-Christoph (24 January 2014). \"Preface Issue 3/4-2013\". Jahresbericht der Deutschen Mathematiker-Vereinigung. 115 (3\u20134): 127\u2013128. doi:10.1365/s13291-013-0077-z.\nGelman, A, Carlin, JB, Stern, HS, and Rubin, DB (2003), \"Bayesian Data Analysis,\" Second Edition, CRC Press.\nGrinstead, CM and Snell, JL (1997), \"Introduction to Probability (2nd edition),\" American Mathematical Society (free pdf available) [1].\n\"Bayes formula\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nMcGrayne, SB (2011). The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines & Emerged Triumphant from Two Centuries of Controversy. Yale University Press. ISBN 978-0-300-18822-6.\nLaplace, Pierre Simon (1986). \"Memoir on the Probability of the Causes of Events\". Statistical Science. 1 (3): 364\u2013378. doi:10.1214/ss/1177013621. JSTOR 2245476.\nLee, Peter M (2012), \"Bayesian Statistics: An Introduction,\" 4th edition. Wiley. ISBN 978-1-118-33257-3.\nPuga JL, Krzywinski M, Altman N (31 March 2015). \"Bayes' theorem\". Nature Methods. 12 (4): 277\u2013278. doi:10.1038/nmeth.3335. PMID 26005726.\nRosenthal, Jeffrey S (2005), \"Struck by Lightning: The Curious World of Probabilities\". HarperCollins. (Granta, 2008. ISBN 9781862079960).\nStigler, Stephen M. (August 1986). \"Laplace's 1774 Memoir on Inverse Probability\". Statistical Science. 1 (3): 359\u2013363. doi:10.1214/ss/1177013620.\nStone, JV (2013), download chapter 1 of \"Bayes' Rule: A Tutorial Introduction to Bayesian Analysis\", Sebtel Press, England.\nBayesian Reasoning for Intelligent People, An introduction and tutorial to the use of Bayes' theorem in statistics and cognitive science.\nMorris, Dan (2016), Read first 6 chapters for free of \"Bayes' Theorem Examples: A Visual Introduction For Beginners\" Blue Windmill ISBN 978-1549761744. A short tutorial on how to understand problem scenarios and find P(B), P(A), and P(B|A).\n\n\n== External links ==\nVisual explanation of Bayes using trees on YouTube\nBayes' frequentist interpretation explained visually on YouTube\nEarliest Known Uses of Some of the Words of Mathematics (B). Contains origins of \"Bayesian\", \"Bayes' Theorem\", \"Bayes Estimate/Risk/Solution\", \"Empirical Bayes\", and \"Bayes Factor\".\nA tutorial on probability and Bayes' theorem devised for Oxford University psychology students\nAn Intuitive Explanation of Bayes' Theorem by Eliezer S. Yudkowsky\nBayesian Clinical Diagnostic Model", "Statistical population": "In statistics, a population is a set of similar items or events which is of interest for some question or experiment.  A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all possible hands in a game of poker). A common aim of statistical analysis is to produce information about some chosen population.In statistical inference, a subset of the population (a statistical sample) is chosen to represent the population in a statistical analysis. Moreover, the statistical sample must be unbiased and accurately model the population (every unit of the population has an equal chance of selection). The ratio of the size of this statistical sample to the size of the population is called a sampling fraction. It is then possible to estimate the population parameters using the appropriate sample statistics.\n\n\n== Mean ==\nThe population mean, or population expected value, is a measure of the central tendency either of a probability distribution or of a random variable characterized by that distribution. In a discrete probability distribution of a random variable X, the mean is equal to the sum over every possible value weighted by the probability of that value; that is, it is computed by taking the product of each possible value x of X and its probability p(x), and then adding all these products together, giving \n  \n    \n      \n        \u03bc\n        =\n        \u2211\n        x\n        p\n        (\n        x\n        )\n        .\n        .\n        .\n        .\n      \n    \n    {\\displaystyle \\mu =\\sum xp(x)....}\n  . An analogous formula applies to the case of a continuous probability distribution. Not every probability distribution has a defined mean (see the Cauchy distribution for an example). Moreover, the mean can be infinite for some distributions.\nFor a finite population, the population mean of a property is equal to the arithmetic mean of the given property, while considering every member of the population. For example, the population mean height is equal to the sum of the heights of every individual\u2014divided by the total number of individuals. The sample mean may differ from the population mean, especially for small samples. The law of large numbers states that the larger the size of the sample, the more likely it is that the sample mean will be close to the population mean.\n\n\n== Sub population ==\nA subset of a population that shares one or more additional properties is called a sub population. For example, if the population is all Egyptian people, a sub population is all Egyptian males; if the population is all pharmacies in the world, a sub population is all pharmacies in Egypt. By contrast, a sample is a subset of a population that is not chosen to share any additional property.\nDescriptive statistics may yield different results for different sub populations. For instance, a particular medicine may have different effects on different sub populations, and these effects may be obscured or dismissed if such special sub populations are not identified and examined in isolation.\nSimilarly, one can often estimate parameters more accurately if one separates out sub populations: the distribution of heights among people is better modeled by considering men and women as separate sub populations, for instance.\nPopulations consisting of sub populations can be modeled by mixture models, which combine the distributions within sub populations into an overall population distribution. Even if sub populations are well-modeled by given simple models, the overall population may be poorly fit by a given simple model \u2013 poor fit may be evidence for the existence of sub populations. For example, given two equal sub populations, both normally distributed, if they have the same standard deviation but different means, the overall distribution will exhibit low kurtosis relative to a single normal distribution \u2013 the means of the sub populations fall on the shoulders of the overall distribution. If sufficiently separated, these form a bimodal distribution; otherwise, it simply has a wide peak. Further, it will exhibit overdispersion relative to a single normal distribution with the given variation. Alternatively, given two sub populations with the same mean but different standard deviations, the overall population will exhibit high kurtosis, with a sharper peak and heavier tails (and correspondingly shallower shoulders) than a single distribution.\n\n\n== See also ==\nData collection system\nHorvitz\u2013Thompson estimator\nSample (statistics)\nSampling (statistics)\nStratum (statistics)\n\n\n== References ==\n\n\n== External links ==\nStatistical Terms Made Simple", "Central tendency": "In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution.Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s.The most common measures of central tendency are the arithmetic mean, the median, and the mode.  A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote \"the tendency of quantitative data to cluster around some central value.\"The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.\n\n\n== Measures ==\nThe following may be applied to one-dimensional data. Depending on the circumstances, it may be appropriate to transform the data before calculating a central tendency. Examples are squaring the values or taking logarithms. Whether a transformation is appropriate and what it should be, depend heavily on the data being analyzed.\n\nArithmetic mean or simply, mean\nthe sum of all measurements divided by the number of observations in the data set.\nMedian\nthe middle value that separates the higher half from the lower half of the data set. The median and the mode are the only measures of central tendency that can be used for ordinal data, in which values are ranked relative to each other but are not measured absolutely.\nMode\nthe most frequent value in the data set. This is the only central tendency measure that can be used with nominal data, which have purely qualitative category assignments.\nGeneralized mean\nA generalization of the Pythagorean means, specified by an exponent.\nGeometric mean\nthe nth root of the product of the data values, where there are n of these. This measure is valid only for data that are measured absolutely on a strictly positive scale.\nHarmonic mean\nthe reciprocal of the arithmetic mean of the reciprocals of the data values. This measure too is valid only for data that are measured absolutely on a strictly positive scale.\nWeighted arithmetic mean\nan arithmetic mean that incorporates weighting to certain data elements.\nTruncated mean or trimmed mean\nthe arithmetic mean of data values after a certain number or proportion of the highest and lowest data values have been discarded.\nInterquartile mean\na truncated mean based on data within the interquartile range.\nMidrange\nthe arithmetic mean of the maximum and minimum values of a data set.\nMidhinge\nthe arithmetic mean of the first and third quartiles.\nQuasi-arithmetic mean\nA generalization of the generalized mean, specified by a continuous injective function.\nTrimean\nthe weighted arithmetic mean of the median and two quartiles.\nWinsorized mean\nan arithmetic mean in which extreme values are replaced by values closer to the median.Any of the above may be applied to each dimension of multi-dimensional data, but the results may not be invariant to rotations of the multi-dimensional space.\n\nGeometric median\nthe point minimizing the sum of distances to a set of sample points. This is the same as the median when applied to one-dimensional data, but it is not the same as taking the median of each dimension independently.  It is not invariant to different rescaling of the different dimensions.\nQuadratic mean (often known as the root mean square)\nuseful in engineering, but not often used in statistics. This is because it is not a good indicator of the center of the distribution when the distribution includes negative values.\nSimplicial depth\nthe probability that a randomly chosen simplex with vertices from the given distribution will contain the given center\nTukey median\na point with the property that every halfspace containing it also contains many sample points\n\n\n== Solutions to variational problems ==\nSeveral measures of central tendency can be characterized as solving a variational problem, in the sense of the calculus of variations, namely minimizing variation from the center. That is, given a measure of statistical dispersion, one asks for a measure of central tendency that minimizes variation: such that variation from the center is minimal among all choices of center. In a quip, \"dispersion precedes location\". These measures are initially defined in one dimension, but can be generalized to multiple dimensions. This center may or may not be unique. In the sense of Lp spaces, the correspondence is:\n\nThe associated functions are called p-norms: respectively 0-\"norm\", 1-norm, 2-norm, and \u221e-norm. The function corresponding to the L0 space is not a norm, and is thus often referred to in quotes: 0-\"norm\".\nIn equations, for a given (finite) data set X, thought of as a vector x = (x1,\u2026,xn), the dispersion about a point c is the \"distance\" from x to the constant vector c = (c,\u2026,c) in the p-norm (normalized by the number of points n):\n\n  \n    \n      \n        \n          f\n          \n            p\n          \n        \n        (\n        c\n        )\n        =\n        \n          \n            \u2016\n            \n              \n                x\n              \n              \u2212\n              \n                c\n              \n            \n            \u2016\n          \n          \n            p\n          \n        \n        :=\n        \n          \n            (\n          \n        \n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            |\n            \n              \n                x\n                \n                  i\n                \n              \n              \u2212\n              c\n            \n            |\n          \n          \n            p\n          \n        \n        \n          \n            \n              )\n            \n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n      \n    \n    {\\displaystyle f_{p}(c)=\\left\\|\\mathbf {x} -\\mathbf {c} \\right\\|_{p}:={\\bigg (}{\\frac {1}{n}}\\sum _{i=1}^{n}\\left|x_{i}-c\\right|^{p}{\\bigg )}^{1/p}}\n  For p = 0 and p = \u221e these functions are defined by taking limits, respectively as p \u2192 0 and p \u2192 \u221e. For p = 0 the limiting values are 00 = 0 and a0 = 0 or a \u2260 0, so the difference becomes simply equality, so the 0-norm counts the number of unequal points. For p = \u221e the largest number dominates, and thus the \u221e-norm is the maximum difference.\n\n\n=== Uniqueness ===\nThe mean (L2 center) and midrange (L\u221e center) are unique (when they exist), while the median (L1 center) and mode (L0 center) are not in general unique. This can be understood in terms of convexity of the associated functions (coercive functions).\nThe 2-norm and \u221e-norm are strictly convex, and thus (by convex optimization) the minimizer is unique (if it exists), and exists for bounded distributions. Thus standard deviation about the mean is lower than standard deviation about any other point, and the maximum deviation about the midrange is lower than the maximum deviation about any other point.\nThe 1-norm is not strictly convex, whereas strict convexity is needed to ensure uniqueness of the minimizer. Correspondingly, the median (in this sense of minimizing) is not in general unique, and in fact any point between the two central points of a discrete distribution minimizes average absolute deviation.\nThe 0-\"norm\" is not convex (hence not a norm). Correspondingly, the mode is not unique \u2013 for example, in a uniform distribution any point is the mode.\n\n\n=== Clustering ===\nInstead of a single central point, one can ask for multiple points such that the variation from these points is minimized. This leads to cluster analysis, where each point in the data set is clustered with the nearest \"center\". Most commonly, using the 2-norm generalizes the mean to k-means clustering, while using the 1-norm generalizes the (geometric) median to k-medians clustering. Using the 0-norm simply generalizes the mode (most common value) to using the k most common values as centers.\nUnlike the single-center statistics, this multi-center clustering cannot in general be computed in a closed-form expression, and instead must be computed or approximated by an iterative method; one general approach is expectation\u2013maximization algorithms.\n\n\n=== Information geometry ===\nThe notion of a \"center\" as minimizing variation can be generalized in information geometry as a distribution that minimizes divergence (a generalized distance) from a data set. The most common case is maximum likelihood estimation, where the maximum likelihood estimate (MLE) maximizes likelihood (minimizes expected surprisal), which can be interpreted geometrically by using entropy to measure variation: the MLE minimizes cross entropy (equivalently, relative entropy, Kullback\u2013Leibler divergence).\nA simple example of this is for the center of nominal data: instead of using the mode (the only single-valued \"center\"), one often uses the empirical measure (the frequency distribution divided by the sample size) as a \"center\". For example, given binary data, say heads or tails, if a data set consists of 2 heads and 1 tails, then the mode is \"heads\", but the empirical measure is 2/3 heads, 1/3 tails, which minimizes the cross-entropy (total surprisal) from the data set. This perspective is also used in regression analysis, where least squares finds the solution that minimizes the distances from it, and analogously in logistic regression, a maximum likelihood estimate minimizes the surprisal (information distance).\n\n\n== Relationships between the mean, median and mode ==\n\nFor unimodal distributions the following bounds are known and are sharp:\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              \u03b8\n              \u2212\n              \u03bc\n              \n                |\n              \n            \n            \u03c3\n          \n        \n        \u2264\n        \n          \n            3\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\theta -\\mu |}{\\sigma }}\\leq {\\sqrt {3}},}\n  \n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              \u03bd\n              \u2212\n              \u03bc\n              \n                |\n              \n            \n            \u03c3\n          \n        \n        \u2264\n        \n          \n            0.6\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\nu -\\mu |}{\\sigma }}\\leq {\\sqrt {0.6}},}\n  \n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              \u03b8\n              \u2212\n              \u03bd\n              \n                |\n              \n            \n            \u03c3\n          \n        \n        \u2264\n        \n          \n            3\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\theta -\\nu |}{\\sigma }}\\leq {\\sqrt {3}},}\n  where \u03bc is the mean, \u03bd is the median, \u03b8 is the mode, and \u03c3 is the standard deviation.\nFor every distribution,\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              \u03bd\n              \u2212\n              \u03bc\n              \n                |\n              \n            \n            \u03c3\n          \n        \n        \u2264\n        1.\n      \n    \n    {\\displaystyle {\\frac {|\\nu -\\mu |}{\\sigma }}\\leq 1.}\n  \n\n\n== See also ==\nCentral moment\nExpected value\nLocation parameter\nMean\nPopulation mean\nSample mean\n\n\n== Notes ==\n\n\n== References ==", "Median": "In statistics and probability theory, the median is the value separating the higher half from the lower half of a data sample, a population, or a probability distribution. For a data set, it may be thought of as \"the middle\" value. The basic feature of the median in describing data compared to the mean (often simply described as the \"average\") is that it is not skewed by a small proportion of extremely large or small values, and therefore provides a better representation of the center. Median income, for example, may be a better way to describe center of the income distribution because increases in the largest incomes alone have no effect on median. For this reason, the median is of central importance in robust statistics.\n\n\n== Finite data set of numbers ==\nThe median of a finite list of numbers is the \"middle\" number, when those numbers are listed in order from smallest to greatest.\nIf the data set has an odd number of observations, the middle one is selected. For example, the following list of seven numbers,\n\n1, 3, 3, 6, 7, 8, 9has the median of 6, which is the fourth value.\nIf the data set has an even number of observations, there is no distinct middle value and the median is usually defined to be the arithmetic mean of the two middle values. For example, this data set of 8 numbers\n\n1, 2, 3, 4, 5, 6, 8, 9has a median value of 4.5, that is \n  \n    \n      \n        (\n        4\n        +\n        5\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle (4+5)/2}\n  . (In more technical terms, this interprets the median as the fully trimmed mid-range).  \nIn general, with this convention, the median can be defined as follows: For a data set \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   elements, ordered from smallest to greatest,\n\nif \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is odd, \n  \n    \n      \n        \n          m\n          e\n          d\n          i\n          a\n          n\n        \n        (\n        x\n        )\n        =\n        \n          x\n          \n            (\n            n\n            +\n            1\n            )\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {median} (x)=x_{(n+1)/2}}\n  \nif \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is even, \n  \n    \n      \n        \n          m\n          e\n          d\n          i\n          a\n          n\n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                x\n                \n                  (\n                  n\n                  \n                    /\n                  \n                  2\n                  )\n                \n              \n              +\n              \n                x\n                \n                  (\n                  (\n                  n\n                  \n                    /\n                  \n                  2\n                  )\n                  +\n                  1\n                  )\n                \n              \n            \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {median} (x)={\\frac {x_{(n/2)}+x_{((n/2)+1)}}{2}}}\n  \n\n\n=== Formal definition ===\nFormally, a median of a population is any value such that at least half of the population is less than or equal to the proposed median and at least half is greater than or equal to the proposed median.  As seen above, medians may not be unique.  If each set contains more than half the population, then some of the population is exactly equal to the unique median.\nThe median is well-defined for any ordered (one-dimensional) data, and is independent of any distance metric.  The median can thus be applied to classes which are ranked but not numerical (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between classes if there is an even number of cases.\nA geometric median, on the other hand, is defined in any number of dimensions.  A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.\nThere is no widely accepted standard notation for the median, but some authors represent the median of a variable x either as x\u0342 or as \u03bc1/2 sometimes also M. In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.\nThe median is a special case of other ways of summarizing the typical values associated with a statistical distribution: it is the 2nd quartile, 5th decile, and 50th percentile.\n\n\n=== Uses ===\nThe median can be used as a measure of location when one attaches reduced importance to extreme values, typically because a distribution is skewed, extreme values are not known, or outliers are untrustworthy, i.e., may be measurement/transcription errors.\nFor example, consider the multiset\n\n1, 2, 2, 2, 3, 14.The median is 2 in this case, as is the mode, and it might be seen as a better indication of the center than the arithmetic mean of 4, which is larger than all but one of the values.  However, the widely cited empirical relationship that the mean is shifted \"further into the tail\" of a distribution than the median is not generally true.  At most, one can say that the two statistics cannot be \"too far\" apart; see \u00a7 Inequality relating means and medians below.As a median is based on the middle data in a set, it is not necessary to know the value of extreme results in order to calculate it. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.Because the median is simple to understand and easy to calculate, while also a robust approximation to the mean, the median is a popular summary statistic in descriptive statistics.  In this context, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.\nFor practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient when\u2014and only when\u2014 data is uncontaminated by data from heavy-tailed distributions or from mixtures of distributions.  Even then, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean.\n\n\n== Probability distributions ==\n\nFor any real-valued probability distribution with cumulative distribution function F, a median is defined as any real number m that satisfies the inequalities\n\nAn equivalent phrasing uses a random variable X distributed according to F:\n\nNote that this definition does not require X to have an absolutely continuous distribution (which has a probability density function f), nor does it require a discrete one.  In the former case, the inequalities can be upgraded to equality: a median satisfies\n\nAny probability distribution on R has at least one median, but in pathological cases there may be more than one median: if F is constant 1/2 on an interval (so that f=0 there), then any value of that interval is a median.\n\n\n=== Medians of particular distributions ===\nThe medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:\n\nThe median of a symmetric unimodal distribution coincides with the mode.\nThe median of a symmetric distribution which possesses a mean \u03bc also takes the value \u03bc.\nThe median of a normal distribution with mean \u03bc and variance \u03c32 is \u03bc. In fact, for a normal distribution, mean = median = mode.\nThe median of a uniform distribution in the interval [a, b] is (a + b) / 2, which is also the mean.\nThe median of a Cauchy distribution with location parameter x0 and scale parameter y is x0, the location parameter.\nThe median of a power law distribution x\u2212a, with exponent a > 1 is 21/(a \u2212 1)xmin, where xmin is the minimum value for which the power law holds\nThe median of an exponential distribution with rate parameter \u03bb is the natural logarithm of 2 divided by the rate parameter: \u03bb\u22121ln 2.\nThe median of a Weibull distribution with shape parameter k and scale parameter \u03bb is \u03bb(ln 2)1/k.\n\n\n== Properties ==\n\n\n=== Optimality property ===\nThe mean absolute error of a real variable c with respect to the random variable X is\n\n  \n    \n      \n        E\n        (\n        \n          |\n          \n            X\n            \u2212\n            c\n          \n          |\n        \n        )\n        \n      \n    \n    {\\displaystyle E(\\left|X-c\\right|)\\,}\n  Provided that the probability distribution of X is such that the above expectation exists, then m is a median of  X if and only if m is a minimizer of the mean absolute error with respect to X. In particular, if m is a sample median, then it minimizes the arithmetic mean of the absolute deviations. Note, however, that in cases where the sample contains an even number of elements, this minimizer is not unique.\nMore generally, a median is defined as a minimum of\n\n  \n    \n      \n        E\n        (\n        \n          |\n        \n        X\n        \u2212\n        c\n        \n          |\n        \n        \u2212\n        \n          |\n        \n        X\n        \n          |\n        \n        )\n        ,\n      \n    \n    {\\displaystyle E(|X-c|-|X|),}\n  as discussed below in the section on multivariate medians (specifically, the spatial median).\nThis optimization-based definition of the median is useful in statistical data-analysis, for example, in k-medians clustering.\n\n\n=== Inequality relating means and medians ===\n\nIf the distribution has finite variance, then the distance between the median \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n   and the mean \n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n   is bounded by one standard deviation.\nThis bound was proved by Book and Sher in 1979 for discrete samples, and more generally by Page and Murty in 1982.  In a comment on a subsequent proof by O'Cinneide, Mallows in 1991 presented a compact proof that uses Jensen's inequality twice, as follows. Using |\u00b7| for the absolute value, we have\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  |\n                \n                \u03bc\n                \u2212\n                m\n                \n                  |\n                \n                =\n                \n                  |\n                \n                E\n                \u2061\n                (\n                X\n                \u2212\n                m\n                )\n                \n                  |\n                \n              \n              \n                \n                \u2264\n                E\n                \u2061\n                (\n                \n                  |\n                \n                X\n                \u2212\n                m\n                \n                  |\n                \n                )\n              \n            \n            \n              \n              \n                \n                \u2264\n                E\n                \u2061\n                (\n                \n                  |\n                \n                X\n                \u2212\n                \u03bc\n                \n                  |\n                \n                )\n              \n            \n            \n              \n              \n                \n                \u2264\n                \n                  \n                    E\n                    \u2061\n                    \n                      (\n                      \n                        (\n                        X\n                        \u2212\n                        \u03bc\n                        \n                          )\n                          \n                            2\n                          \n                        \n                      \n                      )\n                    \n                  \n                \n                =\n                \u03c3\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}|\\mu -m|=|\\operatorname {E} (X-m)|&\\leq \\operatorname {E} (|X-m|)\\\\&\\leq \\operatorname {E} (|X-\\mu |)\\\\&\\leq {\\sqrt {\\operatorname {E} \\left((X-\\mu )^{2}\\right)}}=\\sigma .\\end{aligned}}}\n  The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex.  The second inequality comes from the fact that a median minimizes the absolute deviation function \n  \n    \n      \n        a\n        \u21a6\n        E\n        \u2061\n        (\n        \n          |\n        \n        X\n        \u2212\n        a\n        \n          |\n        \n        )\n      \n    \n    {\\displaystyle a\\mapsto \\operatorname {E} (|X-a|)}\n  .\nMallows's proof can be generalized to obtain a multivariate version of the inequality simply by replacing the absolute value with a norm:\n\n  \n    \n      \n        \u2016\n        \u03bc\n        \u2212\n        m\n        \u2016\n        \u2264\n        \n          \n            E\n            \u2061\n            \n              (\n              \n                \u2016\n                X\n                \u2212\n                \u03bc\n                \n                  \u2016\n                  \n                    2\n                  \n                \n              \n              )\n            \n          \n        \n        =\n        \n          \n            trace\n            \u2061\n            \n              (\n              \n                var\n                \u2061\n                (\n                X\n                )\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\|\\mu -m\\|\\leq {\\sqrt {\\operatorname {E} \\left(\\|X-\\mu \\|^{2}\\right)}}={\\sqrt {\\operatorname {trace} \\left(\\operatorname {var} (X)\\right)}}}\n  where m is a spatial median, that is, a minimizer of the function \n  \n    \n      \n        a\n        \u21a6\n        E\n        \u2061\n        (\n        \u2016\n        X\n        \u2212\n        a\n        \u2016\n        )\n        .\n        \n      \n    \n    {\\displaystyle a\\mapsto \\operatorname {E} (\\|X-a\\|).\\,}\n   The spatial median is unique when the data-set's dimension is two or more.An alternative proof uses the one-sided Chebyshev inequality; it appears in an inequality on location and scale parameters.  This formula also follows directly from Cantelli's inequality.\n\n\n==== Unimodal distributions ====\nFor the case of unimodal distributions, one can achieve a sharper bound on the distance between the median and the mean:\n\n  \n    \n      \n        \n          |\n          \n            \n              \n                \n                  X\n                  ~\n                \n              \n            \n            \u2212\n            \n              \n                \n                  X\n                  \u00af\n                \n              \n            \n          \n          |\n        \n        \u2264\n        \n          \n            (\n            \n              \n                3\n                5\n              \n            \n            )\n          \n          \n            \n              1\n              2\n            \n          \n        \n        \u03c3\n        \u2248\n        0.7746\n        \u03c3\n      \n    \n    {\\displaystyle \\left|{\\tilde {X}}-{\\bar {X}}\\right|\\leq \\left({\\frac {3}{5}}\\right)^{\\frac {1}{2}}\\sigma \\approx 0.7746\\sigma }\n  .A similar relation holds between the median and the mode:\n\n  \n    \n      \n        \n          |\n          \n            \n              \n                \n                  X\n                  ~\n                \n              \n            \n            \u2212\n            \n              m\n              o\n              d\n              e\n            \n          \n          |\n        \n        \u2264\n        \n          3\n          \n            \n              1\n              2\n            \n          \n        \n        \u03c3\n        \u2248\n        1.732\n        \u03c3\n        .\n      \n    \n    {\\displaystyle \\left|{\\tilde {X}}-\\mathrm {mode} \\right|\\leq 3^{\\frac {1}{2}}\\sigma \\approx 1.732\\sigma .}\n  \n\n\n== Jensen's inequality for medians ==\nJensen's inequality states that for any random variable X with a finite expectation E[X] and for any convex function f\n\n  \n    \n      \n        f\n        [\n        E\n        (\n        x\n        )\n        ]\n        \u2264\n        E\n        [\n        f\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle f[E(x)]\\leq E[f(x)]}\n  This inequality generalizes to the median as well.  We say a function f: R \u2192 R is a C function if, for any t,\n\n  \n    \n      \n        \n          f\n          \n            \u2212\n            1\n          \n        \n        \n          (\n          \n            \n            (\n            \u2212\n            \u221e\n            ,\n            t\n            ]\n            \n          \n          )\n        \n        =\n        {\n        x\n        \u2208\n        \n          R\n        \n        \u2223\n        f\n        (\n        x\n        )\n        \u2264\n        t\n        }\n      \n    \n    {\\displaystyle f^{-1}\\left(\\,(-\\infty ,t]\\,\\right)=\\{x\\in \\mathbb {R} \\mid f(x)\\leq t\\}}\n  is a closed interval (allowing the degenerate cases of a single point or an empty set).  Every convex function is a C function, but the reverse does not hold.  If f is a C function, then\n\n  \n    \n      \n        f\n        (\n        Median\n        \u2061\n        [\n        X\n        ]\n        )\n        \u2264\n        Median\n        \u2061\n        [\n        f\n        (\n        X\n        )\n        ]\n      \n    \n    {\\displaystyle f(\\operatorname {Median} [X])\\leq \\operatorname {Median} [f(X)]}\n  If the medians are not unique, the statement holds for the corresponding suprema.\n\n\n== Medians for samples ==\n\n\n=== The sample median ===\n\n\n==== Efficient computation of the sample median ====\nEven though comparison-sorting n items requires \u03a9(n log n) operations, selection algorithms can compute the kth-smallest of n items with only \u0398(n) operations. This includes the median, which is the n/2th order statistic (or for an even number of samples, the arithmetic mean of the two middle order statistics).Selection algorithms still have the downside of requiring \u03a9(n) memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's ninther, which is the median of three rule applied with limited recursion: if A is the sample laid out as an array, and\n\nmed3(A) = median(A[1], A[n/2], A[n]),then\n\nninther(A) = med3(med3(A[1 ... 1/3n]), med3(A[1/3n ... 2/3n]), med3(A[2/3n ... n]))The remedian is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.\n\n\n==== Sampling distribution ====\nThe distributions of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is asymptotically normal with mean \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   and variance\n\n  \n    \n      \n        \n          \n            1\n            \n              4\n              n\n              f\n              (\n              m\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{4nf(m)^{2}}}}\n  where \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is the median of \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   and \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the sample size:\n\n  \n    \n      \n        \n          Sample median\n        \n        \u223c\n        \n          \n            N\n          \n        \n        \n          (\n          \n            \u03bc\n            =\n            m\n            ,\n            \n              \u03c3\n              \n                2\n              \n            \n            =\n            \n              \n                1\n                \n                  4\n                  n\n                  f\n                  (\n                  m\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\text{Sample median}}\\sim {\\mathcal {N}}\\left(\\mu =m,\\sigma ^{2}={\\frac {1}{4nf(m)^{2}}}\\right)}\n  \nA modern proof follows below. Laplace's result is now understood as a special case of the asymptotic distribution of arbitrary quantiles.\nFor normal samples, the density is \n  \n    \n      \n        f\n        (\n        m\n        )\n        =\n        1\n        \n          /\n        \n        \n          \n            2\n            \u03c0\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(m)=1/{\\sqrt {2\\pi \\sigma ^{2}}}}\n  , thus for large samples the variance of the median equals \n  \n    \n      \n        (\n        \n          \u03c0\n        \n        \n          /\n        \n        \n          2\n        \n        )\n        \u22c5\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          /\n        \n        n\n        )\n        .\n      \n    \n    {\\displaystyle ({\\pi }/{2})\\cdot (\\sigma ^{2}/n).}\n    (See also section #Efficiency below.)\n\n\n===== Derivation of the asymptotic distribution =====\nWe take the sample size to be an odd number \n  \n    \n      \n        N\n        =\n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle N=2n+1}\n   and assume our variable continuous; the formula for the case of discrete variables is given below in \u00a7 Empirical local density.  The sample can be summarized as \"below median\", \"at median\", and \"above median\", which corresponds to a trinomial distribution with probabilities  \n  \n    \n      \n        F\n        (\n        v\n        )\n      \n    \n    {\\displaystyle F(v)}\n  , \n  \n    \n      \n        f\n        (\n        v\n        )\n      \n    \n    {\\displaystyle f(v)}\n   and  \n  \n    \n      \n        1\n        \u2212\n        F\n        (\n        v\n        )\n      \n    \n    {\\displaystyle 1-F(v)}\n  .  For a continuous variable, the probability of multiple sample values being exactly equal to the median is 0, so one can calculate the density of at the point \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   directly from the trinomial distribution:\n\n  \n    \n      \n        Pr\n        [\n        Median\n        =\n        v\n        ]\n        \n        d\n        v\n        =\n        \n          \n            \n              (\n              2\n              n\n              +\n              1\n              )\n              !\n            \n            \n              n\n              !\n              n\n              !\n            \n          \n        \n        F\n        (\n        v\n        \n          )\n          \n            n\n          \n        \n        (\n        1\n        \u2212\n        F\n        (\n        v\n        )\n        \n          )\n          \n            n\n          \n        \n        f\n        (\n        v\n        )\n        \n        d\n        v\n      \n    \n    {\\displaystyle \\Pr[\\operatorname {Median} =v]\\,dv={\\frac {(2n+1)!}{n!n!}}F(v)^{n}(1-F(v))^{n}f(v)\\,dv}\n  .Now we introduce the beta function.  For integer arguments \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   and \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  , this can be expressed as \n  \n    \n      \n        \n          B\n        \n        (\n        \u03b1\n        ,\n        \u03b2\n        )\n        =\n        \n          \n            \n              (\n              \u03b1\n              \u2212\n              1\n              )\n              !\n              (\n              \u03b2\n              \u2212\n              1\n              )\n              !\n            \n            \n              (\n              \u03b1\n              +\n              \u03b2\n              \u2212\n              1\n              )\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {B} (\\alpha ,\\beta )={\\frac {(\\alpha -1)!(\\beta -1)!}{(\\alpha +\\beta -1)!}}}\n  .  Also, recall that \n  \n    \n      \n        f\n        (\n        v\n        )\n        \n        d\n        v\n        =\n        d\n        F\n        (\n        v\n        )\n      \n    \n    {\\displaystyle f(v)\\,dv=dF(v)}\n  .  Using these relationships and setting both \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   and \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   equal to  \n  \n    \n      \n        n\n        +\n        1\n      \n    \n    {\\displaystyle n+1}\n   allows the last expression to be written as\n\n  \n    \n      \n        \n          \n            \n              F\n              (\n              v\n              \n                )\n                \n                  n\n                \n              \n              (\n              1\n              \u2212\n              F\n              (\n              v\n              )\n              \n                )\n                \n                  n\n                \n              \n            \n            \n              \n                B\n              \n              (\n              n\n              +\n              1\n              ,\n              n\n              +\n              1\n              )\n            \n          \n        \n        \n        d\n        F\n        (\n        v\n        )\n      \n    \n    {\\displaystyle {\\frac {F(v)^{n}(1-F(v))^{n}}{\\mathrm {B} (n+1,n+1)}}\\,dF(v)}\n  Hence the density function of the median is a symmetric beta distribution pushed forward by \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  .  Its mean, as we would expect, is 0.5 and its variance is  \n  \n    \n      \n        1\n        \n          /\n        \n        (\n        4\n        (\n        N\n        +\n        2\n        )\n        )\n      \n    \n    {\\displaystyle 1/(4(N+2))}\n  .  By the chain rule, the corresponding variance of the sample median is\n\n  \n    \n      \n        \n          \n            1\n            \n              4\n              (\n              N\n              +\n              2\n              )\n              f\n              (\n              m\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{4(N+2)f(m)^{2}}}}\n  .The additional 2 is negligible in the limit.\n\n\n===== Empirical local density =====\nIn practice, the functions \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   and \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   are often not known or assumed.  However, they can be estimated from an observed frequency distribution.  In this section, we give an example.  Consider the following table, representing a sample of 3,800 (discrete-valued) observations:\n\nBecause the observations are discrete-valued, constructing the exact distribution of the median is not an immediate translation of the above expression for \n  \n    \n      \n        Pr\n        (\n        Median\n        =\n        v\n        )\n      \n    \n    {\\displaystyle \\Pr(\\operatorname {Median} =v)}\n  ; one may (and typically does) have multiple instances of the median in one's sample.  So we must sum over all these possibilities:\n\n  \n    \n      \n        Pr\n        (\n        Median\n        =\n        v\n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            0\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            k\n            =\n            0\n          \n          \n            n\n          \n        \n        \n          \n            \n              N\n              !\n            \n            \n              i\n              !\n              (\n              N\n              \u2212\n              i\n              \u2212\n              k\n              )\n              !\n              k\n              !\n            \n          \n        \n        F\n        (\n        v\n        \u2212\n        1\n        \n          )\n          \n            i\n          \n        \n        (\n        1\n        \u2212\n        F\n        (\n        v\n        )\n        \n          )\n          \n            k\n          \n        \n        f\n        (\n        v\n        \n          )\n          \n            N\n            \u2212\n            i\n            \u2212\n            k\n          \n        \n      \n    \n    {\\displaystyle \\Pr(\\operatorname {Median} =v)=\\sum _{i=0}^{n}\\sum _{k=0}^{n}{\\frac {N!}{i!(N-i-k)!k!}}F(v-1)^{i}(1-F(v))^{k}f(v)^{N-i-k}}\n  Here, i is the number of points strictly less than the median and k the number strictly greater.\nUsing these preliminaries, it is possible to investigate the effect of sample size on the standard errors of the mean and median.  The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174.  The following table gives some comparison statistics.\n\nThe expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size.  The asymptotic approximation errs on the side of caution by overestimating the standard error.\n\n\n==== Estimation of variance from sample data ====\nThe value of \n  \n    \n      \n        (\n        2\n        f\n        (\n        x\n        )\n        \n          )\n          \n            \u2212\n            2\n          \n        \n      \n    \n    {\\displaystyle (2f(x))^{-2}}\n  \u2014the asymptotic value of \n  \n    \n      \n        \n          n\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        (\n        \u03bd\n        \u2212\n        m\n        )\n      \n    \n    {\\displaystyle n^{-1/2}(\\nu -m)}\n   where \n  \n    \n      \n        \u03bd\n      \n    \n    {\\displaystyle \\nu }\n   is the population median\u2014has been studied by several authors. The standard \"delete one\" jackknife method produces inconsistent results. An alternative\u2014the \"delete k\" method\u2014where \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of \n  \n    \n      \n        \n          n\n          \n            \u2212\n            \n              \n                1\n                4\n              \n            \n          \n        \n      \n    \n    {\\displaystyle n^{-{\\frac {1}{4}}}}\n  ). Other methods have been proposed but their behavior may differ between large and small samples.\n\n\n==== Efficiency ====\nThe efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size \n  \n    \n      \n        N\n        =\n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle N=2n+1}\n   from the normal distribution, the efficiency for large N is\n\n  \n    \n      \n        \n          \n            2\n            \u03c0\n          \n        \n        \n          \n            \n              N\n              +\n              2\n            \n            N\n          \n        \n      \n    \n    {\\displaystyle {\\frac {2}{\\pi }}{\\frac {N+2}{N}}}\n  The efficiency tends to \n  \n    \n      \n        \n          \n            2\n            \u03c0\n          \n        \n      \n    \n    {\\displaystyle {\\frac {2}{\\pi }}}\n   as \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   tends to infinity.\nIn other words, the relative variance of the median will be \n  \n    \n      \n        \u03c0\n        \n          /\n        \n        2\n        \u2248\n        1.57\n      \n    \n    {\\displaystyle \\pi /2\\approx 1.57}\n  , or 57% greater than the variance of the mean \u2013 the relative standard error of the median will be \n  \n    \n      \n        (\n        \u03c0\n        \n          /\n        \n        2\n        \n          )\n          \n            \n              1\n              2\n            \n          \n        \n        \u2248\n        1.25\n      \n    \n    {\\displaystyle (\\pi /2)^{\\frac {1}{2}}\\approx 1.25}\n  , or 25% greater than the standard error of the mean, \n  \n    \n      \n        \u03c3\n        \n          /\n        \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\sigma /{\\sqrt {n}}}\n   (see also section #Sampling distribution above.).\n\n\n=== Other estimators ===\nFor univariate distributions that are symmetric about one median, the Hodges\u2013Lehmann estimator is a robust and highly efficient estimator of the population median.If data is represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.\n\n\n== Multivariate median ==\nPreviously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.\n\n\n=== Marginal median ===\nThe marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.\n\n\n=== Geometric median ===\nThe geometric median of a discrete set of sample points \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        \n          x\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots x_{N}}\n   in a Euclidean space is the point minimizing the sum of distances to the sample points.\n\n  \n    \n      \n        \n          \n            \n              \u03bc\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            \n              \u03bc\n              \u2208\n              \n                \n                  R\n                \n                \n                  m\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            n\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \u2016\n            \n              \u03bc\n              \u2212\n              \n                x\n                \n                  n\n                \n              \n            \n            \u2016\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mu }}={\\underset {\\mu \\in \\mathbb {R} ^{m}}{\\operatorname {arg\\,min} }}\\sum _{n=1}^{N}\\left\\|\\mu -x_{n}\\right\\|_{2}}\n  In contrast to the marginal median, the geometric median is equivariant with respect to Euclidean similarity transformations such as translations and rotations.\n\n\n=== Median in all directions ===\nIf the marginal medians for all coordinate systems coincide, then their common location may be termed the \"median in all directions\". This concept is relevant to voting theory on account of the median voter theorem. When it exists, the median in all directions coincides with the geometric median (at least for discrete distributions).\n\n\n=== Centerpoint ===\nAn alternative generalization of the median in higher dimensions is the centerpoint.\n\n\n== Other median-related concepts ==\n\n\n=== Interpolated median ===\nWhen dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is 3 since the median is the smallest value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   for which \n  \n    \n      \n        F\n        (\n        x\n        )\n      \n    \n    {\\displaystyle F(x)}\n   is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50.  First we add half of the interval width \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark.  In other words, we split up the interval width pro rata to the numbers of observations.  In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   are known, the interpolated median can be calculated from\n\n  \n    \n      \n        \n          m\n          \n            int\n          \n        \n        =\n        m\n        +\n        w\n        \n          [\n          \n            \n              \n                1\n                2\n              \n            \n            \u2212\n            \n              \n                \n                  F\n                  (\n                  m\n                  )\n                  \u2212\n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \n                  f\n                  (\n                  m\n                  )\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle m_{\\text{int}}=m+w\\left[{\\frac {1}{2}}-{\\frac {F(m)-{\\frac {1}{2}}}{f(m)}}\\right].}\n  Alternatively, if in an observed sample there are \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   scores above the median category, \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   scores in it and \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   scores below it then the interpolated median is given by\n\n  \n    \n      \n        \n          m\n          \n            int\n          \n        \n        =\n        m\n        +\n        \n          \n            w\n            2\n          \n        \n        \n          [\n          \n            \n              \n                k\n                \u2212\n                i\n              \n              j\n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle m_{\\text{int}}=m+{\\frac {w}{2}}\\left[{\\frac {k-i}{j}}\\right].}\n  \n\n\n=== Pseudo-median ===\n\nFor univariate distributions that are symmetric about one median, the Hodges\u2013Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges\u2013Lehmann estimator is a robust and highly efficient estimator of the population pseudo-median, which is the median of a symmetrized distribution and which is close to the population median. The Hodges\u2013Lehmann estimator has been generalized to multivariate distributions.\n\n\n=== Variants of regression ===\nThe Theil\u2013Sen estimator is a method for robust linear regression based on finding medians of slopes.\n\n\n=== Median filter ===\nThe median filter is an important tool of image processing, that can effectively remove any salt and pepper noise from grayscale images.\n\n\n=== Cluster analysis ===\n\nIn cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.\n\n\n=== Median\u2013median line ===\nThis is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  : a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and independent \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.\nNair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.\n\n\n== Median-unbiased estimators ==\n\nAny mean-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A median-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.\nThe theory of median-unbiased estimators was revived by George W. Brown in 1947:\nAn estimate of a one-dimensional parameter \u03b8 will be said to be median-unbiased if, for fixed \u03b8, the median of the distribution of the estimate is at the value \u03b8; i.e., the estimate underestimates just as often as it overestimates. This requirement seems for most purposes to accomplish as much as the mean-unbiased requirement and has the additional property that it is invariant under one-to-one transformation.\nFurther properties of median-unbiased estimators have been reported.  Median-unbiased estimators are invariant under one-to-one transformations.\nThere are methods of constructing median-unbiased estimators that are optimal (in a sense analogous to the minimum-variance property for mean-unbiased estimators). Such constructions exist for probability distributions having monotone likelihood-functions. One such procedure is an analogue of the Rao\u2013Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao\u2014Blackwell procedure but for a larger class of loss functions.\n\n\n== History ==\nScientific researchers in the ancient near east appear not to have used summary statistics altogether, instead choosing values that offered maximal consistency with a broader theory that integrated a wide variety of phenomena.  Within the Mediterranean (and, later, European) scholarly community, statistics like the mean are fundamentally a medieval and early modern development.  (The history of the median outside Europe and its predecessors remains relatively unstudied.)\nThe idea of the median appeared in the 6th century in the Talmud, in order to fairly analyze divergent appraisals.  However, the concept did not spread to the broader scientific community.\nInstead, the closest ancestor of the modern median is the mid-range, invented by Al-Biruni.:\u200a31\u200a  Transmission of Al-Biruni's work to later scholars is unclear.  Al-Biruni applied his technique to assaying metals, but, after he published his work, most assayers still adopted the most unfavorable value from their results, lest they appear to cheat.:\u200a35\u20138\u200a  However, increased navigation at sea during the Age of Discovery meant that ship's navigators increasingly had to attempt to determine latitude in unfavorable weather against hostile shores, leading to renewed interest in summary statistics.  Whether rediscovered or independently invented, the mid-range is recommended to nautical navigators in Harriot's \"Instructions for Raleigh's Voyage to Guiana, 1595\".:\u200a45\u20138\u200aThe idea of the median may have first appeared in Edward Wright's 1599 book Certaine Errors in Navigation on a section about compass navigation.  Wright was reluctant to discard measured values, and may have felt that the median \u2014 incorporating a greater proportion of the dataset than the mid-range \u2014 was more likely to be correct.  However, Wright did not give examples of his technique's use, making it hard to verify that he described the modern notion of median.  The median (in the context of probability) certainly appeared in the correspondence of Christiaan Huygens, but as an example of a statistic that was inappropriate for actuarial practice.The earliest recommendation of the median dates to 1757, when Roger Joseph Boscovich developed a regression method based on the L1 norm and therefore implicitly on the median.  In 1774, Laplace made this desire explicit: he suggested the median be used as the standard estimator of the value of a posterior PDF. The specific criterion was to minimize the expected magnitude of the error; \n  \n    \n      \n        \n          |\n        \n        \u03b1\n        \u2212\n        \n          \u03b1\n          \n            \u2217\n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle |\\alpha -\\alpha ^{*}|}\n   where \n  \n    \n      \n        \n          \u03b1\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\alpha ^{*}}\n   is the estimate and \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is the true value.  To this end, Laplace determined the distributions of both the sample mean and the sample median in the early 1800s.  However, a decade later, Gauss and Legendre developed the least squares method, which minimizes \n  \n    \n      \n        (\n        \u03b1\n        \u2212\n        \n          \u03b1\n          \n            \u2217\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (\\alpha -\\alpha ^{*})^{2}}\n   to obtain the mean.  Within the context of regression, Gauss and Legendre's innovation offers vastly easier computation.  Consequently, Laplaces' proposal was generally rejected until the rise of computing devices 150 years later (and is still a relatively uncommon algorithm).Antoine Augustin Cournot in 1843 was the first to use the term median (valeur m\u00e9diane) for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median (Centralwerth) in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace, and the median appeared in a textbook by F. Y. Edgeworth.  Francis Galton used the English term median in 1881, having earlier used the terms middle-most value in 1869, and the medium in 1880.Statisticians encouraged the use of medians intensely throughout the 19th century for its intuitive clarity and ease of manual computation.  However, the notion of median does not lend itself to the theory of higher moments as well as the arithmetic mean does, and is much harder to compute by computer.  As a result, the median was steadily supplanted as a notion of generic average by the arithmetic mean during the 20th century.\n\n\n== See also ==\nAbsolute deviation\nBias of an estimator\nCentral tendency\nConcentration of measure for Lipschitz functions\nMedian graph\nMedian of medians \u2013 Algorithm to calculate the approximate median in linear time\nMedian search\nMedian slope\nMedian voter theory\nMedoids \u2013 Generalization of the median in higher dimensions\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\"Median (in statistics)\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nMedian as a weighted arithmetic mean of all Sample Observations\nOn-line calculator\nCalculating the median\nA problem involving the mean, the median, and the mode.\nWeisstein, Eric W. \"Statistical Median\". MathWorld.\nPython script for Median computations and income inequality metrics\nFast Computation of the Median by Successive Binning\n'Mean, median, mode and skewness', A tutorial devised for first-year psychology students at Oxford University, based on a worked example.\nThe Complex SAT Math Problem Even the College Board Got Wrong: Andrew Daniels in Popular MechanicsThis article incorporates material from Median of a distribution on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.", "Posterior probability": "The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule. From an epistemological perspective, the posterior probability contains everything there is to know about an uncertain proposition (such as a scientific hypothesis, or parameter values), given prior knowledge and a mathematical model describing the observations available at a particular time. After the arrival of new information, the current posterior probability may serve as the prior in another round of Bayesian updating.In the context of Bayesian statistics, the posterior probability distribution usually describes the epistemic uncertainty about statistical parameters conditional on a collection of observed data. From a given posterior distribution, various point and interval estimates can be derived, such as the maximum a posteriori (MAP) or the highest posterior density interval (HPDI). But while conceptually simple, the posterior distribution is generally not tractable and therefore needs to be either analytically or numerically approximated.\n\n\n== Definition in the distributional case ==\nIn variational Bayesian methods, the posterior probability is the probability of the parameters \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   given the evidence \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , and is denoted \n  \n    \n      \n        p\n        (\n        \u03b8\n        \n          |\n        \n        X\n        )\n      \n    \n    {\\displaystyle p(\\theta |X)}\n  .\nIt contrasts with the likelihood function, which is the probability of the evidence given the parameters: \n  \n    \n      \n        p\n        (\n        X\n        \n          |\n        \n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(X|\\theta )}\n  .\nThe two are related as follows:\nGiven a prior belief that a probability distribution function is \n  \n    \n      \n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(\\theta )}\n   and that the observations \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   have a likelihood \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(x|\\theta )}\n  , then the posterior probability is defined as \n\n  \n    \n      \n        p\n        (\n        \u03b8\n        \n          |\n        \n        x\n        )\n        =\n        \n          \n            \n              p\n              (\n              x\n              \n                |\n              \n              \u03b8\n              )\n            \n            \n              p\n              (\n              x\n              )\n            \n          \n        \n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(\\theta |x)={\\frac {p(x|\\theta )}{p(x)}}p(\\theta )}\n  where \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n   is the normalizing constant and is calculated as\n\n  \n    \n      \n        p\n        (\n        x\n        )\n        =\n        \u222b\n        p\n        (\n        x\n        \n          |\n        \n        \u03b8\n        )\n        p\n        (\n        \u03b8\n        )\n        d\n        \u03b8\n      \n    \n    {\\displaystyle p(x)=\\int p(x|\\theta )p(\\theta )d\\theta }\n  for continuous \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , \nor by summing \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        \u03b8\n        )\n        p\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(x|\\theta )p(\\theta )}\n  \nover all possible values of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   for discrete \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  .The posterior probability is therefore proportional to the product Likelihood \u00b7 Prior probability.\n\n\n== Example ==\nSuppose there is a school with 60% boys and 40% girls as students. The girls wear trousers or skirts in equal numbers; all boys wear trousers. An observer sees a (random) student from a distance; all the observer can see is that this student is wearing trousers. What is the probability this student is a girl? The correct answer can be computed using Bayes' theorem.\nThe event \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   is that the student observed is a girl, and the event \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is that the student observed is wearing trousers. To compute the posterior probability \n  \n    \n      \n        P\n        (\n        G\n        \n          |\n        \n        T\n        )\n      \n    \n    {\\displaystyle P(G|T)}\n  , we first need to know:\n\n  \n    \n      \n        P\n        (\n        G\n        )\n      \n    \n    {\\displaystyle P(G)}\n  , or the probability that the student is a girl regardless of any other information. Since the observer sees a random student, meaning that all students have the same probability of being observed, and the percentage of girls among the students is 40%, this probability equals 0.4.\n\n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n  , or the probability that the student is not a girl (i.e. a boy) regardless of any other information (\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is the complementary event to \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  ). This is 60%, or 0.6.\n\n  \n    \n      \n        P\n        (\n        T\n        \n          |\n        \n        G\n        )\n      \n    \n    {\\displaystyle P(T|G)}\n  , or the probability of the student wearing trousers given that the student is a girl. As they are as likely to wear skirts as trousers, this is 0.5.\n\n  \n    \n      \n        P\n        (\n        T\n        \n          |\n        \n        B\n        )\n      \n    \n    {\\displaystyle P(T|B)}\n  , or the probability of the student wearing trousers given that the student is a boy. This is given as 1.\n\n  \n    \n      \n        P\n        (\n        T\n        )\n      \n    \n    {\\displaystyle P(T)}\n  , or the probability of a (randomly selected) student wearing trousers regardless of any other information. Since \n  \n    \n      \n        P\n        (\n        T\n        )\n        =\n        P\n        (\n        T\n        \n          |\n        \n        G\n        )\n        P\n        (\n        G\n        )\n        +\n        P\n        (\n        T\n        \n          |\n        \n        B\n        )\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(T)=P(T|G)P(G)+P(T|B)P(B)}\n   (via the law of total probability), this is \n  \n    \n      \n        P\n        (\n        T\n        )\n        =\n        0.5\n        \u00d7\n        0.4\n        +\n        1\n        \u00d7\n        0.6\n        =\n        0.8\n      \n    \n    {\\displaystyle P(T)=0.5\\times 0.4+1\\times 0.6=0.8}\n  .Given all this information, the posterior probability of the observer having spotted a girl given that the observed student is wearing trousers can be computed by substituting these values in the formula:\n\n  \n    \n      \n        P\n        (\n        G\n        \n          |\n        \n        T\n        )\n        =\n        \n          \n            \n              P\n              (\n              T\n              \n                |\n              \n              G\n              )\n              P\n              (\n              G\n              )\n            \n            \n              P\n              (\n              T\n              )\n            \n          \n        \n        =\n        \n          \n            \n              0.5\n              \u00d7\n              0.4\n            \n            0.8\n          \n        \n        =\n        0.25.\n      \n    \n    {\\displaystyle P(G|T)={\\frac {P(T|G)P(G)}{P(T)}}={\\frac {0.5\\times 0.4}{0.8}}=0.25.}\n  An intuitive way to solve this is to assume the school has N students. Number of boys = 0.6N and number of girls = 0.4N. If N is sufficiently large, total number of trouser wearers = 0.6N+ 50% of 0.4N. And number of girl trouser wearers = 50% of 0.4N. Therefore, in the population of trousers, girls are (50% of 0.4N)/(0.6N+ 50% of 0.4N) = 25%. In other words, if you separated out the group of trouser wearers, a quarter of that group will be girls. Therefore, if you see trousers, the most you can deduce is that you are looking at a single sample from a subset of students where 25% are girls. And by definition, chance of this random student being a girl is 25%. Every Bayes theorem problem can be solved in this way.\n\n\n== Calculation ==\nThe posterior probability distribution of one random variable given the value of another can be calculated with Bayes' theorem by multiplying the prior probability distribution by the likelihood function, and then dividing by the normalizing constant, as follows:\n\n  \n    \n      \n        \n          f\n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n              \n                \n                  \n                    L\n                  \n                \n                \n                  X\n                  \u2223\n                  Y\n                  =\n                  y\n                \n              \n              (\n              x\n              )\n            \n            \n              \n                \u222b\n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u221e\n                \n              \n              \n                f\n                \n                  X\n                \n              \n              (\n              u\n              )\n              \n                \n                  \n                    L\n                  \n                \n                \n                  X\n                  \u2223\n                  Y\n                  =\n                  y\n                \n              \n              (\n              u\n              )\n              \n              d\n              u\n            \n          \n        \n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)={f_{X}(x){\\mathcal {L}}_{X\\mid Y=y}(x) \\over {\\int _{-\\infty }^{\\infty }f_{X}(u){\\mathcal {L}}_{X\\mid Y=y}(u)\\,du}}}\n  gives the posterior probability density function for a random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given the data \n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n  , where\n\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n   is the prior density of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ,\n\n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          f\n          \n            Y\n            \u2223\n            X\n            =\n            x\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}_{X\\mid Y=y}(x)=f_{Y\\mid X=x}(y)}\n   is the likelihood function as a function of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  ,\n\n  \n    \n      \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        u\n        )\n        \n          \n            \n              L\n            \n          \n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        u\n        )\n        \n        d\n        u\n      \n    \n    {\\displaystyle \\int _{-\\infty }^{\\infty }f_{X}(u){\\mathcal {L}}_{X\\mid Y=y}(u)\\,du}\n   is the normalizing constant, and\n\n  \n    \n      \n        \n          f\n          \n            X\n            \u2223\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)}\n   is the posterior density of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   given the data \n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n  .\n\n\n== Credible interval ==\nPosterior probability is a conditional probability conditioned on randomly observed data. Hence it is a random variable. For a random variable, it is important to summarize its amount of uncertainty. One way to achieve this goal is to provide a credible interval of the posterior probability.\n\n\n== Classification ==\nIn classification, posterior probabilities reflect the uncertainty of assessing an observation to particular class, see also Class membership probabilities. \nWhile statistical classification methods by definition generate posterior probabilities, Machine Learners usually supply membership values which do not induce any probabilistic confidence. It is desirable to transform or re-scale membership values to class membership probabilities, since they are comparable and additionally more easily applicable for post-processing.\n\n\n== See also ==\nPrediction interval\nBernstein\u2013von Mises theorem\nProbability of success\nBayesian epistemology\n\n\n== References ==\n\n\n== Further reading ==\nLancaster, Tony (2004). An Introduction to Modern Bayesian Econometrics. Oxford: Blackwell. ISBN 1-4051-1720-6.\nLee, Peter M. (2004). Bayesian Statistics : An Introduction (3rd ed.). Wiley. ISBN 0-340-81405-5.", "Receiver operating characteristic": "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.  The method was originally developed for operators of military radar receivers starting in 1941, which led to its name.\nThe ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 \u2212 specificity). The ROC can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n   to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.\nThe ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research.\nThe ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.\n\n\n== Basic concept ==\n\nA classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value (for instance, to determine whether a person has hypertension based on a blood pressure measure). Or it can be a discrete class label, indicating one of the classes.\nConsider a two-class prediction problem (binary classification), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p.\nTo get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2\u00d72 contingency table or confusion matrix, as follows:\n\n\n== ROC space ==\n\nThe contingency table can derive several evaluation \"metrics\" (see infobox). To draw a ROC curve, only the true positive rate (TPR) and false positive rate (FPR) are needed (as functions of some classifier parameter). The TPR defines how many correct positive results occur among all positive samples available during the test. FPR, on the other hand, defines how many incorrect positive results occur among all negative samples available during the test.\nA ROC space is defined by FPR and TPR as x and y axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs). Since TPR is equivalent to sensitivity and FPR is equal to 1 \u2212 specificity, the ROC graph is sometimes called the sensitivity vs (1 \u2212 specificity) plot. Each prediction result or instance of a confusion matrix represents one point in the ROC space.\nThe best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing 100% sensitivity (no false negatives) and 100% specificity (no false positives). The (0,1) point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners (regardless of the positive and negative base rates). An intuitive example of random guessing is a decision by flipping coins. As the size of the sample increases, a random classifier's ROC point tends towards the diagonal line. In the case of a balanced coin, it will tend to the point (0.5, 0.5).\nThe diagonal divides the ROC space. Points above the diagonal represent good classification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\nLet us look into four prediction results from 100 positive and 100 negative instances:\n\nPlots of the four results above in the ROC space are given in the figure. The result of method A clearly shows the best predictive power among A, B, and C. The result of B lies on the random guess line (the diagonal line), and it can be seen in the table that the accuracy of B is 50%. However, when C is mirrored across the center point (0.5,0.5), the resulting method C\u2032 is even better than A. This mirrored method simply reverses the predictions of whatever method or test produced the C contingency table. Although the original C method has negative predictive power, simply reversing its decisions leads to a new predictive method C\u2032 which has positive predictive power. When the C method predicts p or n, the C\u2032 method would predict n or p, respectively. In this manner, the C\u2032 test would perform the best. The closer a result from a contingency table is to the upper left corner, the better it predicts, but the distance from the random guess line in either direction is the best indicator of how much predictive power a method has. If the result is below the line (i.e. the method is worse than a random guess), all of the method's predictions must be reversed in order to utilize its power, thereby moving the result above the random guess line.\n\n\n== Curves in ROC space ==\n\nIn binary classification, the class prediction for each instance is often made based on a continuous random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , which is a \"score\" computed for the instance (e.g. the estimated probability in logistic regression). Given a threshold parameter \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  , the instance is classified as \"positive\" if \n  \n    \n      \n        X\n        >\n        T\n      \n    \n    {\\displaystyle X>T}\n  , and \"negative\" otherwise. \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   follows a probability density \n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{1}(x)}\n   if the instance actually belongs to class \"positive\", and \n  \n    \n      \n        \n          f\n          \n            0\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{0}(x)}\n   if otherwise. Therefore, the true positive rate is given by \n  \n    \n      \n        \n          \n            TPR\n          \n        \n        (\n        T\n        )\n        =\n        \n          \u222b\n          \n            T\n          \n          \n            \u221e\n          \n        \n        \n          f\n          \n            1\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle {\\mbox{TPR}}(T)=\\int _{T}^{\\infty }f_{1}(x)\\,dx}\n   and the false positive rate is given by \n  \n    \n      \n        \n          \n            FPR\n          \n        \n        (\n        T\n        )\n        =\n        \n          \u222b\n          \n            T\n          \n          \n            \u221e\n          \n        \n        \n          f\n          \n            0\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle {\\mbox{FPR}}(T)=\\int _{T}^{\\infty }f_{0}(x)\\,dx}\n  . \nThe ROC curve plots parametrically \n  \n    \n      \n        \n          \n            TPR\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle {\\mbox{TPR}}(T)}\n   versus \n  \n    \n      \n        \n          \n            FPR\n          \n        \n        (\n        T\n        )\n      \n    \n    {\\displaystyle {\\mbox{FPR}}(T)}\n   with \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   as the varying parameter.\nFor example, imagine that the blood protein levels in diseased people and healthy people are normally distributed with means of 2 g/dL and 1 g/dL respectively. A medical test might measure the level of a certain protein in a blood sample and classify any number above a certain threshold as indicating disease. The experimenter can adjust the threshold (green vertical line in the figure), which will in turn change the false positive rate. Increasing the threshold would result in fewer false positives (and more false negatives), corresponding to a leftward movement on the curve. The actual shape of the curve is determined by how much overlap the two distributions have.\n\n\n== Further interpretations ==\nSometimes, the ROC is used to generate a summary statistic. Common versions are:\n\nthe intercept of the ROC curve with the line at 45 degrees orthogonal to the no-discrimination line - the balance point where Sensitivity = 1 - Specificity\nthe intercept of the ROC curve with the tangent at 45 degrees parallel to the no-discrimination line that is closest to the error-free point (0,1) - also called Youden's J statistic and generalized as Informedness\nthe area between the ROC curve and the no-discrimination line multiplied by two is called the Gini coefficient.  It should not be confused with the measure of statistical dispersion also called Gini coefficient.\nthe area between the full ROC curve and the triangular ROC curve including only (0,0), (1,1) and one selected operating point \n  \n    \n      \n        (\n        t\n        p\n        r\n        ,\n        f\n        p\n        r\n        )\n      \n    \n    {\\displaystyle (tpr,fpr)}\n   - Consistency\nthe area under the ROC curve, or \"AUC\" (\"area under curve\"), or A' (pronounced \"a-prime\"), or \"c-statistic\" (\"concordance statistic\").\nthe sensitivity index d\u2032 (pronounced \"d-prime\"), the distance between the mean of the distribution of activity in the system under noise-alone conditions and its distribution under signal-alone conditions, divided by their standard deviation, under the assumption that both these distributions are normal with the same standard deviation. Under these assumptions, the shape of the ROC is entirely determined by d\u2032.However, any attempt to summarize the ROC curve into a single number loses information about the pattern of tradeoffs of the particular discriminator algorithm.\n\n\n=== Probabilistic interpretation ===\nWhen using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative'). In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which.\nThis can be seen as follows: the area under the curve is given by (the integral boundaries are reversed as large threshold \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   has\na lower value on the x-axis)\n\n  \n    \n      \n        \n          \n            TPR\n          \n        \n        (\n        T\n        )\n        :\n        T\n        \u21a6\n        y\n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\mbox{TPR}}(T):T\\mapsto y(x)}\n  \n\n  \n    \n      \n        \n          \n            FPR\n          \n        \n        (\n        T\n        )\n        :\n        T\n        \u21a6\n        x\n      \n    \n    {\\displaystyle {\\mbox{FPR}}(T):T\\mapsto x}\n  \n\n  \n    \n      \n        A\n        =\n        \n          \u222b\n          \n            x\n            =\n            0\n          \n          \n            1\n          \n        \n        \n          \n            TPR\n          \n        \n        (\n        \n          \n            \n              FPR\n            \n          \n          \n            \u2212\n            1\n          \n        \n        (\n        x\n        )\n        )\n        \n        d\n        x\n        =\n        \n          \u222b\n          \n            \u221e\n          \n          \n            \u2212\n            \u221e\n          \n        \n        \n          \n            TPR\n          \n        \n        (\n        T\n        )\n        \n          \n            \n              FPR\n            \n          \n          \u2032\n        \n        (\n        T\n        )\n        \n        d\n        T\n        =\n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        I\n        (\n        \n          T\n          \u2032\n        \n        \u2265\n        T\n        )\n        \n          f\n          \n            1\n          \n        \n        (\n        \n          T\n          \u2032\n        \n        )\n        \n          f\n          \n            0\n          \n        \n        (\n        T\n        )\n        \n        d\n        \n          T\n          \u2032\n        \n        \n        d\n        T\n        =\n        P\n        (\n        \n          X\n          \n            1\n          \n        \n        \u2265\n        \n          X\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle A=\\int _{x=0}^{1}{\\mbox{TPR}}({\\mbox{FPR}}^{-1}(x))\\,dx=\\int _{\\infty }^{-\\infty }{\\mbox{TPR}}(T){\\mbox{FPR}}'(T)\\,dT=\\int _{-\\infty }^{\\infty }\\int _{-\\infty }^{\\infty }I(T'\\geq T)f_{1}(T')f_{0}(T)\\,dT'\\,dT=P(X_{1}\\geq X_{0})}\n  where \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n   is the score for a positive instance and \n  \n    \n      \n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle X_{0}}\n   is the score for a negative instance, and \n  \n    \n      \n        \n          f\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle f_{0}}\n   and \n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle f_{1}}\n   are probability densities as defined in previous section.\n\n\n=== Area under the curve ===\nIt can be shown that the AUC is closely related to the Mann\u2013Whitney U, which tests whether positives are ranked higher than negatives. It is also equivalent to the Wilcoxon test of ranks. For a predictor \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  , an unbiased estimator of its AUC can be expressed by the following Wilcoxon-Mann-Whitney statistic:\n\n  \n    \n      \n        A\n        U\n        C\n        (\n        f\n        )\n        =\n        \n          \n            \n              \n                \u2211\n                \n                  \n                    t\n                    \n                      0\n                    \n                  \n                  \u2208\n                  \n                    \n                      \n                        D\n                      \n                    \n                    \n                      0\n                    \n                  \n                \n              \n              \n                \u2211\n                \n                  \n                    t\n                    \n                      1\n                    \n                  \n                  \u2208\n                  \n                    \n                      \n                        D\n                      \n                    \n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n              \n              [\n              f\n              (\n              \n                t\n                \n                  0\n                \n              \n              )\n              <\n              f\n              (\n              \n                t\n                \n                  1\n                \n              \n              )\n              ]\n            \n            \n              \n                |\n              \n              \n                \n                  \n                    D\n                  \n                \n                \n                  0\n                \n              \n              \n                |\n              \n              \u22c5\n              \n                |\n              \n              \n                \n                  \n                    D\n                  \n                \n                \n                  1\n                \n              \n              \n                |\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle AUC(f)={\\frac {\\sum _{t_{0}\\in {\\mathcal {D}}^{0}}\\sum _{t_{1}\\in {\\mathcal {D}}^{1}}{\\textbf {1}}[f(t_{0})<f(t_{1})]}{|{\\mathcal {D}}^{0}|\\cdot |{\\mathcal {D}}^{1}|}},}\n  where, \n  \n    \n      \n        \n          \n            1\n          \n        \n        [\n        f\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        <\n        f\n        (\n        \n          t\n          \n            1\n          \n        \n        )\n        ]\n      \n    \n    {\\textstyle {\\textbf {1}}[f(t_{0})<f(t_{1})]}\n   denotes an indicator function which returns 1 if \n  \n    \n      \n        f\n        (\n        \n          t\n          \n            0\n          \n        \n        )\n        <\n        f\n        (\n        \n          t\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle f(t_{0})<f(t_{1})}\n   otherwise return 0; \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}^{0}}\n   is the set of negative examples, and \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}^{1}}\n   is the set of positive examples.\nThe AUC is related to the  Gini impurity index  (\n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle G_{1}}\n  ) by the formula \n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n        =\n        2\n        \n          \n            AUC\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle G_{1}=2{\\mbox{AUC}}-1}\n  , where:\n\n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n        =\n        1\n        \u2212\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          X\n          \n            k\n          \n        \n        \u2212\n        \n          X\n          \n            k\n            \u2212\n            1\n          \n        \n        )\n        (\n        \n          Y\n          \n            k\n          \n        \n        +\n        \n          Y\n          \n            k\n            \u2212\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle G_{1}=1-\\sum _{k=1}^{n}(X_{k}-X_{k-1})(Y_{k}+Y_{k-1})}\n  In this way, it is possible to calculate the AUC by using an average of a number of trapezoidal approximations.  \n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle G_{1}}\n   should not be confused with the measure of statistical dispersion that is also called Gini coefficient.\nIt is also common to calculate the Area Under the ROC Convex Hull (ROC AUCH = ROCH AUC) as any point on the line segment between two prediction results can be achieved by randomly using one or the other system with probabilities proportional to the relative length of the opposite component of the segment. It is also possible to invert concavities \u2013 just as in the figure the worse solution can be reflected to become a better solution; concavities can be reflected in any line segment, but this more extreme form of fusion is much more likely to overfit the data.The machine learning community most often uses the ROC AUC statistic for model comparison. This practice has been questioned because AUC estimates are quite noisy and suffer from other problems. Nonetheless, the coherence of AUC as a measure of aggregated classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.Another problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system, as well as ignoring the possibility of concavity repair, so that related alternative measures such as Informedness or DeltaP are recommended. These measures are essentially equivalent to the Gini for a single prediction point with DeltaP' = Informedness = 2AUC-1, whilst DeltaP = Markedness represents the dual (viz. predicting the prediction from the real class) and their geometric mean is the Matthews correlation coefficient.Whereas ROC AUC varies between 0 and 1 \u2014 with an uninformative classifier yielding 0.5 \u2014 the alternative measures known as Informedness, Certainty  and Gini Coefficient (in the single parameterization or single system case) all have the advantage that 0 represents chance performance whilst 1 represents perfect performance, and \u22121 represents the \"perverse\" case of full informedness always giving the wrong response. Bringing chance performance to 0 allows these alternative scales to be interpreted as Kappa statistics. Informedness has been shown to have desirable characteristics for Machine Learning versus other common definitions of Kappa such as Cohen Kappa and Fleiss Kappa.Sometimes it can be more useful to look at a specific region of the ROC Curve rather than at the whole curve. It is possible to compute partial AUC. For example, one could focus on the region of the curve with low false positive rate, which is often of prime interest for population screening tests. Another common approach for classification problems in which P \u226a N (common in bioinformatics applications) is to use a logarithmic scale for the x-axis.The ROC area under the curve is also called c-statistic or c statistic.\n\n\n=== Other measures ===\n\nThe Total Operating Characteristic (TOC) also characterizes diagnostic ability while revealing more information than the ROC. For each threshold, ROC reveals two ratios, TP/(TP + FN) and FP/(FP + TN). In other words, ROC reveals \n  \n    \n      \n        \n          \n            hits\n            \n              \n                hits\n              \n              +\n              \n                misses\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\text{hits}}{{\\text{hits}}+{\\text{misses}}}}}\n   and \n  \n    \n      \n        \n          \n            false alarms\n            \n              \n                false alarms\n              \n              +\n              \n                correct rejections\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\text{false alarms}}{{\\text{false alarms}}+{\\text{correct rejections}}}}}\n  . On the other hand, TOC shows the total information in the contingency table for each threshold. The TOC method reveals all of the information that the ROC method provides, plus additional important information that ROC does not reveal, i.e. the size of every entry in the contingency table for each threshold. TOC also provides the popular AUC of the ROC.\n\nThese figures are the TOC and ROC curves using the same data and thresholds.\nConsider the point that corresponds to a threshold of 74. The TOC curve shows the number of hits, which is 3, and hence the number of misses, which is 7. Additionally, the TOC curve shows that the number of false alarms is 4 and the number of correct rejections is 16. At any given point in the ROC curve, it is possible to glean values for the ratios of \n  \n    \n      \n        \n          \n            false alarms\n            \n              \n                false alarms\n              \n              +\n              \n                correct rejections\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\text{false alarms}}{{\\text{false alarms}}+{\\text{correct rejections}}}}}\n   and \n  \n    \n      \n        \n          \n            hits\n            \n              \n                hits\n              \n              +\n              \n                misses\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\text{hits}}{{\\text{hits}}+{\\text{misses}}}}}\n  . For example, at threshold 74, it is evident that the x coordinate is 0.2 and the y coordinate is 0.3. However, these two values are insufficient to construct all entries of the underlying two-by-two contingency table.\n\n\n== Detection error tradeoff graph ==\n\nAn alternative to the ROC curve is the detection error tradeoff (DET) graph, which plots the false negative rate (missed detections) vs. the false positive rate (false alarms) on non-linearly transformed x- and y-axes. The transformation function is the quantile function of the normal distribution, i.e., the inverse of the cumulative normal distribution. It is, in fact, the same transformation as zROC, below, except that the complement of the hit rate, the miss rate or false negative rate, is used. This alternative spends more graph area on the region of interest. Most of the ROC area is of little interest; one primarily cares about the region tight against the y-axis and the top left corner \u2013 which, because of using miss rate instead of its complement, the hit rate, is the lower left corner in a DET plot. Furthermore, DET graphs have the useful property of linearity and a linear threshold behavior for normal distributions. The DET plot is used extensively in the automatic speaker recognition community, where the name DET was first used. The analysis of the ROC performance in graphs with this warping of the axes was used by psychologists in perception studies halfway through the 20th century, where this was dubbed \"double probability paper\".\n\n\n== Z-score ==\nIf a standard score is applied to the ROC curve, the curve will be transformed into a straight line.  This z-score is based on a normal distribution with a mean of zero and a standard deviation of one. In memory strength theory, one must assume that the zROC is not only linear, but has a slope of 1.0. The normal distributions of targets (studied objects that the subjects need to recall) and lures (non studied objects that the subjects attempt to recall) is the factor causing the zROC to be linear.\nThe linearity of the zROC curve depends on the standard deviations of the target and lure strength distributions. If the standard deviations are equal, the slope will be 1.0. If the standard deviation of the target strength distribution is larger than the standard deviation of the lure strength distribution, then the slope will be smaller than 1.0. In most studies, it has been found that the zROC curve slopes constantly fall below 1, usually between 0.5 and 0.9. Many experiments yielded a zROC slope of 0.8. A slope of 0.8 implies that the variability of the target strength distribution is 25% larger than the variability of the lure strength distribution.Another variable used is d' (d prime) (discussed above in \"Other measures\"), which can easily be expressed in terms of z-values. Although d' is a commonly used parameter, it must be recognized that it is only relevant when strictly adhering to the very strong assumptions of strength theory made above.The z-score of an ROC curve is always linear, as assumed, except in special situations. The Yonelinas familiarity-recollection model is a two-dimensional account of recognition memory. Instead of the subject simply answering yes or no to a specific input, the subject gives the input a feeling of familiarity, which operates like the original ROC curve. What changes, though, is a parameter for Recollection (R). Recollection is assumed to be all-or-none, and it trumps familiarity. If there were no recollection component, zROC would have a predicted slope of 1. However, when adding the recollection component, the zROC curve will be concave up, with a decreased slope. This difference in shape and slope result from an added element of variability due to some items being recollected. Patients with anterograde amnesia are unable to recollect, so their Yonelinas zROC curve would have a slope close to 1.0.\n\n\n== History ==\nThe ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory. Following the attack on Pearl Harbor in 1941, the United States army began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals. For these purposes they measured the ability of a radar receiver operator to make these important distinctions, which was called the Receiver Operating Characteristic.In the 1950s, ROC curves were employed in psychophysics to assess human (and occasionally non-human animal) detection of weak signals. In medicine, ROC analysis has been extensively used in the evaluation of diagnostic tests. ROC curves are also used extensively in epidemiology and medical research and are frequently mentioned in conjunction with evidence-based medicine. In radiology, ROC analysis is a common technique to evaluate new radiology techniques. In the social sciences, ROC analysis is often called the ROC Accuracy Ratio, a common technique for judging the accuracy of default probability models.\nROC curves are widely used in laboratory medicine to assess the diagnostic accuracy of a test, to choose the optimal cut-off of a test and to compare diagnostic accuracy of several tests.\nROC curves also proved useful for the evaluation of machine learning techniques. The first application of ROC in machine learning was by Spackman who demonstrated the value of ROC curves in comparing and evaluating different classification algorithms.ROC curves are also used in verification of forecasts in meteorology.\n\n\n== ROC curves beyond binary classification ==\nThe extension of ROC curves for classification problems with more than two classes is cumbersome.  Two common approaches for when there are multiple classes are (1) average over all pairwise AUC values and (2) compute the volume under surface (VUS). To average over all pairwise classes, one computes the AUC for each pair of classes, using only the examples from those two classes as if there were no other classes, and then averages these AUC values over all possible pairs. When there are c classes there will be c(c \u2212 1) / 2 possible pairs of classes.\nThe volume under surface approach has one plot a hypersurface rather than a curve and then measure the hypervolume under that hypersurface. Every possible decision rule that one might use for a classifier for c classes can be described in terms of its true positive rates (TPR1, ..., TPRc). It is this set of rates that defines a point, and the set of all possible decision rules yields a cloud of points that define the hypersurface.  With this definition, the VUS is the probability that the classifier will be able to correctly label all c examples when it is given a set that has one randomly selected example from each class.  The implementation of a classifier that knows that its input set consists of one example from each class might first compute a goodness-of-fit score for each of the c2 possible pairings of an example to a class, and then employ the Hungarian algorithm to maximize the sum of the c selected scores over all c! possible ways to assign exactly one example to each class.\nGiven the success of ROC curves for the assessment of classification models, the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves  and the Regression ROC (RROC) curves. In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the error variance of the regression model.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\nROC demo\nanother ROC demo \nROC video explanation\nAn Introduction to the Total Operating Characteristic: Utility in Land Change Model Evaluation\nHow to run the TOC Package in R\nTOC R package on Github\nExcel Workbook for generating TOC curves\n\n\n== Further reading ==\n\nBalakrishnan, Narayanaswamy (1991); Handbook of the Logistic Distribution, Marcel Dekker, Inc., ISBN 978-0-8247-8587-1\nBrown, Christopher D.; Davis, Herbert T. (2006). \"Receiver operating characteristic curves and related decision measures: a tutorial\". Chemometrics and Intelligent Laboratory Systems. 80: 24\u201338. doi:10.1016/j.chemolab.2005.05.004.\nRotello, Caren M.; Heit, Evan; Dub\u00e9, Chad (2014). \"When more data steer us wrong: replications with the wrong dependent measure perpetuate erroneous conclusions\" (PDF). Psychonomic Bulletin & Review. 22 (4): 944\u2013954. doi:10.3758/s13423-014-0759-2. PMID 25384892. S2CID 6046065.\nFawcett, Tom (2004). \"ROC Graphs: Notes and Practical Considerations for Researchers\" (PDF). Pattern Recognition Letters. 27 (8): 882\u2013891. CiteSeerX 10.1.1.145.4649. doi:10.1016/j.patrec.2005.10.012.\nGonen, Mithat (2007); Analyzing Receiver Operating Characteristic Curves Using SAS, SAS Press, ISBN 978-1-59994-298-8\nGreen, William H., (2003) Econometric Analysis, fifth edition, Prentice Hall, ISBN 0-13-066189-9\nHeagerty, Patrick J.; Lumley, Thomas; Pepe, Margaret S. (2000). \"Time-dependent ROC Curves for Censored Survival Data and a Diagnostic Marker\". Biometrics. 56 (2): 337\u2013344. doi:10.1111/j.0006-341x.2000.00337.x. PMID 10877287. S2CID 8822160.\nHosmer, David W.; and Lemeshow, Stanley (2000); Applied Logistic Regression, 2nd ed., New York, NY: Wiley, ISBN 0-471-35632-8\nLasko, Thomas A.; Bhagwat, Jui G.; Zou, Kelly H.; Ohno-Machado, Lucila (2005). \"The use of receiver operating characteristic curves in biomedical informatics\". Journal of Biomedical Informatics. 38 (5): 404\u2013415. CiteSeerX 10.1.1.97.9674. doi:10.1016/j.jbi.2005.02.008. PMID 16198999.\nMas, Jean-Fran\u00e7ois; Filho, Britaldo Soares; Pontius, Jr, Robert Gilmore; Guti\u00e9rrez, Michelle Farf\u00e1n; Rodrigues, Hermann (2013). \"A suite of tools for ROC analysis of spatial models\". ISPRS International Journal of Geo-Information. 2 (3): 869\u2013887. Bibcode:2013IJGI....2..869M. doi:10.3390/ijgi2030869.\nPontius, Jr, Robert Gilmore; Parmentier, Benoit (2014). \"Recommendations for using the Relative Operating Characteristic (ROC)\". Landscape Ecology. 29 (3): 367\u2013382. doi:10.1007/s10980-013-9984-8. S2CID 15924380.\nPontius, Jr, Robert Gilmore; Pacheco, Pablo (2004). \"Calibration and validation of a model of forest disturbance in the Western Ghats, India 1920\u20131990\". GeoJournal. 61 (4): 325\u2013334. doi:10.1007/s10708-004-5049-5. S2CID 155073463.\nPontius, Jr, Robert Gilmore; Batchu, Kiran (2003). \"Using the relative operating characteristic to quantify certainty in prediction of location of land cover change in India\". Transactions in GIS. 7 (4): 467\u2013484. doi:10.1111/1467-9671.00159. S2CID 14452746.\nPontius, Jr, Robert Gilmore; Schneider, Laura (2001). \"Land-use change model validation by a ROC method for the Ipswich watershed, Massachusetts, USA\". Agriculture, Ecosystems & Environment. 85 (1\u20133): 239\u2013248. doi:10.1016/S0167-8809(01)00187-6.\nStephan, Carsten; Wesseling, Sebastian; Schink, Tania; Jung, Klaus (2003). \"Comparison of Eight Computer Programs for Receiver-Operating Characteristic Analysis\". Clinical Chemistry. 49 (3): 433\u2013439. doi:10.1373/49.3.433. PMID 12600955.\nSwets, John A.; Dawes, Robyn M.; and Monahan, John (2000); Better Decisions through Science, Scientific American, October, pp. 82\u201387\nZou, Kelly H.; O'Malley, A. James; Mauri, Laura (2007). \"Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models\". Circulation. 115 (5): 654\u20137. doi:10.1161/circulationaha.105.594929. PMID 17283280.\nZhou, Xiao-Hua; Obuchowski, Nancy A.; McClish, Donna K. (2002). Statistical Methods in Diagnostic Medicine. New York, NY: Wiley & Sons. ISBN 978-0-471-34772-9.\nChicco D.; Jurman G. (2023). \"The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classification\". BioData Mining. 16 (1). doi:10.1186/s13040-023-00322-4. PMC 9938573.", "Pruning (decision trees)": "Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.\nOne of the questions that arises in a decision tree algorithm is the optimal size of the final tree.  A tree that is too large risks overfitting the training data and poorly generalizing to new samples.  A small tree might not capture important structural information about the sample space.  However, it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error.  This problem is known as the horizon effect.  A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information.Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set.  There are many techniques for tree pruning that differ in the measurement that is used to optimize performance.\n\n\n== Techniques ==\nPruning processes can be divided into two types (pre- and post-pruning).\nPre-pruning procedures prevent a complete induction of the training set by replacing a stop () criterion in the induction algorithm (e.g. max. Tree depth or information gain (Attr)> minGain). Pre-pruning methods are considered to be more efficient because they do not induce an entire set, but rather trees remain small from the start. Prepruning methods share a common problem, the horizon effect. This is to be understood as the undesired premature termination of the induction by the stop () criterion.\nPost-pruning (or just pruning) is the most common way of simplifying trees. Here, nodes and subtrees are replaced with leaves to reduce complexity. Pruning can not only significantly reduce the size but also improve the classification accuracy of unseen objects. It may be the case that the accuracy of the assignment on the train set deteriorates, but the accuracy of the classification properties of the tree increases overall.\nThe procedures are differentiated on the basis of their approach in the tree (top-down or bottom-up).\n\n\n=== Bottom-up pruning ===\nThese procedures start at the last node in the tree (the lowest point). Following recursively upwards, they determine the relevance of each individual node. If the relevance for the classification is not given, the node is dropped or replaced by a leaf. The advantage is that no relevant sub-trees can be lost with this method.\nThese methods include Reduced Error Pruning (REP), Minimum Cost Complexity Pruning (MCCP), or Minimum Error Pruning (MEP).\n\n\n=== Top-down pruning ===\nIn contrast to the bottom-up method, this method starts at the root of the tree. Following the structure below, a relevance check is carried out which decides whether a node is relevant for the classification of all n items or not. By pruning the tree at an inner node, it can happen that an entire sub-tree (regardless of its relevance) is dropped. One of these representatives is pessimistic error pruning (PEP), which brings quite good results with unseen items.\n\n\n== Pruning algorithms ==\n\n\n=== Reduced error pruning ===\nOne of the simplest forms of pruning is reduced error pruning.  Starting at the leaves, each node is replaced with its most popular class.  If the prediction accuracy is not affected then the change is kept.  While somewhat naive, reduced error pruning has the advantage of simplicity and speed.\n\n\n=== Cost complexity pruning ===\nCost complexity pruning generates a series of trees \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n        \u2026\n        \n          T\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle T_{0}\\dots T_{m}}\n   where \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n   is the initial tree and \n  \n    \n      \n        \n          T\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle T_{m}}\n   is the root alone.  At step \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  , the tree is created by removing a subtree from tree \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   and replacing it with a leaf node with value chosen as in the tree building algorithm.  The subtree that is removed is chosen as follows:\n\nDefine the error rate of tree \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   over data set \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   as \n  \n    \n      \n        err\n        \u2061\n        (\n        T\n        ,\n        S\n        )\n      \n    \n    {\\displaystyle \\operatorname {err} (T,S)}\n  .\nThe subtree \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   that minimizes \n  \n    \n      \n        \n          \n            \n              err\n              \u2061\n              (\n              prune\n              \u2061\n              (\n              T\n              ,\n              t\n              )\n              ,\n              S\n              )\n              \u2212\n              err\n              \u2061\n              (\n              T\n              ,\n              S\n              )\n            \n            \n              \n                |\n                \n                  leaves\n                  \u2061\n                  (\n                  T\n                  )\n                \n                |\n              \n              \u2212\n              \n                |\n                \n                  leaves\n                  \u2061\n                  (\n                  prune\n                  \u2061\n                  (\n                  T\n                  ,\n                  t\n                  )\n                  )\n                \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\operatorname {err} (\\operatorname {prune} (T,t),S)-\\operatorname {err} (T,S)}{\\left\\vert \\operatorname {leaves} (T)\\right\\vert -\\left\\vert \\operatorname {leaves} (\\operatorname {prune} (T,t))\\right\\vert }}}\n   is chosen for removal.The function \n  \n    \n      \n        prune\n        \u2061\n        (\n        T\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle \\operatorname {prune} (T,t)}\n   defines the tree obtained by pruning the subtrees \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   from the tree \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  .  Once the series of trees has been created, the best tree is chosen by generalized accuracy as measured by a training set or cross-validation.\n\n\n== See also ==\nAlpha\u2013beta pruning\nArtificial neural network\nNull-move heuristic\n\n\n== References ==\nPearl, Judea (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley. ISBN 978-0-201-05594-8.\nMansour, Y. (1997). \"Pessimistic decision tree pruning based on tree size\". Proc. 14th International Conference on Machine Learning. pp. 195\u2013201.\nBreslow, L. A.; Aha, D. W. (1997). \"Simplifying Decision Trees: A Survey\". The Knowledge Engineering Review. 12 (1): 1\u201347. doi:10.1017/S0269888997000015. S2CID 18782652.\nQuinlan, J. R. (1986). \"Induction of Decision Trees\". Machine Learning. Kluwer. 1: 81\u2013106. doi:10.1007/BF00116251.\n\n\n== Further reading ==\nMDL based decision tree pruning\nDecision tree pruning using backpropagation neural networks\n\n\n== External links ==\nFast, Bottom-Up Decision Tree Pruning Algorithm\nIntroduction to Decision tree pruning", "Multiclass classification": "In machine learning and statistical classification, multiclass classification or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).\nWhile many classification algorithms (notably multinomial logistic regression) naturally permit the use of more than two classes, some are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.\nMulticlass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.\n\n\n== General strategies ==\nThe existing multi-class classification techniques can be categorised into\n\ntransformation to binary\nextension from binary\nhierarchical classification.\n\n\n=== Transformation to binary ===\nThis section discusses strategies for reducing the problem of multiclass classification to multiple binary classification problems. It can be categorized into one vs rest and one vs one. The techniques developed based on reducing the multi-class problem into multiple binary problems can also be called problem transformation techniques.\n\n\n==== One-vs.-rest ====\nOne-vs.-rest:\u200a182,\u200a338\u200a (OvR or one-vs.-all, OvA or one-against-all, OAA) strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a class label; discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample.:\u200a182\u200aIn pseudocode, the training algorithm for an OvR learner constructed from a binary classification learner L is as follows:\n\nInputs:\nL, a learner (training algorithm for binary classifiers)\nsamples X\nlabels y where yi \u2208 {1, \u2026 K} is the label for the sample Xi\nOutput:\na list of classifiers fk for k \u2208 {1, \u2026, K}\nProcedure:\nFor each k in {1, \u2026, K}\nConstruct a new label vector z where zi  = yi  if yi = k and  zi = 0 otherwise\nApply L to X, z to obtain fkMaking decisions means applying all classifiers to an unseen sample x and predicting the label k for which the corresponding classifier reports the highest confidence score:\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              arg\n              \n              max\n            \n            \n              k\n              \u2208\n              {\n              1\n              \u2026\n              K\n              }\n            \n          \n        \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1\\ldots K\\}}{\\arg \\!\\max }}\\;f_{k}(x)}\n  Although this strategy is popular, it is a heuristic that suffers from several problems. Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives.:\u200a338\u200a\n\n\n==== One-vs.-one ====\nIn the one-vs.-one (OvO) reduction, one trains K (K \u2212 1) / 2 binary classifiers for a K-way multiclass problem; each receives the samples of a pair of classes from the original training set, and must learn to distinguish these two classes. At prediction time, a voting scheme is applied: all K (K \u2212 1) / 2 classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier.:\u200a339\u200aLike OvR, OvO suffers from ambiguities in that some regions of its input space may receive the same number of votes.:\u200a183\u200a\n\n\n=== Extension from binary ===\nThis section discusses strategies of extending the existing binary classifiers to solve multi-class classification problems. Several algorithms have been developed based on neural networks, decision trees, k-nearest neighbors, naive Bayes, support vector machines and extreme learning machines to address multi-class classification problems. These types of techniques can also be called algorithm adaptation techniques.\n\n\n==== Neural networks ====\nMulticlass perceptrons provide a natural extension to the multi-class problem. Instead of just having one neuron in the output layer, with binary output, one could have N binary neurons leading to multi-class classification. In practice, the last layer of a neural network is usually a softmax function layer, which is the algebraic simplification of N logistic classifiers, normalized per class by the sum of the N-1 other logistic classifiers.\n\n\n===== Extreme learning machines =====\nExtreme learning machines (ELM) is a special case of single hidden layer feed-forward neural networks (SLFNs) wherein the input weights and the hidden node biases can be chosen at random. Many variants and developments are made to the ELM for multiclass classification.\n\n\n==== k-nearest neighbours ====\nk-nearest neighbors kNN is considered among the oldest non-parametric classification algorithms. To classify an unknown example, the distance from that example to every other training example is measured. The k smallest distances are identified, and the most represented class by these k nearest neighbours is considered the output class label.\n\n\n==== Naive Bayes ====\nNaive Bayes is a successful classifier based upon the principle of maximum a posteriori (MAP). This approach is naturally extensible to the case of having more than two classes, and was shown to perform well in spite of the underlying simplifying assumption of conditional independence.\n\n\n==== Decision trees ====\nDecision tree learning is a powerful classification technique. The tree tries to infer a split of the training data based on the values of the available features to produce a good generalization.  The algorithm can naturally handle binary or multiclass classification problems. The leaf nodes can refer to any of the K classes concerned.\n\n\n==== Support vector machines ====\nSupport vector machines are based upon the idea of maximizing the margin i.e. maximizing the minimum distance from the separating hyperplane to the nearest example. The basic SVM supports only binary classification, but extensions have been proposed to handle the multiclass classification case as well. In these extensions, additional parameters and constraints are added to the optimization problem to handle the separation of the different classes.\n\n\n==== Multi expression programming ====\nMulti expression programming (MEP) is an evolutionary algorithm for generating computer programs (that can be used for classification tasks too). MEP has a unique feature: it encodes multiple programs into a single chromosome. Each of these programs can be used to generate the output for a class, thus making MEP naturally suitable for solving multi-class classification problems.\n\n\n=== Hierarchical classification ===\nHierarchical classification tackles the multi-class classification problem by dividing the output space i.e. into a tree. Each parent node is divided into multiple child nodes and the process is continued until each child node represents only one class. Several methods have been proposed based on hierarchical classification.\n\n\n== Learning paradigms ==\nBased on learning paradigms, the existing multi-class classification techniques can be classified into batch learning and online learning. Batch learning algorithms require all the data samples to be available beforehand. It trains the model using the entire training data and then predicts the test sample using the found relationship. The online learning algorithms, on the other hand, incrementally build their models in sequential iterations. In iteration t, an online algorithm receives a sample, xt and predicts its label \u0177t using the current model; the algorithm then receives yt, the true label of xt and updates its model based on the sample-label pair: (xt, yt). Recently, a new learning paradigm called progressive learning technique has been developed. The progressive learning technique is capable of not only learning from new samples but also capable of learning new classes of data and yet retain the knowledge learnt thus far.\n\n\n== See also ==\nBinary classification\nOne-class classification\nMulti-label classification\nMulticlass perceptron\nMulti-task learning\n\n\n== Notes ==\n\n\n== References ==", "Fuzzy clustering": "Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\nClustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application.\n\n\n== Comparison to hard clustering ==\nIn non-fuzzy clustering (also known as hard clustering), data are divided into distinct clusters, where each data point can only belong to exactly one cluster. In fuzzy clustering, data points can potentially belong to multiple clusters. For example, an apple can be red or green (hard clustering), but an apple can also be red AND green (fuzzy clustering). Here, the apple can be red to a certain degree as well as green to a certain degree. Instead of the apple belonging to green [green = 1] and not red [red = 0], the apple can belong to green [green = 0.5] and red [red = 0.5]. These value are normalized between 0 and 1; however, they do not represent probabilities, so the two values do not need to add up to 1.\n\n\n== Membership ==\nMembership grades are assigned to each of the data points (tags). These membership grades indicate the degree to which data points belong to each cluster. Thus, points on the edge of a cluster, with lower membership grades, may be in the cluster to a lesser degree than points in the center of cluster.\n\n\n== Fuzzy C-means clustering ==\nOne of the most widely used fuzzy clustering algorithms is the Fuzzy C-means clustering (FCM) algorithm.\n\n\n=== History ===\nFuzzy c-means (FCM) clustering was developed by J.C. Dunn in 1973, and improved by J.C. Bezdek in 1981.\n\n\n=== General description ===\nThe fuzzy c-means algorithm is very similar to the k-means algorithm:\n\nChoose a number of clusters.\nAssign coefficients randomly to each data point for being in the clusters.\nRepeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  , the given sensitivity threshold) :\nCompute the centroid for each cluster (shown below).\nFor each data point, compute its coefficients of being in the clusters.\n\n\n=== Centroid ===\nAny point x has a set of coefficients giving the degree of being in the kth cluster wk(x). With fuzzy c-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster, or, mathematically,\n\n  \n    \n      \n        \n          c\n          \n            k\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  x\n                \n              \n              \n                \n                  \n                    w\n                    \n                      k\n                    \n                  \n                  (\n                  x\n                  )\n                \n                \n                  m\n                \n              \n              x\n            \n            \n              \n                \u2211\n                \n                  x\n                \n              \n              \n                \n                  \n                    w\n                    \n                      k\n                    \n                  \n                  (\n                  x\n                  )\n                \n                \n                  m\n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle c_{k}={{\\sum _{x}{w_{k}(x)}^{m}x} \\over {\\sum _{x}{w_{k}(x)}^{m}}},}\n  \nwhere m is the hyper- parameter that controls how fuzzy the cluster will be. The higher it is, the fuzzier the cluster will be in the end.\n\n\n=== Algorithm ===\nThe FCM algorithm attempts to partition a finite collection of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   elements \n\n  \n    \n      \n        X\n        =\n        {\n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            x\n          \n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle X=\\{\\mathbf {x} _{1},...,\\mathbf {x} _{n}\\}}\n   into a collection of c fuzzy clusters with respect to some given criterion.\nGiven a finite set of data, the algorithm returns a list of  \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n    cluster centres  \n  \n    \n      \n        C\n        =\n        {\n        \n          \n            c\n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            c\n          \n          \n            c\n          \n        \n        }\n      \n    \n    {\\displaystyle C=\\{\\mathbf {c} _{1},...,\\mathbf {c} _{c}\\}}\n    and a partition matrix\n\n  \n    \n      \n        W\n        =\n        \n          w\n          \n            i\n            ,\n            j\n          \n        \n        \u2208\n        [\n        0\n        ,\n        1\n        ]\n        ,\n        \n        i\n        =\n        1\n        ,\n        .\n        .\n        .\n        ,\n        n\n        ,\n        \n        j\n        =\n        1\n        ,\n        .\n        .\n        .\n        ,\n        c\n      \n    \n    {\\displaystyle W=w_{i,j}\\in [0,1],\\;i=1,...,n,\\;j=1,...,c}\n  , where each element, \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   , tells\nthe degree to which element, \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  , belongs to cluster \n  \n    \n      \n        \n          \n            c\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {c} _{j}}\n  .\nThe FCM aims to minimize an objective function:\n\n  \n    \n      \n        J\n        (\n        W\n        ,\n        C\n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            c\n          \n        \n        \n          w\n          \n            i\n            j\n          \n          \n            m\n          \n        \n        \n          \n            \u2016\n            \n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  c\n                \n                \n                  j\n                \n              \n            \n            \u2016\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle J(W,C)=\\sum _{i=1}^{n}\\sum _{j=1}^{c}w_{ij}^{m}\\left\\|\\mathbf {x} _{i}-\\mathbf {c} _{j}\\right\\|^{2}}\n  ,where:\n\n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        \n          \n            1\n            \n              \n                \u2211\n                \n                  k\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                \n                  (\n                  \n                    \n                      \n                        \u2016\n                        \n                          \n                            \n                              x\n                            \n                            \n                              i\n                            \n                          \n                          \u2212\n                          \n                            \n                              c\n                            \n                            \n                              j\n                            \n                          \n                        \n                        \u2016\n                      \n                      \n                        \u2016\n                        \n                          \n                            \n                              x\n                            \n                            \n                              i\n                            \n                          \n                          \u2212\n                          \n                            \n                              c\n                            \n                            \n                              k\n                            \n                          \n                        \n                        \u2016\n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    2\n                    \n                      m\n                      \u2212\n                      1\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle w_{ij}={\\frac {1}{\\sum _{k=1}^{c}\\left({\\frac {\\left\\|\\mathbf {x} _{i}-\\mathbf {c} _{j}\\right\\|}{\\left\\|\\mathbf {x} _{i}-\\mathbf {c} _{k}\\right\\|}}\\right)^{\\frac {2}{m-1}}}}}\n  .\n\n\n=== Comparison to K-means clustering ===\nK-means clustering also attempts to minimize the objective function shown above, except that in K-means, the membership values are either zero or one, and cannot take values in between, i.e. \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n        \u2208\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle w_{ij}\\in \\{0,1\\}}\n  . In Fuzzy C-means, the degree of fuzziness is parametrized by \n  \n    \n      \n        m\n        \u2208\n        (\n        1\n        ,\n        \u221e\n        )\n      \n    \n    {\\displaystyle m\\in (1,\\infty )}\n  , where a larger \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   results in fuzzier clusters. In the limit \n  \n    \n      \n        m\n        \u2192\n        1\n      \n    \n    {\\displaystyle m\\rightarrow 1}\n  , the memberships, \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n   , converge to 0 or 1, and the Fuzzy C-means objective coincides with that of K-means. In the absence of experimentation or domain knowledge, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as 'k'-means; the minimum is a local minimum, and the results depend on the initial choice of weights.\n\n\n=== Implementation ===\nThere are several implementations of this algorithm that are publicly available.\n\n\n== Related algorithms ==\nFuzzy C-means (FCM) with automatically determined for the number of clusters could enhance the detection accuracy. Using a mixture of Gaussians along with the expectation-maximization algorithm is a more statistically formalized method which includes some of these ideas: partial membership in classes.\n\n\n== Example ==\nTo better understand this principle, a classic example of mono-dimensional data is given below on an x axis.\n\nThis data set can be traditionally grouped into two clusters. By selecting a threshold on the x-axis, the data is separated into two clusters.  The resulting clusters are labelled 'A' and 'B', as seen in the following image.  Each point belonging to the data set would therefore have a membership coefficient of 1 or 0. This membership coefficient of each corresponding data point is represented by the inclusion of the y-axis.   \n\nIn fuzzy clustering, each data point can have membership to multiple clusters.  By relaxing the definition of membership coefficients from strictly 1 or 0, these values can range from any value from 1 to 0. The following image shows the data set from the previous clustering, but now fuzzy c-means clustering is applied. First, a new threshold value defining two clusters may be generated. Next, new membership coefficients for each data point are generated based on clusters centroids, as well as distance from each cluster centroid.\n\nAs one can see, the middle data point belongs to cluster A and cluster B. the value of 0.3 is this data point's membership coefficient for cluster A .\n\n\n== Applications ==\nClustering problems have applications in surface science, biology, medicine, psychology, economics, and many other disciplines.\n\n\n=== Bioinformatics ===\nIn the field of bioinformatics, clustering is used for a number of applications. One use is as a pattern recognition technique to analyze gene expression data from RNA-sequencing data or other technologies. In this case, genes with similar expression patterns are grouped into the same cluster, and different clusters display distinct, well-separated patterns of expression. Use of clustering can provide insight into gene function and regulation. Because fuzzy clustering allows genes to belong to more than one cluster, it allows for the identification of genes that are conditionally co-regulated or co-expressed. For example, one gene may be acted on by more than one transcription factor, and one gene may encode a protein that has more than one function. Thus, fuzzy clustering is more appropriate than hard clustering.\n\n\n=== Image analysis ===\nFuzzy c-means has been a very important tool for image processing in clustering objects in an image. In the 1970s, mathematicians introduced the spatial term into the FCM algorithm to improve the accuracy of clustering under noise. Furthermore, FCM algorithms have been used to distinguish between different activities using image-based features such as the Hu and the Zernike Moments. Alternatively, A fuzzy logic model can be described on fuzzy sets that are defined on three components of the HSL color space HSL and HSV; The membership functions aim to describe colors follow the human intuition of color identification.\n\n\n=== Marketing ===\nIn marketing, customers can be grouped into fuzzy clusters based on their needs, brand choices, psycho-graphic profiles, or other marketing related partitions.\n\n\n== Image processing example ==\n\nImage segmentation using k-means clustering algorithms has long been used for pattern recognition, object detection, and medical imaging. However, due to real world limitations such as noise, shadowing, and variations in cameras, traditional hard clustering is often unable to reliably perform image processing tasks as stated above.  Fuzzy clustering has been proposed as a more applicable algorithm in the performance to these tasks.  Given is gray scale image that has undergone fuzzy clustering in Matlab.  The original image is seen next to a clustered image.  Colors are used to give a visual representation of the three distinct clusters used to identify the membership of each pixel. Below, a chart is given that defines the fuzzy membership coefficients of their corresponding intensity values.\nDepending on the application for which the fuzzy clustering coefficients are to be used, different pre-processing techniques can be applied to RGB images.  RGB to HCL conversion is common practice.\n\n\n== See also ==\nFLAME Clustering\nCluster Analysis\nExpectation-maximization algorithm (a similar, but more statistically formalized method)\n\n\n== References ==", "Expert system": "In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if\u2013then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. \nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n\n\n== History ==\n\n\n=== Early development ===\nSoon after the dawn of modern computers in the late 1940s and early 1950s, researchers started realizing the immense potential these machines had for modern society.  One of the first challenges was to make such machines capable of \u201cthinking\u201d like humans \u2013 in particular, making these machines capable of making important decisions the way humans do.  The medical/healthcare field presented the tantalizing challenge of enabling these machines to make medical diagnostic decisions.Thus, in the late 1950s, right after the information age had fully arrived, researchers started experimenting with the prospect of using computer technology to emulate human decision making. For example, biomedical researchers started creating computer-aided systems for diagnostic applications in medicine and biology. These early diagnostic systems used patients\u2019 symptoms and laboratory test results as inputs to generate a diagnostic outcome.\nThese systems were often described as the early forms of expert systems.  However, researchers realized that there were significant limitations when using traditional methods such as flow charts, statistical pattern matching, or probability theory.\n\n\n=== Formal introduction and later developments ===\nThis previous situation gradually led to the development of expert systems, which used knowledge-based approaches.  These expert systems in medicine were the MYCIN expert system, the Internist-I expert system and later, in the middle of the 1980s, the CADUCEUS.Expert systems were formally introduced around 1965 by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the \"father of expert systems\"; other key early contributors were Bruce Buchanan and Randall Davis. The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral). The idea that \"intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use\" \u2013 as Feigenbaum said \u2013 was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon). Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.Research on expert systems was also active in France. While in the US the focus tended to be on rules-based systems, first on systems hard coded on top of LISP programming environments and then on expert system shells developed by vendors such as Intellicorp, in France research focused more on systems developed in Prolog. The advantage of expert system shells was that they were somewhat easier for nonprogrammers to use.  The advantage of Prolog environments was that they were not focused only on if-then rules; Prolog environments provided a much better realization of a complete first order logic environment.In the 1980s, expert systems proliferated.  Universities offered expert system courses and two-thirds of the Fortune 500 companies applied the technology in daily business activities. Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.\nIn 1981, the first IBM PC, with the PC DOS operating system, was introduced. The imbalance between the high affordability of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client\u2013server model.  Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. This model also enabled business units to bypass corporate IT departments and directly build their own applications. As a result, client-server had a tremendous impact on the expert systems market. Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments. With the rise of the PC and client-server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC-based tools. Also, new vendors, often financed by venture capital (such as Aion Corporation, Neuron Data, Exsys, and many others), started appearing regularly.\nThe first expert system to be used in a design capacity for a large-scale product was the SID (Synthesis of Integral Design) software program, developed in 1982. Written in LISP, SID generated 93% of the VAX 9000 CPU logic gates. Input to the software was a set of rules created by several expert logic designers. SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves. Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts.  While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker. The program was highly controversial but used nevertheless due to project budget constraints. It was terminated by logic designers after the VAX 9000 project completion.\nDuring the years before the middle of the 1970s, the expectations of what expert systems can accomplish in many fields tended to be extremely optimistic.  At the beginning of these early studies, researchers were hoping to develop entirely automatic (i.e., completely computerized) expert systems. The expectations of people of what computers can do were frequently too idealistic.  This situation radically changed after Richard M. Karp published his breakthrough paper: \u201cReducibility among Combinatorial Problems\u201d in the early 1970s. Thanks to Karp's work, together with other scholars, like Hubert L. Dreyfus, it became clear that there are certain limitations and possibilities when one designs computer algorithms.  His findings describe what computers can do and what they cannot do.  Many of the computational problems related to this type of expert systems have certain pragmatic limitations.   These findings laid down the groundwork that led to the next developments in the field.In the 1990s and beyond, the term expert system and the idea of a standalone AI system mostly dropped from the IT lexicon. There are two interpretations of this. One is that \"expert systems failed\": the IT world moved on because expert systems did not deliver on their over hyped promise. The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose expert systems, to being one of many standard tools. Other researchers suggest that Expert Systems caused inter-company power struggles when the IT organization lost its exclusivity in software modifications to users or Knowledge Engineers.In the first decade of the 2000s, there was a \"resurrection\" for the technology, while using the term Rule Based Systems, with significant success stories and adoption.  Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way of specifying business logic \u2013 rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.\n\n\n=== Current approaches to expert systems ===\nThe limitations of the previous type of expert systems have urged researchers to develop new types of approaches. They have developed more efficient, flexible, and powerful approaches in order to simulate the human decision-making process. Some of the approaches that researchers have developed are based on new methods of artificial intelligence (AI), and in particular in machine learning and data mining approaches with a feedback mechanism. Recurrent neural networks often take advantage of such mechanisms.  Related is the discussion on the disadvantages section.  \nModern systems can incorporate new knowledge more easily and thus update themselves easily.  Such systems can generalize from existing knowledge better and deal with vast amounts of complex data.  Related is the subject of big data here. Sometimes these type of expert systems are called \"intelligent systems.\"\n\n\n== Software architecture ==\n\nAn expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. In general view, an expert system includes the following components: a knowledge base, an inference engine, an explanation facility, a knowledge acquisition facility, and a user interface. The knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.\nThe inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.There are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:\n\nA simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.\nBackward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but does not, then it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.\nThe use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English, if the user asked \"Why is Socrates Mortal?\" the system would reply \"Because all men are mortal and Socrates is a man\".  A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.As expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:\n\nTruth maintenance. These systems record the dependencies in a knowledge-base so that when facts are altered, dependent knowledge can be altered accordingly. For example, if the system learns that Socrates is no longer known to be a man it will revoke the assertion that Socrates is mortal.\nHypothetical reasoning. In this, the knowledge base can be divided up into many possible views, a.k.a. worlds. This allows the inference engine to explore multiple possibilities in parallel. For example, the system may want to explore the consequences of both assertions, what will be true if Socrates is a Man and what will be true if he is not?\nUncertainty systems. One of the first extensions of simply using rules to represent knowledge was also to associate a probability with each rule. So, not to assert that Socrates is mortal, but to assert Socrates may be mortal with some probability value. Simple probabilities were extended in some systems with sophisticated mechanisms for uncertain reasoning, such as Fuzzy logic, and combination of probabilities.\nOntology classification. With the addition of object classes to the knowledge base, a new type of reasoning was possible. Along with reasoning simply about object values, the system could also reason about object structures. In this simple example, Man can represent an object class and R1 can be redefined as a rule that defines the class of all men.  These types of special purpose inference engines are termed classifiers. Although they were not highly used in expert systems, classifiers are very powerful for unstructured volatile domains, and are a key technology for the Internet and the emerging Semantic Web.\n\n\n== Advantages ==\nThe goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit.  In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.\nEase of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems.  Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.\nA claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.Summing up the benefits of using expert systems, the following can be highlighted: \nIncreased availability and reliability: Expertise can be accessed on any computer hardware and the system always completes responses on time.\nMultiple expertise: Several expert systems can be run simultaneously to solve a problem. and gain a higher level of expertise than a human expert.\nExplanation: Expert systems always describe of how the problem was solved.\nFast response: The expert systems are fast and able to solve a problem in real-time.\nReduced cost: The cost of expertise for each user is significantly reduced.\n\n\n== Disadvantages ==\nThe most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems \u2013 essentially the same problems as those of any other large system \u2013 seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.Performance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them. This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C). System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments \u2013 programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client\u2013server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.Another major challenge of expert systems emerges when the size of the knowledge base increases. This causes the processing complexity to increase.  For instance, when an expert system with 100 million rules was envisioned as the ultimate expert system, it became obvious that such system would be too complex and it would face too many computational problems. An inference engine would have to be able to process huge numbers of rules to reach a decision.\nHow to verify that decision rules are consistent with each other is also a challenge when there are too many rules.  Usually such problem leads to a satisfiability (SAT) formulation. This is a well-known NP-complete problem Boolean satisfiability problem.  If we assume only binary variables, say n of them, and then the corresponding search space is of size 2\n  \n    \n      \n        \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle ^{n}}\n  .  Thus, the search space can grow exponentially.\nThere are also questions on how to prioritize the use of the rules in order to operate more efficiently, or how to resolve ambiguities (for instance, if there are too many else-if sub-structures within a single rule) and so on.Other problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base.  Such problems exist with methods that employ machine learning approaches too.Another problem related to the knowledge base is how to make updates of its knowledge quickly and effectively. Also how to add a new piece of knowledge (i.e., where to add it among many rules) is challenging.  Modern approaches that rely on machine learning methods are easier in this regard.\nBecause of the above challenges, it became clear that new approaches to AI were required instead of rule-based technologies.  These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.The key challenges that expert systems in medicine (if one considers computer-aided diagnostic systems as modern expert systems), and perhaps in other application domains, include issues related to aspects such as: big data, existing regulations, healthcare practice, various algorithmic issues, and system assessment.Finally, the following disadvantages of using expert systems can be summarized: \nExpert systems have superficial knowledge, and a simple task can potentially become computationally expensive.\nExpert systems require knowledge engineers to input the data, data acquisition is very hard.\nThe expert system may choose the most inappropriate method for solving a particular problem.\nProblems of ethics in the use of any form of AI are very relevant at present.\nIt is a closed world with specific knowledge, in which there is no deep perception of concepts and their interrelationships until an expert provides them.\n\n\n== Applications ==\nHayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book. Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.\n\nHearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category of expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems\u2014looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.\nCADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.\nDendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved\u2014designing a solution given a set of constraints\u2014was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.\nSMH.PAL is an expert system for the assessment of students with multiple disabilities.GARVAN-ES1 was a medical expert system, developed at the Garvan Institute of Medical Research, that provided automated clinical diagnostic comments on endocrine reports from a pathology laboratory. It was one of the first medical expert systems to go into routine clinical use internationally  and the first expert system to be used for diagnosis daily in Australia. The system was written in \"C\" and ran on a PDP-11 in 64K of memory. It had 661 rules that were compiled; not interpreted.\n\nMistral  is an expert system to monitor dam safety, developed in the 1990s by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos. Mistral is a registered trade mark of CESI.\n\n\n== See also ==\nAI winter\nCLIPS\nConstraint logic programming\nConstraint satisfaction\nKnowledge engineering\nLearning classifier system\nRule-based machine learning\n\n\n== References ==\n\n\n=== Works cited ===\n\n\n== External links ==\nArtificial Intelligence at Curlie\nExpert System tutorial on Code Project", "Quartile": "In statistics, a quartile is a type of quantile which divides the number of data points into four parts, or quarters, of more-or-less equal size. The data must be ordered from smallest to largest to compute quartiles; as such, quartiles are a form of order statistic. The three main quartiles are as follows:\n\nThe first quartile (Q1) is defined as the middle number between the smallest number (minimum) and the median of the data set. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point.\nThe second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point.\nThe third quartile (Q3) is the middle value between the median and the highest value (maximum) of the data set. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point.Along with the minimum and maximum of the data (which are also quartiles), the three quartiles described above provide a five-number summary of the data. This summary is important in statistics because it provides information about both the center and the spread of the data. Knowing the lower and upper quartile provides information on how big the spread is and if the dataset is skewed toward one side. Since quartiles divide the number of data points evenly, the range is not the same between quartiles (i.e., Q3-Q2 \u2260 Q2-Q1) and is instead known as the interquartile range (IQR). While the maximum and minimum also show the spread of the data, the upper and lower quartiles can provide more detailed information on the location of specific data points, the presence of outliers in the data, and the difference in spread between the middle 50% of the data and the outer data points.\n\n\n== Definitions ==\n\n\n== Computing methods ==\n\n\n=== Discrete distributions ===\nFor discrete distributions, there is no universal agreement on selecting the quartile values.\n\n\n==== Method 1 ====\nUse the median to divide the ordered data set into two-halves.\nIf there is an odd number of data points in the original ordered data set, do not include the median (the central value in the ordered list) in either half.\nIf there is an even number of data points in the original ordered data set, split this data set exactly in half.\nThe lower quartile value is the median of the lower half of the data. The upper quartile value is the median of the upper half of the data.This rule is employed by the TI-83 calculator boxplot and \"1-Var Stats\" functions.\n\n\n==== Method 2 ====\nUse the median to divide the ordered data set into two-halves.\nIf there are an odd number of data points in the original ordered data set, include the median (the central value in the ordered list) in both halves.\nIf there are an even number of data points in the original ordered data set, split this data set exactly in half.\nThe lower quartile value is the median of the lower half of the data. The upper quartile value is the median of the upper half of the data.The values found by this method are also known as \"Tukey's hinges\"; see also midhinge.\n\n\n==== Method 3 ====\nIf there are even numbers of data points, then Method 3 starts off the same as Method 1 or Method 2 above and you can choose to include or not include the median as a datapoint. If you choose to include the median as a new datapoint, proceed to step 2 or 3 of Method 3 because you now have an odd number of datapoints.\nIf there are (4n+1) data points, then the lower quartile is 25% of the nth data value plus 75% of the (n+1)th data value; the upper quartile is 75% of the (3n+1)th data point plus 25% of the (3n+2)th data point.\nIf there are (4n+3) data points, then the lower quartile is 75% of the (n+1)th data value plus 25% of the (n+2)th data value; the upper quartile is 25% of the (3n+2)th data point plus 75% of the (3n+3)th data point.\n\n\n==== Method 4 ====\nIf we have an ordered dataset \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{n}}\n  , we can interpolate between data points to find the \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  th empirical quantile if \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   is in the \n  \n    \n      \n        i\n        \n          /\n        \n        (\n        n\n        +\n        1\n        )\n      \n    \n    {\\displaystyle i/(n+1)}\n   quantile. If we denote the integer part of a number \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   by \n  \n    \n      \n        \u230a\n        a\n        \u230b\n      \n    \n    {\\displaystyle \\lfloor a\\rfloor }\n  , then the empirical quantile function is given by,\n\n  \n    \n      \n        q\n        (\n        p\n        \n          /\n        \n        4\n        )\n        =\n        \n          x\n          \n            k\n          \n        \n        +\n        \u03b1\n        (\n        \n          x\n          \n            k\n            +\n            1\n          \n        \n        \u2212\n        \n          x\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle q(p/4)=x_{k}+\\alpha (x_{k+1}-x_{k})}\n  ,\nwhere \n  \n    \n      \n        k\n        =\n        \u230a\n        p\n        (\n        n\n        +\n        1\n        )\n        \n          /\n        \n        4\n        \u230b\n      \n    \n    {\\displaystyle k=\\lfloor p(n+1)/4\\rfloor }\n   and \n  \n    \n      \n        \u03b1\n        =\n        p\n        (\n        n\n        +\n        1\n        )\n        \n          /\n        \n        4\n        \u2212\n        \u230a\n        p\n        (\n        n\n        +\n        1\n        )\n        \n          /\n        \n        4\n        \u230b\n      \n    \n    {\\displaystyle \\alpha =p(n+1)/4-\\lfloor p(n+1)/4\\rfloor }\n  .To find the first, second, and third quartiles of the dataset we would evaluate \n  \n    \n      \n        q\n        (\n        0.25\n        )\n      \n    \n    {\\displaystyle q(0.25)}\n  , \n  \n    \n      \n        q\n        (\n        0.5\n        )\n      \n    \n    {\\displaystyle q(0.5)}\n  , and \n  \n    \n      \n        q\n        (\n        0.75\n        )\n      \n    \n    {\\displaystyle q(0.75)}\n   respectively.\n\n\n==== Example 1 ====\nOrdered Data Set: 6, 7, 15, 36, 39, 40, 41, 42, 43, 47, 49\n\n\n==== Example 2 ====\nOrdered Data Set: 7, 15, 36, 39, 40, 41\nAs there are an even number of data points, the first three methods all give the same results.\n\n\n=== Continuous probability distributions ===\n\nIf we define a continuous probability distributions as \n  \n    \n      \n        P\n        (\n        X\n        )\n      \n    \n    {\\displaystyle P(X)}\n   where \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is a real valued random variable, its cumulative distribution function (CDF) is given by,\n\n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        P\n        (\n        X\n        \u2264\n        x\n        )\n      \n    \n    {\\displaystyle F_{X}(x)=P(X\\leq x)}\n  .The CDF gives the probability that the random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is less than the value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  . Therefore, the first quartile is the value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   when \n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        0.25\n      \n    \n    {\\displaystyle F_{X}(x)=0.25}\n  , the second quartile is \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   when \n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        0.5\n      \n    \n    {\\displaystyle F_{X}(x)=0.5}\n  , and the third quartile is \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   when \n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        0.75\n      \n    \n    {\\displaystyle F_{X}(x)=0.75}\n  . The values of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   can be found with the quantile function \n  \n    \n      \n        Q\n        (\n        p\n        )\n      \n    \n    {\\displaystyle Q(p)}\n   where \n  \n    \n      \n        p\n        =\n        0.25\n      \n    \n    {\\displaystyle p=0.25}\n   for the first quartile, \n  \n    \n      \n        p\n        =\n        0.5\n      \n    \n    {\\displaystyle p=0.5}\n   for the second quartile, and \n  \n    \n      \n        p\n        =\n        0.75\n      \n    \n    {\\displaystyle p=0.75}\n   for the third quartile. The quantile function is the inverse of the cumulative distribution function if the cumulative distribution function is monotonically increasing.\n\n\n== Outliers ==\nThere are methods by which to check for outliers in the discipline of statistics and statistical analysis. Outliers could be a result from a shift in the location (mean) or in the scale (variability) of the process of interest. Outliers could also be evidence of a sample population that has a non-normal distribution or of a contaminated population data set. Consequently, as is the basic idea of descriptive statistics, when encountering an outlier, we have to explain this value by further analysis of the cause or origin of the outlier.  In cases of extreme observations, which are not an infrequent occurrence, the typical values must be analyzed.  In the case of quartiles, the Interquartile Range (IQR) may be used to characterize the data when there may be extremities that skew the data; the interquartile range is a relatively robust statistic (also sometimes called \"resistance\") compared to the range and standard deviation.  There is also a mathematical method to check for outliers and determining \"fences\", upper and lower limits from which to check for outliers.\nAfter determining the first and third quartiles and the interquartile range as outlined above, then fences are calculated using the following formula:\n\n  \n    \n      \n        \n          Lower fence\n        \n        =\n        \n          Q\n          \n            1\n          \n        \n        \u2212\n        1.5\n        (\n        \n          I\n          Q\n          R\n        \n        )\n        \n      \n    \n    {\\displaystyle {\\text{Lower fence}}=Q_{1}-1.5(\\mathrm {IQR} )\\,}\n  \n\n  \n    \n      \n        \n          Upper fence\n        \n        =\n        \n          Q\n          \n            3\n          \n        \n        +\n        1.5\n        (\n        \n          I\n          Q\n          R\n        \n        )\n        ,\n        \n      \n    \n    {\\displaystyle {\\text{Upper fence}}=Q_{3}+1.5(\\mathrm {IQR} ),\\,}\n  where Q1 and Q3 are the first and third quartiles, respectively.  The lower fence is the \"lower limit\" and the upper fence is the \"upper limit\" of data, and any data lying outside these defined bounds can be considered an outlier.  Anything below the Lower fence or above the Upper fence can be considered such a case.  The fences provide a guideline by which to define an outlier, which may be defined in other ways. The fences define a \"range\" outside which an outlier exists; a way to picture this is a boundary of a fence, outside which are \"outsiders\" as opposed to outliers. It is common for the lower and upper fences along with the outliers to be represented by a boxplot. For a boxplot, only the vertical heights correspond to the visualized data set while horizontal width of the box is irrelevant. Outliers located outside the fences in a boxplot can be marked as any choice of symbol, such as an \"x\" or \"o\". The fences are sometimes also referred to as \"whiskers\" while the entire plot visual is called a \"box-and-whisker\" plot.\nWhen spotting an outlier in the data set by calculating the interquartile ranges and boxplot features, it might be simple to mistakenly view it as evidence that the population is non-normal or that the sample is contaminated. However, this method should not take place of a hypothesis test for determining normality of the population. The significance of the outliers vary depending on the sample size. If the sample is small, then it is more probable to get interquartile ranges that are unrepresentatively small, leading to narrower fences. Therefore, it would be more likely to find data that are marked as outliers.\n\n\n== Computer software for quartiles ==\nExcel:\nThe Excel function QUARTILE(array, quart) provides the desired quartile value for a given array of data, using Method 3 from above. In the Quartile function, array is the dataset of numbers that is being analyzed and quart is any of the following 5 values depending on which quartile is being calculated. \nMATLAB:\nIn order to calculate quartiles in Matlab, the function quantile(A,p) can be used. Where A is the vector of data being analyzed and p is the percentage that relates to the quartiles as stated below. \n\n\n== See also ==\nFive-number summary\nRange\nBox plot\nInterquartile range\nSummary statistics\nQuantile\n\n\n== References ==\n\n\n== External links ==\nQuartile \u2013 from MathWorld Includes references and compares various methods to compute quartiles\nQuartiles \u2013 From MathForum.org\nQuartiles calculator \u2013 simple quartiles calculator\nQuartiles \u2013 An example how to calculate it", "Biclustering": "Biclustering, block clustering, Co-clustering or two-mode clustering is a data mining technique which allows simultaneous clustering of the rows and columns of a matrix.\nThe term was first introduced by Boris Mirkin to name a technique introduced many years earlier, in 1972, by John A. Hartigan.Given a set of \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   samples represented by an \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -dimensional feature vector, the entire dataset can be represented as \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   rows in \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   columns (i.e., an \n  \n    \n      \n        m\n        \u00d7\n        n\n      \n    \n    {\\displaystyle m\\times n}\n   matrix). The Biclustering algorithm generates Biclusters. A Bicluster is a subset of rows which exhibit similar behavior across a subset of columns, or vice versa.\n\n\n== Development ==\nBiclustering was originally introduced by John A. Hartigan in 1972. The term \"Biclustering\" was then later used and refined by Boris G. Mirkin. This algorithm was not generalized until 2000, when Y. Cheng and George M. Church proposed a biclustering algorithm based on variance and applied it to biological gene expression data.In 2001 and 2003, I. S. Dhillon published two algorithms applying biclustering to files and words. One version was based on bipartite spectral graph partitioning. The other was based on information theory. Dhillon assumed the loss of mutual information during biclustering was equal to the Kullback\u2013Leibler-distance (KL-distance) between P and Q. P represents the distribution of files and feature words before Biclustering, while Q is the distribution after Biclustering. KL-distance is for measuring the difference between two random distributions. KL = 0 when the two distributions are the same and KL increases as the difference increases. Thus, the aim of the algorithm was to find the minimum KL-distance between P and Q. In 2004, Arindam Banerjee used a weighted-Bregman distance instead of KL-distance to design a Biclustering algorithm that was suitable for any kind of matrix, unlike the KL-distance algorithm.To cluster more than two types of objects, in 2005, Bekkerman expanded the mutual information in Dhillon's theorem from a single pair into multiple pairs.\n\n\n== Complexity ==\nThe complexity of the Biclustering problem depends on the exact problem formulation, and particularly on the merit function used to evaluate the quality of a given Bicluster. However, the most interesting variants of this problem are NP-complete. NP-complete has two conditions. In the simple case that there is an only element a(i,j) either 0 or 1 in the binary matrix A, a Bicluster is equal to a biclique in the corresponding bipartite graph. The maximum size Bicluster is equivalent to the maximum edge biclique in the bipartite graph.  In the complex case, the element in matrix A is used to compute the quality of a given Bicluster and solve the more restricted version of the problem. It requires either large computational effort or the use of lossy heuristics to short-circuit the calculation.\n\n\n== Types of Biclusters ==\nBicluster with constant values (a)\nWhen a Biclustering algorithm tries to find a constant-value Bicluster, it reorders the rows and columns of the matrix to group together similar rows and columns, eventually grouping Biclusters with similar values. This method is sufficient when the data is normalized. \nA perfect constant Bicluster is a matrix(I,J) in which all values a(i,j) are equal to a given constant \u03bc. In tangible data, these entries a(i,j) may be represented with the form n(i,j) + \u03bc where n(i,j) denotes the noise. According to Hartigan's algorithm, by splitting the original data matrix into a set of Biclusters, variance is used to compute constant Biclusters. Hence, a perfect Bicluster may be equivalently defined as a matrix with a variance of zero. In order to prevent the partitioning of the data matrix into Biclusters with the only one row and one column; Hartigan assumes that there are, for example, K Biclusters within the data matrix. When the data matrix is partitioned into K Biclusters, the algorithm ends.\nBicluster with constant values on rows (b) or columns (c)\nUnlike the constant-value Biclusters, these types of Biclusters cannot be evaluated solely based on the variance of their values. To finish the identification, the columns and the rows should be normalized first. There are, however, other algorithms, without the normalization step, that can find Biclusters which have rows and columns with different approaches.\nBicluster with coherent values (d, e)\nFor Biclusters with coherent values on rows and columns, an overall improvement over the algorithms for Biclusters with constant values on rows or on columns should be considered. This algorithm may contain analysis of variance between groups, using co-variance between both rows and columns. In Cheng and Church's theorem, a Bicluster is defined as a subset of rows and columns with almost the same score. The similarity score is used to measure the coherence of rows and columns.\n\nThe relationship between these cluster models and other types of clustering such as correlation clustering is discussed in.\n\n\n== Algorithms ==\nThere are many Biclustering algorithms developed for bioinformatics, including: block clustering, CTWC (Coupled Two-Way Clustering), ITWC (Interrelated Two-Way Clustering), \u03b4-bicluster, \u03b4-pCluster, \u03b4-pattern, FLOC, OPC, Plaid Model, OPSMs (Order-preserving submatrixes), Gibbs, SAMBA (Statistical-Algorithmic Method for Bicluster Analysis), Robust Biclustering Algorithm (RoBA), Crossing Minimization, cMonkey, PRMs, DCC, LEB (Localize and Extract Biclusters), QUBIC (QUalitative BIClustering), BCCA (Bi-Correlation Clustering Algorithm) BIMAX, ISA and FABIA (Factor analysis for Bicluster Acquisition), runibic,\nand recently proposed hybrid method EBIC (evolutionary-based Biclustering), which was shown to detect multiple patterns with very high accuracy. More recently, IMMD-CC is proposed that is developed based on the iterative complexity reduction concept. IMMD-CC is able to identify co-cluster centroids from highly sparse transformation obtained by iterative multi-mode discretization.\nBiclustering algorithms have also been proposed and used in other application fields under the names co-clustering, bi-dimensional clustering, and subspace clustering.Given the known importance of discovering local patterns in time-series data. Recent proposals have addressed the Biclustering problem in the specific case of time-series gene expression data. In this case, the interesting Biclusters can be restricted to those with contiguous columns. This restriction leads to a tractable problem and enables the development of efficient exhaustive enumeration algorithms such as CCC-Biclustering and e-CCC-Biclustering. \nThe approximate patterns in CCC-Biclustering algorithms allow a given number of errors, per gene, relatively to an expression profile representing the expression pattern in the Bicluster. The e-CCC-Biclustering algorithm uses approximate expressions to find and report all maximal CCC-Bicluster's by a discretized matrix A and efficient string processing techniques.\nThese algorithms find and report all maximal Biclusters with coherent and contiguous columns with perfect/approximate expression patterns, in time linear/polynomial which is obtained by manipulating a discretized version of original expression matrix in the size of the time-series gene expression matrix using efficient string processing techniques based on suffix trees. These algorithms are also applied to solve problems and sketch the analysis of computational complexity.\nSome recent algorithms have attempted to include additional support for Biclustering rectangular matrices in the form of other datatypes, including cMonkey.\nThere is an ongoing debate about how to judge the results of these methods, as Biclustering allows overlap between clusters and some algorithms allow the exclusion of hard-to-reconcile columns/conditions. Not all of the available algorithms are deterministic and the analyst must pay attention to the degree to which results represent stable minima. Because this is an unsupervised classification problem, the lack of a gold standard makes it difficult to spot errors in the results. One approach is to utilize multiple Biclustering algorithms, with the majority or super-majority voting amongst them to decide the best result. Another way is to analyze the quality of shifting and scaling patterns in Biclusters. Biclustering has been used in the domain of text mining (or classification) which is popularly known as co-clustering. Text corpora are represented in a vectoral form as a matrix D whose rows denote the documents and whose columns denote the words in the dictionary. Matrix elements Dij denote occurrence of word j in document i. Co-clustering algorithms are then applied to discover blocks in D that correspond to a group of documents (rows) characterized by a group of words(columns).\nText clustering can solve the high-dimensional sparse problem, which means clustering text and words at the same time. When clustering text, we need to think about not only the words information, but also the information of words clusters that was composed by words. Then, according to similarity of feature words in the text, will eventually cluster the feature words. This is called co-clustering. There are two advantages of co-clustering: one is clustering the test based on words clusters can extremely decrease the dimension of clustering, it can also appropriate to measure the distance between the tests. Second is mining more useful information and can get the corresponding information in test clusters and words clusters. This corresponding information can be used to describe the type of texts and words, at the same time, the result of words clustering can be also used to text mining and information retrieval.\nSeveral approaches have been proposed based on the information contents of the resulting blocks: matrix-based approaches such as SVD and BVD, and graph-based approaches. Information-theoretic algorithms iteratively assign each row to a cluster of documents and each column to a cluster of words such that the mutual information is maximized. Matrix-based methods focus on the decomposition of matrices into blocks such that the error between the original matrix and the regenerated matrices from the decomposition is minimized.  Graph-based methods tend to minimize the cuts between the clusters. Given two groups of documents d1 and d2, the number of cuts can be measured as the number of words that occur in documents of groups d1 and d2.\nMore recently (Bisson and Hussain) have proposed a new approach of using the similarity between words and the similarity between documents to co-cluster the matrix. Their method (known as \u03c7-Sim, for cross similarity) is based on finding document-document similarity and word-word similarity, and then using classical clustering methods such as hierarchical clustering. Instead of explicitly clustering rows and columns alternately, they consider higher-order occurrences of words, inherently taking into account the documents in which they occur. Thus, the similarity between two words is calculated based on the documents in which they occur and also the documents in which \"similar\" words occur. The idea here is that two documents about the same topic do not necessarily use the same set of words to describe it, but a subset of the words and other similar words that are characteristic of that topic. This approach of taking higher-order similarities takes the latent semantic structure of the whole corpus into consideration with the result of generating a better clustering of the documents and words.\nIn text databases, for a document collection defined by a document by term D matrix (of size m by n, m: number of documents, n: number of terms) the cover-coefficient based clustering methodology yields the same number of clusters both for documents and terms (words) using a double-stage probability experiment. According to the cover coefficient concept number of clusters can also be roughly estimated by the following formula \n  \n    \n      \n        (\n        m\n        \u00d7\n        n\n        )\n        \n          /\n        \n        t\n      \n    \n    {\\displaystyle (m\\times n)/t}\n   where t is the number of non-zero entries in D. Note that in D each row and each column must contain at least one non-zero element.\nIn contrast to other approaches, FABIA is a multiplicative model that assumes realistic non-Gaussian signal distributions with heavy tails. FABIA utilizes well understood model selection techniques like variational approaches and applies the Bayesian framework. The generative framework allows FABIA to determine the information content of each Bicluster to separate spurious Biclusters from true Biclusters.\n\n\n== See also ==\nFormal concept analysis\nBiclique\nGalois connection\n\n\n== References ==\n\n\n=== Others ===\nN.K. Verma, S. Bajpai, A. Singh, A. Nagrare, S. Meena, Yan Cui, \"A Comparison of Biclustering Algorithms\" in International conference on Systems in Medicine and Biology (ICSMB 2010)in IIT Kharagpur India, pp. 90\u201397, Dec. 16\u201318.\nJ. Gupta, S. Singh and N.K. Verma \"MTBA: MATLAB Toolbox for Biclustering Analysis\", IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions\", IIT Kanpur India, pp. 148\u2013152, Jul. 2013.\nA. Tanay. R. Sharan, and R. Shamir, \"Biclustering Algorithms: A Survey\", In Handbook of Computational Molecular Biology, Edited by Srinivas Aluru, Chapman (2004)\nKluger Y, Basri R, Chang JT, Gerstein MB (2003). \"Spectral Biclustering of Microarray Data: Coclustering Genes and Conditions\". Genome Research. 13 (4): 703\u2013716. doi:10.1101/gr.648603. PMC 430175. PMID 12671006.\nAdetayo Kasim, Ziv Shkedy, Sebastian Kaiser, Sepp Hochreiter, Willem Talloen (2016), Applied Biclustering Methods for Big and High-Dimensional Data Using R, Chapman & Hall/CRC Press\nOrzechowski, P., Sipper, M., Huang, X., & Moore, J. H. (2018). EBIC: an evolutionary-based parallel biclustering algorithm for pattern discovery. Bioinformatics.\n\n\n== External links ==\nFABIA: Factor Analysis for Bicluster Acquisition, an R package \u2014software", "Query language": "A query language, also known as data query language or database query language (DQL), is a computer language used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL).\n\n\n== Types ==\nBroadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry. Other types of query languages include:\n\nFull-text. The simplest query language is treating all terms as bag of words that are to be matched with the postings in the inverted index and where subsequently ranking models are applied to retrieve the most relevant documents. Only tokens are defined in the CFG. Web search engines often use this approach.\nBoolean. A query language that also supports the use of the Boolean operators AND, OR, NOT.\nStructured. A language that supports searching within (a combination of) fields when a document is structured and has been indexed using its document structure.\nNatural language. A query language that supports natural language by parsing the natural language query to a form that can be best used to retrieve relevant documents, for example with Question answering systems or conversational search.\n\n\n== Examples ==\nAttempto Controlled English is a query language that is also a controlled natural language.\nAQL is a query language for the ArangoDB native multi-model database system.\n.QL is a proprietary object-oriented query language for querying relational databases; successor of Datalog;\nCodeQL is the analysis engine used by developers to automate security checks, and by security researchers to perform variant analysis on GitHub.\nContextual Query Language (CQL) a formal language for representing queries to information retrieval systems such as web indexes or bibliographic catalogues.\nCypher is a query language for the Neo4j graph database;\nDMX is a query language for data mining models;\nDatalog is a query language for deductive databases;\nF-logic is a declarative object-oriented language for deductive databases and knowledge representation.\nFQL enables you to use a SQL-style interface to query the data exposed by the Graph API. It provides advanced features not available in the Graph API.\nGellish English is a language that can be used for queries in Gellish English Databases, for dialogues (requests and responses) as well as for information modeling and knowledge modeling;\nGremlin is an Apache Software Foundation graph traversal language for OLTP and OLAP graph systems.\nGraphQL is a data query language developed by Facebook as an alternate to REST and ad-hoc webservice architectures.\nHTSQL is a query language that translates HTTP queries to SQL;\nISBL is a query language for PRTV, one of the earliest relational database management systems;\nJaql is a functional data processing and query language most commonly used for JSON query processing;\njq is a functional programming language often used for processing queries against one or more JSON documents, including very large ones;\nJSONiq is a declarative query language designed for collections of JSON documents;\nLDAP is an application protocol for querying and modifying directory services running over TCP/IP;\nLogiQL is a variant of Datalog and is the query language for the LogicBlox system.\nM Formula language, a mashup query language used in Microsoft's Power Query\nMQL is a cheminformatics query language for a substructure search allowing beside nominal properties also numerical properties;\nMDX is a query language for OLAP databases;\nN1QL is a Couchbase's query language finding data in Couchbase Servers;\nObject Query Language\nOCL (Object Constraint Language). Despite its name, OCL is also an object query language and an OMG standard;\nOPath, intended for use in querying WinFS Stores;\nPoliqarp Query Language is a special query language designed to analyze annotated text. Used in the Poliqarp search engine;\nPQL is a special-purpose programming language for managing process models based on information about scenarios that these models describe;\nPRQL PRQL (Pipelined Relational Query Language) is a modern language for transforming data. Consists of a curated set of orthogonal transformations, which are combined together to form a pipeline.\nPTQL based on relational queries over program traces, allowing programmers to write expressive, declarative queries about program behavior.\nQUEL is a relational database access language, similar in most ways to SQL;\nRDQL is a RDF query language;\nSMARTS is the cheminformatics standard for a substructure search;\nSPARQL is a query language for RDF graphs;\nSQL is a well known query language and data manipulation language for relational databases;\nXQuery is a query language for XML data sources;\nXPath is a declarative language for navigating XML documents;\nYQL is an SQL-like query language created by Yahoo!\nSearch engine query languages, e.g., as used by Google or Bing\n\n\n== See also ==\nData control language\nData definition language\nData manipulation language\nPath expression\n\n\n== References ==", "Maxima and minima": "In mathematical analysis, the maximum and minimum of a function are, respectively, the largest and smallest value taken by the function. Known generically as extremum, they may be defined either within a given range (the local or relative extrema) or on the entire domain (the global or absolute extrema) of a function. Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.\nAs defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.\nIn statistics, the corresponding concept is the sample maximum and minimum.\n\n\n== Definition ==\nA real-valued function f defined on a domain X has a global (or absolute) maximum point at x\u2217, if f(x\u2217) \u2265 f(x) for all x in X. Similarly, the function has a global (or absolute) minimum point at x\u2217, if f(x\u2217) \u2264 f(x) for all x in X. The value of the function at a maximum point is called the maximum value of the function, denoted \n  \n    \n      \n        max\n        (\n        f\n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\max(f(x))}\n  , and the value of the function at a minimum point is called the minimum value of the function. Symbolically, this can be written as follows:\n\n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{0}\\in X}\n   is a global maximum point of function \n  \n    \n      \n        f\n        :\n        X\n        \u2192\n        \n          R\n        \n        ,\n      \n    \n    {\\displaystyle f:X\\to \\mathbb {R} ,}\n   if \n  \n    \n      \n        (\n        \u2200\n        x\n        \u2208\n        X\n        )\n        \n        f\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        \u2265\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle (\\forall x\\in X)\\,f(x_{0})\\geq f(x).}\n  The definition of global minimum point also proceeds similarly.\nIf the domain X is a metric space, then f is said to have a local (or relative) maximum point at the point x\u2217, if there exists some \u03b5 > 0 such that f(x\u2217) \u2265 f(x) for all x in X within distance \u03b5 of x\u2217. Similarly, the function has a local minimum point at x\u2217, if f(x\u2217) \u2264 f(x) for all x in X within distance \u03b5 of x\u2217. A similar definition can be used when X is a topological space, since the definition just given can be rephrased in terms of neighbourhoods. Mathematically, the given definition is written as follows:\n\nLet \n  \n    \n      \n        (\n        X\n        ,\n        \n          d\n          \n            X\n          \n        \n        )\n      \n    \n    {\\displaystyle (X,d_{X})}\n   be a metric space and function \n  \n    \n      \n        f\n        :\n        X\n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f:X\\to \\mathbb {R} }\n  . Then \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{0}\\in X}\n   is a local maximum point of function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   if \n  \n    \n      \n        (\n        \u2203\n        \u03b5\n        >\n        0\n        )\n      \n    \n    {\\displaystyle (\\exists \\varepsilon >0)}\n   such that \n  \n    \n      \n        (\n        \u2200\n        x\n        \u2208\n        X\n        )\n        \n        \n          d\n          \n            X\n          \n        \n        (\n        x\n        ,\n        \n          x\n          \n            0\n          \n        \n        )\n        <\n        \u03b5\n        \n        \u27f9\n        \n        f\n        (\n        \n          x\n          \n            0\n          \n        \n        )\n        \u2265\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle (\\forall x\\in X)\\,d_{X}(x,x_{0})<\\varepsilon \\implies f(x_{0})\\geq f(x).}\n  The definition of local minimum point can also proceed similarly.\nIn both the global and local cases, the concept of a strict extremum can be defined. For example, x\u2217 is a strict global maximum point if for all x in X with x \u2260 x\u2217, we have f(x\u2217) > f(x), and x\u2217 is a strict local maximum point if there exists some \u03b5 > 0 such that, for all x in X within distance \u03b5 of x\u2217 with x \u2260 x\u2217, we have f(x\u2217) > f(x). Note that a point is a strict global maximum point if and only if it is the unique global maximum point, and similarly for minimum points.\nA continuous real-valued function with a compact domain always has a maximum point and a minimum point. An important example is a function whose domain is a closed and bounded interval of real numbers (see the graph above).\n\n\n== Search ==\nFinding global maxima and minima is the goal of mathematical optimization. If a function is continuous on a closed interval, then by the extreme value theorem, global maxima and minima exist. Furthermore, a global maximum (or minimum) either must be a local maximum (or minimum) in the interior of the domain, or must lie on the boundary of the domain. So a method of finding a global maximum (or minimum) is to look at all the local maxima (or minima) in the interior, and also look at the maxima (or minima) of the points on the boundary, and take the largest (or smallest) one.\nFor differentiable functions, Fermat's theorem states that local extrema in the interior of a domain must occur at critical points (or points where the derivative equals zero). However, not all critical points are extrema. One can distinguish whether a critical point is a local maximum or local minimum by using the first derivative test, second derivative test, or higher-order derivative test, given sufficient differentiability.For any function that is defined piecewise, one finds a maximum (or minimum) by finding the maximum (or minimum) of each piece separately, and then seeing which one is largest (or smallest).\n\n\n== Examples ==\n\nFor a practical example, assume a situation where someone has \n  \n    \n      \n        200\n      \n    \n    {\\displaystyle 200}\n   feet of fencing and is trying to maximize the square footage of a rectangular enclosure, where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is the length, \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is the width, and \n  \n    \n      \n        x\n        y\n      \n    \n    {\\displaystyle xy}\n   is the area:\n\n  \n    \n      \n        2\n        x\n        +\n        2\n        y\n        =\n        200\n      \n    \n    {\\displaystyle 2x+2y=200}\n  \n\n  \n    \n      \n        2\n        y\n        =\n        200\n        \u2212\n        2\n        x\n      \n    \n    {\\displaystyle 2y=200-2x}\n  \n\n  \n    \n      \n        \n          \n            \n              2\n              y\n            \n            2\n          \n        \n        =\n        \n          \n            \n              200\n              \u2212\n              2\n              x\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {2y}{2}}={\\frac {200-2x}{2}}}\n  \n\n  \n    \n      \n        y\n        =\n        100\n        \u2212\n        x\n      \n    \n    {\\displaystyle y=100-x}\n  \n\n  \n    \n      \n        x\n        y\n        =\n        x\n        (\n        100\n        \u2212\n        x\n        )\n      \n    \n    {\\displaystyle xy=x(100-x)}\n  The derivative with respect to \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    d\n                    \n                      d\n                      x\n                    \n                  \n                \n                x\n                y\n              \n              \n                \n                =\n                \n                  \n                    d\n                    \n                      d\n                      x\n                    \n                  \n                \n                x\n                (\n                100\n                \u2212\n                x\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    d\n                    \n                      d\n                      x\n                    \n                  \n                \n                \n                  (\n                  \n                    100\n                    x\n                    \u2212\n                    \n                      x\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                100\n                \u2212\n                2\n                x\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {d}{dx}}xy&={\\frac {d}{dx}}x(100-x)\\\\&={\\frac {d}{dx}}\\left(100x-x^{2}\\right)\\\\&=100-2x\\end{aligned}}}\n  Setting this equal to \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n\n  \n    \n      \n        0\n        =\n        100\n        \u2212\n        2\n        x\n      \n    \n    {\\displaystyle 0=100-2x}\n  \n\n  \n    \n      \n        2\n        x\n        =\n        100\n      \n    \n    {\\displaystyle 2x=100}\n  \n\n  \n    \n      \n        x\n        =\n        50\n      \n    \n    {\\displaystyle x=50}\n  reveals that \n  \n    \n      \n        x\n        =\n        50\n      \n    \n    {\\displaystyle x=50}\n   is our only critical point.\nNow retrieve the endpoints by determining the interval to which \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is restricted. Since width is positive, then \n  \n    \n      \n        x\n        >\n        0\n      \n    \n    {\\displaystyle x>0}\n  , and since \n  \n    \n      \n        x\n        =\n        100\n        \u2212\n        y\n      \n    \n    {\\displaystyle x=100-y}\n  , that implies that \n  \n    \n      \n        x\n        <\n        100\n      \n    \n    {\\displaystyle x<100}\n  .\nPlug in critical point \n  \n    \n      \n        50\n      \n    \n    {\\displaystyle 50}\n  , as well as endpoints \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   and \n  \n    \n      \n        100\n      \n    \n    {\\displaystyle 100}\n  , into \n  \n    \n      \n        x\n        y\n        =\n        x\n        (\n        100\n        \u2212\n        x\n        )\n      \n    \n    {\\displaystyle xy=x(100-x)}\n  , and the results are \n  \n    \n      \n        2500\n        ,\n        0\n        ,\n      \n    \n    {\\displaystyle 2500,0,}\n   and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   respectively.\nTherefore, the greatest area attainable with a rectangle of \n  \n    \n      \n        200\n      \n    \n    {\\displaystyle 200}\n   feet of fencing is \n  \n    \n      \n        50\n        \u00d7\n        50\n        =\n        2500\n      \n    \n    {\\displaystyle 50\\times 50=2500}\n  .\n\n\n== Functions of more than one variable ==\n\nFor functions of more than one variable, similar conditions apply. For example, in the (enlargeable) figure on the right, the necessary conditions for a local maximum are similar to those of a function with only one variable. The first partial derivatives as to z (the variable to be maximized) are zero at the maximum (the glowing dot on top in the figure).  The second partial derivatives are negative. These are only necessary, not sufficient, conditions for a local maximum, because of the possibility of a saddle point. For use of these conditions to solve for a maximum, the function z must also be differentiable throughout. The second partial derivative test can help classify the point as a relative maximum or relative minimum.\nIn contrast, there are substantial differences between functions of one variable and functions of more than one variable in the identification of global extrema. For example, if a bounded differentiable function f defined on a closed interval in the real line has a single critical point, which is a local minimum, then it is also a global minimum (use the intermediate value theorem and Rolle's theorem to prove this by contradiction). In two and more dimensions, this argument fails. This is illustrated by the function\n\n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n        (\n        1\n        \u2212\n        x\n        \n          )\n          \n            3\n          \n        \n        ,\n        \n        x\n        ,\n        y\n        \u2208\n        \n          R\n        \n        ,\n      \n    \n    {\\displaystyle f(x,y)=x^{2}+y^{2}(1-x)^{3},\\qquad x,y\\in \\mathbb {R} ,}\n  whose only critical point is at (0,0), which is a local minimum with f(0,0) = 0. However, it cannot be a global one, because f(2,3) = \u22125.\n\n\n== Maxima or minima of a functional ==\nIf the domain of a function for which an extremum is to be found consists itself of functions (i.e. if an extremum is to be found of a functional), then the extremum is found using the calculus of variations.\n\n\n== In relation to sets ==\nMaxima and minima can also be defined for sets. In general, if an ordered set S has a greatest element m, then m is a maximal element of the set, also denoted as \n  \n    \n      \n        max\n        (\n        S\n        )\n      \n    \n    {\\displaystyle \\max(S)}\n  . Furthermore, if S is a subset of an ordered set T and m is the greatest element of S with (respect to order induced by T), then m is a least upper bound of S in T. Similar results hold for least element, minimal element and greatest lower bound. The maximum and minimum function for sets are used in databases, and can be computed rapidly, since the maximum (or minimum) of a set can be computed from the maxima of a partition; formally, they are self-decomposable aggregation functions.\nIn the case of a general partial order, the least element (i.e., one that is smaller than all others) should not be confused with a minimal element (nothing is smaller). Likewise, a greatest element of a partially ordered set (poset) is an upper bound of the set which is contained within the set, whereas a maximal element m of a poset A is an element of A such that if m \u2264 b (for any b in A), then m = b. Any least element or greatest element of a poset is unique, but a poset can have several minimal or maximal elements.  If a poset has more than one maximal element, then these elements will not be mutually comparable.\nIn a totally ordered set, or chain, all elements are mutually comparable, so such a set can have at most one minimal element and at most one maximal element.  Then, due to mutual comparability, the minimal element will also be the least element, and the maximal element will also be the greatest element. Thus in a totally ordered set, we can simply use the terms minimum and maximum. \nIf a chain is finite, then it will always have a maximum and a minimum.  If a chain is infinite, then it need not have a maximum or a minimum.  For example, the set of natural numbers has no maximum, though it has a minimum. If an infinite chain S is bounded, then the closure Cl(S) of the set occasionally has a minimum and a maximum, in which case they are called the greatest lower bound and the least upper bound of the set S, respectively.\n\n\n== Argument of the maximum ==\n\n\n== See also ==\nDerivative test\nInfimum and supremum\nLimit superior and limit inferior\nMaximum-minimums identity\nMechanical equilibrium\nMex (mathematics)\nSample maximum and minimum\nSaddle point\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nThomas Simpson's work on Maxima and Minima at Convergence\nApplication of Maxima and Minima with sub pages of solved problems\nJolliffe, Arthur Ernest (1911). \"Maxima and Minima\" . Encyclop\u00e6dia Britannica. Vol. 17 (11th ed.). pp. 918\u2013920.", "Null hypothesis": "In scientific research, the null hypothesis (often denoted H0) is the claim that no relationship exists between two sets of data or variables being analyzed. The null hypothesis is that any experimentally observed difference is due to chance alone, and an underlying causative relationship does not exist, hence the term \"null\". In addition to the null hypothesis, an alternative hypothesis is also developed, which claims that a relationship does exist between two variables.\n\n\n== Basic definitions ==\nThe null hypothesis and the alternative hypothesis are types of conjectures used in statistical tests, which are formal methods of reaching conclusions or making decisions on the basis of data.  The hypotheses are conjectures about a statistical model of the population, which are based on a sample of the population.  The tests are core elements of statistical inference, heavily used in the interpretation of scientific experimental data, to separate scientific claims from statistical noise.\n\"The statement being tested in a test of statistical significance is called the null hypothesis.  The test of significance is designed to assess the strength of the evidence against the null hypothesis.  Usually, the null hypothesis is a statement of 'no effect' or 'no difference'.\"  It is often symbolized as H0.\nThe statement that is being tested against the null hypothesis is the alternative hypothesis.  Symbols include H1 and Ha.\nStatistical significance test: \"Very roughly, the procedure for deciding goes like this: Take a random sample from the population.  If the sample data are consistent with the null hypothesis, then do not reject the null hypothesis; if the sample data are inconsistent with the null hypothesis, then reject the null hypothesis and conclude that the alternative hypothesis is true.\"The following adds context and nuance to the basic definitions.\nGiven the test scores of two random samples, one of men and one of women, does one group differ from the other? A possible null hypothesis is that the mean male score is the same as the mean female score:\n\nH0: \u03bc1 = \u03bc2where\n\nH0 = the null hypothesis,\n\u03bc1 = the mean of population 1, and\n\u03bc2 = the mean of population 2.A stronger null hypothesis is that the two samples are drawn from the same population, such that the variances and shapes of the distributions are also equal.\n\n\n== Terminology ==\n\nSimple hypothesis\nAny hypothesis which specifies the population distribution completely.  For such a hypothesis the sampling distribution of any statistic is a function of the sample size alone.\nComposite hypothesis\nAny hypothesis which does not specify the population distribution completely.  Example: A hypothesis specifying a normal distribution with a specified mean and an unspecified variance.The simple/composite distinction was made by Neyman and Pearson.\nExact hypothesis\nAny hypothesis that specifies an exact parameter value.  Example: \u03bc = 100.  Synonym: point hypothesis.\nInexact hypothesis\nThose specifying a parameter range or interval.  Examples: \u03bc \u2264 100; 95 \u2264 \u03bc \u2264 105.Fisher required an exact null hypothesis for testing (see the quotations below).\nA  one-tailed hypothesis (tested using a one-sided test) is an inexact hypothesis in which the value of a parameter is specified as being either:\n\nabove or equal to a certain value, or\nbelow or equal to a certain value.A one-tailed hypothesis is said to have directionality.\nFisher's original (lady tasting tea) example was a one-tailed test.  The null hypothesis was asymmetric.  The probability of guessing all cups correctly was the same as guessing all cups incorrectly, but Fisher noted that only guessing correctly was compatible with the lady's claim.\n\n\n== Technical description ==\nThe null hypothesis is a default hypothesis that a quantity to be measured is zero (null). Typically, the quantity to be measured is the difference between two situations. For instance, trying to determine if there is a positive proof that an effect has occurred or that samples derive from different batches.The null hypothesis states that a quantity (of interest) is larger or equal to zero and smaller or equal to zero. If either requirement can be positively overturned, the null hypothesis is \"excluded from the realm of possibilities\".\nThe null hypothesis is generally assumed to remain possibly true. Multiple analyses can be performed to show how the hypothesis should either be rejected or excluded e.g. having a high confidence level, thus demonstrating a statistically significant difference. This is demonstrated by showing that zero is outside of the specified confidence interval of the measurement on either side, typically within the real numbers. Failure to exclude the null hypothesis (with any confidence) does not logically confirm or support the (unprovable) null hypothesis. (When it is proven that something is e.g. bigger than x, it does not necessarily imply it is plausible that it is smaller or equal than x; it may instead be a poor quality measurement with low accuracy. Confirming the null hypothesis two-sided would amount to positively proving it is bigger or equal than 0 and to positively proving it is smaller or equal than 0; this is something for which infinite accuracy is needed as well as exactly zero effect, neither of which normally are realistic. Also measurements will never indicate a non-zero probability of exactly zero difference.) So failure of an exclusion of a null hypothesis amounts to a \"don't know\" at the specified confidence level; it does not immediately imply null somehow, as the data may already show a (less strong) indication for a non-null. The used confidence level does absolutely certainly not correspond to the likelihood of null at failing to exclude; in fact in this case a high used confidence level expands the still plausible range.\nA non-null hypothesis can have the following meanings, depending on the author a) a value other than zero is used, b) some margin other than zero is used and c) the \"alternative\" hypothesis.Testing (excluding or failing to exclude) the null hypothesis provides evidence that there are (or are not) statistically sufficient grounds to believe there is a relationship between two phenomena (e.g., that a potential treatment has a non-zero effect, either way). Testing the null hypothesis is a central task in statistical hypothesis testing in the modern practice of science. There are precise criteria for excluding or not excluding a null hypothesis at a certain confidence level. The confidence level should indicate the likelihood that much more and better data would still be able to exclude the null hypothesis on the same side.The concept of a null hypothesis is used differently in two approaches to statistical inference. In the significance testing approach of Ronald Fisher, a null hypothesis is rejected if the observed data are significantly unlikely to have occurred if the null hypothesis were true. In this case, the null hypothesis is rejected and an alternative hypothesis is accepted in its place. If the data are consistent with the null hypothesis statistically possibly true, then the null hypothesis is not rejected. In neither case is the null hypothesis or its alternative proven; with better or more data, the null may still be rejected. This is analogous to the legal principle of presumption of innocence, in which a suspect or defendant is assumed to be innocent (null is not rejected) until proven guilty (null is rejected) beyond a reasonable doubt (to a statistically significant degree).In the hypothesis testing approach of Jerzy Neyman and Egon Pearson, a null hypothesis is contrasted with an alternative hypothesis, and the two hypotheses are distinguished on the basis of data, with certain error rates. It is used in formulating answers in research.\nStatistical inference can be done without a null hypothesis, by specifying a statistical model corresponding to each candidate hypothesis, and by using model selection techniques to choose the most appropriate model.  (The most common selection techniques are based on either Akaike information criterion or Bayes factor).\n\n\n== Principle ==\nHypothesis testing requires constructing a statistical model of what the data would look like if chance or random processes alone were responsible for the results. The hypothesis that chance alone is responsible for the results is called the null hypothesis. The model of the result of the random process is called the distribution under the null hypothesis. The obtained results are compared with the distribution under the null hypothesis, and the likelihood of finding the obtained results is thereby determined.Hypothesis testing works by collecting data and measuring how likely the particular set of data is (assuming the null hypothesis is true), when the study is on a randomly selected representative sample. The null hypothesis assumes no relationship between variables in the population from which the sample is selected.If the data-set of a randomly selected representative sample is very unlikely relative to the null hypothesis (defined as being part of a class of sets of data that only rarely will be observed), the experimenter rejects the null hypothesis, concluding it (probably) is false. This class of data-sets is usually specified via a test statistic, which is designed to measure the extent of apparent departure from the null hypothesis. The procedure works by assessing whether the observed departure, measured by the test statistic, is larger than a value defined, so that the probability of occurrence of a more extreme value is small under the null hypothesis (usually in less than either 5% or 1% of similar data-sets in which the null hypothesis does hold).\nIf the data do not contradict the null hypothesis, then only a weak conclusion can be made: namely, that the observed data set provides insufficient evidence against the null hypothesis. In this case, because the null hypothesis could be true or false, in some contexts this is interpreted as meaning that the data give insufficient evidence to make any conclusion, while in other contexts, it is interpreted as meaning that there is not sufficient evidence to support changing from a currently useful regime to a different one. Nevertheless, if at this point the effect appears likely and/or large enough, there may be an incentive to further investigate, such as running a bigger sample.\nFor instance, a certain drug may reduce the risk of having a heart attack. Possible null hypotheses are \"this drug does not reduce the risk of having a heart attack\" or \"this drug has no effect on the risk of having a heart attack\". The test of the hypothesis consists of administering the drug to half of the people in a study group as a controlled experiment. If the data show a statistically significant change in the people receiving the drug, the null hypothesis is rejected.\n\n\n== Goals of null hypothesis tests ==\nThere are many types of significance tests for one, two or more samples, for means, variances and proportions, paired or unpaired data, for different distributions, for large and small samples; all have null hypotheses.  There are also at least four goals of null hypotheses for significance tests:\nTechnical null hypotheses are used to verify statistical assumptions.  For example, the residuals between the data and a statistical model cannot be distinguished from random noise.  If true, there is no justification for complicating the model.\nScientific null assumptions are used to directly advance a theory.  For example, the angular momentum of the universe is zero.  If not true, the theory of the early universe may need revision.\nNull hypotheses of homogeneity are used to verify that multiple experiments are producing consistent results.  For example, the effect of a medication on the elderly is consistent with that of the general adult population.  If true, this strengthens the general effectiveness conclusion and simplifies recommendations for use.\nNull hypotheses that assert the equality of effect of two or more alternative treatments, for example, a drug and a placebo, are used to reduce scientific claims based on statistical noise.  This is the most popular null hypothesis; It is so popular that many statements about significant testing assume such null hypotheses.Rejection of the null hypothesis is not necessarily the real goal of a significance tester.  An adequate statistical model may be associated with a failure to reject the null; the model is adjusted until the null is not rejected.  The numerous uses of significance testing were well known to Fisher who discussed many in his book written a decade before defining the null hypothesis.A statistical significance test shares much mathematics with a confidence interval.  They are mutually illuminating.  A result is often significant when there is confidence in the sign of a relationship (the interval does not include 0).  Whenever the sign of a relationship is important, statistical significance is a worthy goal.  This also reveals weaknesses of significance testing: A result can be significant without a good estimate of the strength of a relationship; significance can be a modest goal.  A weak relationship can also achieve significance with enough data.  Reporting both significance and confidence intervals is commonly recommended.\nThe varied uses of significance tests reduce the number of generalizations that can be made about all applications.\n\n\n== Choice of the null hypothesis ==\nThe choice of the null hypothesis is associated with sparse and inconsistent advice.  Fisher mentioned few constraints on the choice and stated that many null hypotheses should be considered and that many tests are possible for each.  The variety of applications and the diversity of goals suggests that the choice can be complicated. In many applications the formulation of the test is traditional.  A familiarity with the range of tests available may suggest a particular null hypothesis and test.  Formulating the null hypothesis is not automated (though the calculations of significance testing usually are).  Sir David Cox said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".A statistical significance test is intended to test a hypothesis.  If the hypothesis summarizes a set of data, there is no value in testing the hypothesis on that set of data.  Example: If a study of last year's weather reports indicates that rain in a region falls primarily on weekends, it is only valid to test that null hypothesis on weather reports from any other year.  Testing hypotheses suggested by the data is circular reasoning that proves nothing; It is a special limitation on the choice of the null hypothesis.\nA routine procedure is as follows: Start from the scientific hypothesis.  Translate this to a statistical alternative hypothesis and proceed: \"Because Ha expresses the effect that we wish to find evidence for, we often begin with Ha and then set up H0 as the statement that the hoped-for effect is not present.\"  This advice is reversed for modeling applications where we hope not to find evidence against the null.\nA complex case example is as follows:  The gold standard in clinical research is the randomized placebo-controlled double-blind clinical trial.  But testing a new drug against a (medically ineffective) placebo may be unethical for a serious illness.  Testing a new drug against an older medically effective drug raises fundamental philosophical issues regarding the goal of the test and the motivation of the experimenters.  The standard \"no difference\" null hypothesis may reward the pharmaceutical company for gathering inadequate data.  \"Difference\" is a better null hypothesis in this case, but statistical significance is not an adequate criterion for reaching a nuanced conclusion which requires a good numeric estimate of the drug's effectiveness.  A \"minor\" or \"simple\" proposed change in the null hypothesis ((new vs old) rather than (new vs placebo)) can have a dramatic effect on the utility of a test for complex non-statistical reasons.\n\n\n=== Directionality ===\n\nThe choice of null hypothesis (H0) and consideration of directionality (see \"one-tailed test\") is critical.\n\n\n==== Tailedness of the null-hypothesis test ====\nConsider the question of whether a tossed coin is fair (i.e. that on average it lands heads up 50% of the time) and an experiment where you toss the coin 5 times.\nA possible result of the experiment that we consider here is 5 heads. \nLet outcomes be considered unlikely with respect to an assumed distribution if their probability is lower than a significance threshold of 0.05.\nA potential null hypothesis implying a one-tail test is \"this coin is not biased toward heads\". \nBeware that, in this context, the word \"tail\" takes two meanings: either as outcome of a single toss, or as region of extremal values in a probability distribution.\nIndeed, with a fair coin the probability of this experiment outcome is 1/25 = 0.031, which would be even lower if the coin were biased in favour of tails. \nTherefore, the observations are not likely enough for the null hypothesis to hold, and the test refutes it. \nSince the coin is ostensibly neither fair nor biased toward tails, the conclusion of the experiment is that the coin is biased towards heads.\nAlternatively, a null hypothesis implying a two-tailed test is \"this coin is fair\". \nThis one null hypothesis could be examined by looking out for either too many tails or too many heads in the experiments.\nThe outcomes that would tend to refuse this null hypothesis are those with a large number of heads or a large number of tails, and our experiment with 5 heads would seem to belong to this class.\nHowever, the probability of 5 tosses of the same kind, irrespective of whether these are head or tails, is twice as much as that of the 5-head occurrence singly considered.\nHence, under this two-tailed null hypothesis, the observation receives a probability value of 0.063.\nHence again, with the same significance threshold used for the one-tailed test (0.05), the same outcome is not statistically significant.\nTherefore, the two-tailed null hypothesis will be preserved in this case, not supporting the conclusion reached with the single-tailed null hypothesis, that the coin is biased towards heads.\nThis example illustrates that the conclusion reached from a statistical test may depend on the precise formulation of the null and alternative hypotheses.\n\n\n==== Discussion ====\nFisher said, \"the null hypothesis must be exact, that is free of vagueness and ambiguity, because it must supply the basis of the 'problem of distribution,' of which the test of significance is the solution\", implying a more restrictive domain for H0. According to this view, the null hypothesis must be numerically exact\u2014it must state that a particular quantity or difference is equal to a particular number. In classical science, it is most typically the statement that there is no effect of a particular treatment; in observations, it is typically that there is no difference between the value of a particular measured variable and that of a prediction.\nMost statisticians believe that it is valid to state direction as a part of null hypothesis, or as part of a null hypothesis/alternative hypothesis pair. However, the results are not a full description of all the results of an experiment, merely a single result tailored to one particular purpose. For example, consider an H0 that claims the population mean for a new treatment is an improvement on a well-established treatment with population mean = 10 (known from long experience), with the one-tailed alternative being that the new treatment's mean > 10. If the sample evidence obtained through x-bar equals \u2212200 and the corresponding t-test statistic equals \u221250, the conclusion from the test would be that there is no evidence that the new treatment is better than the existing one: it would not report that it is markedly worse, but that is not what this particular test is looking for. To overcome any possible ambiguity in reporting the result of the test of a null hypothesis, it is best to indicate whether the test was two-sided and, if one-sided, to include the direction of the effect being tested.\nThe statistical theory required to deal with the simple cases of directionality dealt with here, and more complicated ones, makes use of the concept of an unbiased test.\nThe directionality of hypotheses is not always obvious.  The explicit null hypothesis of Fisher's Lady tasting tea example was that the Lady had no such ability, which led to a symmetric probability distribution.  The one-tailed nature of the test resulted from the one-tailed alternate hypothesis (a term not used by Fisher).  The null hypothesis became implicitly one-tailed.  The logical negation of the Lady's one-tailed claim was also one-tailed.  (Claim: Ability > 0; Stated null: Ability = 0;  Implicit null: Ability \u2264 0).\nPure arguments over the use of one-tailed tests are complicated by the variety of tests.  Some tests (for instance the  \u03c72 goodness of fit test) are inherently one-tailed.  Some probability distributions are asymmetric.  The traditional tests of 3 or more groups are two-tailed.\nAdvice concerning the use of one-tailed hypotheses has been inconsistent and accepted practice varies among fields.  The greatest objection to one-tailed hypotheses is their potential subjectivity.  A non-significant result can sometimes be converted to a significant result by the use of a one-tailed hypothesis (as the fair coin test, at the whim of the analyst).  The flip side of the argument: One-sided tests are less likely to ignore a real effect.  One-tailed tests can suppress the publication of data that differs in sign from predictions.  Objectivity was a goal of the developers of statistical tests.\nIt is a common practice to use a one-tailed hypothesis by default. However, \"If you do not have a specific direction firmly in mind in advance, use a two-sided alternative.  Moreover, some users of statistics argue that we should always work with the two-sided alternative.\"One alternative to this advice is to use three-outcome tests.  It eliminates the issues surrounding directionality of hypotheses by testing twice, once in each direction and combining the results to produce three possible outcomes.  Variations on this approach have a history, being suggested perhaps 10 times since 1950.Disagreements over one-tailed tests flow from the philosophy of science.  While Fisher was willing to ignore the unlikely case of the Lady guessing all cups of tea incorrectly (which may have been appropriate for the circumstances), medicine believes that a proposed treatment that kills patients is significant in every sense and should be reported and perhaps explained.  Poor statistical reporting practices have contributed to disagreements over one-tailed tests.  Statistical significance resulting from two-tailed tests is insensitive to the sign of the relationship; Reporting significance alone is inadequate.  \"The treatment has an effect\" is the uninformative result of a two-tailed test.  \"The treatment has a beneficial effect\" is the more informative result of a one-tailed test.  \"The treatment has an effect, reducing the average length of hospitalization by 1.5 days\" is the most informative report, combining a two-tailed significance test result with a numeric estimate of the relationship between treatment and effect.  Explicitly reporting a numeric result eliminates a philosophical advantage of a one-tailed test.  An underlying issue is the appropriate form of an experimental science without numeric predictive theories: A model of numeric results is more informative than a model of effect signs (positive, negative or unknown) which is more informative than a model of simple significance (non-zero or unknown); in the absence of numeric theory signs may suffice.\n\n\n== History of statistical tests ==\n\nThe history of the null and alternative hypotheses is embedded in the history of statistical tests.\nBefore 1925:  There are occasional transient traces of statistical tests for centuries in the past, which provide early examples of null hypotheses.  In the late 19th century statistical significance was defined.  In the early 20th century important probability distributions were defined. Gossett and Pearson worked on specific cases of significance testing.\n1925:  Fisher published the first edition of Statistical Methods for Research Workers which defined the statistical significance test and made it a mainstream method of analysis for much of experimental science.  The text was devoid of proofs and weak on explanations, but it was filled with real examples.  It placed statistical practice in the sciences well in advance of published statistical theory.\n1933:  In a series of papers (published over a decade starting in 1928) Neyman & Pearson defined the statistical hypothesis test as a proposed improvement on Fisher's test.  The papers provided much of the terminology for statistical tests including alternative hypothesis and H0 as a hypothesis to be tested using observational data (with H1, H2... as alternatives).  Neyman did not use the term null hypothesis in later writings about his method.\n1935:  Fisher published the first edition of the book The Design of Experiments which introduced the null hypothesis (by example rather than by definition) and carefully explained the rationale for significance tests in the context of the interpretation of experimental results; see quotations regarding the null hypothesis.\nFollowing:  Fisher and Neyman  quarreled over the relative merits of their competing formulations until Fisher's death in 1962.  Career changes and World War II ended the partnership of Neyman and Pearson.  The formulations were merged by relatively anonymous textbook writers, experimenters (journal editors) and mathematical statisticians without input from the principals.  The subject today combines much of the terminology and explanatory power of Neyman & Pearson with the scientific philosophy and calculations provided by Fisher.  Whether statistical testing is properly one subject or two remains a source of disagreement.  Sample of two: One text refers to the subject as hypothesis testing (with no mention of significance testing in the index) while another says significance testing (with a section on inference as a decision).  Fisher developed significance testing as a flexible tool for researchers to weigh their evidence.  Instead testing has become institutionalized.  Statistical significance has become a rigidly defined and enforced criterion for the publication of experimental results in many scientific journals.  In some fields significance testing has become the dominant and nearly exclusive form of statistical analysis.  As a consequence the limitations of the tests have been exhaustively studied.  Books have been filled with the collected criticism of significance testing.\n\n\n== See also ==\nBayes factor\nBurden of proof\nCounternull\nEstimation statistics\nLikelihood-ratio test\nPresumption of innocence\nStatistical hypothesis testing\nP-value\n\n\n== References ==\n\n\n== Further reading ==\nAd\u00e8r, H. J.; Mellenbergh, G. J. & Hand, D. J. (2007). Advising on research methods: A consultant's companion. Huizen, The Netherlands: Johannes van Kessel Publishing. ISBN 978-90-79418-01-5.\nEfron, B. (2004). \"Large-Scale Simultaneous Hypothesis Testing\". Journal of the American Statistical Association. 99 (465): 96\u2013104. doi:10.1198/016214504000000089. S2CID 1520711.  The application of significance testing in this paper is an outlier.  Tests to find a null hypothesis?  Not trying to show significance, but to find interesting cases?\nRice, William R.; Gaines, Steven D. (June 1994). \"'Heads I win, tails you lose': testing directional alternative hypotheses in ecological and evolutionary research\". TREE. 9 (6): 235\u2013237. doi:10.1016/0169-5347(94)90258-5. PMID 21236837.  Directed tests combine the attributes of one-tailed and two-tailed tests.  \"...directed tests should be used in virtually all applications where one-sided tests have previously been used, excepting those cases where the data can only deviate from H0, in one direction.\"\n\n\n== External links ==\nHyperStat Online: Null hypothesis", "Expectation\u2013maximization algorithm": "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n\n\n== History ==\nThe EM algorithm was explained and given its name in a classic 1977 paper by Arthur Dempster, Nan Laird, and Donald Rubin. They pointed out that the method had been \"proposed many times in special circumstances\" by earlier authors. One of the earliest is the gene-counting method for estimating allele frequencies by Cedric Smith.  Another was proposed by H.O. Hartley in 1958, and Hartley and Hocking in 1977, from which many of the ideas in the Dempster-Laird-Rubin paper originated. Another one by S.K Ng, Thriyambakam Krishnan and G.J McLachlan in 1977. Hartley\u2019s ideas can be broadened to any grouped discrete distribution. A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers, following his collaboration with Per Martin-L\u00f6f and Anders Martin-L\u00f6f. The Dempster\u2013Laird\u2013Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. The Dempster\u2013Laird\u2013Rubin paper established the EM method as an important tool of statistical analysis. See also Meng and van Dyk (1997).\nThe convergence analysis of the Dempster\u2013Laird\u2013Rubin algorithm was flawed and a correct convergence analysis was published by C. F. Jeff Wu in 1983.\nWu's proof established the EM method's convergence also outside of the exponential family, as claimed by Dempster\u2013Laird\u2013Rubin.\n\n\n== Introduction ==\nThe EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly.  Typically these models involve latent variables in addition to unknown parameters and known data observations.  That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.\nFinding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.\nThe EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points.  It's not obvious that this will work, but it can be proven in this context. Additionally, it can be proven that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a local maximum or a saddle point. In general, multiple maxima may occur, with no guarantee that the global maximum will be found.  Some likelihoods also have singularities in them, i.e., nonsensical maxima.  For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.\n\n\n== Description ==\n\n\n=== The symbols ===\nGiven the statistical model which generates a set \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   of observed data, a set of unobserved latent data or missing values \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n  , and a vector of unknown parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n  , along with a likelihood function \n  \n    \n      \n        L\n        (\n        \n          \u03b8\n        \n        ;\n        \n          X\n        \n        ,\n        \n          Z\n        \n        )\n        =\n        p\n        (\n        \n          X\n        \n        ,\n        \n          Z\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle L({\\boldsymbol {\\theta }};\\mathbf {X} ,\\mathbf {Z} )=p(\\mathbf {X} ,\\mathbf {Z} \\mid {\\boldsymbol {\\theta }})}\n  , the maximum likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data\n\n  \n    \n      \n        L\n        (\n        \n          \u03b8\n        \n        ;\n        \n          X\n        \n        )\n        =\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        =\n        \u222b\n        p\n        (\n        \n          X\n        \n        ,\n        \n          Z\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        \n        d\n        \n          Z\n        \n        =\n        \u222b\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          Z\n        \n        ,\n        \n          \u03b8\n        \n        )\n        p\n        (\n        \n          Z\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        \n        d\n        \n          Z\n        \n      \n    \n    {\\displaystyle L({\\boldsymbol {\\theta }};\\mathbf {X} )=p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})=\\int p(\\mathbf {X} ,\\mathbf {Z} \\mid {\\boldsymbol {\\theta }})\\,d\\mathbf {Z} =\\int p(\\mathbf {X} \\mid \\mathbf {Z} ,{\\boldsymbol {\\theta }})p(\\mathbf {Z} \\mid {\\boldsymbol {\\theta }})\\,d\\mathbf {Z} }\n  However, this quantity is often intractable since \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   is unobserved and the distribution of \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   is unknown before attaining \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n  .\n\n\n=== The EM algorithm ===\nThe EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:\n\nExpectation step (E step): Define \n  \n    \n      \n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})}\n   as the expected value of the log likelihood function of \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n  , with respect to the current conditional distribution of \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   given \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   and the current estimates of the parameters \n  \n    \n      \n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}^{(t)}}\n  :\n\n  \n    \n      \n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        =\n        \n          E\n          \n            \n              Z\n            \n            \u223c\n            p\n            (\n            \u22c5\n            \n              |\n            \n            \n              X\n            \n            ,\n            \n              \n                \u03b8\n              \n              \n                (\n                t\n                )\n              \n            \n            )\n          \n        \n        \u2061\n        \n          [\n          \n            log\n            \u2061\n            p\n            (\n            \n              X\n            \n            ,\n            \n              Z\n            \n            \n              |\n            \n            \n              \u03b8\n            \n            )\n          \n          ]\n        \n        \n      \n    \n    {\\displaystyle Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})=\\operatorname {E} _{\\mathbf {Z} \\sim p(\\cdot |\\mathbf {X} ,{\\boldsymbol {\\theta }}^{(t)})}\\left[\\log p(\\mathbf {X} ,\\mathbf {Z} |{\\boldsymbol {\\theta }})\\right]\\,}\n  Maximization step (M step): Find the parameters that maximize this quantity:\n\n  \n    \n      \n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            \u03b8\n          \n        \n         \n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}^{(t+1)}={\\underset {\\boldsymbol {\\theta }}{\\operatorname {arg\\,max} }}\\ Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})\\,}\n  More succinctly, we can write it as one equation:\n\n\n=== Interpretation of the variables ===\nThe typical models to which EM is applied use \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   as a latent variable indicating membership in one of a set of groups:\n\nThe observed data points \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   may be discrete (taking values in a finite or countably infinite set) or continuous (taking values in an uncountably infinite set). Associated with each data point may be a vector of observations.\nThe missing values (aka latent variables) \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   are discrete, drawn from a fixed number of values, and with one latent variable per observed unit.\nThe parameters are continuous, and are of two kinds: Parameters that are associated with all data points, and those associated with a specific value of a latent variable (i.e., associated with all data points whose corresponding latent variable has that value).However, it is possible to apply EM to other sorts of models.\nThe motivation is as follows.  If the value of the parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   is known, usually the value of the latent variables \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   can be found by maximizing the log-likelihood over all possible values of \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n  , either simply by iterating over \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   or through an algorithm such as the Viterbi algorithm for hidden Markov models.  Conversely, if we know the value of the latent variables \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n  , we can find an estimate of the parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group.  This suggests an iterative algorithm, in the case where both \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   and \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   are unknown:\n\nFirst, initialize the parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   to some random values.\nCompute the probability of each possible value of \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   , given \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n  .\nThen, use the just-computed values of \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   to compute a better estimate for the parameters \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n  .\nIterate steps 2 and 3 until convergence.The algorithm as just described monotonically approaches a local minimum of the cost function.\n\n\n== Properties ==\nAlthough an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a maximum likelihood estimator. For multimodal distributions, this means that an EM algorithm may converge to a local maximum of the observed data likelihood function, depending on starting values. A variety of heuristic or metaheuristic approaches exist to escape a local maximum, such as random-restart hill climbing (starting with several different random initial estimates \n  \n    \n      \n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}^{(t)}}\n  ), or applying simulated annealing methods.\nEM is especially useful when the likelihood is an exponential family, see Sundberg (2019, Ch. 8) for a comprehensive treatment: the E step becomes the sum of expectations of sufficient statistics, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive closed-form expression updates for each step, using the Sundberg formula (proved and published by Rolf Sundberg, based on unpublished results of Per Martin-L\u00f6f and Anders Martin-L\u00f6f).The EM method was modified to compute maximum a posteriori (MAP) estimates for Bayesian inference in the original paper by Dempster, Laird, and Rubin.\nOther methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gauss\u2013Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.\n\n\n== Proof of correctness ==\nExpectation-Maximization works to improve \n  \n    \n      \n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})}\n   rather than directly improving \n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})}\n  .  Here it is shown that improvements to the former imply improvements to the latter.For any \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   with non-zero probability \n  \n    \n      \n        p\n        (\n        \n          Z\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }})}\n  , we can write\n\n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        =\n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        ,\n        \n          Z\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        \u2212\n        log\n        \u2061\n        p\n        (\n        \n          Z\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \u03b8\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})=\\log p(\\mathbf {X} ,\\mathbf {Z} \\mid {\\boldsymbol {\\theta }})-\\log p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }}).}\n  We take the expectation over possible values of the unknown data \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n   under the current parameter estimate \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(t)}}\n   by multiplying both sides by \n  \n    \n      \n        p\n        (\n        \n          Z\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }}^{(t)})}\n   and summing (or integrating) over \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n  .  The left-hand side is the expectation of a constant, so we get:\n\n  \n    \n      \n        \n          \n            \n              \n                log\n                \u2061\n                p\n                (\n                \n                  X\n                \n                \u2223\n                \n                  \u03b8\n                \n                )\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    \n                      Z\n                    \n                  \n                \n                p\n                (\n                \n                  Z\n                \n                \u2223\n                \n                  X\n                \n                ,\n                \n                  \n                    \u03b8\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n                log\n                \u2061\n                p\n                (\n                \n                  X\n                \n                ,\n                \n                  Z\n                \n                \u2223\n                \n                  \u03b8\n                \n                )\n                \u2212\n                \n                  \u2211\n                  \n                    \n                      Z\n                    \n                  \n                \n                p\n                (\n                \n                  Z\n                \n                \u2223\n                \n                  X\n                \n                ,\n                \n                  \n                    \u03b8\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n                log\n                \u2061\n                p\n                (\n                \n                  Z\n                \n                \u2223\n                \n                  X\n                \n                ,\n                \n                  \u03b8\n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                Q\n                (\n                \n                  \u03b8\n                \n                \u2223\n                \n                  \n                    \u03b8\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n                +\n                H\n                (\n                \n                  \u03b8\n                \n                \u2223\n                \n                  \n                    \u03b8\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})&=\\sum _{\\mathbf {Z} }p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }}^{(t)})\\log p(\\mathbf {X} ,\\mathbf {Z} \\mid {\\boldsymbol {\\theta }})-\\sum _{\\mathbf {Z} }p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }}^{(t)})\\log p(\\mathbf {Z} \\mid \\mathbf {X} ,{\\boldsymbol {\\theta }})\\\\&=Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})+H({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)}),\\end{aligned}}}\n  where \n  \n    \n      \n        H\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle H({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})}\n   is defined by the negated sum it is replacing.\nThis last equation holds for every value of \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   including \n  \n    \n      \n        \n          \u03b8\n        \n        =\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}={\\boldsymbol {\\theta }}^{(t)}}\n  ,\n\n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        =\n        Q\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        +\n        H\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }}^{(t)})=Q({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)})+H({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)}),}\n  and subtracting this last equation from the previous equation gives\n\n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        \u2212\n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        =\n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \u2212\n        Q\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        +\n        H\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \u2212\n        H\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})-\\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }}^{(t)})=Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})-Q({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)})+H({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})-H({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)}).}\n  However, Gibbs' inequality tells us that \n  \n    \n      \n        H\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \u2265\n        H\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle H({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})\\geq H({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)})}\n  , so we can conclude that\n\n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n        \u2212\n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \u2265\n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        \u2212\n        Q\n        (\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})-\\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }}^{(t)})\\geq Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})-Q({\\boldsymbol {\\theta }}^{(t)}\\mid {\\boldsymbol {\\theta }}^{(t)}).}\n  In words, choosing \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\theta }}}\n   to improve \n  \n    \n      \n        Q\n        (\n        \n          \u03b8\n        \n        \u2223\n        \n          \n            \u03b8\n          \n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})}\n   causes \n  \n    \n      \n        log\n        \u2061\n        p\n        (\n        \n          X\n        \n        \u2223\n        \n          \u03b8\n        \n        )\n      \n    \n    {\\displaystyle \\log p(\\mathbf {X} \\mid {\\boldsymbol {\\theta }})}\n   to improve at least as much.\n\n\n== As a maximization\u2013maximization procedure ==\nThe EM algorithm can be viewed as two alternating maximization steps, that is, as an example of coordinate descent. Consider the function:\n\n  \n    \n      \n        F\n        (\n        q\n        ,\n        \u03b8\n        )\n        :=\n        \n          E\n          \n            q\n          \n        \n        \u2061\n        [\n        log\n        \u2061\n        L\n        (\n        \u03b8\n        ;\n        x\n        ,\n        Z\n        )\n        ]\n        +\n        H\n        (\n        q\n        )\n        ,\n      \n    \n    {\\displaystyle F(q,\\theta ):=\\operatorname {E} _{q}[\\log L(\\theta ;x,Z)]+H(q),}\n  where q is an arbitrary probability distribution over the unobserved data z and H(q) is the entropy of the distribution q. This function can be written as\n\n  \n    \n      \n        F\n        (\n        q\n        ,\n        \u03b8\n        )\n        =\n        \u2212\n        \n          D\n          \n            \n              K\n              L\n            \n          \n        \n        \n          \n            (\n          \n        \n        q\n        \u2225\n        \n          p\n          \n            Z\n            \u2223\n            X\n          \n        \n        (\n        \u22c5\n        \u2223\n        x\n        ;\n        \u03b8\n        )\n        \n          \n            )\n          \n        \n        +\n        log\n        \u2061\n        L\n        (\n        \u03b8\n        ;\n        x\n        )\n        ,\n      \n    \n    {\\displaystyle F(q,\\theta )=-D_{\\mathrm {KL} }{\\big (}q\\parallel p_{Z\\mid X}(\\cdot \\mid x;\\theta ){\\big )}+\\log L(\\theta ;x),}\n  where  \n  \n    \n      \n        \n          p\n          \n            Z\n            \u2223\n            X\n          \n        \n        (\n        \u22c5\n        \u2223\n        x\n        ;\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p_{Z\\mid X}(\\cdot \\mid x;\\theta )}\n   is the conditional distribution of the unobserved data given the observed data \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and \n  \n    \n      \n        \n          D\n          \n            K\n            L\n          \n        \n      \n    \n    {\\displaystyle D_{KL}}\n   is the Kullback\u2013Leibler divergence.\nThen the steps in the EM algorithm may be viewed as:\n\nExpectation step: Choose \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   to maximize \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  :\n\n  \n    \n      \n        \n          q\n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          \n            a\n            r\n            g\n            \n            m\n            a\n            x\n          \n          \n            q\n          \n        \n        \u2061\n         \n        F\n        (\n        q\n        ,\n        \n          \u03b8\n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle q^{(t)}=\\operatorname {arg\\,max} _{q}\\ F(q,\\theta ^{(t)})}\n  \nMaximization step: Choose \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   to maximize \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  :\n\n  \n    \n      \n        \n          \u03b8\n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            a\n            r\n            g\n            \n            m\n            a\n            x\n          \n          \n            \u03b8\n          \n        \n        \u2061\n         \n        F\n        (\n        \n          q\n          \n            (\n            t\n            )\n          \n        \n        ,\n        \u03b8\n        )\n      \n    \n    {\\displaystyle \\theta ^{(t+1)}=\\operatorname {arg\\,max} _{\\theta }\\ F(q^{(t)},\\theta )}\n  \n\n\n== Applications ==\nEM is frequently used for parameter estimation of mixed models, notably in quantitative genetics.In psychometrics, EM is an important tool for estimating item parameters and latent abilities of item response theory models.\nWith the ability to deal with missing data and observe unidentified variables, EM is becoming a useful tool to price and manage risk of a portfolio.The EM algorithm (and its faster variant ordered subset expectation maximization) is also widely used in medical image reconstruction, especially in positron emission tomography, single-photon emission computed tomography, and x-ray computed tomography. See below for other faster variants of EM.\nIn structural engineering, the Structural Identification using Expectation Maximization (STRIDE) algorithm is an output-only method for identifying natural vibration properties of a structural system using sensor data (see Operational Modal Analysis).\nEM is also used for data clustering. In natural language processing, two prominent instances of the algorithm are the Baum\u2013Welch algorithm for hidden Markov models, and the inside-outside algorithm for unsupervised induction of probabilistic context-free grammars.\nIn the analysis of intertrade waiting times i.e. the time between subsequent trades in shares of stock at a stock exchange the EM algorithm has proved to be very useful.\n\n\n== Filtering and smoothing EM algorithms ==\nA Kalman filter is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.\nFiltering and smoothing EM algorithms arise by repeating this two-step procedure:\n\nE-step\nOperate a Kalman filter or a minimum-variance smoother designed with current parameter estimates to obtain updated state estimates.M-step\nUse the filtered or smoothed state estimates within maximum-likelihood calculations to obtain updated parameter estimates.Suppose that a Kalman filter or minimum-variance smoother operates on measurements of a single-input-single-output system that possess additive white noise. An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            v\n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            (\n            \n              z\n              \n                k\n              \n            \n            \u2212\n            \n              \n                \n                  \n                    x\n                    ^\n                  \n                \n              \n              \n                k\n              \n            \n            )\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}_{v}^{2}={\\frac {1}{N}}\\sum _{k=1}^{N}{(z_{k}-{\\widehat {x}}_{k})}^{2},}\n  where \n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {x}}_{k}}\n   are scalar output estimates calculated by a filter or a smoother from N scalar measurements \n  \n    \n      \n        \n          z\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle z_{k}}\n  . The above update can also be applied to updating a Poisson measurement noise intensity. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            w\n          \n          \n            2\n          \n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    ^\n                  \n                \n              \n              \n                k\n                +\n                1\n              \n            \n            \u2212\n            \n              \n                \n                  F\n                  ^\n                \n              \n            \n            \n              \n                \n                  \n                    x\n                    ^\n                  \n                \n              \n              \n                k\n              \n            \n            )\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\widehat {\\sigma }}_{w}^{2}={\\frac {1}{N}}\\sum _{k=1}^{N}{({\\widehat {x}}_{k+1}-{\\widehat {F}}{\\widehat {x}}_{k})}^{2},}\n  where \n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {x}}_{k}}\n   and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            k\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {x}}_{k+1}}\n   are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via\n\n  \n    \n      \n        \n          \n            \n              F\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  k\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                \n                  (\n                  \n                    \n                      \n                        \n                          x\n                          ^\n                        \n                      \n                    \n                    \n                      k\n                      +\n                      1\n                    \n                  \n                  \u2212\n                  \n                    \n                      \n                        F\n                        ^\n                      \n                    \n                  \n                  \n                    \n                      \n                        \n                          x\n                          ^\n                        \n                      \n                    \n                    \n                      k\n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n            \n              \n                \u2211\n                \n                  k\n                  =\n                  1\n                \n                \n                  N\n                \n              \n              \n                \n                  \n                    \n                      x\n                      ^\n                    \n                  \n                \n                \n                  k\n                \n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {F}}={\\frac {\\sum _{k=1}^{N}{({\\widehat {x}}_{k+1}-{\\widehat {F}}{\\widehat {x}}_{k})}^{2}}{\\sum _{k=1}^{N}{\\widehat {x}}_{k}^{2}}}.}\n  The convergence of parameter estimates such as those above are well studied.\n\n\n== Variants ==\nA number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using conjugate gradient and modified Newton's methods (Newton\u2013Raphson). Also, EM can be used with constrained estimation methods.\nParameter-expanded expectation maximization (PX-EM) algorithm often provides speed up by \"us[ing] a `covariance adjustment' to correct the analysis of the M step, capitalising on extra information captured in the imputed complete data\".Expectation conditional maximization (ECM) replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter \u03b8i is maximized individually, conditionally on the other parameters remaining fixed. Itself can be extended into the Expectation conditional maximization either (ECME) algorithm.This idea is further extended in generalized expectation maximization (GEM) algorithm, in which is sought only an increase in the objective function F for both the E step and M step as described in the As a maximization\u2013maximization procedure section. GEM is further developed in a distributed environment and shows promising results.It is also possible to consider the EM algorithm as a subclass of the MM (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm, and therefore use any machinery developed in the more general case.\n\n\n=== \u03b1-EM algorithm ===\nThe Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the \u03b1-log likelihood ratio. Then, the \u03b1-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the \u03b1-log likelihood ratio and the \u03b1-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the \u03b1-EM algorithm\nwhich contains the log-EM algorithm as its subclass. Thus, the \u03b1-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The \u03b1-EM shows faster convergence than the log-EM algorithm by choosing an appropriate \u03b1. The \u03b1-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm \u03b1-HMM.\n\n\n== Relation to variational Bayes methods ==\nEM is a partially non-Bayesian, maximum likelihood method.  Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for \u03b8 (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over \u03b8 and the latent variables.  The Bayesian approach to inference is simply to treat \u03b8 as another latent variable.  In this paradigm, the distinction between the E and M steps disappears.  If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including \u03b8) and optimize them one at a time.  Now, k steps per iteration are needed, where k is the number of latent variables.  For graphical models this is easy to do as each variable's new Q depends only on its Markov blanket, so local message passing can be used for efficient inference.\n\n\n== Geometric interpretation ==\n\nIn information geometry, the E step and the M step are interpreted as projections under dual affine connections, called the e-connection and the m-connection; the Kullback\u2013Leibler divergence can also be understood in these terms.\n\n\n== Examples ==\n\n\n=== Gaussian mixture ===\n\nLet \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            x\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {x} =(\\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots ,\\mathbf {x} _{n})}\n   be a sample of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   independent observations from a mixture of two multivariate normal distributions of dimension \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  , and let \n  \n    \n      \n        \n          z\n        \n        =\n        (\n        \n          z\n          \n            1\n          \n        \n        ,\n        \n          z\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          z\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{n})}\n   be the latent variables that determine the component from which the observation originates.\n\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        \u2223\n        (\n        \n          Z\n          \n            i\n          \n        \n        =\n        1\n        )\n        \u223c\n        \n          \n            \n              N\n            \n          \n          \n            d\n          \n        \n        (\n        \n          \n            \u03bc\n          \n          \n            1\n          \n        \n        ,\n        \n          \u03a3\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle X_{i}\\mid (Z_{i}=1)\\sim {\\mathcal {N}}_{d}({\\boldsymbol {\\mu }}_{1},\\Sigma _{1})}\n   and \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        \u2223\n        (\n        \n          Z\n          \n            i\n          \n        \n        =\n        2\n        )\n        \u223c\n        \n          \n            \n              N\n            \n          \n          \n            d\n          \n        \n        (\n        \n          \n            \u03bc\n          \n          \n            2\n          \n        \n        ,\n        \n          \u03a3\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle X_{i}\\mid (Z_{i}=2)\\sim {\\mathcal {N}}_{d}({\\boldsymbol {\\mu }}_{2},\\Sigma _{2}),}\n  where\n\n  \n    \n      \n        P\n        \u2061\n        (\n        \n          Z\n          \n            i\n          \n        \n        =\n        1\n        )\n        =\n        \n          \u03c4\n          \n            1\n          \n        \n        \n      \n    \n    {\\displaystyle \\operatorname {P} (Z_{i}=1)=\\tau _{1}\\,}\n   and \n  \n    \n      \n        P\n        \u2061\n        (\n        \n          Z\n          \n            i\n          \n        \n        =\n        2\n        )\n        =\n        \n          \u03c4\n          \n            2\n          \n        \n        =\n        1\n        \u2212\n        \n          \u03c4\n          \n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {P} (Z_{i}=2)=\\tau _{2}=1-\\tau _{1}.}\n  The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:\n\n  \n    \n      \n        \u03b8\n        =\n        \n          \n            (\n          \n        \n        \n          \u03c4\n        \n        ,\n        \n          \n            \u03bc\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \u03bc\n          \n          \n            2\n          \n        \n        ,\n        \n          \u03a3\n          \n            1\n          \n        \n        ,\n        \n          \u03a3\n          \n            2\n          \n        \n        \n          \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\theta ={\\big (}{\\boldsymbol {\\tau }},{\\boldsymbol {\\mu }}_{1},{\\boldsymbol {\\mu }}_{2},\\Sigma _{1},\\Sigma _{2}{\\big )},}\n  where the incomplete-data likelihood function is\n\n  \n    \n      \n        L\n        (\n        \u03b8\n        ;\n        \n          x\n        \n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            2\n          \n        \n        \n          \u03c4\n          \n            j\n          \n        \n         \n        f\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ;\n        \n          \n            \u03bc\n          \n          \n            j\n          \n        \n        ,\n        \n          \u03a3\n          \n            j\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle L(\\theta ;\\mathbf {x} )=\\prod _{i=1}^{n}\\sum _{j=1}^{2}\\tau _{j}\\ f(\\mathbf {x} _{i};{\\boldsymbol {\\mu }}_{j},\\Sigma _{j}),}\n  and the complete-data likelihood function is\n\n  \n    \n      \n        L\n        (\n        \u03b8\n        ;\n        \n          x\n        \n        ,\n        \n          z\n        \n        )\n        =\n        p\n        (\n        \n          x\n        \n        ,\n        \n          z\n        \n        \u2223\n        \u03b8\n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \u220f\n          \n            j\n            =\n            1\n          \n          \n            2\n          \n        \n         \n        [\n        f\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ;\n        \n          \n            \u03bc\n          \n          \n            j\n          \n        \n        ,\n        \n          \u03a3\n          \n            j\n          \n        \n        )\n        \n          \u03c4\n          \n            j\n          \n        \n        \n          ]\n          \n            \n              I\n            \n            (\n            \n              z\n              \n                i\n              \n            \n            =\n            j\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle L(\\theta ;\\mathbf {x} ,\\mathbf {z} )=p(\\mathbf {x} ,\\mathbf {z} \\mid \\theta )=\\prod _{i=1}^{n}\\prod _{j=1}^{2}\\ [f(\\mathbf {x} _{i};{\\boldsymbol {\\mu }}_{j},\\Sigma _{j})\\tau _{j}]^{\\mathbb {I} (z_{i}=j)},}\n  or\n\n  \n    \n      \n        L\n        (\n        \u03b8\n        ;\n        \n          x\n        \n        ,\n        \n          z\n        \n        )\n        =\n        exp\n        \u2061\n        \n          {\n          \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              \u2211\n              \n                j\n                =\n                1\n              \n              \n                2\n              \n            \n            \n              I\n            \n            (\n            \n              z\n              \n                i\n              \n            \n            =\n            j\n            )\n            \n              \n                [\n              \n            \n            log\n            \u2061\n            \n              \u03c4\n              \n                j\n              \n            \n            \u2212\n            \n              \n                \n                  1\n                  2\n                \n              \n            \n            log\n            \u2061\n            \n              |\n            \n            \n              \u03a3\n              \n                j\n              \n            \n            \n              |\n            \n            \u2212\n            \n              \n                \n                  1\n                  2\n                \n              \n            \n            (\n            \n              \n                x\n              \n              \n                i\n              \n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                j\n              \n            \n            \n              )\n              \n                \u22a4\n              \n            \n            \n              \u03a3\n              \n                j\n              \n              \n                \u2212\n                1\n              \n            \n            (\n            \n              \n                x\n              \n              \n                i\n              \n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                j\n              \n            \n            )\n            \u2212\n            \n              \n                \n                  d\n                  2\n                \n              \n            \n            log\n            \u2061\n            (\n            2\n            \u03c0\n            )\n            \n              \n                ]\n              \n            \n          \n          }\n        \n        ,\n      \n    \n    {\\displaystyle L(\\theta ;\\mathbf {x} ,\\mathbf {z} )=\\exp \\left\\{\\sum _{i=1}^{n}\\sum _{j=1}^{2}\\mathbb {I} (z_{i}=j){\\big [}\\log \\tau _{j}-{\\tfrac {1}{2}}\\log |\\Sigma _{j}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{j})^{\\top }\\Sigma _{j}^{-1}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{j})-{\\tfrac {d}{2}}\\log(2\\pi ){\\big ]}\\right\\},}\n  where \n  \n    \n      \n        \n          I\n        \n      \n    \n    {\\displaystyle \\mathbb {I} }\n   is an indicator function and \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is the probability density function of a multivariate normal.\nIn the last equality, for each i, one indicator \n  \n    \n      \n        \n          I\n        \n        (\n        \n          z\n          \n            i\n          \n        \n        =\n        j\n        )\n      \n    \n    {\\displaystyle \\mathbb {I} (z_{i}=j)}\n   is equal to zero, and one indicator is equal to one. The inner sum thus reduces to one term.\n\n\n==== E step ====\nGiven our current estimate of the parameters \u03b8(t), the conditional distribution of the Zi is determined by Bayes theorem to be the proportional height of the normal density weighted by \u03c4:\n\n  \n    \n      \n        \n          T\n          \n            j\n            ,\n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        :=\n        P\n        \u2061\n        (\n        \n          Z\n          \n            i\n          \n        \n        =\n        j\n        \u2223\n        \n          X\n          \n            i\n          \n        \n        =\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ;\n        \n          \u03b8\n          \n            (\n            t\n            )\n          \n        \n        )\n        =\n        \n          \n            \n              \n                \u03c4\n                \n                  j\n                \n                \n                  (\n                  t\n                  )\n                \n              \n               \n              f\n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              ;\n              \n                \n                  \u03bc\n                \n                \n                  j\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              ,\n              \n                \u03a3\n                \n                  j\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              )\n            \n            \n              \n                \u03c4\n                \n                  1\n                \n                \n                  (\n                  t\n                  )\n                \n              \n               \n              f\n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              ;\n              \n                \n                  \u03bc\n                \n                \n                  1\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              ,\n              \n                \u03a3\n                \n                  1\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              )\n              +\n              \n                \u03c4\n                \n                  2\n                \n                \n                  (\n                  t\n                  )\n                \n              \n               \n              f\n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              ;\n              \n                \n                  \u03bc\n                \n                \n                  2\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              ,\n              \n                \u03a3\n                \n                  2\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle T_{j,i}^{(t)}:=\\operatorname {P} (Z_{i}=j\\mid X_{i}=\\mathbf {x} _{i};\\theta ^{(t)})={\\frac {\\tau _{j}^{(t)}\\ f(\\mathbf {x} _{i};{\\boldsymbol {\\mu }}_{j}^{(t)},\\Sigma _{j}^{(t)})}{\\tau _{1}^{(t)}\\ f(\\mathbf {x} _{i};{\\boldsymbol {\\mu }}_{1}^{(t)},\\Sigma _{1}^{(t)})+\\tau _{2}^{(t)}\\ f(\\mathbf {x} _{i};{\\boldsymbol {\\mu }}_{2}^{(t)},\\Sigma _{2}^{(t)})}}.}\n  These are called the \"membership probabilities\", which are normally considered the output of the E step (although this is not the Q function of below).\nThis E step corresponds with setting up this function for Q:\n\n  \n    \n      \n        \n          \n            \n              \n                Q\n                (\n                \u03b8\n                \u2223\n                \n                  \u03b8\n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n              \n              \n                \n                =\n                \n                  E\n                  \n                    \n                      Z\n                    \n                    \u2223\n                    \n                      X\n                    \n                    =\n                    \n                      x\n                    \n                    ;\n                    \n                      \n                        \u03b8\n                      \n                      \n                        (\n                        t\n                        )\n                      \n                    \n                  \n                \n                \u2061\n                [\n                log\n                \u2061\n                L\n                (\n                \u03b8\n                ;\n                \n                  x\n                \n                ,\n                \n                  Z\n                \n                )\n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  E\n                  \n                    \n                      Z\n                    \n                    \u2223\n                    \n                      X\n                    \n                    =\n                    \n                      x\n                    \n                    ;\n                    \n                      \n                        \u03b8\n                      \n                      \n                        (\n                        t\n                        )\n                      \n                    \n                  \n                \n                \u2061\n                [\n                log\n                \u2061\n                \n                  \u220f\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                L\n                (\n                \u03b8\n                ;\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                ,\n                \n                  Z\n                  \n                    i\n                  \n                \n                )\n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  E\n                  \n                    \n                      Z\n                    \n                    \u2223\n                    \n                      X\n                    \n                    =\n                    \n                      x\n                    \n                    ;\n                    \n                      \n                        \u03b8\n                      \n                      \n                        (\n                        t\n                        )\n                      \n                    \n                  \n                \n                \u2061\n                [\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                log\n                \u2061\n                L\n                (\n                \u03b8\n                ;\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                ,\n                \n                  Z\n                  \n                    i\n                  \n                \n                )\n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  E\n                  \n                    \n                      Z\n                      \n                        i\n                      \n                    \n                    \u2223\n                    \n                      X\n                      \n                        i\n                      \n                    \n                    =\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    ;\n                    \n                      \n                        \u03b8\n                      \n                      \n                        (\n                        t\n                        )\n                      \n                    \n                  \n                \n                \u2061\n                [\n                log\n                \u2061\n                L\n                (\n                \u03b8\n                ;\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                ,\n                \n                  Z\n                  \n                    i\n                  \n                \n                )\n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \u2211\n                  \n                    j\n                    =\n                    1\n                  \n                  \n                    2\n                  \n                \n                P\n                (\n                \n                  Z\n                  \n                    i\n                  \n                \n                =\n                j\n                \u2223\n                \n                  X\n                  \n                    i\n                  \n                \n                =\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                ;\n                \n                  \u03b8\n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n                log\n                \u2061\n                L\n                (\n                \n                  \u03b8\n                  \n                    j\n                  \n                \n                ;\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                ,\n                j\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  \u2211\n                  \n                    j\n                    =\n                    1\n                  \n                  \n                    2\n                  \n                \n                \n                  T\n                  \n                    j\n                    ,\n                    i\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                \n                  \n                    [\n                  \n                \n                log\n                \u2061\n                \n                  \u03c4\n                  \n                    j\n                  \n                \n                \u2212\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                log\n                \u2061\n                \n                  |\n                \n                \n                  \u03a3\n                  \n                    j\n                  \n                \n                \n                  |\n                \n                \u2212\n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                (\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                \u2212\n                \n                  \n                    \u03bc\n                  \n                  \n                    j\n                  \n                \n                \n                  )\n                  \n                    \u22a4\n                  \n                \n                \n                  \u03a3\n                  \n                    j\n                  \n                  \n                    \u2212\n                    1\n                  \n                \n                (\n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                \u2212\n                \n                  \n                    \u03bc\n                  \n                  \n                    j\n                  \n                \n                )\n                \u2212\n                \n                  \n                    \n                      d\n                      2\n                    \n                  \n                \n                log\n                \u2061\n                (\n                2\n                \u03c0\n                )\n                \n                  \n                    ]\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}Q(\\theta \\mid \\theta ^{(t)})&=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} =\\mathbf {x} ;\\mathbf {\\theta } ^{(t)}}[\\log L(\\theta ;\\mathbf {x} ,\\mathbf {Z} )]\\\\&=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} =\\mathbf {x} ;\\mathbf {\\theta } ^{(t)}}[\\log \\prod _{i=1}^{n}L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\&=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} =\\mathbf {x} ;\\mathbf {\\theta } ^{(t)}}[\\sum _{i=1}^{n}\\log L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\&=\\sum _{i=1}^{n}\\operatorname {E} _{Z_{i}\\mid X_{i}=x_{i};\\mathbf {\\theta } ^{(t)}}[\\log L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\&=\\sum _{i=1}^{n}\\sum _{j=1}^{2}P(Z_{i}=j\\mid X_{i}=\\mathbf {x} _{i};\\theta ^{(t)})\\log L(\\theta _{j};\\mathbf {x} _{i},j)\\\\&=\\sum _{i=1}^{n}\\sum _{j=1}^{2}T_{j,i}^{(t)}{\\big [}\\log \\tau _{j}-{\\tfrac {1}{2}}\\log |\\Sigma _{j}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{j})^{\\top }\\Sigma _{j}^{-1}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{j})-{\\tfrac {d}{2}}\\log(2\\pi ){\\big ]}.\\end{aligned}}}\n  The expectation of \n  \n    \n      \n        log\n        \u2061\n        L\n        (\n        \u03b8\n        ;\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          Z\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\log L(\\theta ;\\mathbf {x} _{i},Z_{i})}\n   inside the sum is taken with respect to the probability density function \n  \n    \n      \n        P\n        (\n        \n          Z\n          \n            i\n          \n        \n        \u2223\n        \n          X\n          \n            i\n          \n        \n        =\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ;\n        \n          \u03b8\n          \n            (\n            t\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Z_{i}\\mid X_{i}=\\mathbf {x} _{i};\\theta ^{(t)})}\n  , which might be different for each  \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   of the training set. Everything in the E step is known before the step is taken except \n  \n    \n      \n        \n          T\n          \n            j\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle T_{j,i}}\n  , which is computed according to the equation at the beginning of the E step section.\nThis full conditional expectation does not need to be calculated in one step, because \u03c4 and \u03bc/\u03a3 appear in separate linear terms and can thus be maximized independently.\n\n\n==== M step ====\nQ(\u03b8 | \u03b8(t)) being quadratic in form means that determining the maximizing values of \u03b8 is relatively straightforward. Also, \u03c4, (\u03bc1,\u03a31) and (\u03bc2,\u03a32) may all be maximized independently since they all appear in separate linear terms.\nTo begin, consider \u03c4, which has the constraint \u03c41 + \u03c42=1:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \u03c4\n                  \n                  \n                    (\n                    t\n                    +\n                    1\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      a\n                      r\n                      g\n                      \n                      m\n                      a\n                      x\n                    \n                    \u03c4\n                  \n                \n                 \n                Q\n                (\n                \u03b8\n                \u2223\n                \n                  \u03b8\n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      a\n                      r\n                      g\n                      \n                      m\n                      a\n                      x\n                    \n                    \u03c4\n                  \n                \n                 \n                \n                  {\n                  \n                    \n                      [\n                      \n                        \n                          \u2211\n                          \n                            i\n                            =\n                            1\n                          \n                          \n                            n\n                          \n                        \n                        \n                          T\n                          \n                            1\n                            ,\n                            i\n                          \n                          \n                            (\n                            t\n                            )\n                          \n                        \n                      \n                      ]\n                    \n                    log\n                    \u2061\n                    \n                      \u03c4\n                      \n                        1\n                      \n                    \n                    +\n                    \n                      [\n                      \n                        \n                          \u2211\n                          \n                            i\n                            =\n                            1\n                          \n                          \n                            n\n                          \n                        \n                        \n                          T\n                          \n                            2\n                            ,\n                            i\n                          \n                          \n                            (\n                            t\n                            )\n                          \n                        \n                      \n                      ]\n                    \n                    log\n                    \u2061\n                    \n                      \u03c4\n                      \n                        2\n                      \n                    \n                  \n                  }\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\boldsymbol {\\tau }}^{(t+1)}&={\\underset {\\boldsymbol {\\tau }}{\\operatorname {arg\\,max} }}\\ Q(\\theta \\mid \\theta ^{(t)})\\\\&={\\underset {\\boldsymbol {\\tau }}{\\operatorname {arg\\,max} }}\\ \\left\\{\\left[\\sum _{i=1}^{n}T_{1,i}^{(t)}\\right]\\log \\tau _{1}+\\left[\\sum _{i=1}^{n}T_{2,i}^{(t)}\\right]\\log \\tau _{2}\\right\\}.\\end{aligned}}}\n  This has the same form as the MLE for the binomial distribution, so\n\n  \n    \n      \n        \n          \u03c4\n          \n            j\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  j\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                T\n                \n                  1\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              +\n              \n                T\n                \n                  2\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              )\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          T\n          \n            j\n            ,\n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle \\tau _{j}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{j,i}^{(t)}}{\\sum _{i=1}^{n}(T_{1,i}^{(t)}+T_{2,i}^{(t)})}}={\\frac {1}{n}}\\sum _{i=1}^{n}T_{j,i}^{(t)}.}\n  For the next estimates of (\u03bc1,\u03a31):\n\n  \n    \n      \n        \n          \n            \n              \n                (\n                \n                  \n                    \u03bc\n                  \n                  \n                    1\n                  \n                  \n                    (\n                    t\n                    +\n                    1\n                    )\n                  \n                \n                ,\n                \n                  \u03a3\n                  \n                    1\n                  \n                  \n                    (\n                    t\n                    +\n                    1\n                    )\n                  \n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      a\n                      r\n                      g\n                      \n                      m\n                      a\n                      x\n                    \n                    \n                      \n                        \n                          \u03bc\n                        \n                        \n                          1\n                        \n                      \n                      ,\n                      \n                        \u03a3\n                        \n                          1\n                        \n                      \n                    \n                  \n                \n                 \n                Q\n                (\n                \u03b8\n                \u2223\n                \n                  \u03b8\n                  \n                    (\n                    t\n                    )\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      a\n                      r\n                      g\n                      \n                      m\n                      a\n                      x\n                    \n                    \n                      \n                        \n                          \u03bc\n                        \n                        \n                          1\n                        \n                      \n                      ,\n                      \n                        \u03a3\n                        \n                          1\n                        \n                      \n                    \n                  \n                \n                 \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  T\n                  \n                    1\n                    ,\n                    i\n                  \n                  \n                    (\n                    t\n                    )\n                  \n                \n                \n                  {\n                  \n                    \u2212\n                    \n                      \n                        \n                          1\n                          2\n                        \n                      \n                    \n                    log\n                    \u2061\n                    \n                      |\n                    \n                    \n                      \u03a3\n                      \n                        1\n                      \n                    \n                    \n                      |\n                    \n                    \u2212\n                    \n                      \n                        \n                          1\n                          2\n                        \n                      \n                    \n                    (\n                    \n                      \n                        x\n                      \n                      \n                        i\n                      \n                    \n                    \u2212\n                    \n                      \n                        \u03bc\n                      \n                      \n                        1\n                      \n                    \n                    \n                      )\n                      \n                        \u22a4\n                      \n                    \n                    \n                      \u03a3\n                      \n                        1\n                      \n                      \n                        \u2212\n                        1\n                      \n                    \n                    (\n                    \n                      \n                        x\n                      \n                      \n                        i\n                      \n                    \n                    \u2212\n                    \n                      \n                        \u03bc\n                      \n                      \n                        1\n                      \n                    \n                    )\n                  \n                  }\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{aligned}({\\boldsymbol {\\mu }}_{1}^{(t+1)},\\Sigma _{1}^{(t+1)})&={\\underset {{\\boldsymbol {\\mu }}_{1},\\Sigma _{1}}{\\operatorname {arg\\,max} }}\\ Q(\\theta \\mid \\theta ^{(t)})\\\\&={\\underset {{\\boldsymbol {\\mu }}_{1},\\Sigma _{1}}{\\operatorname {arg\\,max} }}\\ \\sum _{i=1}^{n}T_{1,i}^{(t)}\\left\\{-{\\tfrac {1}{2}}\\log |\\Sigma _{1}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{1})^{\\top }\\Sigma _{1}^{-1}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{1})\\right\\}\\end{aligned}}.}\n  This has the same form as a weighted MLE for a normal distribution, so\n\n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            1\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  1\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              \n                \n                  x\n                \n                \n                  i\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  1\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{1}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{1,i}^{(t)}\\mathbf {x} _{i}}{\\sum _{i=1}^{n}T_{1,i}^{(t)}}}}\n   and \n  \n    \n      \n        \n          \u03a3\n          \n            1\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  1\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \u03bc\n                \n                \n                  1\n                \n                \n                  (\n                  t\n                  +\n                  1\n                  )\n                \n              \n              )\n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \u03bc\n                \n                \n                  1\n                \n                \n                  (\n                  t\n                  +\n                  1\n                  )\n                \n              \n              \n                )\n                \n                  \u22a4\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  1\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{1}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{1,i}^{(t)}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{1}^{(t+1)})(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{1}^{(t+1)})^{\\top }}{\\sum _{i=1}^{n}T_{1,i}^{(t)}}}}\n  and, by symmetry,\n\n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            2\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  2\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              \n                \n                  x\n                \n                \n                  i\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  2\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{2}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{2,i}^{(t)}\\mathbf {x} _{i}}{\\sum _{i=1}^{n}T_{2,i}^{(t)}}}}\n   and \n  \n    \n      \n        \n          \u03a3\n          \n            2\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  2\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \u03bc\n                \n                \n                  2\n                \n                \n                  (\n                  t\n                  +\n                  1\n                  )\n                \n              \n              )\n              (\n              \n                \n                  x\n                \n                \n                  i\n                \n              \n              \u2212\n              \n                \n                  \u03bc\n                \n                \n                  2\n                \n                \n                  (\n                  t\n                  +\n                  1\n                  )\n                \n              \n              \n                )\n                \n                  \u22a4\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                T\n                \n                  2\n                  ,\n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\Sigma _{2}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{2,i}^{(t)}(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{2}^{(t+1)})(\\mathbf {x} _{i}-{\\boldsymbol {\\mu }}_{2}^{(t+1)})^{\\top }}{\\sum _{i=1}^{n}T_{2,i}^{(t)}}}.}\n  \n\n\n==== Termination ====\nConclude the iterative process if \n  \n    \n      \n        \n          E\n          \n            Z\n            \u2223\n            \n              \u03b8\n              \n                (\n                t\n                )\n              \n            \n            ,\n            \n              x\n            \n          \n        \n        [\n        log\n        \u2061\n        L\n        (\n        \n          \u03b8\n          \n            (\n            t\n            )\n          \n        \n        ;\n        \n          x\n        \n        ,\n        \n          Z\n        \n        )\n        ]\n        \u2264\n        \n          E\n          \n            Z\n            \u2223\n            \n              \u03b8\n              \n                (\n                t\n                \u2212\n                1\n                )\n              \n            \n            ,\n            \n              x\n            \n          \n        \n        [\n        log\n        \u2061\n        L\n        (\n        \n          \u03b8\n          \n            (\n            t\n            \u2212\n            1\n            )\n          \n        \n        ;\n        \n          x\n        \n        ,\n        \n          Z\n        \n        )\n        ]\n        +\n        \u03b5\n      \n    \n    {\\displaystyle E_{Z\\mid \\theta ^{(t)},\\mathbf {x} }[\\log L(\\theta ^{(t)};\\mathbf {x} ,\\mathbf {Z} )]\\leq E_{Z\\mid \\theta ^{(t-1)},\\mathbf {x} }[\\log L(\\theta ^{(t-1)};\\mathbf {x} ,\\mathbf {Z} )]+\\varepsilon }\n   for \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   below some preset threshold.\n\n\n==== Generalization ====\nThe algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions.\n\n\n=== Truncated and censored regression ===\nThe EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.  Special cases of this model include censored or truncated observations from one normal distribution.\n\n\n== Alternatives ==\nEM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed moment-based approaches or the so-called spectral techniques. Moment-based approaches to learning the parameters of a probabilistic model are of increasing interest recently since they enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions.\n\n\n== See also ==\nmixture distribution\ncompound distribution\ndensity estimation\nPrincipal component analysis\ntotal absorption spectroscopy\nThe EM algorithm can be viewed as a special case of the majorize-minimization (MM) algorithm.\n\n\n== References ==\n\n\n== Further reading ==\nHogg, Robert; McKean, Joseph; Craig, Allen (2005). Introduction to Mathematical Statistics. Upper Saddle River, NJ: Pearson Prentice Hall. pp. 359\u2013364.\nDellaert, Frank (2002). \"The Expectation Maximization Algorithm\". CiteSeerX 10.1.1.9.9735. {{cite journal}}: Cite journal requires |journal= (help) gives an easier explanation of EM algorithm as to lowerbound maximization.\nBishop, Christopher M. (2006). Pattern Recognition and Machine Learning. Springer. ISBN 978-0-387-31073-2.\nGupta, M. R.; Chen, Y. (2010). \"Theory and Use of the EM Algorithm\". Foundations and Trends in Signal Processing. 4 (3): 223\u2013296. CiteSeerX 10.1.1.219.6830. doi:10.1561/2000000034. A well-written short book on EM, including detailed derivation of EM for GMMs, HMMs, and Dirichlet.\nBilmes, Jeff (1998). \"A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models\". CiteSeerX 10.1.1.28.613. {{cite journal}}: Cite journal requires |journal= (help) includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models.\nMcLachlan, Geoffrey J.; Krishnan, Thriyambakam (2008). The EM Algorithm and Extensions (2nd ed.). Hoboken: Wiley. ISBN 978-0-471-20170-0.\n\n\n== External links ==\nVarious 1D, 2D and 3D demonstrations of EM together with Mixture Modeling are provided as part of the paired SOCR activities and applets. These applets and activities show empirically the properties of the EM algorithm for parameter estimation in diverse settings.\nClass hierarchy in C++ (GPL) including Gaussian Mixtures\nThe on-line textbook: Information Theory, Inference, and Learning Algorithms, by David J.C. MacKay includes simple examples of the EM algorithm such as clustering using the soft k-means algorithm, and emphasizes the variational view of the EM algorithm, as described in Chapter 33.7 of version 7.2 (fourth edition).\nVariational Algorithms for Approximate Bayesian Inference, by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs  (chapters).\nThe Expectation Maximization Algorithm: A short tutorial, A self-contained derivation of the EM Algorithm by Sean Borman.\nThe EM Algorithm, by Xiaojin Zhu.\nEM algorithm and variants: an informal tutorial by Alexis Roche.  A concise and very clear description of EM and many interesting variants.", "Dimensionality reduction": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.\n\n\n== Feature selection ==\n\nFeature selection approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the filter strategy (e.g. information gain), the wrapper strategy (e.g. search guided by accuracy), and the embedded strategy (selected features are added or removed while building the model based on prediction errors).\nData analysis such as regression or classification can be done in the reduced space more accurately than in the original space.\n\n\n== Feature projection ==\n\nFeature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.\n\n\n=== Principal component analysis (PCA) ===\n\nThe main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.\n\n\n=== Non-negative matrix factorization (NMF) ===\n\nNMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties, the consideration of missing data and parallel computation, sequential construction which leads to the stability and linearity of NMF, as well as other updates including handling missing data in digital image processing.With a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astronomy, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to unphysical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.\n\n\n=== Kernel PCA ===\n\nPrincipal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA.\n\n\n=== Graph-based kernel PCA ===\nOther prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.\nMore recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.\nAn alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.\nA different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feedforward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.\n\n\n=== Linear discriminant analysis (LDA) ===\n\nLinear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n\n\n=== Generalized discriminant analysis (GDA) ===\nGDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.\n\n\n=== Autoencoder ===\n\nAutoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation.\n\n\n=== t-SNE ===\n\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well.\n\n\n=== UMAP ===\n\nUniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.\n\n\n== Dimension reduction ==\nFor high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality-sensitive hashing, random projection, \"sketches\", or other high-dimensional similarity search techniques from the VLDB conference toolbox might be the only feasible option.\n\n\n== Applications ==\nA dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nJMLR Special Issue on Variable and Feature Selection\nELastic MAPs\nLocally Linear Embedding\nVisual Comparison of various dimensionality reduction methods\nA Global Geometric Framework for Nonlinear Dimensionality Reduction", "Level of measurement": "Level of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and has since had a complex history, being adopted and extended in some disciplines and by some scholars, and criticized or rejected by others. Other classifications include those by Mosteller and Tukey, and by Chrisman.\n\n\n== Stevens's typology ==\n\n\n=== Overview ===\nStevens proposed his typology in a 1946 Science article titled \"On the theory of scales of measurement\". In that article, Stevens claimed that all measurement in science was conducted using four different types of scales that he called \"nominal\", \"ordinal\", \"interval\", and \"ratio\", unifying both \"qualitative\" (which are described by his \"nominal\" type) and \"quantitative\" (to a different degree, all the rest of his scales). The concept of scale types later received the mathematical rigour that it lacked at its inception with the work of mathematical psychologists Theodore Alper (1985, 1987), Louis Narens (1981a, b), and R. Duncan Luce (1986, 1987, 2001). As Luce (1997, p. 395) wrote:\n\nS. S. Stevens (1946, 1951, 1975) claimed that what counted was having an interval or ratio scale. Subsequent research has given meaning to this assertion, but given his attempts to invoke scale type ideas it is doubtful if he understood it himself ... no measurement theorist I know accepts Stevens's broad definition of measurement ... in our view, the only sensible meaning for 'rule' is empirically testable laws about the attribute.\n\n\n==== Comparison ====\n\n\n=== Nominal level ===\nThe nominal type differentiates between items or subjects based only on their names or (meta-)categories and other qualitative classifications they belong to; thus dichotomous data involves the construction of classifications as well as the classification of items. Discovery of an exception to a classification can be viewed as progress. Numbers may be used to represent the variables but the numbers do not have numerical value or relationship:  for example, a globally unique identifier.\nExamples of these classifications include gender, nationality, ethnicity, language, genre, style, biological species, and form. In a university one could also use hall of affiliation as an example. Other concrete examples are \n\nin grammar, the parts of speech: noun, verb, preposition, article, pronoun, etc.\nin politics, power projection: hard power, soft power, etc.\nin biology, the taxonomic ranks below domains: Archaea, Bacteria, and Eukarya\nin software engineering, type of faults: specification faults, design faults, and code faultsNominal scales were often called qualitative scales, and measurements made on qualitative scales were called qualitative data. However, the rise of qualitative research has made this usage confusing. If numbers are assigned as labels in nominal measurement, they have no specific numerical value or meaning. No form of arithmetic computation (+, \u2212, \u00d7, etc.) may be performed on nominal measures. The nominal level is the lowest measurement level  used from a statistical point of view.\n\n\n==== Mathematical operations ====\nEquality and other operations that can be defined in terms of equality, such as inequality and set membership, are the only non-trivial operations that generically apply to objects of the nominal type.\n\n\n==== Central tendency ====\nThe mode, i.e. the most common item, is allowed as the measure of central tendency for the nominal type. On the other hand, the median, i.e. the middle-ranked item, makes no sense for the nominal type of data since ranking is meaningless for the nominal type.\n\n\n=== Ordinal scale ===\n\nThe ordinal type allows for rank order (1st, 2nd, 3rd, etc.) by which data can be sorted but still does not allow for a relative degree of difference between them. Examples include, on one hand, dichotomous data with dichotomous (or dichotomized) values such as 'sick' vs. 'healthy' when measuring health, 'guilty' vs. 'not-guilty' when making judgments in courts, 'wrong/false' vs. 'right/true' when measuring truth value, and, on the other hand, non-dichotomous data consisting of a spectrum of values, such as 'completely agree', 'mostly agree', 'mostly disagree', 'completely disagree' when measuring opinion.\nThe ordinal scale places events in order, but there is no attempt to make the intervals of the scale equal in terms of some rule. Rank orders represent ordinal scales and are frequently used in research relating to qualitative phenomena. A student's rank in his graduation class involves the use of an ordinal scale. One has to be very careful in making a statement about scores based on ordinal scales. For instance, if Devi's position in his class is 10 and Ganga's position is 40, it cannot be said that Devi's position is four times as good as that of Ganga. The statement would make no sense at all.\nOrdinal scales only permit the ranking of items from highest to lowest. Ordinal measures have no absolute values, and the real differences between adjacent ranks may not be equal. All that can be said is that one person is higher or lower on the scale than another, but more precise comparisons cannot be made. Thus, the use of an ordinal scale implies a statement of 'greater than' or 'less than' (an equality statement is also acceptable) without our being able to state how much greater or less. The real difference between ranks 1 and 2, for instance, may be more or less than the difference between ranks 5 and 6. Since the numbers of this scale have only a rank meaning, the appropriate measure of central tendency is the median. A percentile or quartile measure is used for measuring dispersion. Correlations are restricted to various rank order methods. Measures of statistical significance are restricted to the non-parametric methods (R. M. Kothari, 2004).\n\n\n==== Central tendency ====\nThe median, i.e. middle-ranked, item is allowed as the measure of central tendency; however, the mean (or average) as the measure of central tendency is not allowed. The mode is allowed.\nIn 1946, Stevens observed that psychological measurement, such as measurement of opinions, usually operates on ordinal scales; thus means and standard deviations have no validity, but they can be used to get ideas for how to improve operationalization of variables used in questionnaires. Most psychological data collected by psychometric instruments and tests, measuring cognitive and other abilities, are ordinal, although some theoreticians have argued they can be treated as interval or ratio scales. However, there is little prima facie evidence to suggest that such attributes are anything more than ordinal (Cliff, 1996; Cliff & Keats, 2003; Michell, 2008). In particular, IQ scores reflect an ordinal scale, in which all scores are meaningful for comparison only. There is no absolute zero, and a 10-point difference may carry different meanings at different points of the scale.\n\n\n=== Interval scale ===\nThe interval type allows for the degree of difference between items, but not the ratio between them. Examples include temperature scales with the Celsius scale, which has two defined points (the freezing and boiling point of water at specific conditions) and then separated into 100 intervals, date when measured from an arbitrary epoch (such as AD), location in Cartesian coordinates, and direction measured in degrees from true or magnetic north. Ratios are not meaningful since 20 \u00b0C cannot be said to be \"twice as hot\" as 10 \u00b0C (unlike temperature in Kelvins), nor can multiplication/division be carried out between any two dates directly. However, ratios of differences can be expressed; for example, one difference can be twice another. Interval type variables are sometimes also called \"scaled variables\", but the formal mathematical term is an affine space (in this case an affine line).\n\n\n==== Central tendency and statistical dispersion ====\nThe mode, median, and arithmetic mean are allowed to measure central tendency of interval variables, while measures of statistical dispersion include range and standard deviation. Since one can only divide by differences, one cannot define measures that require some ratios, such as  the coefficient of variation. More subtly, while one can define moments about the origin, only central moments are meaningful, since the choice of origin is arbitrary. One can define standardized moments, since ratios of differences are meaningful, but one cannot define the coefficient of variation, since the mean is a moment about the origin, unlike the standard deviation, which is (the square root of) a central moment.\n\n\n=== Ratio scale ===\nSee also: Positive real numbers \u00a7 Ratio scaleThe ratio type takes its name from the fact that measurement is the estimation of the ratio between a magnitude of a continuous quantity and a unit of measurement of the same kind (Michell, 1997, 1999). Most measurement in the physical sciences and engineering is done on ratio scales. Examples include mass, length, duration, plane angle, energy and electric charge. In contrast to interval scales, ratios can be compared using division. Very informally, many ratio scales can be described as specifying \"how much\" of something (i.e. an amount or magnitude). Ratio scale is often used to express an order  of magnitude such as for temperature in Orders of magnitude (temperature).\n\n\n==== Central tendency and statistical dispersion ====\nThe geometric mean and the harmonic mean are allowed to measure the central tendency, in addition to the mode, median, and arithmetic mean. The studentized range and the coefficient of variation are allowed to measure statistical dispersion. All statistical measures are allowed because all necessary mathematical operations are defined for the ratio scale.\n\n\n== Debate on Stevens's typology ==\nWhile Stevens's typology is widely adopted, it is still being challenged by other theoreticians, particularly in the cases of the nominal and ordinal types (Michell, 1986).. Duncan (1986), for example, objected to the use of the word measurement in relation to the nominal type and Luce (1997) disagreed with Steven's definition of measurement. \nOn the other hand, Stevens (1975) said of his own definition of measurement that \"the assignment can be any consistent rule. The only rule not allowed would be random assignment, for randomness amounts in effect to a nonrule\". Hand says, \"Basic psychology texts often begin with Stevens's framework and the ideas are ubiquitous. Indeed, the essential soundness of his hierarchy has been established for representational measurement by mathematicians, determining the invariance properties of mappings from empirical systems to real number continua. Certainly the ideas have been revised, extended, and elaborated, but the remarkable thing is his insight given the relatively limited formal apparatus available to him and how many decades have passed since he coined them.\"The use of the mean as a measure of the central tendency for the ordinal type is still debatable among those who accept Stevens's typology. Many behavioural scientists use the mean for ordinal data, anyway. This is often justified on the basis that the ordinal type in behavioural science is in fact somewhere between the true ordinal and interval types; although the interval difference between two ordinal ranks is not constant, it is often of the same order of magnitude.\nFor example, applications of measurement models in educational contexts often indicate that total scores have a fairly linear relationship with measurements across the range of an assessment. Thus, some argue that so long as the unknown interval difference between ordinal scale ranks is not too variable, interval scale statistics such as means can meaningfully be used on ordinal scale variables. Statistical analysis software such as SPSS requires the user to select the appropriate measurement class for each variable. This ensures that subsequent user errors cannot inadvertently perform meaningless analyses (for example correlation analysis with a variable on a nominal level).\nL. L. Thurstone made progress toward developing a justification for obtaining the interval type, based on the law of comparative judgment. A common application of the law is the analytic hierarchy process. Further progress was made by Georg Rasch (1960), who developed the probabilistic Rasch model that provides a theoretical basis and justification for obtaining interval-level measurements from counts of observations such as total scores on assessments.\n\n\n=== Other proposed typologies ===\nTypologies aside from Stevens's typology have been proposed. For instance, Mosteller and Tukey (1977), Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).\n\n\n==== Mosteller and Tukey's typology (1977) ====\nMosteller and Tukey noted that the four levels are not exhaustive and proposed:\n\nNames\nGrades (ordered labels like beginner, intermediate, advanced)\nRanks (orders with 1 being the smallest or largest, 2 the next smallest or largest, and so on)\nCounted fractions (bound by 0 and 1)\nCounts (non-negative integers)\nAmounts (non-negative real numbers)\nBalances (any real number)For example, percentages (a variation on fractions in the Mosteller\u2013Tukey framework) do not fit well into Stevens's framework: No transformation is fully admissible.\n\n\n==== Chrisman's typology (1998) ====\nNicholas R. Chrisman introduced an expanded list of levels of measurement to account for various measurements that do not necessarily fit with the traditional notions of levels of measurement. Measurements bound to a range and repeating (like degrees in a circle, clock time, etc.), graded membership categories, and other types of measurement do not fit to Stevens's original work, leading to the introduction of six new levels of measurement, for a total of ten:\n\nNominal\nGradation of membership\nOrdinal\nInterval\nLog-interval\nExtensive ratio\nCyclical ratio\nDerived ratio\nCounts\nAbsoluteWhile some claim that the extended levels of measurement are rarely used outside of academic geography, graded membership is central to fuzzy set theory, while absolute measurements include probabilities and the plausibility and ignorance in Dempster\u2013Shafer theory. Cyclical ratio measurements include angles and times. Counts appear to be ratio measurements, but the scale is not arbitrary and fractional counts are commonly meaningless. Log-interval measurements are commonly displayed in stock market graphics. All these types of measurements are commonly used outside academic geography, and do not fit well to Stevens' original work.\n\n\n=== Scale types and Stevens's \"operational theory of measurement\" ===\nThe theory of scale types is the intellectual handmaiden to Stevens's \"operational theory of measurement\", which was to become definitive within psychology and the behavioral sciences, despite Michell's characterization as its being quite at odds with measurement in the natural sciences (Michell, 1999). Essentially, the operational theory of measurement was a reaction to the conclusions of a committee established in 1932 by the British Association for the Advancement of Science to investigate the possibility of genuine scientific measurement in the psychological and behavioral sciences. This committee, which became known as the Ferguson committee, published a Final Report (Ferguson, et al., 1940, p. 245) in which Stevens's sone scale (Stevens & Davis, 1938) was an object of criticism:\n\n \u2026any law purporting to express a quantitative relation between sensation intensity and stimulus intensity is not merely false but is in fact meaningless unless and until a meaning can be given to the concept of addition as applied to sensation.\nThat is, if Stevens's sone scale genuinely measured the intensity of auditory sensations, then evidence for such sensations as being quantitative attributes needed to be produced. The evidence needed was the presence of additive structure \u2013 a concept comprehensively treated by the German mathematician Otto H\u00f6lder (H\u00f6lder, 1901). Given that the physicist and measurement theorist Norman Robert Campbell dominated the Ferguson committee's deliberations, the committee concluded that measurement in the social sciences was impossible due to the lack of concatenation operations. This conclusion was later rendered false by the discovery of the theory of conjoint measurement by Debreu (1960) and independently by Luce & Tukey (1964). However, Stevens's reaction was not to conduct experiments to test for the presence of additive structure in sensations, but instead to render the conclusions of the Ferguson committee null and void by proposing a new theory of measurement:\n\nParaphrasing N. R. Campbell (Final Report, p.340), we may say that measurement, in the broadest sense, is defined as the assignment of numerals to objects and events according to rules (Stevens, 1946, p.677).\nStevens was greatly influenced by the ideas of another Harvard academic, the Nobel laureate physicist Percy Bridgman (1927), whose doctrine of operationism Stevens used to define measurement. In Stevens's definition, for example, it is the use of a tape measure that defines length (the object of measurement) as being measurable (and so by implication quantitative). Critics of operationism object that it confuses the relations between two objects or events for properties of one of those of objects or events (Hardcastle, 1995; Michell, 1999; Moyer, 1981a,b; Rogers, 1989).\nThe Canadian measurement theorist William Rozeboom (1966) was an early and trenchant critic of Stevens's theory of scale types.\n\n\n==== Same variable may be different scale type depending on context ====\nAnother issue is that the same variable may be a different scale type depending on how it is measured and on the goals of the analysis. For example, hair color is usually thought of as a nominal variable, since it has no apparent ordering. However, it is possible to order colors (including hair colors) in various ways, including by hue; this is known as colorimetry. Hue is an interval level variable.\n\n\n== See also ==\nCohen's kappa\nCoherence (units of measurement)\nHume's principle\nInter-rater reliability\nLogarithmic scale\nRamsey\u2013Lewis method\nSet theory\nStatistical data type\nTransition (linguistics)\n\n\n== References ==\n\n\n== Further reading ==", "Genetic algorithm": "In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, etc.\n\n\n== Methodology ==\n\n\n=== Optimization problems ===\nIn a genetic algorithm, a population of candidate solutions (called individuals, creatures, organisms, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.\nA typical genetic algorithm requires:\n\na genetic representation of the solution domain,\na fitness function to evaluate the solution domain.A standard representation of each candidate solution is as an array of bits (also called bit set or bit string). Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.\nOnce the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.\n\n\n==== Initialization ====\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space). Occasionally, the solutions may be \"seeded\" in areas where optimal solutions are likely to be found.\n\n\n==== Selection ====\n\nDuring each successive generation, a portion of the existing population is selected to reproduce for a new generation. Individual solutions are selected through a fitness-based process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.\nThe fitness function is defined over the genetic representation and measures the quality of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.\nIn some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.\n\n\n==== Genetic operators ====\n\nThe next step is to generate a second generation population of solutions from those selected, through a combination of genetic operators: crossover (also called recombination), and mutation.\nFor each new solution to be produced, a pair of \"parent\" solutions is selected for breeding from the pool selected previously. By producing a \"child\" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its \"parents\". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.\nAlthough reproduction methods that are based on the use of two parents are more \"biology inspired\", some research suggests that more than two \"parents\" generate higher quality chromosomes.\nThese processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally, the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.\nOpinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.\nAlthough crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.It is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed. An adequate population size ensures sufficient genetic diversity for the problem at hand, but can lead to a waste of computational resources if set to a value larger than required.\n\n\n==== Heuristics ====\nIn addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The speciation heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.\n\n\n==== Termination ====\nThis generational process is repeated until a termination condition has been reached. Common terminating conditions are:\n\nA solution is found that satisfies minimum criteria\nFixed number of generations reached\nAllocated budget (computation time/money) reached\nThe highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results\nManual inspection\nCombinations of the above\n\n\n== The building block hypothesis ==\nGenetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:\n\nA description of a heuristic that performs adaptation by identifying and recombining \"building blocks\", i.e. low order, low defining-length schemata with above average fitness.\nA hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.Goldberg describes the heuristic as follows:\n\n\"Short, low order, and highly fit schemata are sampled, recombined [crossed over], and resampled to form strings of potentially higher fitness. In a way, by working with these particular schemata [the building blocks], we have reduced the complexity of our problem; instead of building high-performance strings by trying every conceivable combination, we construct better and better strings from the best partial solutions of past samplings.\"Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks. Just as a child creates magnificent fortresses through the arrangement of simple blocks of wood, so does a genetic algorithm seek near optimal performance through the juxtaposition of short, low-order, high-performance schemata, or building blocks.\"Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.\n\n\n== Limitations ==\nThere are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:\n\nRepeated fitness function evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive fitness function evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods cannot deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an approximated fitness that is computationally efficient. It is apparent that amalgamation of approximate models may be one of the most promising approaches to convincingly use GA to solve complex real life problems.\nGenetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or a plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.\nThe \"better\" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.\nIn many problems, GAs have a tendency to converge towards local optima or even arbitrary points rather than the global optimum of the problem. This means that it does not \"know how\" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the fitness landscape: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions, although the No Free Lunch theorem proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a \"niche penalty\", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and genetic programming) because crossing over a homogeneous population does not yield new solutions. In evolution strategies and evolutionary programming, diversity is not essential because of a greater reliance on mutation.\nOperating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called triggered hypermutation), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called random immigrants). Again, evolution strategies and evolutionary programming can be implemented with a so-called \"comma strategy\" in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.\nGAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like decision problems), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.\nFor specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence (e.g.: ant colony optimization, particle swarm optimization) and methods based on integer linear programming. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\n\n== Variants ==\n\n\n=== Chromosome representation ===\n\nThe simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.\nWhen bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so-called Hamming walls, in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.\nOther approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.An expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome. This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.\n\n\n=== Elitism ===\nA practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.\n\n\n=== Parallel implementations ===\nParallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.\nOther variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.\n\n\n=== Adaptive GAs ===\nGenetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Researchers have analyzed GA convergence analytically. Instead of using fixed values of pc and pm, AGAs utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm), the adjustment of pc and pm depends on the fitness values of the solutions. There are more examples of AGA variants: Successive zooming method is an early example of improving convergence. In CAGA (clustering-based adaptive genetic algorithm), through the use of clustering analysis to judge the optimization states of the population, the adjustment of pc and pm depends on these optimization states. Recent approaches use more abstract variables for deciding pc and pm. Examples are dominance & co-dominance principles and LIGA (levelized interpolative genetic algorithm), which combines a flexible GA with modified A* search to tackle search space anisotropicity.It can be quite effective to combine GA with other optimization methods. A GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA while overcoming the lack of robustness of hill climbing.\nThis means that the rules of genetic variation may have a different meaning in the natural case. For instance \u2013 provided that steps are stored in consecutive order \u2013 crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency.A variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.\nA number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA, GEMGA and LLGA.\n\n\n== Problem domains ==\nProblems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.\nAs a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).\nExamples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector, antennae designed to pick up radio signals in space, walking methods for computer figures, optimal design of aerodynamic bodies in complex flowfieldsIn his Algorithm Design Manual, Skiena advises against genetic algorithms for any task:\n\n[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution\u2014where significant progress require [sic] millions of years\u2014can be quite appropriate.\n[...]\n\nI have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to simulated annealing for your heuristic search voodoo needs.\n\n\n== History ==\nIn 1950, Alan Turing proposed a \"learning machine\" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s \u2013 Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.\n\n\n=== Commercial products ===\nIn the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. \nIn 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995. Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version. Since the 1990s, MATLAB has built in three derivative-free optimization heuristic algorithms (simulated annealing, particle swarm optimization, genetic algorithm) and two direct search algorithms (simplex search, pattern search).\n\n\n== Related techniques ==\n\n\n=== Parent fields ===\nGenetic algorithms are a sub-field:\n\nEvolutionary algorithms\nEvolutionary computing\nMetaheuristics\nStochastic optimization\nOptimization\n\n\n=== Related fields ===\n\n\n==== Evolutionary algorithms ====\n\nEvolutionary algorithms is a sub-field of evolutionary computing.\n\nEvolution strategies (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain. They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy (CMA-ES).\nEvolutionary programming (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.\nEstimation of Distribution Algorithm (EDA) substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as Probabilistic Graphical Models, from which new solutions can be sampled or generated from guided-crossover.\nGenetic programming (GP) is a related technique popularized by John Koza in which computer programs, rather than function parameters, are optimized. Genetic programming often uses tree-based internal data structures to represent the computer programs for adaptation instead of the list structures typical of genetic algorithms. There are many variants of Genetic Programming, including Cartesian genetic programming, Gene expression programming, grammatical evolution, Linear genetic programming, Multi expression programming etc.\nGrouping genetic algorithm (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items. The idea behind this GA evolution proposed by Emanuel Falkenauer is that solving some complex problems, a.k.a. clustering or partitioning problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include bin packing, line balancing, clustering with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.\nInteractive evolutionary algorithms are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.\n\n\n==== Swarm intelligence ====\n\nSwarm intelligence is a sub-field of evolutionary computing.\n\nAnt colony optimization (ACO) uses many ants (or agents) equipped with a pheromone model to traverse the solution space and find locally productive areas.\nAlthough considered an Estimation of distribution algorithm, Particle swarm optimization (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables.\n\n\n==== Other evolutionary computing algorithms ====\nEvolutionary computation is a sub-field of the metaheuristic methods.\n\nMemetic algorithm (MA), often called hybrid genetic algorithm among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from memes, which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.\nBacteriologic algorithms (BA) inspired by evolutionary ecology and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, there is not one individual that fits the whole environment. So, one needs to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining.\nCultural algorithm (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.\nDifferential evolution (DE) inspired by migration of superorganisms.\nGaussian adaptation (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information. Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain \"ambition\" to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder (average information) of the Gaussian simultaneously keeping the mean fitness constant.\n\n\n==== Other metaheuristic methods ====\nMetaheuristic methods broadly fall within stochastic optimisation methods.\n\nSimulated annealing (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.\nTabu search (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.\nExtremal optimization (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). The governing principle behind this algorithm is that of emergent improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.\n\n\n==== Other stochastic optimisation methods ====\nThe cross-entropy (CE) method generates candidate solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.\nReactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and metaheuristics.\n\n\n== See also ==\nGenetic programming\nList of genetic algorithm applications\nGenetic algorithms in signal processing (a.k.a. particle filters)\nPropagation of schema\nUniversal Darwinism\nMetaheuristics\nLearning classifier system\nRule-based machine learning\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\n\n=== Resources ===\n[1] Provides a list of resources in the genetic algorithms field\nAn Overview of the History and Flavors of Evolutionary Algorithms\n\n\n=== Tutorials ===\nGenetic Algorithms - Computer programs that \"evolve\" in ways that resemble natural selection can solve complex problems even their creators do not fully understand An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma\nAn online interactive Genetic Algorithm tutorial for a reader to practise or learn how a GA works: Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.\nA Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University An excellent tutorial with much theory\n\"Essentials of Metaheuristics\", 2009 (225 p). Free open text by Sean Luke.\nGlobal Optimization Algorithms \u2013 Theory and Application\nGenetic Algorithms in Python Tutorial with the intuition behind GAs and Python implementation.\nGenetic Algorithms evolves to solve the prisoner's dilemma. Written by Robert Axelrod.", "Hyperplane": "In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space.  For example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.  This notion can be used in any general space in which the concept of the dimension of a subspace is defined.\nIn different settings, hyperplanes may have different properties.  For instance, a hyperplane of an n-dimensional affine space is a flat subset with dimension n \u2212 1 and it separates the space into two half spaces. While a hyperplane of an n-dimensional projective space does not have this property.\nThe difference in dimension between a subspace S and its ambient space X is known as the codimension of S with respect to X.  Therefore, a necessary and sufficient condition for S to be a hyperplane in X is for S to have codimension one in X.\n\n\n== Technical description ==\nIn geometry, a hyperplane of an n-dimensional space V is a subspace of dimension n \u2212 1, or equivalently, of codimension 1 in V. The space V may be a Euclidean space or more generally an affine space, or a vector space or a projective space, and the notion of hyperplane varies correspondingly since the definition of subspace differs in these settings; in all cases however, any hyperplane can be given in coordinates as the solution of a single (due to the \"codimension 1\" constraint) algebraic equation of degree 1. \nIf V is a vector space, one distinguishes \"vector hyperplanes\" (which are linear subspaces, and therefore must pass through the origin) and \"affine hyperplanes\" (which need not pass through the origin; they can be obtained by translation of a vector hyperplane). A hyperplane in a Euclidean space separates that space into two half spaces, and defines a reflection that fixes the hyperplane and interchanges those two half spaces.\n\n\n== Special types of hyperplanes ==\nSeveral specific types of hyperplanes are defined with properties that are well suited for particular purposes. Some of these specializations are described here.\n\n\n=== Affine hyperplanes ===\nAn affine hyperplane is  an affine subspace of codimension 1 in an affine space.\nIn Cartesian coordinates, such a hyperplane can be described with a single linear equation of the following form (where at least one of the \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle a_{i}}\n  s is non-zero and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   is an arbitrary constant):\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        .\n         \n      \n    \n    {\\displaystyle a_{1}x_{1}+a_{2}x_{2}+\\cdots +a_{n}x_{n}=b.\\ }\n  In the case of a real affine space, in other words when the coordinates are real numbers, this affine space separates the space into two half-spaces, which are the connected components of the complement of the hyperplane, and are given by the inequalities\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        <\n        b\n         \n      \n    \n    {\\displaystyle a_{1}x_{1}+a_{2}x_{2}+\\cdots +a_{n}x_{n}<b\\ }\n  and\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        >\n        b\n        .\n         \n      \n    \n    {\\displaystyle a_{1}x_{1}+a_{2}x_{2}+\\cdots +a_{n}x_{n}>b.\\ }\n  As an example, a point is a hyperplane in 1-dimensional space, a line is a hyperplane in 2-dimensional space, and a plane is a hyperplane in 3-dimensional space. A line in 3-dimensional space is not a hyperplane, and does not separate the space into two parts (the complement of such a line is connected).\nAny hyperplane of a Euclidean space has exactly two unit normal vectors.\nAffine hyperplanes are used to define decision boundaries in many machine learning algorithms such as linear-combination (oblique) decision trees, and perceptrons.\n\n\n=== Vector hyperplanes ===\nIn a vector space, a vector hyperplane is a subspace of codimension 1, only possibly shifted from the origin by a vector, in which case it is referred to as a flat. Such a hyperplane is the solution of a single linear equation.\n\n\n=== Projective hyperplanes ===\nProjective hyperplanes, are used in projective geometry.  A projective subspace is a set of points with the property that for any two points of the set, all the points on the line determined by the two points are contained in the set. Projective geometry can be viewed as affine geometry with vanishing points (points at infinity) added.  An affine hyperplane together with the associated points at infinity forms a projective hyperplane.  One special case of a projective hyperplane is the infinite or ideal hyperplane, which is defined with the set of all points at infinity.\nIn  projective space, a hyperplane does not divide the space into two parts; rather, it takes two hyperplanes to separate points and divide up the space.  The reason for this is that the space essentially \"wraps around\" so that both sides of a lone hyperplane are connected to each other.\n\n\n== Applications ==\nIn convex geometry, two disjoint convex sets in n-dimensional Euclidean space are separated by a hyperplane, a result called the hyperplane separation theorem.\nIn machine learning, hyperplanes are a key tool to create support vector machines for such tasks as computer vision and natural language processing.\nThe datapoint and its predicted value via a linear model is a hyperplane. \n\n\n== Dihedral angles ==\nThe dihedral angle between two non-parallel hyperplanes of a Euclidean space is the angle between the corresponding normal vectors. The product of the transformations in the two hyperplanes is a rotation whose axis is the subspace of codimension 2 obtained by intersecting the hyperplanes, and whose angle is twice the angle between the hyperplanes.\n\n\n=== Support hyperplanes ===\nA hyperplane H is called a \"support\" hyperplane of the polyhedron P if P is contained in one of the two closed half-spaces bounded by H and \n  \n    \n      \n        H\n        \u2229\n        P\n        \u2260\n        \u2205\n      \n    \n    {\\displaystyle H\\cap P\\neq \\varnothing }\n  . The intersection of P and H is defined to be a \"face\" of the polyhedron. The theory of polyhedra and the dimension of the faces are analyzed by looking at these intersections involving hyperplanes.\n\n\n== See also ==\nHypersurface\nDecision boundary\nHam sandwich theorem\nArrangement of hyperplanes\nSupporting hyperplane theorem\n\n\n== References ==\n\nBinmore, Ken G. (1980). The Foundations of Topological Analysis: A Straightforward Introduction: Book 2 Topological Ideas. Cambridge University Press. p. 13. ISBN 0-521-29930-6.\nCharles W. Curtis (1968) Linear Algebra, page 62, Allyn & Bacon, Boston.\nHeinrich Guggenheimer (1977) Applicable Geometry, page 7, Krieger, Huntington ISBN 0-88275-368-1 .\nVictor V. Prasolov & VM Tikhomirov (1997,2001) Geometry, page 22, volume 200 in Translations of Mathematical Monographs, American Mathematical Society, Providence ISBN 0-8218-2038-9 .\n\n\n== External links ==\n\nWeisstein, Eric W. \"Hyperplane\". MathWorld.\nWeisstein, Eric W. \"Flat\". MathWorld.", "Single-linkage clustering": "In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.\nThis method tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than two elements of other clusters. For some classes of data, this may lead to difficulties in defining classes that could usefully subdivide the data. However, it is popular in astronomy for analyzing galaxy clusters, which may often involve long strings of matter; in this application, it is also known as the friends-of-friends algorithm.\n\n\n== Overview of agglomerative clustering methods ==\nIn the beginning of the agglomerative clustering process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters, until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined. The function used to determine the distance between two clusters, known as the linkage function, is what differentiates the agglomerative clustering methods.\nIn single-linkage clustering, the distance between two clusters is determined by a single pair of elements: those two elements (one in each cluster) that are closest to each other. The shortest of these pairwise distances that remain at any step causes the two clusters whose elements are involved to be merged. The method is also known as nearest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence in which clusters were merged and the distance at which each merge took place.Mathematically, the linkage function \u2013 the distance D(X,Y) between clusters X and Y \u2013 is described by the expression\n\n  \n    \n      \n        D\n        (\n        X\n        ,\n        Y\n        )\n        =\n        \n          min\n          \n            x\n            \u2208\n            X\n            ,\n            y\n            \u2208\n            Y\n          \n        \n        d\n        (\n        x\n        ,\n        y\n        )\n        ,\n      \n    \n    {\\displaystyle D(X,Y)=\\min _{x\\in X,y\\in Y}d(x,y),}\n  where X and Y are any two sets of elements considered as clusters, and d(x,y) denotes the distance between the two elements x and y.\n\n\n== Naive algorithm ==\nThe following algorithm is an agglomerative scheme that erases rows and columns in a proximity matrix as old clusters are merged into new ones. The \n  \n    \n      \n        N\n        \u00d7\n        N\n      \n    \n    {\\displaystyle N\\times N}\n   proximity matrix \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   contains all distances \n  \n    \n      \n        d\n        (\n        i\n        ,\n        j\n        )\n      \n    \n    {\\displaystyle d(i,j)}\n  . The clusterings are assigned sequence numbers \n  \n    \n      \n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        n\n        \u2212\n        1\n      \n    \n    {\\displaystyle 0,1,\\ldots ,n-1}\n   and \n  \n    \n      \n        L\n        (\n        k\n        )\n      \n    \n    {\\displaystyle L(k)}\n   is the level of the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -th clustering. A cluster with sequence number m is denoted (m) and the proximity between clusters \n  \n    \n      \n        (\n        r\n        )\n      \n    \n    {\\displaystyle (r)}\n   and \n  \n    \n      \n        (\n        s\n        )\n      \n    \n    {\\displaystyle (s)}\n   is denoted \n  \n    \n      \n        d\n        [\n        (\n        r\n        )\n        ,\n        (\n        s\n        )\n        ]\n      \n    \n    {\\displaystyle d[(r),(s)]}\n  .\nThe single linkage algorithm is composed of the following steps:\n\nBegin with the disjoint clustering having level  \n  \n    \n      \n        L\n        (\n        0\n        )\n        =\n        0\n      \n    \n    {\\displaystyle L(0)=0}\n   and sequence number \n  \n    \n      \n        m\n        =\n        0\n      \n    \n    {\\displaystyle m=0}\n  .\nFind the most similar pair of clusters in the current clustering, say pair \n  \n    \n      \n        (\n        r\n        )\n        ,\n        (\n        s\n        )\n      \n    \n    {\\displaystyle (r),(s)}\n  , according to \n  \n    \n      \n        d\n        [\n        (\n        r\n        )\n        ,\n        (\n        s\n        )\n        ]\n        =\n        min\n        d\n        [\n        (\n        i\n        )\n        ,\n        (\n        j\n        )\n        ]\n      \n    \n    {\\displaystyle d[(r),(s)]=\\min d[(i),(j)]}\n  where the minimum is over all pairs of clusters in the current clustering.\nIncrement the sequence number: \n  \n    \n      \n        m\n        =\n        m\n        +\n        1\n      \n    \n    {\\displaystyle m=m+1}\n  . Merge clusters \n  \n    \n      \n        (\n        r\n        )\n      \n    \n    {\\displaystyle (r)}\n   and \n  \n    \n      \n        (\n        s\n        )\n      \n    \n    {\\displaystyle (s)}\n   into a single cluster to form the next clustering \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  . Set the level of this clustering to \n  \n    \n      \n        L\n        (\n        m\n        )\n        =\n        d\n        [\n        (\n        r\n        )\n        ,\n        (\n        s\n        )\n        ]\n      \n    \n    {\\displaystyle L(m)=d[(r),(s)]}\n  \nUpdate the proximity matrix, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  , by deleting the rows and columns corresponding to clusters \n  \n    \n      \n        (\n        r\n        )\n      \n    \n    {\\displaystyle (r)}\n   and \n  \n    \n      \n        (\n        s\n        )\n      \n    \n    {\\displaystyle (s)}\n   and adding a row and column corresponding to the newly formed cluster. The proximity between the new cluster, denoted \n  \n    \n      \n        (\n        r\n        ,\n        s\n        )\n      \n    \n    {\\displaystyle (r,s)}\n   and an old cluster \n  \n    \n      \n        (\n        k\n        )\n      \n    \n    {\\displaystyle (k)}\n   is defined as \n  \n    \n      \n        d\n        [\n        (\n        r\n        ,\n        s\n        )\n        ,\n        (\n        k\n        )\n        ]\n        =\n        min\n        {\n        d\n        [\n        (\n        k\n        )\n        ,\n        (\n        r\n        )\n        ]\n        ,\n        d\n        [\n        (\n        k\n        )\n        ,\n        (\n        s\n        )\n        ]\n        }\n      \n    \n    {\\displaystyle d[(r,s),(k)]=\\min\\{d[(k),(r)],d[(k),(s)]\\}}\n  .\nIf all objects are in one cluster, stop. Else, go to step 2.\n\n\n== Working example ==\nThis working example is based on a JC69 genetic distance matrix computed from the 5S ribosomal RNA sequence alignment of five bacteria: Bacillus subtilis (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  ), Bacillus stearothermophilus (\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  ), Lactobacillus viridescens (\n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  ), Acholeplasma modicum (\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  ), and Micrococcus luteus (\n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  ).\n\n\n=== First step ===\nFirst clusteringLet us assume that we have five elements \n  \n    \n      \n        (\n        a\n        ,\n        b\n        ,\n        c\n        ,\n        d\n        ,\n        e\n        )\n      \n    \n    {\\displaystyle (a,b,c,d,e)}\n   and the following matrix \n  \n    \n      \n        \n          D\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle D_{1}}\n   of pairwise distances between them:\n\nIn this example, \n  \n    \n      \n        \n          D\n          \n            1\n          \n        \n        (\n        a\n        ,\n        b\n        )\n        =\n        17\n      \n    \n    {\\displaystyle D_{1}(a,b)=17}\n   is the lowest value of \n  \n    \n      \n        \n          D\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle D_{1}}\n  , so we cluster elements a and b.\n\nFirst branch length estimationLet u denote the node to which a and b are now connected. Setting  \n  \n    \n      \n        \u03b4\n        (\n        a\n        ,\n        u\n        )\n        =\n        \u03b4\n        (\n        b\n        ,\n        u\n        )\n        =\n        \n          D\n          \n            1\n          \n        \n        (\n        a\n        ,\n        b\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle \\delta (a,u)=\\delta (b,u)=D_{1}(a,b)/2}\n    ensures that elements a and b are equidistant from u. This corresponds to the expectation of the ultrametricity hypothesis.\nThe branches joining a and b to u then have lengths  \n  \n    \n      \n        \u03b4\n        (\n        a\n        ,\n        u\n        )\n        =\n        \u03b4\n        (\n        b\n        ,\n        u\n        )\n        =\n        17\n        \n          /\n        \n        2\n        =\n        8.5\n      \n    \n    {\\displaystyle \\delta (a,u)=\\delta (b,u)=17/2=8.5}\n   (see the final dendrogram)\n\nFirst distance matrix updateWe then proceed to update the initial proximity matrix \n  \n    \n      \n        \n          D\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle D_{1}}\n   into a new proximity matrix \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n   (see below), reduced in size by one row and one column because of the clustering of a with b.\nBold values in \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n   correspond to the new distances, calculated by retaining the minimum distance between each element of the first cluster \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle (a,b)}\n   and each of the remaining elements:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  D\n                  \n                    2\n                  \n                \n                (\n                (\n                a\n                ,\n                b\n                )\n                ,\n                c\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                a\n                ,\n                c\n                )\n                ,\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                b\n                ,\n                c\n                )\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                21\n                ,\n                30\n                )\n              \n              \n                =\n              \n              \n                21\n              \n            \n            \n              \n                \n                  D\n                  \n                    2\n                  \n                \n                (\n                (\n                a\n                ,\n                b\n                )\n                ,\n                d\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                a\n                ,\n                d\n                )\n                ,\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                b\n                ,\n                d\n                )\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                31\n                ,\n                34\n                )\n              \n              \n                =\n              \n              \n                31\n              \n            \n            \n              \n                \n                  D\n                  \n                    2\n                  \n                \n                (\n                (\n                a\n                ,\n                b\n                )\n                ,\n                e\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                a\n                ,\n                e\n                )\n                ,\n                \n                  D\n                  \n                    1\n                  \n                \n                (\n                b\n                ,\n                e\n                )\n                )\n              \n              \n                =\n              \n              \n                min\n                (\n                23\n                ,\n                21\n                )\n              \n              \n                =\n              \n              \n                21\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lllllll}D_{2}((a,b),c)&=&\\min(D_{1}(a,c),D_{1}(b,c))&=&\\min(21,30)&=&21\\\\D_{2}((a,b),d)&=&\\min(D_{1}(a,d),D_{1}(b,d))&=&\\min(31,34)&=&31\\\\D_{2}((a,b),e)&=&\\min(D_{1}(a,e),D_{1}(b,e))&=&\\min(23,21)&=&21\\end{array}}}\n  Italicized values in \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n   are not affected by the matrix update as they correspond to distances between elements not involved in the first cluster.\n\n\n=== Second step ===\nSecond clusteringWe now reiterate the three previous actions, starting from the new distance matrix \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n   :\n\nHere, \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        )\n        =\n        21\n      \n    \n    {\\displaystyle D_{2}((a,b),c)=21}\n    and  \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        e\n        )\n        =\n        21\n      \n    \n    {\\displaystyle D_{2}((a,b),e)=21}\n    are the lowest values of \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n  , so we join cluster \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle (a,b)}\n   with element c and with element e.\n\nSecond branch length estimationLet v denote the node to which \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle (a,b)}\n  , c and e are now connected. Because of the ultrametricity constraint, the branches joining a or b to v, and c to v, and also e to v are equal and have the following total length:\n\n  \n    \n      \n        \u03b4\n        (\n        a\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        b\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        c\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        e\n        ,\n        v\n        )\n        =\n        21\n        \n          /\n        \n        2\n        =\n        10.5\n      \n    \n    {\\displaystyle \\delta (a,v)=\\delta (b,v)=\\delta (c,v)=\\delta (e,v)=21/2=10.5}\n  We deduce the missing branch length:\n\n  \n    \n      \n        \u03b4\n        (\n        u\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        c\n        ,\n        v\n        )\n        \u2212\n        \u03b4\n        (\n        a\n        ,\n        u\n        )\n        =\n        \u03b4\n        (\n        c\n        ,\n        v\n        )\n        \u2212\n        \u03b4\n        (\n        b\n        ,\n        u\n        )\n        =\n        10.5\n        \u2212\n        8.5\n        =\n        2\n      \n    \n    {\\displaystyle \\delta (u,v)=\\delta (c,v)-\\delta (a,u)=\\delta (c,v)-\\delta (b,u)=10.5-8.5=2}\n   (see the final dendrogram)Second distance matrix updateWe then proceed to update the \n  \n    \n      \n        \n          D\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle D_{2}}\n   matrix into a new distance matrix \n  \n    \n      \n        \n          D\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle D_{3}}\n   (see below), reduced in size by two rows and two columns because of the clustering of \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle (a,b)}\n   with c and with e :\n\n  \n    \n      \n        \n          D\n          \n            3\n          \n        \n        (\n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        ,\n        e\n        )\n        ,\n        d\n        )\n        =\n        min\n        (\n        \n          D\n          \n            2\n          \n        \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        d\n        )\n        ,\n        \n          D\n          \n            2\n          \n        \n        (\n        c\n        ,\n        d\n        )\n        ,\n        \n          D\n          \n            2\n          \n        \n        (\n        e\n        ,\n        d\n        )\n        )\n        =\n        min\n        (\n        31\n        ,\n        28\n        ,\n        43\n        )\n        =\n        28\n      \n    \n    {\\displaystyle D_{3}(((a,b),c,e),d)=\\min(D_{2}((a,b),d),D_{2}(c,d),D_{2}(e,d))=\\min(31,28,43)=28}\n  \n\n\n=== Final step ===\nThe final \n  \n    \n      \n        \n          D\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle D_{3}}\n   matrix is:\n\nSo we join clusters \n  \n    \n      \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ((a,b),c,e)}\n   and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  .\nLet \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   denote the (root) node to which \n  \n    \n      \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ((a,b),c,e)}\n   and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   are now connected.\nThe branches joining \n  \n    \n      \n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ((a,b),c,e)}\n   and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   to \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   then have lengths:\n\n  \n    \n      \n        \u03b4\n        (\n        (\n        (\n        a\n        ,\n        b\n        )\n        ,\n        c\n        ,\n        e\n        )\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        d\n        ,\n        r\n        )\n        =\n        28\n        \n          /\n        \n        2\n        =\n        14\n      \n    \n    {\\displaystyle \\delta (((a,b),c,e),r)=\\delta (d,r)=28/2=14}\n  \nWe deduce the remaining branch length:\n\n  \n    \n      \n        \u03b4\n        (\n        v\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        a\n        ,\n        r\n        )\n        \u2212\n        \u03b4\n        (\n        a\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        b\n        ,\n        r\n        )\n        \u2212\n        \u03b4\n        (\n        b\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        c\n        ,\n        r\n        )\n        \u2212\n        \u03b4\n        (\n        c\n        ,\n        v\n        )\n        =\n        \u03b4\n        (\n        e\n        ,\n        r\n        )\n        \u2212\n        \u03b4\n        (\n        e\n        ,\n        v\n        )\n        =\n        14\n        \u2212\n        10.5\n        =\n        3.5\n      \n    \n    {\\displaystyle \\delta (v,r)=\\delta (a,r)-\\delta (a,v)=\\delta (b,r)-\\delta (b,v)=\\delta (c,r)-\\delta (c,v)=\\delta (e,r)-\\delta (e,v)=14-10.5=3.5}\n  \n\n\n=== The single-linkage dendrogram ===\n\nThe dendrogram is now complete. It is ultrametric because all tips (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  , \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  , \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  , and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  ) are equidistant from \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   :\n\n  \n    \n      \n        \u03b4\n        (\n        a\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        b\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        c\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        e\n        ,\n        r\n        )\n        =\n        \u03b4\n        (\n        d\n        ,\n        r\n        )\n        =\n        14\n      \n    \n    {\\displaystyle \\delta (a,r)=\\delta (b,r)=\\delta (c,r)=\\delta (e,r)=\\delta (d,r)=14}\n  \nThe dendrogram is therefore rooted by \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , its deepest node.\n\n\n== Other linkages ==\nThe naive algorithm for single linkage clustering is essentially the same as Kruskal's algorithm for minimum spanning trees. However, in single linkage clustering, the order in which clusters are formed is important, while for minimum spanning trees what matters is the set of pairs of points that form distances chosen by the algorithm.\nAlternative linkage schemes include complete linkage clustering, average linkage clustering (UPGMA and WPGMA), and Ward's method. In the naive algorithm for agglomerative clustering, implementing a different linkage scheme may be accomplished simply by using a different formula to calculate inter-cluster distances in the algorithm. The formula that should be adjusted has been highlighted using bold text in the above algorithm description. However, more efficient algorithms such as the one described below do not generalize to all linkage schemes in the same way.\n\n\n== Faster algorithms ==\nThe naive algorithm for single-linkage clustering is easy to understand but slow, with time complexity \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3})}\n  . In 1973, R. Sibson proposed an algorithm with time complexity \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n   and space complexity \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   (both optimal) known as SLINK. The slink algorithm represents a clustering on a set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   numbered items by two functions. These functions are both determined by finding the smallest cluster \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   that contains both item \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   and at least one larger-numbered item.\nThe first function, \n  \n    \n      \n        \u03c0\n      \n    \n    {\\displaystyle \\pi }\n  , maps item \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   to the largest-numbered item in cluster \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\nThe second function, \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  , maps item \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   to the distance associated with the creation of cluster \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\nStoring these functions in two arrays that map each item number to its function value takes space \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  , and this information is sufficient to determine the clustering itself. As Sibson shows, when a new item is added to the set of items, the updated functions representing the new single-linkage clustering for the augmented set, represented in the same way, can be constructed from the old clustering in time \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  . The SLINK algorithm then loops over the items, one by one, adding them to the representation of the clustering.An alternative algorithm, running in the same optimal time and space bounds, is based on the equivalence between the naive algorithm and Kruskal's algorithm for minimum spanning trees. Instead of using Kruskal's algorithm, one can use Prim's algorithm, in a variation without binary heaps that takes time \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n   and space \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   to construct the minimum spanning tree (but not the clustering) of the given items and distances. Then, applying Kruskal's algorithm to the sparse graph formed by the edges of the minimum spanning tree produces the clustering itself in an additional time \n  \n    \n      \n        O\n        (\n        n\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle O(n\\log n)}\n   and space \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  .\n\n\n== See also ==\nCluster analysis\nComplete-linkage clustering\nHierarchical clustering\nMolecular clock\nNeighbor-joining\nUPGMA\nWPGMA\n\n\n== References ==\n\n\n== External links ==\nLinkages used in Matlab", "Positive and negative predictive values": "The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively.  The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test (as true positive rate and true negative rate are); they depend also on the prevalence. Both PPV and NPV can be derived using Bayes' theorem.\nAlthough sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal.\nIn information retrieval, the PPV statistic is often called the precision.\n\n\n== Definition ==\n\n\n=== Positive predictive value (PPV) ===\nThe positive predictive value (PPV), or precision, is defined as \n\n  \n    \n      \n        \n          PPV\n        \n        =\n        \n          \n            Number of true positives\n            \n              \n                Number of true positives\n              \n              +\n              \n                Number of false positives\n              \n            \n          \n        \n        =\n        \n          \n            Number of true positives\n            Number of positive calls\n          \n        \n      \n    \n    {\\displaystyle {\\text{PPV}}={\\frac {\\text{Number of true positives}}{{\\text{Number of true positives}}+{\\text{Number of false positives}}}}={\\frac {\\text{Number of true positives}}{\\text{Number of positive calls}}}}\n  where a \"true positive\" is the event that the test makes a positive prediction, and the subject has a positive result under the gold standard, and a \"false positive\" is the event that the test makes a positive prediction, and the subject has a negative result under the gold standard. The ideal value of the PPV, with a perfect test, is 1 (100%), and the worst possible value would be zero.  \nThe PPV can also be computed from sensitivity, specificity, and the prevalence of the condition:\n\n  \n    \n      \n        \n          PPV\n        \n        =\n        \n          \n            \n              \n                sensitivity\n              \n              \u00d7\n              \n                prevalence\n              \n            \n            \n              \n                sensitivity\n              \n              \u00d7\n              \n                prevalence\n              \n              +\n              (\n              1\n              \u2212\n              \n                specificity\n              \n              )\n              \u00d7\n              (\n              1\n              \u2212\n              \n                prevalence\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{PPV}}={\\frac {{\\text{sensitivity}}\\times {\\text{prevalence}}}{{\\text{sensitivity}}\\times {\\text{prevalence}}+(1-{\\text{specificity}})\\times (1-{\\text{prevalence}})}}}\n  cf. Bayes' theorem\nThe complement of the PPV is the false discovery rate (FDR):\n\n  \n    \n      \n        \n          FDR\n        \n        =\n        1\n        \u2212\n        \n          PPV\n        \n        =\n        \n          \n            Number of false positives\n            \n              \n                Number of true positives\n              \n              +\n              \n                Number of false positives\n              \n            \n          \n        \n        =\n        \n          \n            Number of false positives\n            Number of positive calls\n          \n        \n      \n    \n    {\\displaystyle {\\text{FDR}}=1-{\\text{PPV}}={\\frac {\\text{Number of false positives}}{{\\text{Number of true positives}}+{\\text{Number of false positives}}}}={\\frac {\\text{Number of false positives}}{\\text{Number of positive calls}}}}\n  \n\n\n=== Negative predictive value (NPV) ===\nThe negative predictive value is defined as:\n\n  \n    \n      \n        \n          NPV\n        \n        =\n        \n          \n            Number of true negatives\n            \n              \n                Number of true negatives\n              \n              +\n              \n                Number of false negatives\n              \n            \n          \n        \n        =\n        \n          \n            Number of true negatives\n            Number of negative calls\n          \n        \n      \n    \n    {\\displaystyle {\\text{NPV}}={\\frac {\\text{Number of true negatives}}{{\\text{Number of true negatives}}+{\\text{Number of false negatives}}}}={\\frac {\\text{Number of true negatives}}{\\text{Number of negative calls}}}}\n  where a \"true negative\" is the event that the test makes a negative prediction, and the subject has a negative result under the gold standard, and a \"false negative\" is the event that the test makes a negative prediction, and the subject has a positive result under the gold standard. With a perfect test, one which returns no false negatives, the value of the NPV is 1 (100%), and with a test which returns no true negatives the NPV value is zero.  \nThe NPV can also be computed from sensitivity, specificity, and prevalence:\n\n  \n    \n      \n        \n          NPV\n        \n        =\n        \n          \n            \n              \n                specificity\n              \n              \u00d7\n              (\n              1\n              \u2212\n              \n                prevalence\n              \n              )\n            \n            \n              \n                specificity\n              \n              \u00d7\n              (\n              1\n              \u2212\n              \n                prevalence\n              \n              )\n              +\n              (\n              1\n              \u2212\n              \n                sensitivity\n              \n              )\n              \u00d7\n              \n                prevalence\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{NPV}}={\\frac {{\\text{specificity}}\\times (1-{\\text{prevalence}})}{{\\text{specificity}}\\times (1-{\\text{prevalence}})+(1-{\\text{sensitivity}})\\times {\\text{prevalence}}}}}\n  \n  \n    \n      \n        \n          NPV\n        \n        =\n        \n          \n            \n              T\n              N\n            \n            \n              T\n              N\n              +\n              F\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{NPV}}={\\frac {TN}{TN+FN}}}\n  The complement of the NPV is the false omission rate (FOR):\n\n  \n    \n      \n        \n          FOR\n        \n        =\n        1\n        \u2212\n        \n          NPV\n        \n        =\n        \n          \n            Number of false negatives\n            \n              \n                Number of true negatives\n              \n              +\n              \n                Number of false negatives\n              \n            \n          \n        \n        =\n        \n          \n            Number of false negatives\n            Number of negative calls\n          \n        \n      \n    \n    {\\displaystyle {\\text{FOR}}=1-{\\text{NPV}}={\\frac {\\text{Number of false negatives}}{{\\text{Number of true negatives}}+{\\text{Number of false negatives}}}}={\\frac {\\text{Number of false negatives}}{\\text{Number of negative calls}}}}\n  Although sometimes used synonymously, a negative predictive value generally refers to what is established by control groups, while a negative post-test probability rather refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the negative predictive value, then the two are numerically equal.\n\n\n=== Relationship ===\nThe following diagram illustrates how the positive predictive value, negative predictive value, sensitivity, and specificity are related.\n\nNote that the positive and negative predictive values can only be estimated using data from a cross-sectional study or other population-based study in which valid prevalence estimates may be obtained. In contrast, the sensitivity and specificity can be estimated from case-control studies.\n\n\n=== Worked example ===\nSuppose the fecal occult blood (FOB) screen test is used in 2030 people to look for bowel cancer:\n\nThe small positive predictive value (PPV = 10%) indicates that many of the positive results from this testing procedure are false positives. Thus it will be necessary to follow up any positive result with a more reliable test to obtain a more accurate assessment as to whether cancer is present. Nevertheless, such a test may be useful if it is inexpensive and convenient. The strength of the FOB screen test is instead in its negative predictive value \u2014 which, if negative for an individual, gives us a high confidence that its negative result is true.\n\n\n== Problems ==\n\n\n=== Other individual factors ===\nNote that the PPV is not intrinsic to the test\u2014it depends also on the prevalence. Due to the large effect of prevalence upon predictive values, a standardized approach has been proposed, where the PPV is normalized to a prevalence of 50%. PPV is directly proportional to the prevalence of the disease or condition. In the above example, if the group of people tested had included a higher proportion of people with bowel cancer, then the PPV would probably come out higher and the NPV lower. If everybody in the group had bowel cancer, the PPV would be 100% and the NPV 0%.\nTo overcome this problem, NPV and PPV should only be used if the ratio of the number of patients in the disease group and the number of patients in the healthy control group used to establish the NPV and PPV is equivalent to the prevalence of the diseases in the studied population, or, in case two disease groups are compared, if the ratio of the number of patients in disease group 1 and the number of patients in disease group 2 is equivalent to the ratio of the prevalences of the two diseases studied. Otherwise, positive and negative likelihood ratios are more accurate than NPV and PPV, because likelihood ratios do not depend on prevalence.\nWhen an individual being tested has a different pre-test probability of having a condition than the control groups used to establish the PPV and NPV, the PPV and NPV are generally distinguished from the positive and negative post-test probabilities, with the PPV and NPV referring to the ones established by the control groups, and the post-test probabilities referring to the ones for the tested individual (as estimated, for example, by likelihood ratios). Preferably, in such cases, a large group of equivalent individuals should be studied, in order to establish separate positive and negative predictive values for use of the test in such individuals.\n\n\n=== Bayesian updating ===\nBayes' theorem confers inherent limitations on the accuracy of screening tests as a function of disease prevalence or pre-test probability. It has been shown that a testing system can tolerate significant drops in prevalence, up to a certain well-defined point known as the prevalence threshold, below which the reliability of a positive screening test drops precipitously. That said, Balayla et al. showed that sequential testing overcomes the aforementioned Bayesian limitations and thus improves the reliability of screening tests. For a desired positive predictive value \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   that approaches some constant \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , the number of positive test iterations \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   needed is:\n\n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n        =\n        \n          lim\n          \n            \u03c1\n            \u2192\n            k\n          \n        \n        \n          \u2308\n          \n            \n              \n                ln\n                \u2061\n                \n                  [\n                  \n                    \n                      \n                        \u03c1\n                        (\n                        \u03d5\n                        \u2212\n                        1\n                        )\n                      \n                      \n                        \u03d5\n                        (\n                        \u03c1\n                        \u2212\n                        1\n                        )\n                      \n                    \n                  \n                  ]\n                \n              \n              \n                ln\n                \u2061\n                \n                  [\n                  \n                    \n                      a\n                      \n                        1\n                        \u2212\n                        b\n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n          \u2309\n        \n      \n    \n    {\\displaystyle n_{i}=\\lim _{\\rho \\to k}\\left\\lceil {\\frac {\\ln \\left[{\\frac {\\rho (\\phi -1)}{\\phi (\\rho -1)}}\\right]}{\\ln \\left[{\\frac {a}{1-b}}\\right]}}\\right\\rceil }\n  where \n\n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   is the desired PPV\n\n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   is the number of testing iterations necessary to achieve \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  \n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   is the sensitivity\n\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   is the specificity\n\n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   is disease prevalence, and\n\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is a constant.Of note, the denominator of the above equation is the natural logarithm of the positive likelihood ratio (LR+).\n\n\n=== Different target conditions ===\nPPV is used to indicate the probability that in case of a positive test, that the patient really has the specified disease. However, there may be more than one cause for a disease and any single potential cause may not always result in the overt disease seen in a patient. There is potential to mix up related target conditions of PPV and NPV, such as interpreting the PPV or NPV of a test as having a disease, when that PPV or NPV value actually refers only to a predisposition of having that disease.\nAn example is the microbiological throat swab used in patients with a sore throat. Usually publications stating PPV of a throat swab are reporting on the probability that this bacterium is present in the throat, rather than that the patient is ill from the bacteria found. If presence of this bacterium always resulted in a sore throat, then the PPV would be very useful. However the bacteria may colonise individuals in a harmless way and never result in infection or disease. Sore throats occurring in these individuals are caused by other agents such as a virus. In this situation the gold standard used in the evaluation study represents only the presence of bacteria (that might be harmless) but not a causal bacterial sore throat illness. It can be proven that this problem will affect positive predictive value far more than negative predictive value. To evaluate diagnostic tests where the gold standard looks only at potential causes of disease, one may use an extension of the predictive value termed the Etiologic Predictive Value.\n\n\n== See also ==\nBinary classification\nSensitivity and specificity\nFalse discovery rate\nRelevance (information retrieval)\nReceiver-operator characteristic\nDiagnostic odds ratio\nSensitivity index\n\n\n== References ==", "Fitness function": "A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims. Fitness functions are used in evolutionary algorithms (EA), such as genetic programming and genetic algorithms to guide simulations towards optimal design solutions.In the field of EAs, each design solution is commonly represented as a string of numbers (referred to as a chromosome). After each round of testing, or simulation, the idea is to delete the n worst design solutions, and to breed n new ones from the best design solutions. Each design solution, therefore, needs to be awarded a figure of merit, to indicate how close it came to meeting the overall specification, and this is generated by applying the fitness function to the test, or simulation, results obtained from that solution.Two main classes of fitness functions exist: one where the fitness function does not change, as in optimizing a fixed function or testing with a fixed set of test cases; and one where the fitness function is mutable, as in niche differentiation or co-evolving the set of test cases. Another way of looking at fitness functions is in terms of a fitness landscape, which shows the fitness for each possible chromosome. In the following, it is assumed that the fitness is determined based on an evaluation that remains unchanged during an optimization run.\nA fitness function does not necessarily have to be able to calculate an absolute value, as it is sometimes sufficient to compare candidates in order to select the better one. A relative indication of fitness (candidate a is better than b) is sufficient in some cases, such as tournament selection or Pareto optimization.\n\n\n== Requirements of evaluation and fitness function ==\nThe quality of the evaluation and calculation of a fitness function is fundamental to the success of an EA optimisation. It implements Darwin's principle of \"survival of the fittest\". Without fitness-based selection mechanisms for mate selection and offspring acceptance, EA search would be blind and hardly distinguishable from the Monte Carlo method. When setting up a fitness function, one must always be aware that it is about more than just describing the desired target state. Rather, the evolutionary search on the way to the optimum should also be supported as much as possible (see also section on auxiliary objectives), if and insofar as this is not already done by the fitness function alone. If the fitness function is designed badly, the algorithm will either converge on an inappropriate solution, or will have difficulty converging at all.\nDefinition of the fitness function is not straightforward in many cases and often is performed iteratively if the fittest solutions produced by an EA is not what is desired. Interactive genetic algorithms address this difficulty by outsourcing evaluation to external agents which are normally humans.\n\n\n== Computational efficiency ==\nThe fitness function should not only correlate closely with the designer's goal, but it also should be computationally efficient. Speed of execution is very important, as a typical genetic algorithm must be iterated many times in order to produce a usable result for a non-trivial problem.\nFitness approximation may be appropriate, especially in the following cases: \n\nFitness computation time of a single solution is extremely high\nPrecise model for fitness computation is missing\nThe fitness function is uncertain or noisy.Alternatively or also in addition to the fitness approximation, the fitness calculations can also be distributed to a parallel computer in order to reduce the execution times. Depending on the population model of the EA used, both the EA itself and the fitness calculations of all offspring of one generation can be executed in parallel.\n\n\n== Multi-objective optimization ==\nPractical applications usually aim at optimizing multiple and at least partially conflicting objectives. Two fundamentally different approaches are often used for this purpose, Pareto optimization and optimization based on fitness calculated using the weighted sum.\n\n\n=== Weighted sum and penalty functions ===\nWhen optimizing with the weighted sum, the single values of the \n  \n    \n      \n        O\n      \n    \n    {\\displaystyle O}\n   objectives are first normalized so that they can be compared. This can be done with the help of costs or by specifying target values and determining the current value as the degree of fulfillment. Costs or degrees of fulfillment can then be compared with each other and, if required, can also be mapped to a uniform fitness scale. Without loss of generality, fitness is assumed to represent a value to be maximized. Each objective \n  \n    \n      \n        \n          o\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle o_{i}}\n   is assigned a weight \n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}}\n   in the form of a percentage value so that the overall raw fitness \n  \n    \n      \n        \n          f\n          \n            r\n            a\n            w\n          \n        \n      \n    \n    {\\displaystyle f_{raw}}\n   can be calculated as a weighted sum: \n  \n    \n      \n        \n          f\n          \n            r\n            a\n            w\n          \n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            O\n          \n        \n        \n          \n            o\n            \n              i\n            \n          \n          \u22c5\n          \n            w\n            \n              i\n            \n          \n        \n        \n        \n          \n            w\n            i\n            t\n            h\n          \n        \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            O\n          \n        \n        \n          \n            w\n            \n              i\n            \n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle f_{raw}=\\sum _{i=1}^{O}{o_{i}\\cdot w_{i}}\\quad {\\mathsf {with}}\\quad \\sum _{i=1}^{O}{w_{i}}=1}\n   A violation of \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n   restrictions \n  \n    \n      \n        \n          r\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle r_{j}}\n   can be included in the fitness determined in this way in the form of penalty functions. For this purpose, a function \n  \n    \n      \n        p\n        \n          f\n          \n            j\n          \n        \n        (\n        \n          r\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle pf_{j}(r_{j})}\n   can be defined for each restriction which returns a value between \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   and \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   depending on the degree of violation, with the result being \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   if there is no violation. The previously determined raw fitness is multiplied by the penalty function(s) and the result is then the final fitness \n  \n    \n      \n        \n          f\n          \n            f\n            i\n            n\n            a\n            l\n          \n        \n      \n    \n    {\\displaystyle f_{final}}\n  : \n  \n    \n      \n        \n          f\n          \n            f\n            i\n            n\n            a\n            l\n          \n        \n        =\n        \n          f\n          \n            r\n            a\n            w\n          \n        \n        \u22c5\n        \n          \u220f\n          \n            j\n            =\n            1\n          \n          \n            R\n          \n        \n        \n          p\n          \n            f\n            \n              j\n            \n          \n          (\n          \n            r\n            \n              j\n            \n          \n          )\n        \n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            O\n          \n        \n        \n          (\n          \n            o\n            \n              i\n            \n          \n          \u22c5\n          \n            w\n            \n              i\n            \n          \n          )\n        \n        \u22c5\n        \n          \u220f\n          \n            j\n            =\n            1\n          \n          \n            R\n          \n        \n        \n          p\n          \n            f\n            \n              j\n            \n          \n          (\n          \n            r\n            \n              j\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{final}=f_{raw}\\cdot \\prod _{j=1}^{R}{pf_{j}(r_{j})}=\\sum _{i=1}^{O}{(o_{i}\\cdot w_{i})}\\cdot \\prod _{j=1}^{R}{pf_{j}(r_{j})}}\n   This approach is simple and has the advantage of being able to combine any number of objectives and restrictions. The disadvantage is that different objectives can compensate each other and that the weights have to be defined before the optimization. In addition, certain solutions may not be obtained, see the section on the comparison of both types of optimization.\n\n\n=== Pareto optimization ===\nA solution is called Pareto-optimal if the improvement of one objective is only possible with a deterioration of at least one other objective. The set of all Pareto-optimal solutions, also called Pareto set, represents the set of all optimal compromises between the objectives. The figure below on the right shows an example of the Pareto set of two objectives \n  \n    \n      \n        \n          f\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle f_{1}}\n   and \n  \n    \n      \n        \n          f\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f_{2}}\n   to be maximized. The elements of the set form the Pareto front (green line). From this set, a human decision maker must subsequently select the desired compromise solution. Constraints are included in Pareto optimization in that solutions without constraint violations are per se better than those with violations. If two solutions to be compared each have constraint violations, the respective extent of the violations decides.It was recognized early on that EAs with their simultaneously considered solution set are well suited to finding solutions in one run that cover the Pareto front sufficiently well. Besides the SPEA2, the NSGA-II and NSGA-III have established themselves as standard methods.\nThe advantage of Pareto optimization is that, in contrast to the weighted sum, it provides all alternatives that are equivalent in terms of the objectives as an overall solution. The disadvantage is that a visualization of the alternatives becomes problematic or even impossible from four objectives on. Furthermore, the effort increases exponentially with the number of objectives. If there are more than three or four objectives, some have to be combined using the weighted sum or other aggregation methods.\n\n\n=== Comparison of both types of assessment ===\n\nWith the help of the weighted sum, the total Pareto front can be obtained by a suitable choice of weights, provided that it is convex. This is illustrated by the adjacent picture on the left. The point \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {P}}}\n   on the green Pareto front is reached by the weights \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle w_{1}}\n   and \n  \n    \n      \n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{2}}\n  , provided that the EA converges to the optimum. The direction with the largest fitness gain in the solution set \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   is shown by the drawn arrows.\nIn case of a non-convex front, however, non-convex front sections are not reachable by the weighted sum. In the adjacent image on the right, this is the section between points \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {A}}}\n   and \n  \n    \n      \n        \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {B}}}\n  . This can be remedied to a limited extent by using an extension of the weighted sum, the cascaded weighted sum.Comparing both assessment approaches, the use of Pareto optimization is certainly advantageous when little is known about the possible solutions of a task and when the number of optimization objectives can be narrowed down to three, at most four. However, in the case of repeated optimization of variations of one and the same task, the desired lines of compromise are usually known and the effort to determine the entire Pareto front is no longer justified. This is also true when no human decision is desired or possible after optimization, such as in automated decision processes.\n\n\n== Auxiliary objectives ==\n\nIn addition to the primary objectives resulting from the task itself, it may be necessary to include auxiliary objectives in the assessment to support the achievement of one or more primary objectives. An example of a scheduling task is used for illustration purposes. The optimization goals include not only a general fast processing of all orders but also the compliance with a latest completion time. The latter is especially necessary for the scheduling of rush orders. The second goal is not achieved by the exemplary initial schedule, as shown in the adjacent figure. A following mutation does not change this, but schedules the work step d earlier, which is a necessary intermediate step for an earlier start of the last work step e of the order. As long as only the latest completion time is evaluated, however, the fitness of the mutated schedule remains unchanged, even though it represents a relevant step towards the objective of a timely completion of the order. This can be remedied, for example, by an additional evaluation of the delay of work steps. The new objective is an auxiliary one, since it was introduced in addition to the actual optimization objectives to support their achievement. A more detailed description of this approach and another example can be found in.\n\n\n== See also ==\nEvolutionary computation\nInferential programming\nTest functions for optimization\nLoss function\n\n\n== External links ==\nA Nice Introduction to Adaptive Fuzzy Fitness Granulation (AFFG) (PDF), A promising approach to accelerate the convergence rate of EAs.\nThe cyber shack of Adaptive Fuzzy Fitness Granulation (AFFG) That is designed to accelerate the convergence rate of EAs.\nFitness functions in evolutionary robotics: A survey and analysis (AFFG) (PDF), A review of fitness functions used in evolutionary robotics.\nFord, Neal; Richards, Mark, Sadalage, Pramod; Dehghani, Zhamak. (2021) Software Architecture: The Hard Parts O'Reilly Media, Inc. ISBN 9781492086895.\n\n\n== References ==", "Poisson regression": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.\nNegative binomial regression is a popular generalization of Poisson regression because it loosens the highly restrictive assumption that the variance is equal to the mean made by the Poisson model. The traditional negative binomial regression model is based on the Poisson-gamma mixture distribution. This model is popular because it models the Poisson heterogeneity with a gamma distribution.\nPoisson regression models are generalized linear models with the logarithm as the (canonical) link function, and the Poisson distribution function as the assumed probability distribution of the response.\n\n\n== Regression models ==\nIf \n  \n    \n      \n        \n          x\n        \n        \u2208\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{n}}\n   is a vector of independent variables, then the model takes the form\n\n  \n    \n      \n        log\n        \u2061\n        (\n        E\n        \u2061\n        (\n        Y\n        \u2223\n        \n          x\n        \n        )\n        )\n        =\n        \u03b1\n        +\n        \n          \n            \u03b2\n          \n          \u2032\n        \n        \n          x\n        \n        ,\n      \n    \n    {\\displaystyle \\log(\\operatorname {E} (Y\\mid \\mathbf {x} ))=\\alpha +\\mathbf {\\beta } '\\mathbf {x} ,}\n  where \n  \n    \n      \n        \u03b1\n        \u2208\n        \n          R\n        \n      \n    \n    {\\displaystyle \\alpha \\in \\mathbb {R} }\n   and \n  \n    \n      \n        \n          \u03b2\n        \n        \u2208\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\beta } \\in \\mathbb {R} ^{n}}\n  . Sometimes this is written more compactly as\n\n  \n    \n      \n        log\n        \u2061\n        (\n        E\n        \u2061\n        (\n        Y\n        \u2223\n        \n          x\n        \n        )\n        )\n        =\n        \n          \n            \u03b8\n          \n          \u2032\n        \n        \n          x\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\log(\\operatorname {E} (Y\\mid \\mathbf {x} ))={\\boldsymbol {\\theta }}'\\mathbf {x} ,\\,}\n  where \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   is now an (n + 1)-dimensional vector consisting of n independent variables concatenated to the number one. Here \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is simply \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   concatenated to \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  .\nThus, when given a Poisson regression model \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   and an input vector \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  , the predicted mean of the associated Poisson distribution is given by\n\n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        \n          x\n        \n        )\n        =\n        \n          e\n          \n            \n              \n                \u03b8\n              \n              \u2032\n            \n            \n              x\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid \\mathbf {x} )=e^{{\\boldsymbol {\\theta }}'\\mathbf {x} }.\\,}\n  If \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   are independent observations with corresponding values \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n   of the predictor variables, then \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   can be estimated by maximum likelihood.  The maximum-likelihood estimates lack a closed-form expression and must be found by numerical methods.  The probability surface for maximum-likelihood Poisson regression is always concave, making Newton\u2013Raphson or other gradient-based methods appropriate estimation techniques.\n\n\n== Maximum likelihood-based parameter estimation ==\nGiven a set of parameters \u03b8 and an input vector x, the mean of the predicted Poisson distribution, as stated above, is given by\n\n  \n    \n      \n        \u03bb\n        :=\n        E\n        \u2061\n        (\n        Y\n        \u2223\n        x\n        )\n        =\n        \n          e\n          \n            \n              \u03b8\n              \u2032\n            \n            x\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle \\lambda :=\\operatorname {E} (Y\\mid x)=e^{\\theta 'x},\\,}\n  and thus, the Poisson distribution's probability mass function is given by\n\n  \n    \n      \n        p\n        (\n        y\n        \u2223\n        x\n        ;\n        \u03b8\n        )\n        =\n        \n          \n            \n              \u03bb\n              \n                y\n              \n            \n            \n              y\n              !\n            \n          \n        \n        \n          e\n          \n            \u2212\n            \u03bb\n          \n        \n        =\n        \n          \n            \n              \n                e\n                \n                  y\n                  \n                    \u03b8\n                    \u2032\n                  \n                  x\n                \n              \n              \n                e\n                \n                  \u2212\n                  \n                    e\n                    \n                      \n                        \u03b8\n                        \u2032\n                      \n                      x\n                    \n                  \n                \n              \n            \n            \n              y\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle p(y\\mid x;\\theta )={\\frac {\\lambda ^{y}}{y!}}e^{-\\lambda }={\\frac {e^{y\\theta 'x}e^{-e^{\\theta 'x}}}{y!}}}\n  Now suppose we are given a data set consisting of m vectors \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \u2208\n        \n          \n            R\n          \n          \n            n\n            +\n            1\n          \n        \n        ,\n        \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        m\n      \n    \n    {\\displaystyle x_{i}\\in \\mathbb {R} ^{n+1},\\,i=1,\\ldots ,m}\n  , along with a set of m values \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            m\n          \n        \n        \u2208\n        \n          N\n        \n      \n    \n    {\\displaystyle y_{1},\\ldots ,y_{m}\\in \\mathbb {N} }\n  . Then, for a given set of parameters \u03b8, the probability of attaining this particular set of data is given by\n\n  \n    \n      \n        p\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            m\n          \n        \n        \u2223\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            m\n          \n        \n        ;\n        \u03b8\n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              \n                e\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  \n                    \u03b8\n                    \u2032\n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n              \n              \n                e\n                \n                  \u2212\n                  \n                    e\n                    \n                      \n                        \u03b8\n                        \u2032\n                      \n                      \n                        x\n                        \n                          i\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                y\n                \n                  i\n                \n              \n              !\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle p(y_{1},\\ldots ,y_{m}\\mid x_{1},\\ldots ,x_{m};\\theta )=\\prod _{i=1}^{m}{\\frac {e^{y_{i}\\theta 'x_{i}}e^{-e^{\\theta 'x_{i}}}}{y_{i}!}}.}\n  By the method of maximum likelihood, we wish to find the set of parameters \u03b8 that makes this probability as large as possible. To do this, the equation is first rewritten as a likelihood function in terms of \u03b8:\n\n  \n    \n      \n        L\n        (\n        \u03b8\n        \u2223\n        X\n        ,\n        Y\n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              \n                e\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  \n                    \u03b8\n                    \u2032\n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n              \n              \n                e\n                \n                  \u2212\n                  \n                    e\n                    \n                      \n                        \u03b8\n                        \u2032\n                      \n                      \n                        x\n                        \n                          i\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                y\n                \n                  i\n                \n              \n              !\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle L(\\theta \\mid X,Y)=\\prod _{i=1}^{m}{\\frac {e^{y_{i}\\theta 'x_{i}}e^{-e^{\\theta 'x_{i}}}}{y_{i}!}}.}\n  Note that the expression on the right hand side has not actually changed. A formula in this form is typically difficult to work with; instead, one uses the log-likelihood:\n\n  \n    \n      \n        \u2113\n        (\n        \u03b8\n        \u2223\n        X\n        ,\n        Y\n        )\n        =\n        log\n        \u2061\n        L\n        (\n        \u03b8\n        \u2223\n        X\n        ,\n        Y\n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          (\n          \n            \n              y\n              \n                i\n              \n            \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n            \u2212\n            \n              e\n              \n                \n                  \u03b8\n                  \u2032\n                \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \u2212\n            log\n            \u2061\n            (\n            \n              y\n              \n                i\n              \n            \n            !\n            )\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\ell (\\theta \\mid X,Y)=\\log L(\\theta \\mid X,Y)=\\sum _{i=1}^{m}\\left(y_{i}\\theta 'x_{i}-e^{\\theta 'x_{i}}-\\log(y_{i}!)\\right).}\n  Notice that the parameters \u03b8 only appear in the first two terms of each term in the summation. Therefore, given that we are only interested in finding the best value for \u03b8 we may drop the yi! and simply write\n\n  \n    \n      \n        \u2113\n        (\n        \u03b8\n        \u2223\n        X\n        ,\n        Y\n        )\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          (\n          \n            \n              y\n              \n                i\n              \n            \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n            \u2212\n            \n              e\n              \n                \n                  \u03b8\n                  \u2032\n                \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\ell (\\theta \\mid X,Y)=\\sum _{i=1}^{m}\\left(y_{i}\\theta 'x_{i}-e^{\\theta 'x_{i}}\\right).}\n  To find a maximum, we need to solve an equation \n  \n    \n      \n        \n          \n            \n              \u2202\n              \u2113\n              (\n              \u03b8\n              \u2223\n              X\n              ,\n              Y\n              )\n            \n            \n              \u2202\n              \u03b8\n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial \\ell (\\theta \\mid X,Y)}{\\partial \\theta }}=0}\n   which has no closed-form solution. However, the negative log-likelihood, \n  \n    \n      \n        \u2212\n        \u2113\n        (\n        \u03b8\n        \u2223\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle -\\ell (\\theta \\mid X,Y)}\n  , is a convex function, and so standard convex optimization techniques such as gradient descent can be applied to find the optimal value of \u03b8.\n\n\n== Poisson regression in practice ==\nPoisson regression may be appropriate when the dependent variable is a count, for instance of events such as the arrival of a telephone call at a call centre. The events must be independent in the sense that the arrival of one call will not make another more or less likely, but the probability per unit time of events is understood to be related to covariates such as time of day.\n\n\n=== \"Exposure\" and offset ===\nPoisson regression may also be appropriate for rate data, where the rate is a count of events divided by some measure of that unit's exposure (a particular unit of observation). For example, biologists may count the number of tree species in a forest:  events would be tree observations, exposure would be unit area, and rate would be the number of species per unit area. Demographers may model death rates in geographic areas as the count of deaths divided by person\u2212years. More generally, event rates can be calculated as events per unit time, which allows the observation window to vary for each unit. In these examples, exposure is respectively unit area, person\u2212years and unit time. In Poisson regression this is handled as an offset. If the rate is count/exposure, multiplying both sides of the equation by exposure moves it to the right side of the equation.  When both sides of the equation are then logged, the final model contains log(exposure) as a term that is added to the regression coefficients. This logged variable, log(exposure), is called the offset variable and enters on the right-hand side of the equation with a parameter estimate (for log(exposure)) constrained to 1. \n\n  \n    \n      \n        log\n        \u2061\n        (\n        E\n        \u2061\n        (\n        Y\n        \u2223\n        x\n        )\n        )\n        =\n        \n          \u03b8\n          \u2032\n        \n        x\n      \n    \n    {\\displaystyle \\log(\\operatorname {E} (Y\\mid x))=\\theta 'x}\n  which implies\n\n  \n    \n      \n        log\n        \u2061\n        \n          (\n          \n            \n              \n                E\n                \u2061\n                (\n                Y\n                \u2223\n                x\n                )\n              \n              exposure\n            \n          \n          )\n        \n        =\n        log\n        \u2061\n        (\n        E\n        \u2061\n        (\n        Y\n        \u2223\n        x\n        )\n        )\n        \u2212\n        log\n        \u2061\n        (\n        \n          exposure\n        \n        )\n        =\n        \n          \u03b8\n          \u2032\n        \n        x\n        \u2212\n        log\n        \u2061\n        (\n        \n          exposure\n        \n        )\n      \n    \n    {\\displaystyle \\log \\left({\\frac {\\operatorname {E} (Y\\mid x)}{\\text{exposure}}}\\right)=\\log(\\operatorname {E} (Y\\mid x))-\\log({\\text{exposure}})=\\theta 'x-\\log({\\text{exposure}})}\n  Offset in the case of a GLM in R can be achieved using the offset() function:\n\n\n=== Overdispersion and zero inflation ===\nA characteristic of the Poisson distribution is that its mean is equal to its variance. In certain circumstances, it will be found that the observed variance is greater than the mean;  this is known as overdispersion and indicates that the model is not appropriate. A common reason is the omission of relevant explanatory variables, or dependent observations. Under some circumstances, the problem of overdispersion can be solved by using quasi-likelihood estimation or a negative binomial distribution instead.Ver Hoef and Boveng described the difference between quasi-Poisson (also called overdispersion with quasi-likelihood) and negative binomial (equivalent to gamma-Poisson) as follows:  If E(Y) = \u03bc, the quasi-Poisson model assumes var(Y) = \u03b8\u03bc while the gamma-Poisson assumes var(Y) = \u03bc(1 + \u03ba\u03bc), where \u03b8 is the quasi-Poisson overdispersion parameter, and \u03ba is the shape parameter of the negative binomial distribution.  For both models, parameters are estimated using iteratively reweighted least squares.  For quasi-Poisson, the weights are \u03bc/\u03b8.  For negative binomial, the weights are \u03bc/(1 + \u03ba\u03bc).  With large \u03bc and substantial extra-Poisson variation, the negative binomial weights are capped at 1/\u03ba.  Ver Hoef and Boveng discussed an example where they selected between the two by plotting mean squared residuals vs. the mean.Another common problem with Poisson regression is excess zeros: if there are two processes at work, one determining whether there are zero events or any events, and a Poisson process determining how many events there are, there will be more zeros than a Poisson regression would predict. An example would be the distribution of cigarettes smoked in an hour by members of a group where some individuals are non-smokers.\nOther generalized linear models such as the negative binomial model or zero-inflated model may function better in these cases.\nOn the contrary, underdispersion may pose an issue for parameter estimation.\n\n\n=== Use in survival analysis ===\nPoisson regression creates proportional hazards models, one class of survival analysis: see proportional hazards models for descriptions of Cox models.\n\n\n== Extensions ==\n\n\n=== Regularized Poisson regression ===\nWhen estimating the parameters for Poisson regression, one typically tries to find values for \u03b8 that maximize the likelihood of an expression of the form\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        log\n        \u2061\n        (\n        p\n        (\n        \n          y\n          \n            i\n          \n        \n        ;\n        \n          e\n          \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n          \n        \n        )\n        )\n        ,\n      \n    \n    {\\displaystyle \\sum _{i=1}^{m}\\log(p(y_{i};e^{\\theta 'x_{i}})),}\n  where m is the number of examples in the data set, and \n  \n    \n      \n        p\n        (\n        \n          y\n          \n            i\n          \n        \n        ;\n        \n          e\n          \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle p(y_{i};e^{\\theta 'x_{i}})}\n   is the probability mass function of the Poisson distribution with the mean set to \n  \n    \n      \n        \n          e\n          \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle e^{\\theta 'x_{i}}}\n  . Regularization can be added to this optimization problem by instead maximizing\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        log\n        \u2061\n        (\n        p\n        (\n        \n          y\n          \n            i\n          \n        \n        ;\n        \n          e\n          \n            \n              \u03b8\n              \u2032\n            \n            \n              x\n              \n                i\n              \n            \n          \n        \n        )\n        )\n        \u2212\n        \u03bb\n        \n          \n            \u2016\n            \u03b8\n            \u2016\n          \n          \n            2\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sum _{i=1}^{m}\\log(p(y_{i};e^{\\theta 'x_{i}}))-\\lambda \\left\\|\\theta \\right\\|_{2}^{2},}\n  for some positive constant \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  . This technique, similar to ridge regression, can reduce overfitting.\n\n\n== See also ==\nZero-inflated model\nPoisson distribution\nFixed-effect Poisson model\nPartial likelihood methods for panel data \u00a7 Pooled QMLE for Poisson models\nControl function (econometrics) \u00a7 Endogeneity in Poisson regression\n\n\n== References ==\n\n\n== Further reading ==\nCameron, A. C.; Trivedi, P. K. (1998). Regression analysis of count data. Cambridge University Press. ISBN 978-0-521-63201-0.\nChristensen, Ronald (1997). Log-linear models and logistic regression. Springer Texts in Statistics (Second ed.). New York: Springer-Verlag. ISBN 978-0-387-98247-2. MR 1633357.\nGouri\u00e9roux, Christian (2000). \"The Econometrics of Discrete Positive Variables: the Poisson Model\". Econometrics of Qualitative Dependent Variables. New York: Cambridge University Press. pp. 270\u201383. ISBN 978-0-521-58985-7.\nGreene, William H. (2008). \"Models for Event Counts and Duration\". Econometric Analysis (8th ed.). Upper Saddle River: Prentice Hall. pp. 906\u2013944. ISBN 978-0-13-600383-0.\nHilbe, J. M. (2007). Negative Binomial Regression. Cambridge University Press. ISBN 978-0-521-85772-7.\nJones, Andrew M.;  et al. (2013). \"Models for count data\". Applied Health Economics. London: Routledge. pp. 295\u2013341. ISBN 978-0-415-67682-3.\nMyers, Raymond H.;  et al. (2010). \"Logistic and Poisson Regression Models\". Generalized Linear Models With Applications in Engineering and the Sciences (Second ed.). New Jersey: Wiley. pp. 176\u2013183. ISBN 978-0-470-45463-3.", "Information retrieval": "Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications.\n\n\n== Overview ==\nAn information retrieval process begins when a user or searcher enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance.\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.\n\n\n== History ==\nthere is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute\nThe idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n\n\n== Applications ==\nAreas where information retrieval techniques are employed include (the entries are in alphabetical order within each category):\n\n\n=== General applications ===\nDigital libraries\nInformation filtering\nRecommender systems\nMedia search\nBlog search\nImage retrieval\n3D retrieval\nMusic retrieval\nNews search\nSpeech retrieval\nVideo retrieval\nSearch engines\nSite search\nDesktop search\nEnterprise search\nFederated search\nMobile search\nSocial search\nWeb search\n\n\n=== Domain-specific applications ===\nExpert search finding\nGenomic information retrieval\nGeographic information retrieval\nInformation retrieval for chemical structures\nInformation retrieval in software engineering\nLegal information retrieval\nVertical search\n\n\n=== Other retrieval methods ===\nMethods/Techniques in which information retrieval techniques are employed include:\n\nAdversarial information retrieval\nAutomatic summarization\nMulti-document summarization\nCompound term processing\nCross-lingual retrieval\nDocument classification\nSpam filtering\nQuestion answering\n\n\n== Model types ==\n\nFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\n\n=== First dimension: mathematical basis ===\nSet-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\nStandard Boolean model\nExtended Boolean model\nFuzzy retrieval\nAlgebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\nVector space model\nGeneralized vector space model\n(Enhanced) Topic-based Vector Space Model\nExtended Boolean model\nLatent semantic indexing a.k.a. latent semantic analysis\nProbabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.\nBinary Independence Model\nProbabilistic relevance model on which is based the okapi (BM25) relevance function\nUncertain inference\nLanguage models\nDivergence-from-randomness model\nLatent Dirichlet allocation\nFeature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\n\n\n=== Second dimension: properties of the model ===\nModels without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\nModels with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\nModels with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\n\n\n== Performance and correctness measures ==\n\nThe evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.\n\n\n== Timeline ==\nBefore the 1900s\n1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.\n1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\n1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data.\n1920s-1930s\nEmanuel Goldberg submits patents for his \"Statistical Machine\u201d a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\n1940s\u20131950s\nlate 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\n1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.\n1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\n1950s: Growing concern in the US for a \"science gap\" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of the citation index by Eugene Garfield.\n1950: The term \"information retrieval\" was coined by Calvin Mooers.\n1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.\n1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed \"framework\" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.\n1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)\n1959: Hans Peter Luhn published \"Auto-encoding of documents for information retrieval.\"\n1960s:\nearly 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.\n1960: Melvin Earl Maron and John Lary Kuhns published \"On relevance, probabilistic indexing, and information retrieval\" in the Journal of the ACM 7(3):216\u2013244, July 1960.\n1962:\nCyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, \"Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems\". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\nKent published Information Analysis and Retrieval.\n1963:\nWeinberg report \"Science, Government and Information\" gave a full articulation of the idea of a \"crisis of scientific information.\"  The report was named after Dr. Alvin Weinberg.\nJoseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).\n1964:\nKaren Sp\u00e4rck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.\nThe National Bureau of Standards sponsored a symposium titled \"Statistical Association Methods for Mechanized Documentation.\" Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.\nmid-1960s:\nNational Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\nProject Intrex at MIT.\n1965: J. C. R. Licklider published Libraries of the Future.\n1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.\nlate 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\n1968:Gerard Salton published Automatic Information Organization and Retrieval.\nJohn W. Sammon, Jr.'s RADC Tech report \"Some Mathematics of Information Storage and Retrieval...\" outlined the vector model.1969: Sammon's \"A nonlinear mapping for data structure analysis Archived 2017-08-08 at the Wayback Machine\" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\n1970s\nearly 1970s:\nFirst online systems\u2014NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\nTheodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.\n1971: Nicholas Jardine and Cornelis J. van Rijsbergen published \"The use of hierarchic clustering in information retrieval\", which articulated the \"cluster hypothesis.\"\n1975: Three highly influential publications by Salton fully articulated his vector processing framework and  term discrimination model:\nA Theory of Indexing (Society for Industrial and Applied Mathematics)\nA Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)\nA Vector Space Model for Automatic Indexing (CACM 18:11)\n1978: The First ACM SIGIR conference.\n1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.\n1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.\n1980s\n1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\n1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\n1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.\n1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\nmid-1980s: Efforts to develop end-user versions of commercial IR systems.\n1985\u20131993: Key papers on and experimental systems for visualization interfaces.\nWork by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.\n1989: First World Wide Web proposals by Tim Berners-Lee at CERN.\n1990s\n1992: First TREC conference.\n1997: Publication of Korfhage's Information Storage and Retrieval with emphasis on visualization and multi-reference point systems.\n1999: Publication of Ricardo Baeza-Yates and Berthier Ribeiro-Neto's Modern Information Retrieval by Addison Wesley, the first book that attempts to cover all IR.\nlate 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\n\n\n== Major conferences ==\nSIGIR: Conference on Research and Development in Information Retrieval\nECIR: European Conference on Information Retrieval\nCIKM: Conference on Information and Knowledge Management\nWWW: International World Wide Web Conference\nWSDM: Conference on Web Search and Data Mining\nICTIR: International Conference on Theory of Information Retrieval\n\n\n== Awards in the field ==\nTony Kent Strix award\nGerard Salton Award\nKaren Sp\u00e4rck Jones Award\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nRicardo Baeza-Yates, Berthier Ribeiro-Neto. Modern Information Retrieval: The Concepts and Technology behind Search (second edition) Archived 2017-09-18 at the Wayback Machine. Addison-Wesley, UK, 2011.\nStefan B\u00fcttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines Archived 2020-10-05 at the Wayback Machine. MIT Press, Cambridge, Massachusetts, 2010.\n\"Information Retrieval System\". Library & Information Science Network. 24 April 2015.\nChristopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. Introduction to Information Retrieval. Cambridge University Press, 2008.\n\n\n== External links ==\n\nACM SIGIR: Information Retrieval Special Interest Group\nBCS IRSG: British Computer Society - Information Retrieval Specialist Group\nText Retrieval Conference (TREC)\nForum for Information Retrieval Evaluation (FIRE)\nInformation Retrieval (online book) by C. J. van Rijsbergen\nInformation Retrieval Wiki Archived 2015-11-24 at the Wayback Machine\nInformation Retrieval Facility Archived 2008-05-22 at the Wayback Machine\nInformation Retrieval @ DUTH\nTREC report on information retrieval evaluation techniques\nHow eBay measures search relevance\nInformation retrieval performance evaluation tool @ Athena Research Centre", "Mode (statistics)": "The mode is the value that appears most often in a set of data values. If X is a discrete random variable, the mode is the value x at which the probability mass function takes its maximum value (i.e, x=argmaxxi P(X = xi)). In other words, it is the value that is most likely to be sampled.\nLike the statistical mean and median, the mode is a way of expressing, in a (usually) single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions.\nThe mode is not necessarily unique to a given discrete distribution, since the probability mass function may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently.\nWhen the probability density function of a continuous distribution has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal).  A mode of a continuous probability distribution is often considered to be any value x at which its probability density function has a locally maximum value, so any peak is a mode.In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric unimodal distribution, the sample mean can be used as an estimate of the population mode.\n\n\n== Mode of a sample ==\nThe mode of a sample is the element that occurs most often in the collection. For example, the mode of the sample [1, 3, 6, 6, 6, 6, 7, 7, 12, 12, 17] is 6. Given the list of data [1, 1, 2, 4, 4] its mode is not unique. A dataset, in such a case, is said to be bimodal, while a set with more than two modes may be described as multimodal.\nFor a sample from a continuous distribution, such as [0.935..., 1.211..., 2.430..., 3.668..., 3.874...], the concept is unusable in its raw form, since no two values will be exactly the same, so each value will occur precisely once. In order to estimate the mode of the underlying distribution, the usual practice is to discretize the data by assigning frequency values to intervals of equal distance, as for making a histogram, effectively replacing the values by the midpoints of the\nintervals they are assigned to. The mode is then the value where the histogram reaches its peak. For small or middle-sized samples the outcome of this procedure is sensitive to the choice of interval width if chosen too narrow or too wide; typically one should have a sizable fraction of the data concentrated in a relatively small number of intervals (5 to 10), while the fraction of the data falling outside these intervals is also sizable. An alternate approach is kernel density estimation, which essentially blurs point samples to produce a continuous estimate of the probability density function which can provide an estimate of the mode.\nThe following MATLAB (or Octave) code example computes the mode of a sample:\n\nThe algorithm requires as a first step to sort the sample in ascending order. It then computes the discrete derivative of the sorted list, and finds the indices where this derivative is positive. Next it computes the discrete derivative of this set of indices, locating the maximum of this derivative of indices, and finally evaluates the sorted sample at the point where that maximum occurs, which corresponds to the last member of the stretch of repeated values.\n\n\n== Comparison of mean, median and mode ==\n\n\n=== Use ===\nUnlike mean and median, the concept of mode also makes sense for \"nominal data\" (i.e., not consisting of numerical values in the case of mean, or even of ordered values in the case of median). For example, taking a sample of Korean family names, one might find that \"Kim\" occurs more often than any other name. Then \"Kim\" would be the mode of the sample. In any voting system where a plurality determines victory, a single modal value determines the victor, while a multi-modal outcome would require some tie-breaking procedure to take place.\nUnlike median, the concept of mode makes sense for any random variable assuming values from a vector space, including the real numbers (a one-dimensional vector space) and the integers (which can be considered embedded in the reals). For example, a distribution of points in the plane will typically have a mean and a mode, but the concept of median does not apply. The median makes sense when there is a linear order on the possible values. Generalizations of the concept of median to higher-dimensional spaces are the geometric median and the centerpoint.\n\n\n=== Uniqueness and definedness ===\nFor some probability distributions, the expected value may be infinite or undefined, but if defined, it is unique. The mean of a (finite) sample is always defined. The median is the value such that the fractions not exceeding it and not falling below it are each at least 1/2. It is not necessarily unique, but never infinite or totally undefined. For a data sample it is the \"halfway\" value when the list of values is ordered in increasing value, where usually for a list of even length the numerical average is taken of the two values closest to \"halfway\". Finally, as said before, the mode is not necessarily unique. Certain pathological distributions (for example, the Cantor distribution) have no defined mode at all. For a finite data sample, the mode is one (or more) of the values in the sample.\n\n\n=== Properties ===\nAssuming definedness, and for simplicity uniqueness, the following are some of the most interesting properties.\n\nAll three measures have the following property: If the random variable (or each value from the sample) is subjected to the linear or affine transformation, which replaces X by aX + b, so are the mean, median and mode.\nExcept for extremely small samples, the mode is insensitive to \"outliers\" (such as occasional, rare, false experimental readings). The median is also very robust in the presence of outliers, while the mean is rather sensitive.\nIn continuous unimodal distributions the median often lies between the mean and the mode, about one third of the way going from mean to mode. In a formula, median \u2248 (2 \u00d7 mean + mode)/3. This rule, due to Karl Pearson, often applies to slightly non-symmetric distributions that resemble a normal distribution, but it is not always true and in general the three statistics can appear in any order.\nFor unimodal distributions, the mode is within \u221a3 standard deviations of the mean, and the root mean square deviation about the mode is between the standard deviation and twice the standard deviation.\n\n\n=== Example for a skewed distribution ===\nAn example of a skewed distribution is personal wealth: Few people are very rich, but among those some are extremely rich. However, many are rather poor.\n\nA well-known class of distributions that can be arbitrarily skewed is given by the log-normal distribution. It is obtained by transforming a random variable X having a normal distribution into random variable Y = eX. Then the logarithm of random variable Y is normally distributed, hence the name.\nTaking the mean \u03bc of X to be 0, the median of Y will be 1, independent of the standard deviation \u03c3 of X. This is so because X has a symmetric distribution, so its median is also 0. The transformation from X to Y is monotonic, and so we find the median e0 = 1 for Y.\nWhen X has standard deviation \u03c3 = 0.25, the distribution of Y is weakly skewed. Using formulas for the log-normal distribution, we find:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  mean\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                    +\n                    \n                      \u03c3\n                      \n                        2\n                      \n                    \n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                    +\n                    \n                      0.25\n                      \n                        2\n                      \n                    \n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                \u2248\n                1.032\n              \n            \n            \n              \n                \n                  mode\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                    \u2212\n                    \n                      \u03c3\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                    \u2212\n                    \n                      0.25\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                \u2248\n                0.939\n              \n            \n            \n              \n                \n                  median\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                  \n                \n              \n              \n                =\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{rlll}{\\text{mean}}&=e^{\\mu +\\sigma ^{2}/2}&=e^{0+0.25^{2}/2}&\\approx 1.032\\\\{\\text{mode}}&=e^{\\mu -\\sigma ^{2}}&=e^{0-0.25^{2}}&\\approx 0.939\\\\{\\text{median}}&=e^{\\mu }&=e^{0}&=1\\end{array}}}\n  Indeed, the median is about one third on the way from mean to mode.\nWhen X has a larger standard deviation, \u03c3 = 1, the distribution of Y is strongly skewed. Now\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  mean\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                    +\n                    \n                      \u03c3\n                      \n                        2\n                      \n                    \n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                    +\n                    \n                      1\n                      \n                        2\n                      \n                    \n                    \n                      /\n                    \n                    2\n                  \n                \n              \n              \n                \u2248\n                1.649\n              \n            \n            \n              \n                \n                  mode\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                    \u2212\n                    \n                      \u03c3\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                    \u2212\n                    \n                      1\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                \u2248\n                0.368\n              \n            \n            \n              \n                \n                  median\n                \n              \n              \n                =\n                \n                  e\n                  \n                    \u03bc\n                  \n                \n              \n              \n                =\n                \n                  e\n                  \n                    0\n                  \n                \n              \n              \n                =\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{rlll}{\\text{mean}}&=e^{\\mu +\\sigma ^{2}/2}&=e^{0+1^{2}/2}&\\approx 1.649\\\\{\\text{mode}}&=e^{\\mu -\\sigma ^{2}}&=e^{0-1^{2}}&\\approx 0.368\\\\{\\text{median}}&=e^{\\mu }&=e^{0}&=1\\end{array}}}\n  Here, Pearson's rule of thumb fails.\n\n\n=== Van Zwet condition ===\nVan Zwet derived an inequality which provides sufficient conditions for this inequality to hold. The inequality\n\nMode \u2264 Median \u2264 Meanholds if\n\nF( Median - x ) + F( Median + x ) \u2265 1for all x where F() is the cumulative distribution function of the distribution.\n\n\n== Unimodal distributions ==\nIt can be shown for a unimodal distribution that the median \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n   and the  mean \n  \n    \n      \n        \n          \n            \n              X\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n   lie within (3/5)1/2 \u2248 0.7746 standard deviations of each other. In symbols,\n\n  \n    \n      \n        \n          \n            \n              |\n              \n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                \u2212\n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n              \n              |\n            \n            \u03c3\n          \n        \n        \u2264\n        (\n        3\n        \n          /\n        \n        5\n        \n          )\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\left|{\\tilde {X}}-{\\bar {X}}\\right|}{\\sigma }}\\leq (3/5)^{1/2}}\n  where \n  \n    \n      \n        \n          |\n        \n        \u22c5\n        \n          |\n        \n      \n    \n    {\\displaystyle |\\cdot |}\n   is the absolute value.\nA similar relation holds between the median and the mode: they lie within 31/2 \u2248 1.732 standard deviations of each other:\n\n  \n    \n      \n        \n          \n            \n              |\n              \n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                \u2212\n                \n                  m\n                  o\n                  d\n                  e\n                \n              \n              |\n            \n            \u03c3\n          \n        \n        \u2264\n        \n          3\n          \n            1\n            \n              /\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\left|{\\tilde {X}}-\\mathrm {mode} \\right|}{\\sigma }}\\leq 3^{1/2}.}\n  \n\n\n== History ==\nThe term mode originates with Karl Pearson in 1895.Pearson uses the term mode interchangeably with maximum-ordinate. In a footnote he says, \"I have found it convenient to use the term mode for the abscissa corresponding to the ordinate of maximum frequency.\"\n\n\n== See also ==\nArg max\nCentral tendency\nDescriptive statistics\nMoment (mathematics)\nSummary statistics\nUnimodal function\n\n\n== References ==\n\n\n== External links ==\n\"Mode\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nA Guide to Understanding & Calculating the Mode\nWeisstein, Eric W. \"Mode\". MathWorld.\nMean, Median and Mode short beginner video from Khan Academy", "F-test": "An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact \"F-tests\" mainly arise when the models have been fitted to the data using least squares. The name was coined by George W. Snedecor, in honour of Ronald Fisher. Fisher initially developed the statistic as the variance ratio in the 1920s.\n\n\n== Common examples ==\nCommon examples of the use of F-tests include the study of the following cases:\n\nThe hypothesis that the means of a given set of normally distributed populations, all having the same standard deviation, are equal.  This is perhaps the best-known F-test, and plays an important role in the analysis of variance (ANOVA).\nThe hypothesis that a proposed regression model fits the data well.  See Lack-of-fit sum of squares.\nThe hypothesis that a data set in a regression analysis follows the simpler of two proposed linear models that are nested within each other.In addition, some statistical procedures, such as Scheff\u00e9's method for multiple comparisons adjustment in linear models, also use F-tests.\n\n\n=== F-test of the equality of two variances ===\n\nThe F-test is sensitive to non-normality. In the analysis of variance (ANOVA), alternative tests include Levene's test, Bartlett's test, and the Brown\u2013Forsythe test. However, when any of these tests are conducted to test the underlying assumption of homoscedasticity (i.e. homogeneity of variance), as a preliminary step to testing for mean effects, there is an increase in the experiment-wise Type I error rate.\n\n\n== Formula and calculation ==\nMost F-tests arise by considering a decomposition of the variability in a collection of data in terms of sums of squares.  The test statistic in an F-test is the ratio of two scaled sums of squares reflecting different sources of variability. These sums of squares are constructed so that the statistic tends to be greater when the null hypothesis is not true. In order for the statistic to follow the F-distribution under the null hypothesis, the sums of squares should be statistically independent, and each should follow a scaled \u03c7\u00b2-distribution. The latter condition is guaranteed if the data values are independent and normally distributed with a common variance.\n\n\n=== Multiple-comparison ANOVA problems ===\nThe F-test in one-way analysis of variance (ANOVA) is used to assess whether the expected values of a quantitative variable within several pre-defined groups differ from each other. For example, suppose that a medical trial compares four treatments.  The ANOVA F-test can be used to assess whether any of the treatments are on average superior, or inferior, to the others versus the null hypothesis that all four treatments yield the same mean response.  This is an example of an \"omnibus\" test, meaning that a single test is performed to detect any of several possible differences.  Alternatively, we could carry out pairwise tests among the treatments (for instance, in the medical trial example with four treatments we could carry out six tests among pairs of treatments).  The advantage of the ANOVA F-test is that we do not need to pre-specify which treatments are to be compared, and we do not need to adjust for making multiple comparisons.  The disadvantage of the ANOVA F-test is that if we reject the null hypothesis, we do not know which treatments can be said to be significantly different from the others, nor, if the F-test is performed at level \u03b1, can we state that the treatment pair with the greatest mean difference is significantly different at level \u03b1.\nThe formula for the one-way ANOVA F-test statistic is\n\n  \n    \n      \n        F\n        =\n        \n          \n            explained variance\n            unexplained variance\n          \n        \n        ,\n      \n    \n    {\\displaystyle F={\\frac {\\text{explained variance}}{\\text{unexplained variance}}},}\n  or\n\n  \n    \n      \n        F\n        =\n        \n          \n            between-group variability\n            within-group variability\n          \n        \n        .\n      \n    \n    {\\displaystyle F={\\frac {\\text{between-group variability}}{\\text{within-group variability}}}.}\n  The \"explained variance\", or \"between-group variability\" is\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            K\n          \n        \n        \n          n\n          \n            i\n          \n        \n        (\n        \n          \n            \n              \n                Y\n                \u00af\n              \n            \n          \n          \n            i\n            \u22c5\n          \n        \n        \u2212\n        \n          \n            \n              Y\n              \u00af\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        \n          /\n        \n        (\n        K\n        \u2212\n        1\n        )\n      \n    \n    {\\displaystyle \\sum _{i=1}^{K}n_{i}({\\bar {Y}}_{i\\cdot }-{\\bar {Y}})^{2}/(K-1)}\n  where \n  \n    \n      \n        \n          \n            \n              \n                Y\n                \u00af\n              \n            \n          \n          \n            i\n            \u22c5\n          \n        \n      \n    \n    {\\displaystyle {\\bar {Y}}_{i\\cdot }}\n   denotes the sample mean in the i-th group, \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   is the number of observations in the i-th group,\n  \n    \n      \n        \n          \n            \n              Y\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {Y}}}\n   denotes the overall mean of the data, and \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   denotes the number of groups.\nThe \"unexplained variance\", or \"within-group variability\" is\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            K\n          \n        \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            \n              n\n              \n                i\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                Y\n                \n                  i\n                  j\n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      Y\n                      \u00af\n                    \n                  \n                \n                \n                  i\n                  \u22c5\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        \n          /\n        \n        (\n        N\n        \u2212\n        K\n        )\n        ,\n      \n    \n    {\\displaystyle \\sum _{i=1}^{K}\\sum _{j=1}^{n_{i}}\\left(Y_{ij}-{\\bar {Y}}_{i\\cdot }\\right)^{2}/(N-K),}\n  where \n  \n    \n      \n        \n          Y\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle Y_{ij}}\n   is the jth observation in the ith out of \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   groups and \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the overall sample size.  This F-statistic follows the F-distribution with degrees of freedom \n  \n    \n      \n        \n          d\n          \n            1\n          \n        \n        =\n        K\n        \u2212\n        1\n      \n    \n    {\\displaystyle d_{1}=K-1}\n   and \n  \n    \n      \n        \n          d\n          \n            2\n          \n        \n        =\n        N\n        \u2212\n        K\n      \n    \n    {\\displaystyle d_{2}=N-K}\n   under the null hypothesis.  The statistic will be large if the between-group variability is large relative to the within-group variability, which is unlikely to happen if the population means of the groups all have the same value.\nNote that when there are only two groups for the one-way ANOVA F-test, \n  \n    \n      \n        F\n        =\n        \n          t\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle F=t^{2}}\n  where t is the Student's \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   statistic.\n\n\n=== Regression problems ===\n\nConsider two models, 1 and 2, where model 1 is 'nested' within model 2.  Model 1 is the restricted model, and model 2 is the unrestricted one.  That is, model 1 has p1 parameters, and model 2 has p2 parameters, where p1 < p2, and for any choice of parameters in model 1, the same regression curve can be achieved by some choice of the parameters of model 2.\nOne common context in this regard is that of deciding whether a model fits the data significantly better than does a naive model, in which the only explanatory term is the intercept term, so that all predicted values for the dependent variable are set equal to that variable's sample mean. The naive model is the restricted model, since the coefficients of all potential explanatory variables are restricted to equal zero.\nAnother common context is deciding whether there is a structural break in the data: here the restricted model uses all data in one regression, while the unrestricted model uses separate regressions for two different subsets of the data. This use of the F-test is known as the Chow test.\nThe model with more parameters will always be able to fit the data at least as well as the model with fewer parameters.  Thus typically model 2 will give a better (i.e. lower error) fit to the data than model 1.  But one often wants to determine whether model 2 gives a significantly better fit to the data.  One approach to this problem is to use an F-test.\nIf there are n data points to estimate parameters of both models from, then one can calculate the F statistic, given by\n\n  \n    \n      \n        F\n        =\n        \n          \n            \n              (\n              \n                \n                  \n                    \n                      \n                        RSS\n                      \n                      \n                        1\n                      \n                    \n                    \u2212\n                    \n                      \n                        RSS\n                      \n                      \n                        2\n                      \n                    \n                  \n                  \n                    \n                      p\n                      \n                        2\n                      \n                    \n                    \u2212\n                    \n                      p\n                      \n                        1\n                      \n                    \n                  \n                \n              \n              )\n            \n            \n              (\n              \n                \n                  \n                    \n                      RSS\n                    \n                    \n                      2\n                    \n                  \n                  \n                    n\n                    \u2212\n                    \n                      p\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle F={\\frac {\\left({\\frac {{\\text{RSS}}_{1}-{\\text{RSS}}_{2}}{p_{2}-p_{1}}}\\right)}{\\left({\\frac {{\\text{RSS}}_{2}}{n-p_{2}}}\\right)}},}\n  where RSSi is the residual sum of squares of model i. If the regression model has been calculated with weights, then replace RSSi with \u03c72, the weighted sum of squared residuals.  Under the null hypothesis that model 2 does not provide a significantly better fit than model 1, F will have an F distribution, with (p2\u2212p1, n\u2212p2) degrees of freedom.  The null hypothesis is rejected if the F calculated from the data is greater than the critical value of the F-distribution for some desired false-rejection probability (e.g. 0.05). Since F is a monotone function of the likelihood ratio statistic, the F-test is a likelihood ratio test.\n\n\n== See also ==\nGoodness of fit\n\n\n== References ==\n\n\n== Further reading ==\nFox, Karl A. (1980). Intermediate Economic Statistics (Second ed.). New York: John Wiley & Sons. pp. 290\u2013310. ISBN 0-88275-521-8.\nJohnston, John (1972). Econometric Methods (Second ed.). New York: McGraw-Hill. pp. 35\u201338.\nKmenta, Jan (1986). Elements of Econometrics (Second ed.). New York: Macmillan. pp. 147\u2013148. ISBN 0-02-365070-2.\nMaddala, G. S.; Lahiri, Kajal (2009). Introduction to Econometrics (Fourth ed.). Chichester: Wiley. pp. 155\u2013160. ISBN 978-0-470-01512-4.\n\n\n== External links ==\nTable of F-test critical values\nFree calculator for F-testing\nThe F-test for Linear Regression\nEconometrics lecture (topic: hypothesis testing) on YouTube by Mark Thoma", "Linear regression": "In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is error reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n\n\n== Formulation ==\n\nGiven a data set \n  \n    \n      \n        {\n        \n          y\n          \n            i\n          \n        \n        ,\n        \n        \n          x\n          \n            i\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            i\n            p\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}}\n   of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable \u03b5 \u2014 an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the form\n\nwhere T denotes the transpose, so that xiT\u03b2 is the inner product between vectors xi and \u03b2.\nOften these n equations are stacked together and written in matrix notation as\n\n  \n    \n      \n        \n          y\n        \n        =\n        \n          X\n        \n        \n          \u03b2\n        \n        +\n        \n          \u03b5\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {X} {\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }},\\,}\n  where\n\n  \n    \n      \n        \n          y\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n              \n              \n                \n                  \n                    y\n                    \n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle \\mathbf {y} ={\\begin{bmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{bmatrix}},\\quad }\n  \n\n  \n    \n      \n        \n          X\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      x\n                    \n                    \n                      1\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      x\n                    \n                    \n                      2\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n              \n              \n                \n                  \n                    \n                      x\n                    \n                    \n                      n\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      11\n                    \n                  \n                \n                \n                  \u22ef\n                \n                \n                  \n                    x\n                    \n                      1\n                      p\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      21\n                    \n                  \n                \n                \n                  \u22ef\n                \n                \n                  \n                    x\n                    \n                      2\n                      p\n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n                \n                  \u22ee\n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      n\n                      1\n                    \n                  \n                \n                \n                  \u22ef\n                \n                \n                  \n                    x\n                    \n                      n\n                      p\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {X} ={\\begin{bmatrix}\\mathbf {x} _{1}^{\\mathsf {T}}\\\\\\mathbf {x} _{2}^{\\mathsf {T}}\\\\\\vdots \\\\\\mathbf {x} _{n}^{\\mathsf {T}}\\end{bmatrix}}={\\begin{bmatrix}1&x_{11}&\\cdots &x_{1p}\\\\1&x_{21}&\\cdots &x_{2p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{np}\\end{bmatrix}},}\n  \n\n  \n    \n      \n        \n          \u03b2\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b2\n                    \n                      0\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      p\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          \u03b5\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b5\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \u22ee\n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}={\\begin{bmatrix}\\beta _{0}\\\\\\beta _{1}\\\\\\beta _{2}\\\\\\vdots \\\\\\beta _{p}\\end{bmatrix}},\\quad {\\boldsymbol {\\varepsilon }}={\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{bmatrix}}.}\n  \n\n\n=== Notation and terminology ===\n\n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n   is a vector of observed values \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n         \n        (\n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle y_{i}\\ (i=1,\\ldots ,n)}\n   of the variable called the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable. This variable is also sometimes known as the predicted variable, but this should not be confused with predicted values, which are denoted \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}}\n  . The decision as to which variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality.\n\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   may be seen as a matrix of row-vectors \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n            \u22c5\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i\\cdot }}\n   or of n-dimensional column-vectors \n  \n    \n      \n        \n          \n            x\n          \n          \n            \u22c5\n            j\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{\\cdot j}}\n  , which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables (not to be confused with the concept of independent random variables). The matrix \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   is sometimes called the design matrix.\nUsually a constant is included as one of the regressors. In particular, \n  \n    \n      \n        \n          x\n          \n            i\n            0\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x_{i0}=1}\n   for \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ldots ,n}\n  . The corresponding element of \u03b2 is called the intercept. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.\nSometimes one of the regressors can be a non-linear function of another regressor or of the data, as in polynomial regression and segmented regression. The model remains linear as long as it is linear in the parameter vector \u03b2.\nThe values xij may be viewed as either observed values of random variables Xj or as fixed values chosen prior to observing the dependent variable. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations.\n\n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   is a \n  \n    \n      \n        (\n        p\n        +\n        1\n        )\n      \n    \n    {\\displaystyle (p+1)}\n  -dimensional parameter vector, where \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n   is the intercept term (if one is included in the model\u2014otherwise \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   is p-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects). In simple linear regression, p=1, and the coefficient is known as regression slope. Statistical estimation and inference in linear regression focuses on \u03b2. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.\n\n  \n    \n      \n        \n          \u03b5\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}}\n   is a vector of values \n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}}\n  . This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the \"signal\" provided by the rest of the model). This variable captures all other factors which influence the dependent variable y other than the regressors x. The relationship between the error term and the regressors, for example their correlation, is a crucial consideration in formulating a linear regression model, as it will determine the appropriate estimation method.Fitting a linear model to a given data set usually requires estimating the regression coefficients  \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   such that the error term \n  \n    \n      \n        \n          \u03b5\n        \n        =\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}=\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }}}\n   is minimized. For example, it is common to use the sum of squared errors \n  \n    \n      \n        \u2016\n        \n          \u03b5\n        \n        \n          \u2016\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2}}\n   as a measure of \n  \n    \n      \n        \n          \u03b5\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\varepsilon }}}\n   for minimization.\n\n\n=== Example ===\nConsider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as\n\n  \n    \n      \n        \n          h\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          t\n          \n            i\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          t\n          \n            i\n          \n          \n            2\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle h_{i}=\\beta _{1}t_{i}+\\beta _{2}t_{i}^{2}+\\varepsilon _{i},}\n  where \u03b21 determines the initial velocity of the ball, \u03b22 is proportional to the standard gravity, and \u03b5i is due to measurement errors. Linear regression can be used to estimate the values of \u03b21 and \u03b22 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters \u03b21 and \u03b22; if we take regressors xi = (xi1, xi2)  = (ti, ti2), the model takes on the standard form\n\n  \n    \n      \n        \n          h\n          \n            i\n          \n        \n        =\n        \n          \n            x\n          \n          \n            i\n          \n          \n            \n              T\n            \n          \n        \n        \n          \u03b2\n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle h_{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i}.}\n  \n\n\n=== Assumptions ===\n\nStandard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship.  Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.\n\nThe following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):\n\nWeak exogeneity.  This essentially means that the predictor variables x can be treated as fixed values, rather than random variables.  This means, for example, that the predictor variables are assumed to be error-free\u2014that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.\nLinearity.  This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables.  Note that this assumption is much less restrictive than it may at first seem.  Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters.  The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently.  This technique is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given degree) of a predictor variable. With this much flexibility, models such as polynomial regression often have \"too much power\", in that they tend to overfit the data.  As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process.  Common examples are ridge regression and lasso regression.  Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as    special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)\nConstant variance (a.k.a. homoscedasticity). This means that the variance of the errors does not depend on the values of the predictor variables. Thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are. This is often not the case, as a variable whose mean is large will typically have a greater variance than one whose mean is small. For example, a person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000\u2014i.e., a standard deviation of around $20,000\u2014while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, since that would imply their actual income could vary anywhere between \u2212$10,000 and $30,000. (In fact, as this shows, in many cases\u2014often the same cases where the assumption of normally distributed errors fails\u2014the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) The absence of homoscedasticity is called heteroscedasticity. In order to check this assumption, a plot of residuals versus predicted values (or the values of each individual predictor) can be examined for a \"fanning effect\" (i.e., increasing or decreasing vertical spread as one moves left to right on the plot). A plot of the absolute or squared residuals versus the predicted values (or each predictor) can also be examined for a trend or curvature. Formal tests can also be used; see Heteroscedasticity. The presence of heteroscedasticity will result in an overall \"average\" estimate of variance being used instead of one that takes into account the true variance structure. This leads to less precise (but in the case of ordinary least squares, not biased) parameter estimates and biased standard errors, resulting in misleading tests and interval estimates. The mean squared error for the model will also be wrong. Various estimation techniques including weighted least squares and the use of heteroscedasticity-consistent standard errors can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g., fitting the logarithm of the response variable using a linear regression model, which implies that the response variable itself has a log-normal distribution rather than a normal distribution).\nIndependence of errors.  This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods such as generalized least squares are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.\nLack of perfect multicollinearity in the predictors. For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise perfect multicollinearity exists in the predictor variables, meaning a linear relationship exists between two or more predictor variables. This can be caused by accidentally duplicating a variable in the data, using a linear transformation of a variable along with the original (e.g., the same temperature measurements expressed in Fahrenheit and Celsius), or including a linear combination of multiple variables in the model, such as their mean. It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g., fewer data points than regression coefficients). Near violations of this assumption, where predictors are highly but not perfectly correlated, can reduce the precision of parameter estimates (see Variance inflation factor). In the case of perfect multicollinearity, the parameter vector \u03b2 will be non-identifiable\u2014it has no unique solution. In such a case, only some of the parameters can be identified (i.e., their values can only be estimated within some linear subspace of the full parameter space Rp). See partial least squares regression. Methods for fitting linear models with multicollinearity have been developed, some of which require additional assumptions such as \"effect sparsity\"\u2014that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:\n\nThe statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.\nThe arrangement, or probability distribution of the predictor variables x has a major influence on the precision of estimates of \u03b2. Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of \u03b2.\n\n\n=== Interpretation ===\n\nA fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are \"held fixed\". Specifically, the interpretation of \u03b2j is the expected change in y for a one-unit change in xj when the other covariates are held fixed\u2014that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.\nCare must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to \"hold ti fixed\" and at the same time change the value of ti2).\nIt is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.\nThe meaning of the expression \"held fixed\" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been \"held fixed\" by the experimenter. Alternatively, the expression \"held fixed\" can refer to a selection that takes place in the context of data analysis. In this case, we \"hold a variable fixed\" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of \"held fixed\" that can be used in an observational study.\nThe notion of a \"unique effect\" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.\n\n\n== Group effects ==\nIn a multiple linear regression model\n\n  \n    \n      \n        y\n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          \u03b2\n          \n            p\n          \n        \n        \n          x\n          \n            p\n          \n        \n        +\n        \u03b5\n        ,\n      \n    \n    {\\displaystyle y=\\beta _{0}+\\beta _{1}x_{1}+\\cdots +\\beta _{p}x_{p}+\\varepsilon ,}\n  parameter \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\beta _{j}}\n   of predictor variable \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   represents the individual effect of \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n  . It has an interpretation as the expected change in the response variable \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   when \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   increases by one unit with other predictor variables held constant. When \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   is strongly correlated with other predictor variables, it is improbable that \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   can increase by one unit with other variables held constant. In this case, the interpretation of \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\beta _{j}}\n   becomes problematic as it is based on an improbable condition, and the effect of \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   cannot be evaluated in isolation.\nFor a group of predictor variables, say, \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n  , a group effect \n  \n    \n      \n        \u03be\n        (\n        \n          w\n        \n        )\n      \n    \n    {\\displaystyle \\xi (\\mathbf {w} )}\n   is defined as a linear combination of their parameters\n\n  \n    \n      \n        \u03be\n        (\n        \n          w\n        \n        )\n        =\n        \n          w\n          \n            1\n          \n        \n        \n          \u03b2\n          \n            1\n          \n        \n        +\n        \n          w\n          \n            2\n          \n        \n        \n          \u03b2\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          w\n          \n            q\n          \n        \n        \n          \u03b2\n          \n            q\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\xi (\\mathbf {w} )=w_{1}\\beta _{1}+w_{2}\\beta _{2}+\\dots +w_{q}\\beta _{q},}\n  where \n  \n    \n      \n        \n          w\n        \n        =\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            q\n          \n        \n        \n          )\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} =(w_{1},w_{2},\\dots ,w_{q})^{\\intercal }}\n   is a weight vector satisfying \n  \n    \n      \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            q\n          \n        \n        \n          |\n        \n        \n          w\n          \n            j\n          \n        \n        \n          |\n        \n        =\n        1\n      \n    \n    {\\textstyle \\sum _{j=1}^{q}|w_{j}|=1}\n  . Because of the constraint on \n  \n    \n      \n        \n          \n            w\n            \n              j\n            \n          \n        \n      \n    \n    {\\displaystyle {w_{j}}}\n  , \n  \n    \n      \n        \u03be\n        (\n        \n          w\n        \n        )\n      \n    \n    {\\displaystyle \\xi (\\mathbf {w} )}\n   is also referred to as a normalized group effect. A group effect \n  \n    \n      \n        \u03be\n        (\n        \n          w\n        \n        )\n      \n    \n    {\\displaystyle \\xi (\\mathbf {w} )}\n   has an interpretation as the expected change in \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   when variables in the group \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},\\dots ,x_{q}}\n   change by the amount \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            q\n          \n        \n      \n    \n    {\\displaystyle w_{1},w_{2},\\dots ,w_{q}}\n  , respectively, at the same time with variables not in the group held constant. It generalizes the individual effect of a variable to a group of variables in that (\n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  ) if \n  \n    \n      \n        q\n        =\n        1\n      \n    \n    {\\displaystyle q=1}\n  , then the group effect reduces to an individual effect, and (\n  \n    \n      \n        i\n        i\n      \n    \n    {\\displaystyle ii}\n  ) if \n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle w_{i}=1}\n   and \n  \n    \n      \n        \n          w\n          \n            j\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle w_{j}=0}\n   for \n  \n    \n      \n        j\n        \u2260\n        i\n      \n    \n    {\\displaystyle j\\neq i}\n  , then the group effect also reduces to an individual effect.\nA group effect \n  \n    \n      \n        \u03be\n        (\n        \n          w\n        \n        )\n      \n    \n    {\\displaystyle \\xi (\\mathbf {w} )}\n   is said to be meaningful if the underlying simultaneous changes of the \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   variables \n  \n    \n      \n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            q\n          \n        \n        \n          )\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle (w_{1},w_{2},\\dots ,w_{q})^{\\intercal }}\n   is probable.\nGroup effects provide a means to study the collective impact of strongly correlated predictor variables in linear regression models. Individual effects of such variables are not well-defined as their parameters do not have good interpretations. Furthermore, when the sample size is not large, none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem. Nevertheless, there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression. A simple way to identify these meaningful group effects is to use an all positive correlations (APC) arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive, and standardize all \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   predictor variables in the model so that they all have mean zero and length one.  To illustrate this, suppose that \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n   is a group of strongly correlated variables in an APC arrangement and that they are not strongly correlated with predictor variables outside the group. Let \n  \n    \n      \n        \n          y\n          \u2032\n        \n      \n    \n    {\\displaystyle y'}\n   be the centred \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and \n  \n    \n      \n        \n          x\n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle x_{j}'}\n   be the standardized \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n  . Then, the standardized linear regression model is\n\n  \n    \n      \n        \n          y\n          \u2032\n        \n        =\n        \n          \u03b2\n          \n            1\n          \n          \u2032\n        \n        \n          x\n          \n            1\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          \u03b2\n          \n            p\n          \n          \u2032\n        \n        \n          x\n          \n            p\n          \n          \u2032\n        \n        +\n        \u03b5\n        .\n      \n    \n    {\\displaystyle y'=\\beta _{1}'x_{1}'+\\cdots +\\beta _{p}'x_{p}'+\\varepsilon .}\n  Parameters \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\beta _{j}}\n   in the original model, including \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n  , are simple functions of \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\beta _{j}'}\n   in the standardized model. The standardization of variables does not change their correlations, so \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n          \u2032\n        \n        ,\n        \n          x\n          \n            2\n          \n          \u2032\n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n          \u2032\n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n   is a group of strongly correlated variables in an APC arrangement and they are not strongly correlated with other predictor variables in the standardized model. A group effect of \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n          \u2032\n        \n        ,\n        \n          x\n          \n            2\n          \n          \u2032\n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n          \u2032\n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n   is\n\n  \n    \n      \n        \n          \u03be\n          \u2032\n        \n        (\n        \n          w\n        \n        )\n        =\n        \n          w\n          \n            1\n          \n        \n        \n          \u03b2\n          \n            1\n          \n          \u2032\n        \n        +\n        \n          w\n          \n            2\n          \n        \n        \n          \u03b2\n          \n            2\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          w\n          \n            q\n          \n        \n        \n          \u03b2\n          \n            q\n          \n          \u2032\n        \n        ,\n      \n    \n    {\\displaystyle \\xi '(\\mathbf {w} )=w_{1}\\beta _{1}'+w_{2}\\beta _{2}'+\\dots +w_{q}\\beta _{q}',}\n  and its minimum-variance unbiased linear estimator is\n\n  \n    \n      \n        \n          \n            \n              \n                \u03be\n                ^\n              \n            \n          \n          \u2032\n        \n        (\n        \n          w\n        \n        )\n        =\n        \n          w\n          \n            1\n          \n        \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n          \u2032\n        \n        +\n        \n          w\n          \n            2\n          \n        \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            2\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          w\n          \n            q\n          \n        \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            q\n          \n          \u2032\n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {\\xi }}'(\\mathbf {w} )=w_{1}{\\hat {\\beta }}_{1}'+w_{2}{\\hat {\\beta }}_{2}'+\\dots +w_{q}{\\hat {\\beta }}_{q}',}\n  where \n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}_{j}'}\n   is the least squares estimator of \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\beta _{j}'}\n  . In particular, the average group effect of the \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   standardized variables is\n\n  \n    \n      \n        \n          \u03be\n          \n            A\n          \n        \n        =\n        \n          \n            1\n            q\n          \n        \n        (\n        \n          \u03b2\n          \n            1\n          \n          \u2032\n        \n        +\n        \n          \u03b2\n          \n            2\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          \u03b2\n          \n            q\n          \n          \u2032\n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\xi _{A}={\\frac {1}{q}}(\\beta _{1}'+\\beta _{2}'+\\dots +\\beta _{q}'),}\n  which has an interpretation as the expected change in \n  \n    \n      \n        \n          y\n          \u2032\n        \n      \n    \n    {\\displaystyle y'}\n   when all \n  \n    \n      \n        \n          x\n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle x_{j}'}\n   in the strongly correlated group increase by \n  \n    \n      \n        (\n        1\n        \n          /\n        \n        q\n        )\n      \n    \n    {\\displaystyle (1/q)}\n  th of a unit at the same time with variables outside the group held constant. With strong positive correlations and in standardized units, variables in the group are approximately equal, so they are likely to increase at the same time and in similar amount. Thus, the average group effect \n  \n    \n      \n        \n          \u03be\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\xi _{A}}\n   is a meaningful effect. It can be accurately estimated by its minimum-variance unbiased linear estimator \n  \n    \n      \n        \n          \n            \n              \n                \u03be\n                ^\n              \n            \n          \n          \n            A\n          \n        \n        =\n        \n          \n            1\n            q\n          \n        \n        (\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n          \u2032\n        \n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            2\n          \n          \u2032\n        \n        +\n        \u22ef\n        +\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            q\n          \n          \u2032\n        \n        )\n      \n    \n    {\\textstyle {\\hat {\\xi }}_{A}={\\frac {1}{q}}({\\hat {\\beta }}_{1}'+{\\hat {\\beta }}_{2}'+\\dots +{\\hat {\\beta }}_{q}')}\n  , even when individually none of the \n  \n    \n      \n        \n          \u03b2\n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\beta _{j}'}\n   can be accurately estimated by \n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            j\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}_{j}'}\n  .\nNot all group effects are meaningful or can be accurately estimated. For example, \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle \\beta _{1}'}\n   is a special group effect with weights \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle w_{1}=1}\n   and \n  \n    \n      \n        \n          w\n          \n            j\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle w_{j}=0}\n   for \n  \n    \n      \n        j\n        \u2260\n        1\n      \n    \n    {\\displaystyle j\\neq 1}\n  , but it cannot be accurately estimated by \n  \n    \n      \n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            1\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}'_{1}}\n  . It is also not a meaningful effect.  In general, for a group of \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   strongly correlated predictor variables in an APC arrangement in the standardized model, group effects whose weight vectors \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   are at or near the centre of the simplex \n  \n    \n      \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            q\n          \n        \n        \n          w\n          \n            j\n          \n        \n        =\n        1\n      \n    \n    {\\textstyle \\sum _{j=1}^{q}w_{j}=1}\n    (\n  \n    \n      \n        \n          w\n          \n            j\n          \n        \n        \u2265\n        0\n      \n    \n    {\\displaystyle w_{j}\\geq 0}\n  ) are meaningful and can be accurately estimated by their minimum-variance unbiased linear estimators. Effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an APC arrangement. As such, they are not probable. These effects also cannot be accurately estimated. \nApplications of the group effects include (1) estimation and inference for meaningful group effects on the response variable, (2) testing for \"group significance\" of the \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   variables via testing \n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n        :\n        \n          \u03be\n          \n            A\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle H_{0}:\\xi _{A}=0}\n   versus \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n        :\n        \n          \u03be\n          \n            A\n          \n        \n        \u2260\n        0\n      \n    \n    {\\displaystyle H_{1}:\\xi _{A}\\neq 0}\n  , and (3) characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate.\nA group effect of the original variables \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n   can be expressed as a constant times a group effect of the standardized variables \n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n          \u2032\n        \n        ,\n        \n          x\n          \n            2\n          \n          \u2032\n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            q\n          \n          \u2032\n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n  . The former is meaningful when the latter is. Thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables.\n\n\n== Extensions ==\nNumerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.\n\n\n=== Simple and multiple linear regression ===\n\nThe very simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression.  The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression (not to be confused with multivariate linear regression).\nMultiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. The basic model for multiple linear regression is\n\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          X\n          \n            i\n            1\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          X\n          \n            i\n            2\n          \n        \n        +\n        \u2026\n        +\n        \n          \u03b2\n          \n            p\n          \n        \n        \n          X\n          \n            i\n            p\n          \n        \n        +\n        \n          \u03f5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}=\\beta _{0}+\\beta _{1}X_{i1}+\\beta _{2}X_{i2}+\\ldots +\\beta _{p}X_{ip}+\\epsilon _{i}}\n  for each observation i = 1, ... , n.\nIn the formula above we consider n observations of one dependent variable and p independent variables. Thus, Yi is the ith observation of the dependent variable, Xij is ith observation of the jth independent variable, j = 1, 2, ..., p. The values \u03b2j represent parameters to be estimated, and \u03b5i is the ith independent identically distributed normal error.\nIn the more general multivariate linear regression, there is one equation of the above form for each of m > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other:\n\n  \n    \n      \n        \n          Y\n          \n            i\n            j\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n            j\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n            j\n          \n        \n        \n          X\n          \n            i\n            1\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n            j\n          \n        \n        \n          X\n          \n            i\n            2\n          \n        \n        +\n        \u2026\n        +\n        \n          \u03b2\n          \n            p\n            j\n          \n        \n        \n          X\n          \n            i\n            p\n          \n        \n        +\n        \n          \u03f5\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle Y_{ij}=\\beta _{0j}+\\beta _{1j}X_{i1}+\\beta _{2j}X_{i2}+\\ldots +\\beta _{pj}X_{ip}+\\epsilon _{ij}}\n  for all observations indexed as i = 1, ... , n and for all dependent variables indexed as j = 1, ... , m.\nNearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model.  Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.\n\n\n=== General linear models ===\nThe general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi. Conditional linearity of \n  \n    \n      \n        E\n        (\n        \n          y\n        \n        \u2223\n        \n          \n            x\n          \n          \n            i\n          \n        \n        )\n        =\n        \n          \n            x\n          \n          \n            i\n          \n          \n            \n              T\n            \n          \n        \n        B\n      \n    \n    {\\displaystyle E(\\mathbf {y} \\mid \\mathbf {x} _{i})=\\mathbf {x} _{i}^{\\mathsf {T}}B}\n   is still assumed, with a matrix B replacing the vector \u03b2 of the classical linear regression model. Multivariate analogues of ordinary least squares (OLS) and generalized least squares (GLS) have been developed. \"General linear models\" are also called \"multivariate linear models\". These are not the same as multivariable linear models (also called \"multiple linear models\").\n\n\n=== Heteroscedastic models ===\nVarious models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances.  For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.\n\n\n=== Generalized linear models ===\nGeneralized linear models (GLMs) are a framework for modeling response variables that are bounded or discrete. This is used, for example:\n\nwhen modeling positive quantities (e.g. prices or populations) that vary over a large scale\u2014which are better described using a skewed distribution such as the log-normal distribution or Poisson distribution (although GLMs are not used for log-normal data, instead the response variable is simply transformed using the logarithm function);\nwhen modeling categorical data, such as the choice of a given candidate in an election (which is better described using a Bernoulli distribution/binomial distribution for binary choices, or a categorical distribution/multinomial distribution for multi-way choices), where there are a fixed number of choices that cannot be meaningfully ordered;\nwhen modeling ordinal data, e.g. ratings on a scale from 0 to 5, where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning (e.g. a rating of 4 may not be \"twice as good\" in any objective sense as a rating of 2, but simply indicates that it is better than 2 or 3 but not as good as 5).Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: \n  \n    \n      \n        E\n        (\n        Y\n        )\n        =\n        \n          g\n          \n            \u2212\n            1\n          \n        \n        (\n        X\n        B\n        )\n      \n    \n    {\\displaystyle E(Y)=g^{-1}(XB)}\n  . The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the \n  \n    \n      \n        (\n        \u2212\n        \u221e\n        ,\n        \u221e\n        )\n      \n    \n    {\\displaystyle (-\\infty ,\\infty )}\n   range of the linear predictor and the range of the response variable.\nSome common examples of GLMs are:\n\nPoisson regression for count data.\nLogistic regression and probit regression for binary data.\nMultinomial logistic regression and multinomial probit regression for categorical data.\nOrdered logit and ordered probit regression for ordinal data.Single index models allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor \u03b2\u2032x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate \u03b2 up to a proportionality constant.\n\n\n=== Hierarchical linear models ===\nHierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.\n\n\n=== Errors-in-variables ===\nErrors-in-variables models (or \"measurement error models\") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of \u03b2 to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.\n\n\n=== Others ===\nIn Dempster\u2013Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.\n\n\n== Estimation methods ==\nA large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.\nSome of the more common estimation techniques for linear regression are summarized below.\n\n\n=== Least-squares estimation and related techniques ===\n\nAssuming that the independent variable is \n  \n    \n      \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              \u2192\n            \n          \n        \n        =\n        \n          [\n          \n            \n              x\n              \n                1\n              \n              \n                i\n              \n            \n            ,\n            \n              x\n              \n                2\n              \n              \n                i\n              \n            \n            ,\n            \u2026\n            ,\n            \n              x\n              \n                m\n              \n              \n                i\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\vec {x_{i}}}=\\left[x_{1}^{i},x_{2}^{i},\\ldots ,x_{m}^{i}\\right]}\n   and the model's parameters are \n  \n    \n      \n        \n          \n            \n              \u03b2\n              \u2192\n            \n          \n        \n        =\n        \n          [\n          \n            \n              \u03b2\n              \n                0\n              \n            \n            ,\n            \n              \u03b2\n              \n                1\n              \n            \n            ,\n            \u2026\n            ,\n            \n              \u03b2\n              \n                m\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\vec {\\beta }}=\\left[\\beta _{0},\\beta _{1},\\ldots ,\\beta _{m}\\right]}\n  , then the model's prediction would be\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \u2248\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \u03b2\n          \n            j\n          \n        \n        \u00d7\n        \n          x\n          \n            j\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}\\approx \\beta _{0}+\\sum _{j=1}^{m}\\beta _{j}\\times x_{j}^{i}}\n  .If \n  \n    \n      \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {x_{i}}}}\n   is extended to \n  \n    \n      \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              \u2192\n            \n          \n        \n        =\n        \n          [\n          \n            1\n            ,\n            \n              x\n              \n                1\n              \n              \n                i\n              \n            \n            ,\n            \n              x\n              \n                2\n              \n              \n                i\n              \n            \n            ,\n            \u2026\n            ,\n            \n              x\n              \n                m\n              \n              \n                i\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\vec {x_{i}}}=\\left[1,x_{1}^{i},x_{2}^{i},\\ldots ,x_{m}^{i}\\right]}\n   then \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   would become a dot product of the parameter and the independent variable, i.e.\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \u2248\n        \n          \u2211\n          \n            j\n            =\n            0\n          \n          \n            m\n          \n        \n        \n          \u03b2\n          \n            j\n          \n        \n        \u00d7\n        \n          x\n          \n            j\n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              \u03b2\n              \u2192\n            \n          \n        \n        \u22c5\n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}\\approx \\sum _{j=0}^{m}\\beta _{j}\\times x_{j}^{i}={\\vec {\\beta }}\\cdot {\\vec {x_{i}}}}\n  .In the least-squares setting, the optimum parameter is defined as such that minimizes the sum of mean squared loss:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b2\n                  ^\n                \n              \n              \u2192\n            \n          \n        \n        =\n        \n          \n            \n              arg min\n            \n            \n              \n                \u03b2\n                \u2192\n              \n            \n          \n        \n        \n        L\n        \n          (\n          \n            D\n            ,\n            \n              \n                \n                  \u03b2\n                  \u2192\n                \n              \n            \n          \n          )\n        \n        =\n        \n          \n            \n              arg min\n            \n            \n              \n                \u03b2\n                \u2192\n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    \u03b2\n                    \u2192\n                  \n                \n              \n              \u22c5\n              \n                \n                  \n                    \n                      x\n                      \n                        i\n                      \n                    \n                    \u2192\n                  \n                \n              \n              \u2212\n              \n                y\n                \n                  i\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\hat {\\beta }}}={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\,L\\left(D,{\\vec {\\beta }}\\right)={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\sum _{i=1}^{n}\\left({\\vec {\\beta }}\\cdot {\\vec {x_{i}}}-y_{i}\\right)^{2}}\n  Now putting the independent and dependent variables in matrices \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   respectively, the loss function can be rewritten as:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n                \n                  (\n                  \n                    D\n                    ,\n                    \n                      \n                        \n                          \u03b2\n                          \u2192\n                        \n                      \n                    \n                  \n                  )\n                \n              \n              \n                \n                =\n                \u2016\n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n                \u2212\n                Y\n                \n                  \u2016\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    (\n                    \n                      X\n                      \n                        \n                          \n                            \u03b2\n                            \u2192\n                          \n                        \n                      \n                      \u2212\n                      Y\n                    \n                    )\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  (\n                  \n                    X\n                    \n                      \n                        \n                          \u03b2\n                          \u2192\n                        \n                      \n                    \n                    \u2212\n                    Y\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  Y\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n                \u2212\n                \n                  Y\n                  \n                    \n                      T\n                    \n                  \n                \n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n                \u2212\n                \n                  \n                    \n                      \n                        \u03b2\n                        \u2192\n                      \n                    \n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n                +\n                \n                  \n                    \n                      \n                        \u03b2\n                        \u2192\n                      \n                    \n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}L\\left(D,{\\vec {\\beta }}\\right)&=\\|X{\\vec {\\beta }}-Y\\|^{2}\\\\&=\\left(X{\\vec {\\beta }}-Y\\right)^{\\textsf {T}}\\left(X{\\vec {\\beta }}-Y\\right)\\\\&=Y^{\\textsf {T}}Y-Y^{\\textsf {T}}X{\\vec {\\beta }}-{\\vec {\\beta }}^{\\textsf {T}}X^{\\textsf {T}}Y+{\\vec {\\beta }}^{\\textsf {T}}X^{\\textsf {T}}X{\\vec {\\beta }}\\end{aligned}}}\n  As the loss is convex the optimum solution lies at gradient zero. The gradient of the loss function is (using Denominator layout convention):\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \u2202\n                      L\n                      \n                        (\n                        \n                          D\n                          ,\n                          \n                            \n                              \n                                \u03b2\n                                \u2192\n                              \n                            \n                          \n                        \n                        )\n                      \n                    \n                    \n                      \u2202\n                      \n                        \n                          \n                            \u03b2\n                            \u2192\n                          \n                        \n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \u2202\n                      \n                        (\n                        \n                          \n                            Y\n                            \n                              \n                                T\n                              \n                            \n                          \n                          Y\n                          \u2212\n                          \n                            Y\n                            \n                              \n                                T\n                              \n                            \n                          \n                          X\n                          \n                            \n                              \n                                \u03b2\n                                \u2192\n                              \n                            \n                          \n                          \u2212\n                          \n                            \n                              \n                                \n                                  \u03b2\n                                  \u2192\n                                \n                              \n                            \n                            \n                              \n                                T\n                              \n                            \n                          \n                          \n                            X\n                            \n                              \n                                T\n                              \n                            \n                          \n                          Y\n                          +\n                          \n                            \n                              \n                                \n                                  \u03b2\n                                  \u2192\n                                \n                              \n                            \n                            \n                              \n                                T\n                              \n                            \n                          \n                          \n                            X\n                            \n                              \n                                T\n                              \n                            \n                          \n                          X\n                          \n                            \n                              \n                                \u03b2\n                                \u2192\n                              \n                            \n                          \n                        \n                        )\n                      \n                    \n                    \n                      \u2202\n                      \n                        \n                          \n                            \u03b2\n                            \u2192\n                          \n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \u2212\n                2\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n                +\n                2\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {\\partial L\\left(D,{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}&={\\frac {\\partial \\left(Y^{\\textsf {T}}Y-Y^{\\textsf {T}}X{\\vec {\\beta }}-{\\vec {\\beta }}^{\\textsf {T}}X^{\\textsf {T}}Y+{\\vec {\\beta }}^{\\textsf {T}}X^{\\textsf {T}}X{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}\\\\&=-2X^{\\textsf {T}}Y+2X^{\\textsf {T}}X{\\vec {\\beta }}\\end{aligned}}}\n  Setting the gradient to zero produces the optimum parameter:\n\n  \n    \n      \n        \n          \n            \n              \n                \u2212\n                2\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n                +\n                2\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n              \n              \n                \n                =\n                0\n              \n            \n            \n              \n                \u21d2\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                X\n                \n                  \n                    \n                      \u03b2\n                      \u2192\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n              \n            \n            \n              \n                \u21d2\n                \n                  \n                    \n                      \n                        \n                          \u03b2\n                          ^\n                        \n                      \n                      \u2192\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    (\n                    \n                      \n                        X\n                        \n                          \n                            T\n                          \n                        \n                      \n                      X\n                    \n                    )\n                  \n                  \n                    \u2212\n                    1\n                  \n                \n                \n                  X\n                  \n                    \n                      T\n                    \n                  \n                \n                Y\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}-2X^{\\textsf {T}}Y+2X^{\\textsf {T}}X{\\vec {\\beta }}&=0\\\\\\Rightarrow X^{\\textsf {T}}X{\\vec {\\beta }}&=X^{\\textsf {T}}Y\\\\\\Rightarrow {\\vec {\\hat {\\beta }}}&=\\left(X^{\\textsf {T}}X\\right)^{-1}X^{\\textsf {T}}Y\\end{aligned}}}\n  Note: To prove that the \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}}\n   obtained is indeed the local minimum, one needs to differentiate once more to obtain the Hessian matrix and show that it is positive definite. This is provided by the Gauss\u2013Markov theorem.\nLinear least squares methods include mainly:\n\nOrdinary least squares\nWeighted least squares\nGeneralized least squares\n\n\n=== Maximum-likelihood estimation and related techniques ===\nMaximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family \u0192\u03b8 of probability distributions. When f\u03b8 is a normal distribution with zero mean and variance \u03b8, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when \u03b5 follows a multivariate normal distribution with a known covariance matrix.\nRidge regression and other forms of penalized estimation, such as Lasso regression, deliberately introduce bias into the estimation of \u03b2 in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.\nLeast absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for \u03b5.\nAdaptive estimation. If we assume that error terms are independent of the regressors, \n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n        \u22a5\n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}\\perp \\mathbf {x} _{i}}\n  , then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.\n\n\n=== Other estimation techniques ===\n\nBayesian linear regression applies the framework of Bayesian statistics to linear regression. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients \u03b2 are assumed to be random variables with a specified prior distribution.  The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression.  In addition, the Bayesian estimation process produces not a single point estimate for the \"best\" values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity.  This can be used to estimate the \"best\" coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.\nQuantile regression focuses on the conditional quantiles of y given X rather than the conditional mean of y given X. Linear quantile regression models a particular conditional quantile, for example the conditional median, as a linear function \u03b2Tx of the predictors.\nMixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have a known structure. Common applications of mixed models include analysis of data involving repeated measurements, such as longitudinal data, or data obtained from cluster sampling. They are generally fit as parametric models, using maximum likelihood or Bayesian estimation. In the case where the errors are modeled as normal random variables, there is a close connection between mixed models and generalized least squares. Fixed effects estimation is an alternative approach to analyzing this type of data.\nPrincipal component regression (PCR) is used when the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal component analysis, and then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.\nLeast-angle regression is an estimation procedure for linear regression models that was developed to handle high-dimensional covariate vectors, potentially with more covariates than observations.\nThe Theil\u2013Sen estimator is a simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers.\nOther robust estimation techniques, including the \u03b1-trimmed mean approach, and L-, M-, S-, and R-estimators have been introduced.\n\n\n== Applications ==\n\nLinear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.\n\n\n=== Trend line ===\n\nA trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.\nTrend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.\n\n\n=== Epidemiology ===\nEarly evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.\n\n\n=== Finance ===\nThe capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.\n\n\n=== Economics ===\n\nLinear regression is the predominant empirical tool in economics.  For example, it is used to predict consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand, and labor supply.\n\n\n=== Environmental science ===\nLinear regression finds application in a wide range of environmental science applications. In Canada, the Environmental Effects Monitoring Program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem.\n\n\n=== Machine learning ===\nLinear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.\n\n\n== History ==\nLeast squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss (1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== Further reading ==\nPedhazur, Elazar J (1982). Multiple regression in behavioral research: Explanation and prediction (2nd ed.). New York: Holt, Rinehart and Winston. ISBN 978-0-03-041760-3.\nMathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression.\nNational Physical Laboratory (1961). \"Chapter 1: Linear Equations and Matrices: Direct Methods\". Modern Computing Methods. Notes on Applied Science. Vol. 16 (2nd ed.). Her Majesty's Stationery Office.\n\n\n== External links ==\n\nLeast-Squares Regression, PhET Interactive simulations, University of Colorado at Boulder\nDIY Linear Fit", "Conditional independence": "In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is the hypothesis, and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   are observations, conditional independence can be stated as an equality:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)=P(A\\mid C)}\n  where \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)}\n   is the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given both \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  . Since the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is the same as the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given both \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , this equality expresses that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   contributes nothing to the certainty of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . In this case, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are said to be conditionally independent given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , written symbolically as: \n  \n    \n      \n        (\n        A\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle (A\\perp \\!\\!\\!\\perp B\\mid C)}\n  . In the language of causal equality notation, two functions \n  \n    \n      \n        f\n        (\n        y\n        )\n      \n    \n    {\\displaystyle f(y)}\n   and \n  \n    \n      \n        g\n        (\n        y\n        )\n      \n    \n    {\\displaystyle g(y)}\n   which both depend on a common variable \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   are described as conditionally independent using the notation \n  \n    \n      \n        f\n        \n          (\n          y\n          )\n        \n         \n        \n          \n            =\n            \n              \u21b6\u21b7\n            \n          \n        \n         \n        g\n        \n          (\n          y\n          )\n        \n      \n    \n    {\\displaystyle f\\left(y\\right)~{\\overset {\\curvearrowleft \\curvearrowright }{=}}~g\\left(y\\right)}\n  , which is equivalent to the notation \n  \n    \n      \n        P\n        (\n        f\n        \u2223\n        g\n        ,\n        y\n        )\n        =\n        P\n        (\n        f\n        \u2223\n        y\n        )\n      \n    \n    {\\displaystyle P(f\\mid g,y)=P(f\\mid y)}\n  .\nThe concept of conditional independence is essential to graph-based theories of statistical inference, as it establishes a mathematical relation between a collection of conditional statements and a graphoid.\n\n\n== Conditional independence of events ==\nLet \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   be events. \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are said to be conditionally independent given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   if and only if \n  \n    \n      \n        P\n        (\n        C\n        )\n        >\n        0\n      \n    \n    {\\displaystyle P(C)>0}\n   and:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)=P(A\\mid C)}\n  This property is often written: \n  \n    \n      \n        (\n        A\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle (A\\perp \\!\\!\\!\\perp B\\mid C)}\n  , which should be read \n  \n    \n      \n        (\n        (\n        A\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n        )\n        |\n        C\n        )\n      \n    \n    {\\displaystyle ((A\\perp \\!\\!\\!\\perp B)\\vert C)}\n  .\nEquivalently, conditional independence may be stated as:\n\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        \n          |\n        \n        C\n        )\n        =\n        P\n        (\n        A\n        \n          |\n        \n        C\n        )\n        P\n        (\n        B\n        \n          |\n        \n        C\n        )\n      \n    \n    {\\displaystyle P(A,B|C)=P(A|C)P(B|C)}\n  where \n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        \n          |\n        \n        C\n        )\n      \n    \n    {\\displaystyle P(A,B|C)}\n   is the joint probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  . This alternate formulation states that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are independent events, given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\nIt demonstrates that \n  \n    \n      \n        (\n        A\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle (A\\perp \\!\\!\\!\\perp B\\mid C)}\n   is equivalent to \n  \n    \n      \n        (\n        B\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle (B\\perp \\!\\!\\!\\perp A\\mid C)}\n  .\n\n\n=== Proof of the equivalent definition ===\n\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        \u2223\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n        P\n        (\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(A,B\\mid C)=P(A\\mid C)P(B\\mid C)}\n  iff \n  \n    \n      \n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              C\n              )\n            \n          \n        \n        =\n        \n          (\n          \n            \n              \n                P\n                (\n                A\n                ,\n                C\n                )\n              \n              \n                P\n                (\n                C\n                )\n              \n            \n          \n          )\n        \n        \n          (\n          \n            \n              \n                P\n                (\n                B\n                ,\n                C\n                )\n              \n              \n                P\n                (\n                C\n                )\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\frac {P(A,B,C)}{P(C)}}=\\left({\\frac {P(A,C)}{P(C)}}\\right)\\left({\\frac {P(B,C)}{P(C)}}\\right)}\n        (definition of conditional probability)iff \n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        ,\n        C\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              C\n              )\n              P\n              (\n              B\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              C\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A,B,C)={\\frac {P(A,C)P(B,C)}{P(C)}}}\n         (multiply both sides by \n  \n    \n      \n        P\n        (\n        C\n        )\n      \n    \n    {\\displaystyle P(C)}\n  )iff \n  \n    \n      \n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              B\n              ,\n              C\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              C\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {P(A,B,C)}{P(B,C)}}={\\frac {P(A,C)}{P(C)}}}\n         (divide both sides by \n  \n    \n      \n        P\n        (\n        B\n        ,\n        C\n        )\n      \n    \n    {\\displaystyle P(B,C)}\n  )iff \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)=P(A\\mid C)}\n         (definition of conditional probability) \n  \n    \n      \n        \u2234\n      \n    \n    {\\displaystyle \\therefore }\n  \n\n\n=== Examples ===\n\n\n==== Coloured boxes ====\nEach cell represents a possible outcome. The events \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\color {red}R}\n  , \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\color {blue}B}\n   and \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\color {gold}Y}\n   are represented by the areas shaded red, blue and yellow respectively. The overlap between the events \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\color {red}R}\n   and \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\color {blue}B}\n   is shaded   purple.\n\nThe probabilities of these events are shaded areas with respect to the total area. In both examples \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\color {red}R}\n   and \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\color {blue}B}\n   are conditionally independent given \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\color {gold}Y}\n   because:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            R\n          \n        \n        ,\n        \n          \n            B\n          \n        \n        \u2223\n        \n          \n            Y\n          \n        \n        )\n        =\n        Pr\n        (\n        \n          \n            R\n          \n        \n        \u2223\n        \n          \n            Y\n          \n        \n        )\n        Pr\n        (\n        \n          \n            B\n          \n        \n        \u2223\n        \n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Pr({\\color {red}R},{\\color {blue}B}\\mid {\\color {gold}Y})=\\Pr({\\color {red}R}\\mid {\\color {gold}Y})\\Pr({\\color {blue}B}\\mid {\\color {gold}Y})}\n  but not conditionally independent given \n  \n    \n      \n        \n          [\n          \n            \n              not \n            \n            \n              \n                Y\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\left[{\\text{not }}{\\color {gold}Y}\\right]}\n   because:\n\n  \n    \n      \n        Pr\n        (\n        \n          \n            R\n          \n        \n        ,\n        \n          \n            B\n          \n        \n        \u2223\n        \n          not \n        \n        \n          \n            Y\n          \n        \n        )\n        \u2260\n        Pr\n        (\n        \n          \n            R\n          \n        \n        \u2223\n        \n          not \n        \n        \n          \n            Y\n          \n        \n        )\n        Pr\n        (\n        \n          \n            B\n          \n        \n        \u2223\n        \n          not \n        \n        \n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Pr({\\color {red}R},{\\color {blue}B}\\mid {\\text{not }}{\\color {gold}Y})\\not =\\Pr({\\color {red}R}\\mid {\\text{not }}{\\color {gold}Y})\\Pr({\\color {blue}B}\\mid {\\text{not }}{\\color {gold}Y})}\n  \n\n\n==== Proximity and delays ====\nLet events A and B be defined as the probability that person A and person B will be home in time for dinner where both people are randomly sampled from the entire world. Events A and B can be assumed to be independent i.e. knowledge that A is late has minimal to no change on the probability that B will be late. However, if a third event is introduced, person A and person B live in the same neighborhood, the two events are now considered not conditionally independent. Traffic conditions and weather-related events that might delay person A, might delay person B as well. Given the third event and knowledge that person A was late, the probability that person B will be late does meaningfully change.\n\n\n==== Dice rolling ====\nConditional independence depends on the nature of the third event. If you roll two dice, one may assume that the two dice behave independently of each other. Looking at the results of one dice will not tell you about the result of the second dice. (That is, the two dice are independent.) If, however, the 1st dice's result is a 3, and someone tells you about a third event - that the sum of the two results is even - then this extra unit of information restricts the options for the 2nd result to an odd number. In other words, two events can be independent, but NOT conditionally independent.\n\n\n==== Height and vocabulary ====\nHeight and vocabulary are dependent since very small people tend to be children, known for their more basic vocabularies. But knowing that two people are 19 years old (i.e., conditional on age) there is no reason to think that one person's vocabulary is larger if we are told that they are taller.\n\n\n== Conditional independence of random variables ==\nTwo discrete random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are conditionally independent given a third discrete random variable \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   if and only if they are independent in their conditional probability distribution given \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  . That is, \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are conditionally independent given \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   if and only if, given any value of \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  , the probability distribution of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is the same for all values of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   and the probability distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is the same for all values of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . Formally:\n\nwhere \n  \n    \n      \n        \n          F\n          \n            X\n            ,\n            Y\n            \n            \u2223\n            \n            Z\n            \n            =\n            \n            z\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        Pr\n        (\n        X\n        \u2264\n        x\n        ,\n        Y\n        \u2264\n        y\n        \u2223\n        Z\n        =\n        z\n        )\n      \n    \n    {\\displaystyle F_{X,Y\\,\\mid \\,Z\\,=\\,z}(x,y)=\\Pr(X\\leq x,Y\\leq y\\mid Z=z)}\n   is the conditional cumulative distribution function of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  .\nTwo events \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are conditionally independent given a \u03c3-algebra \n  \n    \n      \n        \u03a3\n      \n    \n    {\\displaystyle \\Sigma }\n   if\n\n  \n    \n      \n        Pr\n        (\n        R\n        ,\n        B\n        \u2223\n        \u03a3\n        )\n        =\n        Pr\n        (\n        R\n        \u2223\n        \u03a3\n        )\n        Pr\n        (\n        B\n        \u2223\n        \u03a3\n        )\n        \n           a.s.\n        \n      \n    \n    {\\displaystyle \\Pr(R,B\\mid \\Sigma )=\\Pr(R\\mid \\Sigma )\\Pr(B\\mid \\Sigma ){\\text{ a.s.}}}\n  where \n  \n    \n      \n        Pr\n        (\n        A\n        \u2223\n        \u03a3\n        )\n      \n    \n    {\\displaystyle \\Pr(A\\mid \\Sigma )}\n   denotes the conditional expectation of the indicator function of the event \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , \n  \n    \n      \n        \n          \u03c7\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\chi _{A}}\n  , given the sigma algebra \n  \n    \n      \n        \u03a3\n      \n    \n    {\\displaystyle \\Sigma }\n  . That is,\n\n  \n    \n      \n        Pr\n        (\n        A\n        \u2223\n        \u03a3\n        )\n        :=\n        E\n        \u2061\n        [\n        \n          \u03c7\n          \n            A\n          \n        \n        \u2223\n        \u03a3\n        ]\n        .\n      \n    \n    {\\displaystyle \\Pr(A\\mid \\Sigma ):=\\operatorname {E} [\\chi _{A}\\mid \\Sigma ].}\n  Two random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are conditionally independent given a \u03c3-algebra \n  \n    \n      \n        \u03a3\n      \n    \n    {\\displaystyle \\Sigma }\n   if the above equation holds for all \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n   in \n  \n    \n      \n        \u03c3\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\sigma (X)}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   in \n  \n    \n      \n        \u03c3\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle \\sigma (Y)}\n  .\nTwo random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are conditionally independent given a random variable \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   if they are independent given \u03c3(W): the \u03c3-algebra generated by \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  . This is commonly written:\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        \u2223\n        W\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\mid W}\n   or\n\n  \n    \n      \n        X\n        \u22a5\n        Y\n        \u2223\n        W\n      \n    \n    {\\displaystyle X\\perp Y\\mid W}\n  This it read \"\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is independent of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , given \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \"; the conditioning applies to the whole statement: \"(\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is independent of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  ) given \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \".\n\n  \n    \n      \n        (\n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        )\n        \u2223\n        W\n      \n    \n    {\\displaystyle (X\\perp \\!\\!\\!\\perp Y)\\mid W}\n  This notation extends \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y}\n   for \"\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is independent of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\"\nIf \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   assumes a countable set of values, this is equivalent to the conditional independence of X and Y for the events of the form \n  \n    \n      \n        [\n        W\n        =\n        w\n        ]\n      \n    \n    {\\displaystyle [W=w]}\n  .\nConditional independence of more than two events, or of more than two random variables, is defined analogously.\nThe following two examples show that \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y}\n   neither implies nor is implied by \n  \n    \n      \n        (\n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        )\n        \u2223\n        W\n      \n    \n    {\\displaystyle (X\\perp \\!\\!\\!\\perp Y)\\mid W}\n  .\nFirst, suppose \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   is 0 with probability 0.5 and 1 otherwise.  When W = 0 take \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   to be independent, each having the value 0 with probability 0.99 and the value 1 otherwise.  When \n  \n    \n      \n        W\n        =\n        1\n      \n    \n    {\\displaystyle W=1}\n  , \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are again independent, but this time they take the value 1 with probability 0.99.  Then \n  \n    \n      \n        (\n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        )\n        \u2223\n        W\n      \n    \n    {\\displaystyle (X\\perp \\!\\!\\!\\perp Y)\\mid W}\n  . But \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are dependent, because Pr(X = 0) < Pr(X = 0|Y = 0).  This is because Pr(X = 0) = 0.5, but if Y = 0 then it's very likely that W = 0 and thus that X = 0 as well, so Pr(X = 0|Y = 0) > 0.5.\nFor the second example, suppose \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y}\n  , each taking the values 0 and 1 with probability 0.5. Let \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   be the product \n  \n    \n      \n        X\n        \u22c5\n        Y\n      \n    \n    {\\displaystyle X\\cdot Y}\n  .  Then when \n  \n    \n      \n        W\n        =\n        0\n      \n    \n    {\\displaystyle W=0}\n  , Pr(X = 0) = 2/3, but Pr(X = 0|Y = 0) = 1/2, so \n  \n    \n      \n        (\n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        )\n        \u2223\n        W\n      \n    \n    {\\displaystyle (X\\perp \\!\\!\\!\\perp Y)\\mid W}\n   is false.\nThis is also an example of Explaining Away. See Kevin Murphy's tutorial  where \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   take the values \"brainy\" and \"sporty\".\n\n\n== Conditional independence of random vectors ==\nTwo random vectors \n  \n    \n      \n        \n          X\n        \n        =\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            l\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} =(X_{1},\\ldots ,X_{l})^{\\mathrm {T} }}\n   and \n  \n    \n      \n        \n          Y\n        \n        =\n        (\n        \n          Y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          Y\n          \n            m\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Y} =(Y_{1},\\ldots ,Y_{m})^{\\mathrm {T} }}\n   are conditionally independent given a third random vector \n  \n    \n      \n        \n          Z\n        \n        =\n        (\n        \n          Z\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          Z\n          \n            n\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Z} =(Z_{1},\\ldots ,Z_{n})^{\\mathrm {T} }}\n   if and only if they are independent in their conditional cumulative distribution given \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbf {Z} }\n  . Formally:\n\nwhere \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} =(x_{1},\\ldots ,x_{l})^{\\mathrm {T} }}\n  , \n  \n    \n      \n        \n          y\n        \n        =\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            m\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {y} =(y_{1},\\ldots ,y_{m})^{\\mathrm {T} }}\n   and \n  \n    \n      \n        \n          z\n        \n        =\n        (\n        \n          z\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          z\n          \n            n\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {z} =(z_{1},\\ldots ,z_{n})^{\\mathrm {T} }}\n   and the conditional cumulative distributions are defined as follows.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                    \n                      X\n                    \n                    ,\n                    \n                      Y\n                    \n                    \n                    \u2223\n                    \n                    \n                      Z\n                    \n                    \n                    =\n                    \n                    \n                      z\n                    \n                  \n                \n                (\n                \n                  x\n                \n                ,\n                \n                  y\n                \n                )\n              \n              \n                \n                =\n                Pr\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                \u2264\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  X\n                  \n                    l\n                  \n                \n                \u2264\n                \n                  x\n                  \n                    l\n                  \n                \n                ,\n                \n                  Y\n                  \n                    1\n                  \n                \n                \u2264\n                \n                  y\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  Y\n                  \n                    m\n                  \n                \n                \u2264\n                \n                  y\n                  \n                    m\n                  \n                \n                \u2223\n                \n                  Z\n                  \n                    1\n                  \n                \n                =\n                \n                  z\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  Z\n                  \n                    n\n                  \n                \n                =\n                \n                  z\n                  \n                    n\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  F\n                  \n                    \n                      X\n                    \n                    \n                    \u2223\n                    \n                    \n                      Z\n                    \n                    \n                    =\n                    \n                    \n                      z\n                    \n                  \n                \n                (\n                \n                  x\n                \n                )\n              \n              \n                \n                =\n                Pr\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                \u2264\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  X\n                  \n                    l\n                  \n                \n                \u2264\n                \n                  x\n                  \n                    l\n                  \n                \n                \u2223\n                \n                  Z\n                  \n                    1\n                  \n                \n                =\n                \n                  z\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  Z\n                  \n                    n\n                  \n                \n                =\n                \n                  z\n                  \n                    n\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  F\n                  \n                    \n                      Y\n                    \n                    \n                    \u2223\n                    \n                    \n                      Z\n                    \n                    \n                    =\n                    \n                    \n                      z\n                    \n                  \n                \n                (\n                \n                  y\n                \n                )\n              \n              \n                \n                =\n                Pr\n                (\n                \n                  Y\n                  \n                    1\n                  \n                \n                \u2264\n                \n                  y\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  Y\n                  \n                    m\n                  \n                \n                \u2264\n                \n                  y\n                  \n                    m\n                  \n                \n                \u2223\n                \n                  Z\n                  \n                    1\n                  \n                \n                =\n                \n                  z\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n                ,\n                \n                  Z\n                  \n                    n\n                  \n                \n                =\n                \n                  z\n                  \n                    n\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}F_{\\mathbf {X} ,\\mathbf {Y} \\,\\mid \\,\\mathbf {Z} \\,=\\,\\mathbf {z} }(\\mathbf {x} ,\\mathbf {y} )&=\\Pr(X_{1}\\leq x_{1},\\ldots ,X_{l}\\leq x_{l},Y_{1}\\leq y_{1},\\ldots ,Y_{m}\\leq y_{m}\\mid Z_{1}=z_{1},\\ldots ,Z_{n}=z_{n})\\\\[6pt]F_{\\mathbf {X} \\,\\mid \\,\\mathbf {Z} \\,=\\,\\mathbf {z} }(\\mathbf {x} )&=\\Pr(X_{1}\\leq x_{1},\\ldots ,X_{l}\\leq x_{l}\\mid Z_{1}=z_{1},\\ldots ,Z_{n}=z_{n})\\\\[6pt]F_{\\mathbf {Y} \\,\\mid \\,\\mathbf {Z} \\,=\\,\\mathbf {z} }(\\mathbf {y} )&=\\Pr(Y_{1}\\leq y_{1},\\ldots ,Y_{m}\\leq y_{m}\\mid Z_{1}=z_{1},\\ldots ,Z_{n}=z_{n})\\end{aligned}}}\n  \n\n\n== Uses in Bayesian inference ==\nLet p be the proportion of voters who will vote \"yes\" in an upcoming referendum. In taking an opinion poll, one chooses n voters randomly from the population. For i = 1, ..., n, let Xi = 1 or 0 corresponding, respectively, to whether or not the ith chosen voter will or will not vote \"yes\".\nIn a frequentist approach to statistical inference one would not attribute any probability distribution to p (unless the probabilities could be somehow interpreted as relative frequencies of occurrence of some event or as proportions of some population) and one would say that X1, ..., Xn are independent random variables.\nBy contrast, in a Bayesian approach to statistical inference, one would assign a probability distribution to p regardless of the non-existence of any such \"frequency\" interpretation, and one would construe the probabilities as degrees of belief that p is in any interval to which a probability is assigned. In that model, the random variables X1, ..., Xn are not independent, but they are conditionally independent given the value of p. In particular, if a large number of the Xs are observed to be equal to 1, that would imply a high conditional probability, given that observation, that p is near 1, and thus a high conditional probability, given that observation, that the next X to be observed will be equal to 1.\n\n\n== Rules of conditional independence ==\nA set of rules governing statements of conditional independence have been derived from the basic definition.The these rules were termed \"Graphoid Axioms\"\nby Pearl and Paz, because they hold in graphs, where \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        \u2223\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A\\mid B}\n   is interpreted to mean: \"All paths from X to A are intercepted by the set B\".\n\n\n=== Symmetry ===\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        \n        \u21d2\n        \n        Y\n        \u22a5\n        \n        \n        \n        \u22a5\n        X\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\quad \\Rightarrow \\quad Y\\perp \\!\\!\\!\\perp X}\n  \n\n\n=== Decomposition ===\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        ,\n        B\n        \n        \u21d2\n        \n        \n           and \n        \n        \n          \n            {\n            \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  A\n                \n              \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  B\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B\\quad \\Rightarrow \\quad {\\text{ and }}{\\begin{cases}X\\perp \\!\\!\\!\\perp A\\\\X\\perp \\!\\!\\!\\perp B\\end{cases}}}\n  Proof\n\n  \n    \n      \n        \n          p\n          \n            X\n            ,\n            A\n            ,\n            B\n          \n        \n        (\n        x\n        ,\n        a\n        ,\n        b\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n            ,\n            B\n          \n        \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle p_{X,A,B}(x,a,b)=p_{X}(x)p_{A,B}(a,b)}\n        (meaning of \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        ,\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B}\n  )\n\n  \n    \n      \n        \n          \u222b\n          \n            B\n          \n        \n        \n          p\n          \n            X\n            ,\n            A\n            ,\n            B\n          \n        \n        (\n        x\n        ,\n        a\n        ,\n        b\n        )\n        \n        d\n        b\n        =\n        \n          \u222b\n          \n            B\n          \n        \n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n            ,\n            B\n          \n        \n        (\n        a\n        ,\n        b\n        )\n        \n        d\n        b\n      \n    \n    {\\displaystyle \\int _{B}p_{X,A,B}(x,a,b)\\,db=\\int _{B}p_{X}(x)p_{A,B}(a,b)\\,db}\n        (ignore variable B by integrating it out)\n\n  \n    \n      \n        \n          p\n          \n            X\n            ,\n            A\n          \n        \n        (\n        x\n        ,\n        a\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n          \n        \n        (\n        a\n        )\n      \n    \n    {\\displaystyle p_{X,A}(x,a)=p_{X}(x)p_{A}(a)}\n       A similar proof shows the independence of X and B.\n\n\n=== Weak union ===\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        ,\n        B\n        \n        \u21d2\n        \n        \n           and \n        \n        \n          \n            {\n            \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  A\n                  \u2223\n                  B\n                \n              \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  B\n                  \u2223\n                  A\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B\\quad \\Rightarrow \\quad {\\text{ and }}{\\begin{cases}X\\perp \\!\\!\\!\\perp A\\mid B\\\\X\\perp \\!\\!\\!\\perp B\\mid A\\end{cases}}}\n  Proof\n\nBy assumption, \n  \n    \n      \n        Pr\n        (\n        X\n        )\n        =\n        Pr\n        (\n        X\n        \u2223\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X)=\\Pr(X\\mid A,B)}\n  .\nDue to the property of decomposition \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp B}\n  , \n  \n    \n      \n        Pr\n        (\n        X\n        )\n        =\n        Pr\n        (\n        X\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X)=\\Pr(X\\mid B)}\n  .\nCombining the above two equalities gives \n  \n    \n      \n        Pr\n        (\n        X\n        \u2223\n        B\n        )\n        =\n        Pr\n        (\n        X\n        \u2223\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X\\mid B)=\\Pr(X\\mid A,B)}\n  , which establishes \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        \u2223\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A\\mid B}\n  .The second condition can be proved similarly.\n\n\n=== Contraction ===\n\n  \n    \n      \n        \n          \n          \n            \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  A\n                  \u2223\n                  B\n                \n              \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  B\n                \n              \n            \n          \n          }\n        \n        \n           and \n        \n        \n        \u21d2\n        \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        ,\n        B\n      \n    \n    {\\displaystyle \\left.{\\begin{aligned}X\\perp \\!\\!\\!\\perp A\\mid B\\\\X\\perp \\!\\!\\!\\perp B\\end{aligned}}\\right\\}{\\text{ and }}\\quad \\Rightarrow \\quad X\\perp \\!\\!\\!\\perp A,B}\n  Proof\nThis property can be proved by noticing \n  \n    \n      \n        Pr\n        (\n        X\n        \u2223\n        A\n        ,\n        B\n        )\n        =\n        Pr\n        (\n        X\n        \u2223\n        B\n        )\n        =\n        Pr\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\Pr(X\\mid A,B)=\\Pr(X\\mid B)=\\Pr(X)}\n  , each equality of which is asserted by \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        A\n        \u2223\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A\\mid B}\n   and \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp B}\n  , respectively.\n\n\n=== Intersection ===\nFor strictly positive probability distributions, the following also holds:\n\n  \n    \n      \n        \n          \n          \n            \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  Y\n                  \u2223\n                  Z\n                  ,\n                  W\n                \n              \n              \n                \n                  X\n                  \u22a5\n                  \n                  \n                  \n                  \u22a5\n                  W\n                  \u2223\n                  Z\n                  ,\n                  Y\n                \n              \n            \n          \n          }\n        \n        \n           and \n        \n        \n        \u21d2\n        \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        W\n        ,\n        Y\n        \u2223\n        Z\n      \n    \n    {\\displaystyle \\left.{\\begin{aligned}X\\perp \\!\\!\\!\\perp Y\\mid Z,W\\\\X\\perp \\!\\!\\!\\perp W\\mid Z,Y\\end{aligned}}\\right\\}{\\text{ and }}\\quad \\Rightarrow \\quad X\\perp \\!\\!\\!\\perp W,Y\\mid Z}\n  Proof\nBy assumption:\n\n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        )\n        \u2227\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        Y\n        )\n        \n        \u27f9\n        \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        )\n      \n    \n    {\\displaystyle P(X|Z,W,Y)=P(X|Z,W)\\land P(X|Z,W,Y)=P(X|Z,Y)\\implies P(X|Z,Y)=P(X|Z,W)}\n  Using this equality, together with the Law of total probability applied to \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        )\n      \n    \n    {\\displaystyle P(X|Z)}\n  :\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                X\n                \n                  |\n                \n                Z\n                )\n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    w\n                    \u2208\n                    W\n                  \n                \n                P\n                (\n                X\n                \n                  |\n                \n                Z\n                ,\n                W\n                =\n                w\n                )\n                P\n                (\n                W\n                =\n                w\n                \n                  |\n                \n                Z\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    w\n                    \u2208\n                    W\n                  \n                \n                P\n                (\n                X\n                \n                  |\n                \n                Y\n                ,\n                Z\n                )\n                P\n                (\n                W\n                =\n                w\n                \n                  |\n                \n                Z\n                )\n              \n            \n            \n              \n              \n                \n                =\n                P\n                (\n                X\n                \n                  |\n                \n                Z\n                ,\n                Y\n                )\n                \n                  \u2211\n                  \n                    w\n                    \u2208\n                    W\n                  \n                \n                P\n                (\n                W\n                =\n                w\n                \n                  |\n                \n                Z\n                )\n              \n            \n            \n              \n              \n                \n                =\n                P\n                (\n                X\n                \n                  |\n                \n                Z\n                ,\n                Y\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(X|Z)&=\\sum _{w\\in W}P(X|Z,W=w)P(W=w|Z)\\\\[4pt]&=\\sum _{w\\in W}P(X|Y,Z)P(W=w|Z)\\\\[4pt]&=P(X|Z,Y)\\sum _{w\\in W}P(W=w|Z)\\\\[4pt]&=P(X|Z,Y)\\end{aligned}}}\n  Since \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X|Z,W,Y)=P(X|Z,Y)}\n   and \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        )\n      \n    \n    {\\displaystyle P(X|Z,Y)=P(X|Z)}\n  , it follows that \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Z\n        ,\n        W\n        ,\n        Y\n        )\n        =\n        P\n        (\n        X\n        \n          |\n        \n        Z\n        )\n        \n        \u27fa\n        \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        ,\n        W\n        \n          |\n        \n        Z\n      \n    \n    {\\displaystyle P(X|Z,W,Y)=P(X|Z)\\iff X\\perp \\!\\!\\!\\perp Y,W|Z}\n  .\nTechnical note: since these implications hold for any probability space, they will still hold if one considers a sub-universe by conditioning everything on another variable, say K. For example, \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        \u21d2\n        Y\n        \u22a5\n        \n        \n        \n        \u22a5\n        X\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\Rightarrow Y\\perp \\!\\!\\!\\perp X}\n   would also mean that \n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        Y\n        \u2223\n        K\n        \u21d2\n        Y\n        \u22a5\n        \n        \n        \n        \u22a5\n        X\n        \u2223\n        K\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\mid K\\Rightarrow Y\\perp \\!\\!\\!\\perp X\\mid K}\n  .\n\n\n== See also ==\nGraphoid\nConditional dependence\nde Finetti's theorem\nConditional expectation\n\n\n== References ==\n\n\n== External links ==\n Media related to Conditional independence at Wikimedia Commons", "Data matrix (multivariate statistics)": "In statistics and in particular in regression analysis, a design matrix, also known as model matrix or regressor matrix and often denoted by X, is a matrix of values of explanatory variables of a set of objects. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The design matrix is used in certain statistical models, e.g., the general linear model. It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables.\nThe design matrix contains data on the independent variables (also called explanatory variables) in statistical models which attempt to explain observed data on a response variable (often called a dependent variable) in terms of the explanatory variables. The theory relating to such models makes substantial use of matrix manipulations involving the design matrix: see for example linear regression. A notable feature of the concept of a design matrix is that it is able to represent a number of different experimental designs and statistical models, e.g., ANOVA, ANCOVA, and linear regression.\n\n\n== Definition ==\nThe design matrix is defined to be a matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   such that \n  \n    \n      \n        \n          X\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle X_{ij}}\n   (the jth column of the ith row of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ) represents the value of the jth variable associated with the ith object.\nA regression model may be represented via matrix multiplication as\n\n  \n    \n      \n        y\n        =\n        X\n        \u03b2\n        +\n        e\n        ,\n      \n    \n    {\\displaystyle y=X\\beta +e,}\n  where X is the design matrix, \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   is a vector of the model's coefficients (one for each variable), \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n   is a vector of random errors with mean zero, and y is the vector of predicted outputs for each object.\n\n\n== Size ==\nThe matrix of data has dimension n-by-p, where n is the number of samples observed, and p is the number of variables (features) measured in all samples.In this representation different rows typically represent different repetitions of an experiment, while columns represent different types of data (say, the results from particular probes).  For example, suppose an experiment is run where 10 people are pulled off the street and asked four questions.  The data matrix M would be a 10\u00d74 matrix (meaning 10 rows and 4 columns).  The datum in row i and column j of this matrix would be the answer of the i th person to the j th question.\n\n\n== Examples ==\n\n\n=== Arithmetic mean ===\nThe design matrix for an arithmetic mean is a column vector of ones.\n\n\n=== Simple linear regression ===\nThis section gives an example of simple linear regression\u2014that is, regression with only a single explanatory variable\u2014with seven observations.\nThe seven data points are {yi, xi}, for i = 1, 2, \u2026, 7. The simple linear regression model is\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          x\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i}+\\varepsilon _{i},\\,}\n  where \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n   is the y-intercept and \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   is the slope of the regression line. This model can be represented in matrix form as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b2\n                    \n                      0\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      1\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        +\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b5\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\y_{4}\\\\y_{5}\\\\y_{6}\\\\y_{7}\\end{bmatrix}}={\\begin{bmatrix}1&x_{1}\\\\1&x_{2}\\\\1&x_{3}\\\\1&x_{4}\\\\1&x_{5}\\\\1&x_{6}\\\\1&x_{7}\\end{bmatrix}}{\\begin{bmatrix}\\beta _{0}\\\\\\beta _{1}\\end{bmatrix}}+{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\\\\\varepsilon _{7}\\end{bmatrix}}}\n  where the first column of 1s in the design matrix allows estimation of the y-intercept while the second column contains the x-values associated with the corresponding y-values.\n\n\n=== Multiple regression ===\nThis section contains an example of multiple regression with two covariates (explanatory variables): w and x.\nAgain suppose that the data consist of seven observations, and that for each observed value to be predicted (\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  ), values wi and xi of the two covariates are also observed. The model to be considered is\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        \n          w\n          \n            i\n          \n        \n        +\n        \n          \u03b2\n          \n            2\n          \n        \n        \n          x\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}w_{i}+\\beta _{2}x_{i}+\\varepsilon _{i}}\n  This model can be written in matrix terms as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      3\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      4\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      5\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      6\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    w\n                    \n                      7\n                    \n                  \n                \n                \n                  \n                    x\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b2\n                    \n                      0\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b2\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        +\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b5\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\y_{4}\\\\y_{5}\\\\y_{6}\\\\y_{7}\\end{bmatrix}}={\\begin{bmatrix}1&w_{1}&x_{1}\\\\1&w_{2}&x_{2}\\\\1&w_{3}&x_{3}\\\\1&w_{4}&x_{4}\\\\1&w_{5}&x_{5}\\\\1&w_{6}&x_{6}\\\\1&w_{7}&x_{7}\\end{bmatrix}}{\\begin{bmatrix}\\beta _{0}\\\\\\beta _{1}\\\\\\beta _{2}\\end{bmatrix}}+{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\\\\\varepsilon _{7}\\end{bmatrix}}}\n  Here the 7\u00d73 matrix on the right side is the design matrix.\n\n\n=== One-way ANOVA (cell means model) ===\nThis section contains an example with a one-way analysis of variance (ANOVA) with three groups and seven observations. The given data set has the first three observations belonging to the first group, the following two observations belonging to the second group and the final two observations belonging to the third group.\nIf the model to be fit is just the mean of each group, then the model is\n\n  \n    \n      \n        \n          y\n          \n            i\n            j\n          \n        \n        =\n        \n          \u03bc\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle y_{ij}=\\mu _{i}+\\varepsilon _{ij}}\n  which can be written\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    \u03bc\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03bc\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03bc\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        +\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b5\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\y_{4}\\\\y_{5}\\\\y_{6}\\\\y_{7}\\end{bmatrix}}={\\begin{bmatrix}1&0&0\\\\1&0&0\\\\1&0&0\\\\0&1&0\\\\0&1&0\\\\0&0&1\\\\0&0&1\\end{bmatrix}}{\\begin{bmatrix}\\mu _{1}\\\\\\mu _{2}\\\\\\mu _{3}\\end{bmatrix}}+{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\\\\\varepsilon _{7}\\end{bmatrix}}}\n  In this model \n  \n    \n      \n        \n          \u03bc\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mu _{i}}\n   represents the mean of the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  th group.\n\n\n=== One-way ANOVA (offset from reference group) ===\nThe ANOVA model could be equivalently written as each group parameter \n  \n    \n      \n        \n          \u03c4\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\tau _{i}}\n   being an offset from some overall reference.  Typically this reference point is taken to be one of the groups under consideration. This makes sense in the context of comparing multiple treatment groups to a control group and the control group is considered the \"reference\". In this example, group 1 was chosen to be the reference group. As such the model to be fit is\n\n  \n    \n      \n        \n          y\n          \n            i\n            j\n          \n        \n        =\n        \u03bc\n        +\n        \n          \u03c4\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle y_{ij}=\\mu +\\tau _{i}+\\varepsilon _{ij}}\n  with the constraint that \n  \n    \n      \n        \n          \u03c4\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\tau _{1}}\n   is zero.\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n              \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \u03bc\n                \n              \n              \n                \n                  \n                    \u03c4\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03c4\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        +\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03b5\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      5\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      6\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03b5\n                    \n                      7\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\y_{4}\\\\y_{5}\\\\y_{6}\\\\y_{7}\\end{bmatrix}}={\\begin{bmatrix}1&0&0\\\\1&0&0\\\\1&0&0\\\\1&1&0\\\\1&1&0\\\\1&0&1\\\\1&0&1\\end{bmatrix}}{\\begin{bmatrix}\\mu \\\\\\tau _{2}\\\\\\tau _{3}\\end{bmatrix}}+{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\\\\\varepsilon _{7}\\end{bmatrix}}}\n  In this model \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the mean of the reference group and \n  \n    \n      \n        \n          \u03c4\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\tau _{i}}\n   is the difference from group \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   to the reference group. \n  \n    \n      \n        \n          \u03c4\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\tau _{1}}\n   is not included in the matrix because its difference from the reference group (itself) is necessarily zero.\n\n\n== See also ==\nMoment matrix\nProjection matrix\nJacobian matrix and determinant\nScatter matrix\nGram matrix\nVandermonde matrix\n\n\n== References ==\n\n\n== Further reading ==\nVerbeek, Albert (1984). \"The Geometry of Model Selection in Regression\".  In Dijkstra, Theo K. (ed.). Misspecification Analysis. New York: Springer. pp. 20\u201336. ISBN 0-387-13893-5.", "Minimum description length": "Minimum Description Length (MDL) is a model selection principle where the shortest description of the data is the best model. MDL methods learn through a data compression perspective and are sometimes described as mathematical applications of Occam's razor. The MDL principle can be extended to other forms of inductive inference and learning, for example to estimation and sequential prediction, without explicitly identifying a single model of the data. \nMDL has its origins mostly in information theory and has been further developed within the general fields of statistics, theoretical computer science and machine learning, and more narrowly computational learning theory.\nHistorically, there are different, yet interrelated, usages of the definite noun phrase \"the minimum description length principle\" that vary in what is meant by description:\n\nWithin Jorma Rissanen's theory of learning, a central concept of information theory, models are statistical hypotheses and descriptions are defined as universal codes.\nRissanen's 1978 pragmatic first attempt to automatically derive short descriptions, relates to the Bayesian Information Criterion (BIC).\nWithin Algorithmic Information Theory, where the description length of a data sequence is the length of the smallest program that outputs that data set.  In this context, it is also known as 'idealized' MDL principle and it is closely related to Solomonoff's theory of inductive inference, which is that the best model of a data set is represented by its shortest self-extracting archive.\n\n\n== Overview ==\nSelecting the minimum length description of the available data as the best model observes the principle identified as Occam's razor.  Prior to the advent of computer programming, generating such descriptions was the intellectual labor of scientific theorists.  It was far less formal than it has become in the computer age.  If two scientists had a theoretic disagreement, they rarely could formally apply Occam's razor to choose between their theories.  They would have different data sets and possibly different descriptive languages.  Nevertheless, science advanced as Occam's razor was an informal guide in deciding which model was best.\nWith the advent of formal languages and computer programming Occam's razor was mathematically defined.  Models of a given set of observations, encoded as bits of data, could be created in the form of computer programs that output that data.  Occam's razor could then formally select the shortest program, measured in bits of this algorithmic information, as the best model.\nTo avoid confusion, note that there is nothing in the MDL principle that implies a machine produced the program embodying the model.  It can be entirely the product of humans.  The MDL principle applies regardless of whether the description to be run on a computer is the product of humans, machines or any combination thereof.  The MDL principle requires only that the shortest description, when executed, produce the original data set without error.\n\n\n=== Two-Part codes ===\nThe distinction in computer programs between programs and literal data applies to all formal descriptions and is sometimes referred to as \"two parts\" of a description.  In statistical MDL learning, such a description is frequently called a two-part code.\n\n\n== MDL in machine learning ==\nMDL applies in machine learning when algorithms (machines) generate descriptions.  Learning occurs when an algorithm generates a shorter description of the same data set.\nThe theoretic minimum description length of a data set, called its Kolmogorov complexity, cannot, however, be computed. That is to say, even if by random chance an algorithm generates the shortest program of all that outputs the data set, an automated theorem prover cannot prove there is no shorter such program.  Nevertheless, given two programs that output the dataset, the MDL principle selects the shorter of the two as embodying the best model.\n\n\n== Recent work on algorithmic MDL learning ==\nRecent machine MDL learning of algorithmic, as opposed to statistical, data models have received increasing attention with increasing availability of data, computation resources and theoretic advances. Approaches are informed by the burgeoning field of artificial general intelligence.  Shortly before his death, Marvin Minsky came out strongly in favor of this line of research, saying:\nIt seems to me that the most important discovery since G\u00f6del was the discovery by Chaitin, Solomonoff and Kolmogorov of the concept called Algorithmic Probability which is a fundamental new theory of how to make predictions given a collection of experiences and this is a beautiful theory, everybody should learn it, but it\u2019s got one problem, that is, that you cannot actually calculate what this theory predicts because it is too hard, it requires an infinite amount of work. However, it should be possible to make practical approximations to the Chaitin, Kolmogorov, Solomonoff theory that would make better predictions than anything we have today. Everybody should learn all about that and spend the rest of their lives working on it.\n\n\n== Statistical MDL learning ==\nAny set of data can be represented by a string of symbols from a finite (say, binary) alphabet.\n\n[The MDL Principle] is based on the following insight: any regularity in a given set of data can be used to compress the data, i.e. to describe it using fewer symbols than needed to describe the data literally. (Gr\u00fcnwald, 2004)\n\nBased on this, in 1978, Jorma Rissanen published an MDL learning algorithm using the statistical notion of information rather than algorithmic information.  Over the past 40 years this has developed into a rich theory of statistical and machine learning procedures with connections to Bayesian model selection and averaging, penalization methods such as Lasso and Ridge, and so on - Gr\u00fcnwald and Roos (2020) give an introduction including all modern developments. Rissanen started out with this idea: all statistical learning is about finding regularities in data, and the best hypothesis to describe the regularities in data is also the one that is able to statistically compress the data most. Like other statistical methods, it can be used for learning the parameters of a model using some data. Usually though, standard statistical methods assume  that the general form of a model is fixed. MDL's main strength is that it can also be used for selecting the general form of a model and its parameters. The quantity of interest (sometimes just a model, sometimes just parameters, sometimes both at the same time)  is called a hypothesis. The basic idea is then to consider the (lossless) two-stage code that encodes data \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   with length \n  \n    \n      \n        \n          L\n          (\n          D\n          )\n        \n      \n    \n    {\\displaystyle {L(D)}}\n   by first encoding a hypothesis \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   in the set of considered hypotheses \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}}\n    and then coding \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   \"with the help of\" \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  ; in the simplest context this just means \"encoding the deviations of the data from the predictions made by \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  :\n\nThe \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   achieving this minimum is then viewed as the best explanation of data \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  . As a simple example, take a regression problem: the data \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   could consist of a sequence of points \n  \n    \n      \n        D\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        (\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle D=(x_{1},y_{1}),\\ldots ,(x_{n},y_{n})}\n  , the set \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}}\n   could be the set of all polynomials from \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   to \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  . To describe a polynomial \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   of degree (say) \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , one would first have to discretize the parameters to some precision; one would then have to describe this precision (a natural number); next, one would have to describe the degree \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   (another natural number), and in the final step, one would have to describe \n  \n    \n      \n        k\n        +\n        1\n      \n    \n    {\\displaystyle k+1}\n   parameters; the total length would be \n  \n    \n      \n        L\n        (\n        H\n        )\n      \n    \n    {\\displaystyle L(H)}\n  . One would then describe the points in \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   using some fixed code for the x-values and then using a code for the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   deviations \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \u2212\n        H\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle y_{i}-H(x_{i})}\n  .\nIn practice, one often (but not always) uses a probabilistic model. For example, one associates each polynomial \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   with the corresponding conditional distribution expressing that given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   is normally distributed with mean \n  \n    \n      \n        H\n        (\n        X\n        )\n      \n    \n    {\\displaystyle H(X)}\n   and some variance \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n   which could either be fixed or added as a free parameter. Then the set of hypotheses \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}}\n   reduces to the assumption of a linear model, \n  \n    \n      \n        Y\n        =\n        H\n        (\n        X\n        )\n        +\n        \u03f5\n      \n    \n    {\\displaystyle Y=H(X)+\\epsilon }\n   , with \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   a polynomial.\nFurthermore, one is often not directly interested in specific parameters values, but just, for example, the degree of the polynomial. In that case, one sets \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}}\n   to be \n  \n    \n      \n        \n          \n            H\n          \n        \n        =\n        {\n        \n          \n            \n              H\n            \n          \n          \n            0\n          \n        \n        ,\n        \n          \n            \n              H\n            \n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        }\n      \n    \n    {\\displaystyle {\\cal {H}}=\\{{\\cal {H}}_{0},{\\cal {H}}_{1},\\ldots \\}}\n  where each \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}_{j}}\n   represents the hypothesis that the data is best described as a j-th degree polynomial. One then codes data \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   given hypothesis \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\cal {H}}_{j}}\n   using a one-part code designed such that, whenever some hypothesis \n  \n    \n      \n        H\n        \u2208\n        \n          \n            \n              H\n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle H\\in {\\cal {H}}_{j}}\n   fits the data well, the codelength \n  \n    \n      \n        L\n        (\n        D\n        \n          |\n        \n        H\n        )\n      \n    \n    {\\displaystyle L(D|H)}\n   is short. The design of such codes is called universal coding. There are various types of universal codes one could use, often giving similar lengths for long data sequences but differing for short ones. The 'best' (in the sense that it has a minimax optimality property) are the normalized maximum likelihood (NML)  or Shtarkov codes. A quite useful class of codes are the Bayesian marginal likelihood codes. For exponential families of distributions, when Jeffreys prior is used and the parameter space is suitably restricted, these asymptotically coincide with the NML codes; this brings MDL theory in close contact with objective Bayes model selection, in which one also sometimes adopts Jeffreys'  prior, albeit for different reasons. The MDL approach to model selection \"gives a selection criterion formally identical to the BIC approach\" for large number of samples.\n\n\n=== Example of Statistical MDL Learning ===\nA coin is flipped 1000 times, and the numbers of heads and tails are recorded. Consider two model classes: \n\nThe first is a code that represents outcomes with a 0 for heads or a 1 for tails. This code represents the hypothesis that the coin is fair. The code length according to this code is always exactly 1000 bits.\nThe second consists of all codes that are efficient for a coin with some specific bias, representing the hypothesis that the coin is not fair. Say that we observe 510 heads and 490 tails. Then the code length according to the best code in the second model class is shorter than 1000 bits.For this reason, a naive statistical method might choose the second model as a better explanation for the data. However, an MDL approach would construct a single code based on the hypothesis, instead of just using the best one. This code could be the normalized maximum likelihood code or a Bayesian code.  If such a code is used, then the total codelength based on the second model class would be larger than 1000 bits. Therefore, the conclusion when following an MDL approach is inevitably that there is not enough evidence to support the hypothesis of the biased coin, even though the best element of the second model class provides better fit to the data.\n\n\n=== Statistical MDL Notation ===\nCentral to MDL theory is the one-to-one correspondence between code length functions and probability distributions (this follows from the Kraft\u2013McMillan inequality). For any probability distribution \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  , it is possible to construct a code \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   such that the length (in bits) of \n  \n    \n      \n        C\n        (\n        x\n        )\n      \n    \n    {\\displaystyle C(x)}\n   is equal to \n  \n    \n      \n        \u2212\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle -\\log _{2}P(x)}\n  ; this code minimizes the expected code length. Conversely, given a code \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , one can construct a probability distribution \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   such that the same holds. (Rounding issues are ignored here.) In other words, searching for an efficient code is equivalent to searching for a good probability distribution.\n\n\n=== Limitations of Statistical MDL Learning ===\nThe description language of statistical MDL is not computationally universal.  Therefore it cannot, even in principle, learn models of recursive natural processes.\n\n\n=== Related concepts ===\nStatistical MDL learning is very strongly connected to probability theory and statistics through the correspondence between codes and probability distributions mentioned above. This has led some researchers to view MDL as equivalent to Bayesian inference: code length of model and data together in MDL correspond respectively to prior probability and marginal likelihood in the Bayesian framework.While Bayesian machinery is often useful in constructing efficient MDL codes, the MDL framework also accommodates other codes that are not Bayesian. An example is the Shtarkov normalized maximum likelihood code, which plays a central role in current MDL theory, but has no equivalent in Bayesian inference.  Furthermore, Rissanen stresses that we should make no assumptions about the true data-generating process: in practice, a model class is typically a simplification of reality and thus does not contain any code or probability distribution that is true in any objective sense. In the last mentioned reference Rissanen bases the mathematical underpinning of MDL on the Kolmogorov structure function.\nAccording to the MDL philosophy, Bayesian methods should be dismissed if they are based on unsafe priors that would lead to poor results. The priors that are acceptable from an MDL point of view also tend to be favored in so-called objective Bayesian analysis; there, however, the motivation is usually different.\n\n\n== Other systems ==\nRissanen's was not the first information-theoretic approach to learning; as early as 1968 Wallace and Boulton pioneered a related concept called minimum message length (MML). The difference between MDL and MML is a source of ongoing confusion. Superficially, the methods appear mostly equivalent, but there are some significant differences, especially in interpretation:\n\nMML is a fully subjective Bayesian approach: it starts from the idea that one represents one's beliefs about the data-generating process in the form of a prior distribution. MDL avoids assumptions about the data-generating process.\nBoth methods make use of two-part codes: the first part always represents the information that one is trying to learn, such as the index of a model class (model selection) or parameter values (parameter estimation); the second part is an encoding of the data given the information in the first part. The difference between the methods is that, in the MDL literature, it is advocated that unwanted parameters should be moved to the second part of the code, where they can be represented with the data by using a so-called one-part code, which is often more efficient than a two-part code. In the original description of MML, all parameters are encoded in the first part, so all parameters are learned.\nWithin the MML framework, each parameter is stated to exactly that precision, which results in the optimal overall message length: the preceding example might arise if some parameter was originally considered \"possibly useful\" to a model but was subsequently found to be unable to help to explain the data (such a parameter will be assigned a code length corresponding to the (Bayesian) prior probability that the parameter would be found to be unhelpful).  In the MDL framework, the focus is more on comparing model classes than models, and it is more natural to approach the same question by comparing the class of models that explicitly include such a parameter against some other class that doesn't.  The difference lies in the machinery applied to reach the same conclusion.\n\n\n== See also ==\nAlgorithmic probability\nAlgorithmic information theory\nInductive inference\nInductive probability\nLempel\u2013Ziv complexity\n\n\n== References ==\n\n\n== Further reading ==\nMinimum Description Length on the Web, by the University of Helsinki. Features readings, demonstrations, events and links to MDL researchers.\nHomepage of Jorma Rissanen, containing lecture notes and other recent material on MDL.\nAdvances in Minimum Description Length, MIT Press, ISBN 0-262-07262-9.", "Descriptive statistics": "A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics (in the mass noun sense) is the process of using and analysing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics) by its aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of probability theory, and are frequently nonparametric statistics. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example, in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, the proportion of subjects with related co-morbidities, etc.\nSome measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness.\n\n\n== Use in statistical analysis ==\nDescriptive statistics provide simple summaries about the sample and about the observations that have been made. Such summaries may be either quantitative, i.e. summary statistics, or visual, i.e. simple-to-understand graphs. These summaries may either form the basis of the initial description of the data as part of a more extensive statistical analysis, or they may be sufficient in and of themselves for a particular investigation.\nFor example, the shooting percentage in basketball is a descriptive statistic that summarizes the performance of a player or a team. This number is the number of shots made divided by the number of shots taken. For example, a player who shoots 33% is making approximately one shot in every three. The percentage summarizes or describes multiple discrete events. Consider also the grade point average. This single number describes the general performance of a student across the range of their course experiences.The use of descriptive and summary statistics has an extensive history and, indeed, the simple tabulation of populations and of economic data was the first way the topic of statistics appeared. More recently, a collection of summarisation techniques has been formulated under the heading of exploratory data analysis: an example of such a technique is the box plot.\nIn the business world, descriptive statistics provides a useful summary of many types of data. For example, investors and brokers may use a historical account of return behaviour by performing empirical and analytical analyses on their investments in order to make better investing decisions in the future.\n\n\n=== Univariate analysis ===\nUnivariate analysis involves describing the distribution of a single variable, including its central tendency (including the mean, median, and mode) and dispersion (including the range and quartiles of the data-set, and measures of spread such as the variance and standard deviation). The shape of the distribution may also be described via indices such as skewness and kurtosis. Characteristics of a variable's distribution may also be depicted in graphical or tabular format, including histograms and stem-and-leaf display.\n\n\n=== Bivariate and multivariate analysis ===\nWhen a sample consists of more than one variable, descriptive statistics may be used to describe the relationship between pairs of variables. In this case, descriptive statistics include:\n\nCross-tabulations and contingency tables\nGraphical representation via scatterplots\nQuantitative measures of dependence\nDescriptions of conditional distributionsThe main reason for differentiating univariate and bivariate analysis is that bivariate analysis is not only a simple descriptive analysis, but also it describes the relationship between two different variables. Quantitative measures of dependence include correlation (such as Pearson's r when both variables are continuous, or Spearman's rho if one or both are not) and covariance (which reflects the scale variables are measured on). The slope, in regression analysis, also reflects the relationship between variables. The unstandardised slope indicates the unit change in the criterion variable for a one unit change in the predictor. The standardised slope indicates this change in standardised (z-score) units. Highly skewed data are often transformed by taking logarithms. The use of logarithms makes graphs more symmetrical and look more similar to the normal distribution, making them easier to interpret intuitively.:\u200a47\u200a\n\n\n== References ==\n\n\n== External links ==\nDescriptive Statistics Lecture: University of Pittsburgh Supercourse: http://www.pitt.edu/~super1/lecture/lec0421/index.htm", "Statistical dispersion": "In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range. For instance, when the variance of data in a set is large, the data is widely scattered. On the other hand, when the variance is small, the data in the set is clustered.\nDispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.\n\n\n== Measures of statistical dispersion ==\nA measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse.\nMost measures of dispersion have the same units as the quantity being measured. In other words, if the measurements are in metres or seconds, so is the measure of dispersion. Examples of dispersion measures include:\n\nStandard deviation\nInterquartile range (IQR)\nRange\nMean absolute difference (also known as Gini mean absolute difference)\nMedian absolute deviation (MAD)\nAverage absolute deviation (or simply called average deviation)\nDistance standard deviationThese are frequently used (together with scale factors) as estimators of scale parameters, in which capacity they are called estimates of scale. Robust measures of scale are those unaffected by a small number of outliers, and include the IQR and MAD.\nAll the above measures of statistical dispersion have the useful property that they are location-invariant and linear in scale. This means that if a random variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   has a dispersion of \n  \n    \n      \n        \n          S\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle S_{X}}\n   then a linear transformation \n  \n    \n      \n        Y\n        =\n        a\n        X\n        +\n        b\n      \n    \n    {\\displaystyle Y=aX+b}\n   for real \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   should have dispersion \n  \n    \n      \n        \n          S\n          \n            Y\n          \n        \n        =\n        \n          |\n        \n        a\n        \n          |\n        \n        \n          S\n          \n            X\n          \n        \n      \n    \n    {\\displaystyle S_{Y}=|a|S_{X}}\n  , where \n  \n    \n      \n        \n          |\n        \n        a\n        \n          |\n        \n      \n    \n    {\\displaystyle |a|}\n   is the absolute value of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , that is, ignores a preceding negative sign \n  \n    \n      \n        \u2212\n      \n    \n    {\\displaystyle -}\n  .\nOther measures of dispersion are dimensionless. In other words, they have no units even if the variable itself has units. These include:\n\nCoefficient of variation\nQuartile coefficient of dispersion\nRelative mean difference, equal to twice the Gini coefficient\nEntropy: While the entropy of a discrete variable is  location-invariant and scale-independent, and therefore not a measure of dispersion in the above sense, the entropy of a continuous variable is location invariant and additive in scale: If \n  \n    \n      \n        H\n        (\n        z\n        )\n      \n    \n    {\\displaystyle H(z)}\n   is the entropy of a continuous variable \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n   and \n  \n    \n      \n        z\n        =\n        a\n        x\n        +\n        b\n      \n    \n    {\\displaystyle z=ax+b}\n  , then \n  \n    \n      \n        H\n        (\n        z\n        )\n        =\n        H\n        (\n        x\n        )\n        +\n        log\n        \u2061\n        (\n        a\n        )\n      \n    \n    {\\displaystyle H(z)=H(x)+\\log(a)}\n  .There are other measures of dispersion:\n\nVariance (the square of the standard deviation) \u2013 location-invariant but not linear in scale.\nVariance-to-mean ratio \u2013 mostly used for count data when the term coefficient of dispersion is used and when this ratio is dimensionless, as count data are themselves dimensionless, not otherwise.Some measures of dispersion have specialized purposes. The Allan variance can be used for applications where the noise disrupts convergence. The Hadamard variance can be used to counteract linear frequency drift sensitivity.For categorical variables, it is less common to measure dispersion by a single number; see qualitative variation. One measure that does so is the discrete entropy.\n\n\n== Sources ==\nIn the physical sciences, such variability may result from random measurement errors: instrument measurements are often not perfectly precise, i.e., reproducible, and there is additional inter-rater variability in interpreting and reporting the measured results. One may assume that the quantity being measured is stable, and that the variation between measurements is due to observational error.  A system of a large number of particles is characterized by the mean values of a relatively few number of macroscopic quantities such as temperature, energy, and density. The standard deviation is an important measure in fluctuation theory, which explains many physical phenomena, including why the sky is blue.In the biological sciences, the quantity being measured is seldom unchanging and stable, and the variation observed might additionally be intrinsic to the phenomenon: It may be due to inter-individual variability, that is, distinct members of a population differing from each other. Also, it may be due to intra-individual variability, that is, one and the same subject differing in tests taken at different times or in other differing conditions. Such types of variability are also seen in the arena of manufactured products; even there, the meticulous scientist finds variation.\n\n\n== A partial ordering of dispersion ==\nA mean-preserving spread (MPS) is a change from one probability distribution A to another probability distribution B, where B is formed by spreading out one or more portions of A's probability density function while leaving the mean (the expected value) unchanged. The concept of a mean-preserving spread provides a partial ordering of probability distributions according to their dispersions: of two probability distributions, one may be ranked as having more dispersion than the other, or alternatively neither may be ranked as having more dispersion.\n\n\n== See also ==\n\nAverage\nCircular dispersion\nDispersion matrix\nQualitative variation\nMeasurement uncertainty\nRobust measures of scale\nSummary statistics\n\n\n== References ==", "Bootstrapping (statistics)": "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.Bootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of  resamples with replacement, of the observed data set (and of equal size to the observed data set).\nIt may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\n\n== History ==\nThe bootstrap was published by Bradley Efron in \"Bootstrap methods: another look at the jackknife\" (1979), inspired by earlier work on the jackknife. Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992.\n\n\n== Approach ==\nThe basic idea of bootstrapping is that inference about a population from sample data (sample \u2192 population) can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled \u2192 sample). As the population is unknown, the true error in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference of the 'true' sample from resampled data (resampled \u2192 sample) is measurable.\nMore formally, the bootstrap works by treating inference of the true probability distribution J, given the original data, as being analogous to an inference of the empirical distribution \u0134, given the resampled data. The accuracy of inferences regarding \u0134 using the resampled data can be assessed because we know \u0134. If \u0134 is a reasonable approximation to J, then the quality of inference on J can in turn be inferred.\nAs an example, assume we are interested in the average (or mean) height of people worldwide.  We cannot measure all the people in the global population, so instead, we sample only a tiny part of it, and measure that.  Assume the sample is of size N; that is, we measure the heights of N individuals.  From that single sample, only one estimate of the mean can be obtained.  In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N.  The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample.  This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples, we compute its mean (each of these is called a \"bootstrap estimate\").  We now can create a histogram of bootstrap means.  This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples.  (The method here, described for the mean, can be applied to almost any other statistic or estimator.)\n\n\n== Discussion ==\n\n\n=== Advantages ===\nA great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. However, despite its simplicity, bootstrapping can be applied to complex sampling designs (e.g. for population divided into s strata with ns observations per strata, bootstrapping can be applied for each stratum). Bootstrap is also an appropriate way to control and check the stability of the results.  Although for most problems it is impossible to know the true confidence interval, bootstrap is asymptotically more accurate than the standard intervals obtained using sample variance and assumptions of normality. Bootstrapping is also a convenient method that avoids the cost of repeating the experiment to get other groups of sample data.\n\n\n=== Disadvantages ===\nBootstrapping depends heavily on the estimator used and, though simple, naive use of bootstrapping will not always yield asymptotically valid results and can lead to inconsistency. Although bootstrapping is (under some conditions) asymptotically consistent, it does not provide general finite-sample guarantees. The result may depend on the representative sample. The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples or large enough of a sample size) where these would be more formally stated in other approaches. Also, bootstrapping can be time-consuming and there are not many available software for bootstrapping as it is difficult to automate using traditional statistical computer packages.\n\n\n=== Recommendations ===\nScholars have recommended more bootstrap samples as available computing power has increased. If the results may have substantial real-world consequences, then one should use as many samples as is reasonable, given available computing power and time. Increasing the number of samples cannot increase the amount of information in the original data; it can only reduce the effects of random sampling errors which can arise from a bootstrap procedure itself. Moreover, there is evidence that numbers of samples greater than 100 lead to negligible improvements in the estimation of standard errors. In fact, according to the original developer of the bootstrapping method, even setting the number of samples at 50 is likely to lead to fairly good standard error estimates.Ad\u00e8r et al. recommend the bootstrap procedure for the following situations:\nWhen the theoretical distribution of a statistic of interest is complicated or unknown. Since the bootstrapping procedure is distribution-independent it provides an indirect method to assess the properties of the distribution underlying the sample and the parameters of interest that are derived from this distribution.When the sample size is insufficient for straightforward statistical inference. If the underlying distribution is well-known, bootstrapping provides a way to account for the distortions caused by the specific sample that may not be fully representative of the population.When power calculations have to be performed, and a small pilot sample is available. Most power and sample size calculations are heavily dependent on the standard deviation of the statistic of interest. If the estimate used is incorrect, the required sample size will also be wrong. One method to get an impression of the variation of the statistic is to use a small pilot sample and perform bootstrapping on it to get impression of the variance.However, Athreya has shown that if one performs a naive bootstrap on the sample mean when the underlying population lacks a finite variance (for example, a power law distribution), then the bootstrap distribution will not converge to the same limit as the sample mean. As a result, confidence intervals on the basis of a Monte Carlo simulation of the bootstrap could be misleading. Athreya states that \"Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap\".\n\n\n== Types of bootstrap scheme ==\nIn univariate problems, it is usually acceptable to resample the individual observations with replacement (\"case resampling\" below) unlike subsampling, in which resampling is without replacement and is valid under much weaker conditions compared to the bootstrap. In small samples, a parametric bootstrap approach might be preferred. For other problems, a smooth bootstrap will likely be preferred.\nFor regression problems, various other alternatives are available.\n\n\n=== Case resampling ===\nThe bootstrap is generally useful for estimating the distribution of a statistic (e.g. mean, variance) without using normality assumptions (as required, e.g., for a z-statistic or a t-statistic). In particular, the bootstrap is useful when there is no analytical form or an asymptotic theory (e.g., an applicable central limit theorem) to help estimate the distribution of the statistics of interest. This is because bootstrap methods can apply to most random quantities, e.g., the ratio of variance and mean. There are at least two ways of performing case resampling.\n\nThe Monte Carlo algorithm for case resampling is quite simple. First, we resample the data with replacement, and the size of the resample must be equal to the size of the original data set. Then the statistic of interest is computed from the resample from the first step. We repeat this routine many times to get a more precise estimate of the Bootstrap distribution of the statistic.\nThe 'exact' version for case resampling is similar, but we exhaustively enumerate every possible resample of the data set. This can be computationally expensive as there are a total of \n  \n    \n      \n        \n          \n            \n              (\n            \n            \n              \n                2\n                n\n                \u2212\n                1\n              \n              n\n            \n            \n              )\n            \n          \n        \n        =\n        \n          \n            \n              (\n              2\n              n\n              \u2212\n              1\n              )\n              !\n            \n            \n              n\n              !\n              (\n              n\n              \u2212\n              1\n              )\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle {\\binom {2n-1}{n}}={\\frac {(2n-1)!}{n!(n-1)!}}}\n   different resamples, where n is the size of the data set. Thus for n = 5, 10, 20, 30 there are 126, 92378, 6.89 \u00d7 1010 and 5.91 \u00d7 1016  different resamples respectively.\n\n\n==== Estimating the distribution of sample mean ====\nConsider a coin-flipping experiment. We flip the coin and record whether it lands heads or tails. Let X = x1, x2, \u2026, x10 be 10 observations from the experiment. xi = 1 if the i th flip lands heads, and 0 otherwise. By invoking the assumption that the average of the coin flips is normally distributed, we can use the t-statistic to estimate the distribution of the sample mean,\n\n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        =\n        \n          \n            1\n            10\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          x\n          \n            10\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\bar {x}}={\\frac {1}{10}}(x_{1}+x_{2}+\\cdots +x_{10}).}\n  Such a normality assumption can be justified either as an approximation of the distribution of each individual coin flip or as an approximation of the distribution of the average of a large number of coin flips. The former is a poor approximation because the true distribution of the coin flips is Bernoulli instead of normal. The latter is a valid approximation in infinitely large samples due to the central limit theorem.\nHowever, if we are not ready to make such a justification, then we can use the bootstrap instead. Using case resampling, we can derive the distribution of \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n  . We first resample the data to obtain a bootstrap resample. An example of the first resample might look like this X1* = x2, x1, x10, x10, x3, x4, x6, x7, x1, x9. There are some duplicates since a bootstrap resample comes from sampling with replacement from the data. Also the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first bootstrap mean: \u03bc1*. We repeat this process to obtain the second resample X2* and compute the second bootstrap mean \u03bc2*. If we repeat this 100 times, then we have \u03bc1*, \u03bc2*, ..., \u03bc100*.  This represents an empirical bootstrap distribution of sample mean. From this empirical distribution, one can derive a bootstrap confidence interval for the purpose of hypothesis testing.\n\n\n==== Regression ====\nIn regression problems, case resampling refers to the simple scheme of resampling individual cases \u2013 often rows of a data set. For regression problems, as long as the data set is fairly large, this simple scheme is often acceptable. However, the method is open to criticism.In regression problems, the explanatory variables are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.\n\n\n=== Bayesian bootstrap ===\nBootstrapping can be interpreted in a Bayesian framework using a scheme that creates new data sets through reweighting the initial data. Given a set of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   data points, the weighting assigned to data point \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   in a new data set \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            J\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}^{J}}\n   is \n  \n    \n      \n        \n          w\n          \n            i\n          \n          \n            J\n          \n        \n        =\n        \n          x\n          \n            i\n          \n          \n            J\n          \n        \n        \u2212\n        \n          x\n          \n            i\n            \u2212\n            1\n          \n          \n            J\n          \n        \n      \n    \n    {\\displaystyle w_{i}^{J}=x_{i}^{J}-x_{i-1}^{J}}\n  , where \n  \n    \n      \n        \n          \n            x\n          \n          \n            J\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} ^{J}}\n   is a low-to-high ordered list of \n  \n    \n      \n        N\n        \u2212\n        1\n      \n    \n    {\\displaystyle N-1}\n   uniformly distributed random numbers on \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  , preceded by 0 and succeeded by 1. The distributions of a parameter inferred from considering many such data sets \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            J\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}^{J}}\n   are then interpretable as posterior distributions on that parameter.\n\n\n=== Smooth bootstrap ===\nUnder this scheme, a small amount of (usually normally distributed) zero-centered random noise is added onto each resampled observation. This is equivalent to sampling from a kernel density estimate of the data. Assume K to be a symmetric kernel density function with unit variance. The standard kernel estimator \n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {f\\,}}_{h}(x)}\n   of \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              n\n              h\n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        K\n        \n          (\n          \n            \n              \n                x\n                \u2212\n                \n                  X\n                  \n                    i\n                  \n                \n              \n              h\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {f\\,}}_{h}(x)={1 \\over nh}\\sum _{i=1}^{n}K\\left({x-X_{i} \\over h}\\right),}\n   where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is the smoothing parameter. And the corresponding distribution function estimator \n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {F\\,}}_{h}(x)}\n   is\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  F\n                  \n                \n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        x\n        )\n        =\n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            x\n          \n        \n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            h\n          \n        \n        (\n        t\n        )\n        \n        d\n        t\n        .\n      \n    \n    {\\displaystyle {\\hat {F\\,}}_{h}(x)=\\int _{-\\infty }^{x}{\\hat {f}}_{h}(t)\\,dt.}\n   \n\n\n=== Parametric bootstrap ===\nBased on the assumption that the original data set is a realization of a random sample from a distribution of a specific parametric type, in this case a parametric model is fitted by parameter \u03b8,  often by maximum likelihood, and samples of random numbers are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the estimate of original function F can be written as \n  \n    \n      \n        \n          \n            \n              F\n              ^\n            \n          \n        \n        =\n        \n          F\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {F}}=F_{\\hat {\\theta }}}\n  . This sampling process is repeated many times as for other bootstrap methods. Considering the centered sample mean in this case, the random sample original distribution function \n  \n    \n      \n        \n          F\n          \n            \u03b8\n          \n        \n      \n    \n    {\\displaystyle F_{\\theta }}\n   is replaced by a bootstrap random sample with function \n  \n    \n      \n        \n          F\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\hat {\\theta }}}\n  , and the probability distribution of \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  n\n                \n              \n              \u00af\n            \n          \n        \n        \u2212\n        \n          \u03bc\n          \n            \u03b8\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{n}}}-\\mu _{\\theta }}\n   is approximated by that of \n  \n    \n      \n        \n          \n            \n              \n                X\n                \u00af\n              \n            \n          \n          \n            n\n          \n          \n            \u2217\n          \n        \n        \u2212\n        \n          \u03bc\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}_{n}^{*}-\\mu ^{*}}\n  , where \n  \n    \n      \n        \n          \u03bc\n          \n            \u2217\n          \n        \n        =\n        \n          \u03bc\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mu ^{*}=\\mu _{\\hat {\\theta }}}\n  , which is the expectation corresponding to \n  \n    \n      \n        \n          F\n          \n            \n              \n                \u03b8\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle F_{\\hat {\\theta }}}\n  . The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.\n\n\n=== Resampling residuals ===\nAnother approach to bootstrapping in regression problems is to resample residuals. The method proceeds as follows.\n\nFit the model and retain the fitted values \n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {y\\,}}_{i}}\n   and the residuals \n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b5\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        ,\n        (\n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle {\\widehat {\\varepsilon \\,}}_{i}=y_{i}-{\\widehat {y\\,}}_{i},(i=1,\\dots ,n)}\n  .\nFor each pair, (xi, yi), in which xi is the (possibly multivariate) explanatory variable, add a randomly resampled residual, \n  \n    \n      \n        \n          \n            \n              \n                \n                  \u03b5\n                  \n                \n                ^\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\varepsilon \\,}}_{j}}\n  , to the fitted value \n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {y\\,}}_{i}}\n  . In other words, create synthetic response variables \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2217\n          \n        \n        =\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        +\n        \n          \n            \n              \n                \n                  \u03b5\n                  \n                \n                ^\n              \n            \n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}={\\widehat {y\\,}}_{i}+{\\widehat {\\varepsilon \\,}}_{j}}\n   where j is selected randomly from the list (1, ..., n) for every i.\nRefit the model using the fictitious response variables \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}}\n  , and retain the quantities of interest (often the parameters, \n  \n    \n      \n        \n          \n            \n              \n                \u03bc\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\mu }}_{i}^{*}}\n  , estimated from the synthetic \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}}\n  ).\nRepeat steps 2 and 3 a large number of times.This scheme has the advantage that it retains the information in the explanatory variables. However, a question arises as to which residuals to resample. Raw residuals are one option; another is studentized residuals (in linear regression). Although there are arguments in favor of using studentized residuals; in practice, it often makes little difference, and it is easy to compare the results of both schemes.\n\n\n=== Gaussian process regression bootstrap ===\nWhen data are temporally correlated, straightforward bootstrapping destroys the inherent correlations. This method uses Gaussian process regression (GPR) to fit a probabilistic model from which replicates may then be drawn. GPR is a Bayesian non-linear regression method. A Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian (normal) distribution. A GP is defined by a mean function and a covariance function, which specify the mean vectors and covariance matrices for each finite collection of the random variables.Regression model:\n\n  \n    \n      \n        y\n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n        +\n        \u03b5\n        ,\n         \n         \n        \u03b5\n        \u223c\n        \n          \n            N\n          \n        \n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle y(x)=f(x)+\\varepsilon ,\\ \\ \\varepsilon \\sim {\\mathcal {N}}(0,\\sigma ^{2}),}\n   \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   is a noise term.Gaussian process prior:\nFor any finite collection of variables, x1, ..., xn, the function outputs \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        f\n        (\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle f(x_{1}),\\ldots ,f(x_{n})}\n   are jointly distributed according to a multivariate Gaussian with mean \n  \n    \n      \n        m\n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        m\n        (\n        \n          x\n          \n            n\n          \n        \n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle m=[m(x_{1}),\\ldots ,m(x_{n})]^{\\intercal }}\n   and covariance matrix \n  \n    \n      \n        (\n        K\n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K)_{ij}=k(x_{i},x_{j}).}\n  \nAssume \n  \n    \n      \n        f\n        (\n        x\n        )\n        \u223c\n        \n          \n            G\n            P\n          \n        \n        (\n        m\n        ,\n        k\n        )\n        .\n      \n    \n    {\\displaystyle f(x)\\sim {\\mathcal {GP}}(m,k).}\n   Then \n  \n    \n      \n        y\n        (\n        x\n        )\n        \u223c\n        \n          \n            G\n            P\n          \n        \n        (\n        m\n        ,\n        l\n        )\n      \n    \n    {\\displaystyle y(x)\\sim {\\mathcal {GP}}(m,l)}\n  ,\nwhere \n  \n    \n      \n        l\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        +\n        \n          \u03c3\n          \n            2\n          \n        \n        \u03b4\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle l(x_{i},x_{j})=k(x_{i},x_{j})+\\sigma ^{2}\\delta (x_{i},x_{j})}\n  , and \n  \n    \n      \n        \u03b4\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta (x_{i},x_{j})}\n   is the standard Kronecker delta function.Gaussian process posterior:\nAccording to GP prior, we can get\n\n  \n    \n      \n        [\n        y\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        y\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        ]\n        \u223c\n        \n          \n            N\n          \n        \n        (\n        \n          m\n          \n            0\n          \n        \n        ,\n        \n          K\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle [y(x_{1}),\\ldots ,y(x_{r})]\\sim {\\mathcal {N}}(m_{0},K_{0})}\n  ,where \n  \n    \n      \n        \n          m\n          \n            0\n          \n        \n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        m\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle m_{0}=[m(x_{1}),\\ldots ,m(x_{r})]^{\\intercal }}\n   and \n  \n    \n      \n        (\n        \n          K\n          \n            0\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        +\n        \n          \u03c3\n          \n            2\n          \n        \n        \u03b4\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K_{0})_{ij}=k(x_{i},x_{j})+\\sigma ^{2}\\delta (x_{i},x_{j}).}\n  \nLet x1*,...,xs* be another finite collection of variables, it's obvious that\n\n  \n    \n      \n        [\n        y\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        y\n        (\n        \n          x\n          \n            r\n          \n        \n        )\n        ,\n        f\n        (\n        \n          x\n          \n            1\n          \n          \n            \u2217\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        f\n        (\n        \n          x\n          \n            s\n          \n          \n            \u2217\n          \n        \n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n        \u223c\n        \n          \n            N\n          \n        \n        (\n        \n          \n            \n              (\n            \n            \n              \n                m\n                \n                  0\n                \n              \n              \n                m\n                \n                  \u2217\n                \n              \n            \n            \n              )\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    K\n                    \n                      0\n                    \n                  \n                \n                \n                  \n                    K\n                    \n                      \u2217\n                    \n                  \n                \n              \n              \n                \n                  \n                    K\n                    \n                      \u2217\n                    \n                    \n                      \u22ba\n                    \n                  \n                \n                \n                  \n                    K\n                    \n                      \u2217\n                      \u2217\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle [y(x_{1}),\\ldots ,y(x_{r}),f(x_{1}^{*}),\\ldots ,f(x_{s}^{*})]^{\\intercal }\\sim {\\mathcal {N}}({\\binom {m_{0}}{m_{*}}}{\\begin{pmatrix}K_{0}&K_{*}\\\\K_{*}^{\\intercal }&K_{**}\\end{pmatrix}})}\n  ,where \n  \n    \n      \n        \n          m\n          \n            \u2217\n          \n        \n        =\n        [\n        m\n        (\n        \n          x\n          \n            1\n          \n          \n            \u2217\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        m\n        (\n        \n          x\n          \n            s\n          \n          \n            \u2217\n          \n        \n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle m_{*}=[m(x_{1}^{*}),\\ldots ,m(x_{s}^{*})]^{\\intercal }}\n  , \n  \n    \n      \n        (\n        \n          K\n          \n            \u2217\n            \u2217\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n          \n            \u2217\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n          \n            \u2217\n          \n        \n        )\n      \n    \n    {\\displaystyle (K_{**})_{ij}=k(x_{i}^{*},x_{j}^{*})}\n  , \n  \n    \n      \n        (\n        \n          K\n          \n            \u2217\n          \n        \n        \n          )\n          \n            i\n            j\n          \n        \n        =\n        k\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            j\n          \n          \n            \u2217\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (K_{*})_{ij}=k(x_{i},x_{j}^{*}).}\n  \nAccording to the equations above, the outputs y are also jointly distributed according to a multivariate Gaussian. Thus,\n\n  \n    \n      \n        [\n        f\n        (\n        \n          x\n          \n            1\n          \n          \n            \u2217\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        f\n        (\n        \n          x\n          \n            s\n          \n          \n            \u2217\n          \n        \n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n        \u2223\n        (\n        [\n        y\n        (\n        x\n        )\n        \n          ]\n          \n            \u22ba\n          \n        \n        =\n        y\n        )\n        \u223c\n        \n          \n            N\n          \n        \n        (\n        \n          m\n          \n            post\n          \n        \n        ,\n        \n          K\n          \n            post\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle [f(x_{1}^{*}),\\ldots ,f(x_{s}^{*})]^{\\intercal }\\mid ([y(x)]^{\\intercal }=y)\\sim {\\mathcal {N}}(m_{\\text{post}},K_{\\text{post}}),}\n  where \n  \n    \n      \n        y\n        =\n        [\n        \n          y\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          y\n          \n            r\n          \n        \n        \n          ]\n          \n            \u22ba\n          \n        \n      \n    \n    {\\displaystyle y=[y_{1},...,y_{r}]^{\\intercal }}\n  , \n  \n    \n      \n        \n          m\n          \n            post\n          \n        \n        =\n        \n          m\n          \n            \u2217\n          \n        \n        +\n        \n          K\n          \n            \u2217\n          \n          \n            \u22ba\n          \n        \n        (\n        \n          K\n          \n            O\n          \n        \n        +\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          I\n          \n            r\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        (\n        y\n        \u2212\n        \n          m\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle m_{\\text{post}}=m_{*}+K_{*}^{\\intercal }(K_{O}+\\sigma ^{2}I_{r})^{-1}(y-m_{0})}\n  , \n  \n    \n      \n        \n          K\n          \n            post\n          \n        \n        =\n        \n          K\n          \n            \u2217\n            \u2217\n          \n        \n        \u2212\n        \n          K\n          \n            \u2217\n          \n          \n            \u22ba\n          \n        \n        (\n        \n          K\n          \n            O\n          \n        \n        +\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          I\n          \n            r\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        \n          K\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle K_{\\text{post}}=K_{**}-K_{*}^{\\intercal }(K_{O}+\\sigma ^{2}I_{r})^{-1}K_{*}}\n  , and \n  \n    \n      \n        \n          I\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle I_{r}}\n   is \n  \n    \n      \n        r\n        \u00d7\n        r\n      \n    \n    {\\displaystyle r\\times r}\n   identity matrix.\n\n\n=== Wild bootstrap ===\nThe wild bootstrap, proposed originally by Wu (1986), is suited when the model exhibits heteroskedasticity. The idea is, as the residual bootstrap, to leave the regressors at their sample value, but to resample the response variable based on the residuals values. That is, for each replicate, one computes a new \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   based on\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2217\n          \n        \n        =\n        \n          \n            \n              \n                \n                  y\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        +\n        \n          \n            \n              \n                \n                  \u03b5\n                  \n                \n                ^\n              \n            \n          \n          \n            i\n          \n        \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}={\\widehat {y\\,}}_{i}+{\\widehat {\\varepsilon \\,}}_{i}v_{i}}\n  so the residuals are randomly multiplied by a random variable \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n   with mean 0 and variance 1. For most distributions of \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n   (but not Mammen's), this method assumes that the 'true' residual distribution is symmetric and can offer advantages over simple residual sampling for smaller sample sizes. Different forms are used for the random variable \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  , such as\n\nThe standard normal distributionA distribution suggested by Mammen (1993).\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \u2212\n                  (\n                  \n                    \n                      5\n                    \n                  \n                  \u2212\n                  1\n                  )\n                  \n                    /\n                  \n                  2\n                \n                \n                  \n                    with probability \n                  \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  +\n                  1\n                  )\n                  \n                    /\n                  \n                  (\n                  2\n                  \n                    \n                      5\n                    \n                  \n                  )\n                  ,\n                \n              \n              \n                \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  +\n                  1\n                  )\n                  \n                    /\n                  \n                  2\n                \n                \n                  \n                    with probability \n                  \n                  (\n                  \n                    \n                      5\n                    \n                  \n                  \u2212\n                  1\n                  )\n                  \n                    /\n                  \n                  (\n                  2\n                  \n                    \n                      5\n                    \n                  \n                  )\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-({\\sqrt {5}}-1)/2&{\\text{with probability }}({\\sqrt {5}}+1)/(2{\\sqrt {5}}),\\\\({\\sqrt {5}}+1)/2&{\\text{with probability }}({\\sqrt {5}}-1)/(2{\\sqrt {5}})\\end{cases}}}\n  Approximately, Mammen's distribution is:\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \u2212\n                  0.6180\n                  \n                  \n                    (with a 0 in the units' place)\n                  \n                \n                \n                  \n                    with probability \n                  \n                  0.7236\n                  ,\n                \n              \n              \n                \n                  +\n                  1.6180\n                  \n                  \n                    (with a 1 in the units' place)\n                  \n                \n                \n                  \n                    with probability \n                  \n                  0.2764.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-0.6180\\quad {\\text{(with a 0 in the units' place)}}&{\\text{with probability }}0.7236,\\\\+1.6180\\quad {\\text{(with a 1 in the units' place)}}&{\\text{with probability }}0.2764.\\end{cases}}}\n  Or the simpler distribution, linked to the Rademacher distribution:\n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  \n                    with probability \n                  \n                  1\n                  \n                    /\n                  \n                  2\n                  ,\n                \n              \n              \n                \n                  +\n                  1\n                \n                \n                  \n                    with probability \n                  \n                  1\n                  \n                    /\n                  \n                  2.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle v_{i}={\\begin{cases}-1&{\\text{with probability }}1/2,\\\\+1&{\\text{with probability }}1/2.\\end{cases}}}\n  \n\n\n=== Block bootstrap ===\nThe block bootstrap is used when the data, or the errors in a model, are correlated. In this case, a simple case or residual resampling will fail, as it is not able to replicate the correlation in the data. The block bootstrap tries to replicate the correlation by resampling inside blocks of data (see Blocking (statistics)). The block bootstrap has been used mainly with data correlated in time (i.e. time series) but can also be used with data correlated in space, or among groups (so-called cluster data).\n\n\n==== Time series: Simple block bootstrap ====\nIn the (simple) block bootstrap, the variable of interest is split into non-overlapping blocks.\n\n\n==== Time series: Moving block bootstrap ====\nIn the moving block bootstrap, introduced by K\u00fcnsch (1989), data is split into n \u2212 b + 1 overlapping blocks of length b: Observation 1 to b will be block 1, observation 2 to b + 1 will be block 2, etc. Then from these n \u2212 b + 1 blocks, n/b blocks will be drawn at random with replacement. Then aligning these n/b blocks in the order they were picked, will give the bootstrap observations.\nThis bootstrap works with dependent data, however, the bootstrapped observations will not be stationary anymore by construction. But, it was shown that varying randomly the block length can avoid this problem. This method is known as the stationary bootstrap. Other related modifications of the moving block bootstrap are the Markovian bootstrap and a stationary bootstrap method that matches subsequent blocks based on standard deviation matching.\n\n\n==== Time series: Maximum entropy bootstrap ====\nVinod (2006), presents a method that bootstraps time series data using  maximum entropy principles satisfying the Ergodic theorem with mean-preserving and mass-preserving constraints. There is an R package, meboot, that utilizes the method, which  has applications in econometrics and computer science.\n\n\n==== Cluster data: block bootstrap ====\nCluster data describes data where many observations per unit are observed. This could be observing many firms in many states or observing students in many classes. In such cases, the correlation structure is simplified, and one does usually make the assumption that data is correlated within a group/cluster, but independent between groups/clusters. The structure of the block bootstrap is easily obtained (where the block just corresponds to the group), and usually only the groups are resampled, while the observations within the groups are left unchanged. Cameron et al. (2008) discusses this for clustered errors in linear regression.\n\n\n== Methods for improving computational efficiency ==\nThe bootstrap is a powerful technique although may require substantial computing resources in both time and memory. Some techniques have been developed to reduce this burden. They can generally be combined with many of the different types of Bootstrap schemes and various choices of statistics.\n\n\n=== Poisson bootstrap ===\n\nThe ordinary bootstrap requires the random selection of n elements from a list, which is equivalent to drawing from a multinomial distribution. This may require a large number of passes over the data and is challenging to run these computations in parallel. For large values of n, the Poisson bootstrap is an efficient method of generating bootstrapped data sets. When generating a single bootstrap sample, instead of randomly drawing from the sample data with replacement, each data point is assigned a random weight distributed according to the Poisson distribution with \n  \n    \n      \n        \u03bb\n        =\n        1\n      \n    \n    {\\displaystyle \\lambda =1}\n  . For large sample data, this will approximate random sampling with replacement. This is due to the following approximation:\n\n  \n    \n      \n        \n          lim\n          \n            n\n            \u2192\n            \u221e\n          \n        \n        Binomial\n        \u2061\n        (\n        n\n        ,\n        1\n        \n          /\n        \n        n\n        )\n        =\n        Poisson\n        \u2061\n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\lim _{n\\to \\infty }\\operatorname {Binomial} (n,1/n)=\\operatorname {Poisson} (1)}\n  This method also lends itself well to streaming data and growing data sets, since the total number of samples does not need to be known in advance of beginning to take bootstrap samples.\nFor large enough n, the results are relatively similar to the original bootstrap estimations.A way to improve on the poisson bootstrap, termed \"sequential bootstrap\", is by taking the first samples so that the proportion of unique values is \u22480.632 of the original sample size n. This provides a distribution with main empirical characteristics being within a distance of \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3\n            \n              /\n            \n            4\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{3/4})}\n  . Empirical investigation has shown this method can yield good results. This is related to the reduced bootstrap method.\n\n\n=== Bag of Little Bootstraps ===\nFor massive data sets, it is often computationally prohibitive to hold all the sample data in memory and resample from the sample data. The Bag of Little Bootstraps (BLB) provides a method of pre-aggregating data before bootstrapping to reduce computational constraints. This works by partitioning the data set into \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   equal-sized buckets and aggregating the data within each bucket. This pre-aggregated data set becomes the new sample data over which to draw samples with replacement. This method is similar to the Block Bootstrap, but the motivations and definitions of the blocks are very different. Under certain assumptions, the sample distribution should approximate the full bootstrapped scenario. One constraint is the number of buckets \n  \n    \n      \n        b\n        =\n        \n          n\n          \n            \u03b3\n          \n        \n      \n    \n    {\\displaystyle b=n^{\\gamma }}\n  where \n  \n    \n      \n        \u03b3\n        \u2208\n        [\n        0.5\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\gamma \\in [0.5,1]}\n   and the authors recommend usage of \n  \n    \n      \n        b\n        =\n        \n          n\n          \n            0.7\n          \n        \n      \n    \n    {\\displaystyle b=n^{0.7}}\n   as a general solution.\n\n\n== Choice of statistic ==\nThe bootstrap distribution of a point estimator of a population parameter has been used to produce a bootstrapped confidence interval for the parameter's true value if the parameter can be written as a function of the population's distribution.\nPopulation parameters are estimated with many point estimators. Popular families of point-estimators include mean-unbiased minimum-variance estimators, median-unbiased estimators, Bayesian estimators (for example, the posterior distribution's mode, median, mean), and maximum-likelihood estimators.\nA Bayesian point estimator and a maximum-likelihood estimator have good performance when the sample size is infinite, according to asymptotic theory. For practical problems with finite samples, other estimators may be preferable. Asymptotic theory suggests techniques that often improve the performance of bootstrapped estimators; the bootstrapping of a maximum-likelihood estimator may often be improved using transformations related to pivotal quantities.\n\n\n== Deriving confidence intervals from the bootstrap distribution ==\nThe bootstrap distribution of a parameter-estimator has been used to calculate confidence intervals for its population-parameter.\n\n\n=== Bias, asymmetry, and confidence intervals ===\nBias: The bootstrap distribution and the sample may disagree systematically, in which case  bias may occur.\nIf the bootstrap distribution of an estimator is symmetric, then percentile confidence-interval are often used; such intervals are appropriate especially for median-unbiased estimators of minimum risk (with respect to an absolute loss function).  Bias in the bootstrap distribution will lead to bias in the confidence interval.\nOtherwise, if the bootstrap distribution is non-symmetric, then percentile confidence intervals are often inappropriate.\n\n\n=== Methods for bootstrap confidence intervals ===\nThere are several methods for constructing confidence intervals from the bootstrap distribution of a real parameter:\n\nBasic bootstrap, also known as the Reverse Percentile Interval. The basic bootstrap is a simple scheme to construct the confidence interval: one simply takes the empirical quantiles from the bootstrap distribution of the parameter (see Davison and Hinkley 1997, equ. 5.6 p. 194):\n  \n    \n      \n        (\n        2\n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \u2212\n        \n          \u03b8\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        ,\n        2\n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \u2212\n        \n          \u03b8\n          \n            (\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        )\n      \n    \n    {\\displaystyle (2{\\widehat {\\theta \\,}}-\\theta _{(1-\\alpha /2)}^{*},2{\\widehat {\\theta \\,}}-\\theta _{(\\alpha /2)}^{*})}\n   where \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\theta _{(1-\\alpha /2)}^{*}}\n   denotes the \n  \n    \n      \n        1\n        \u2212\n        \u03b1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 1-\\alpha /2}\n   percentile of the bootstrapped coefficients \n  \n    \n      \n        \n          \u03b8\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{*}}\n  .Percentile bootstrap. The percentile bootstrap proceeds in a similar way to the basic bootstrap, using percentiles of the bootstrap distribution, but with a different formula (note the inversion of the left and right quantiles):\n  \n    \n      \n        (\n        \n          \u03b8\n          \n            (\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        ,\n        \n          \u03b8\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\theta _{(\\alpha /2)}^{*},\\theta _{(1-\\alpha /2)}^{*})}\n   where \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\theta _{(1-\\alpha /2)}^{*}}\n   denotes the \n  \n    \n      \n        1\n        \u2212\n        \u03b1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 1-\\alpha /2}\n   percentile of the bootstrapped coefficients \n  \n    \n      \n        \n          \u03b8\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{*}}\n  .\nSee Davison and Hinkley (1997, equ. 5.18 p. 203) and Efron and Tibshirani (1993, equ 13.5 p. 171).\nThis method can be applied to any statistic. It will work well in cases where the bootstrap distribution is symmetrical and centered on the observed statistic and where the sample statistic is median-unbiased and has maximum concentration (or minimum risk with respect to an absolute value loss function).  When working with small sample sizes (i.e., less than 50), the basic / reversed percentile and percentile confidence intervals for (for example) the variance statistic will be too narrow. So that with a sample of 20 points, 90% confidence interval will include the true variance only 78% of the time.  The basic / reverse percentile confidence intervals are easier to justify mathematically but they are less accurate in general than percentile confidence intervals, and some authors discourage their use.Studentized bootstrap. The studentized bootstrap, also called bootstrap-t, is computed analogously to the standard confidence interval, but replaces the quantiles from the normal or student approximation by the quantiles from the bootstrap distribution of the Student's t-test (see Davison and Hinkley 1997, equ. 5.7 p. 194 and Efron and Tibshirani 1993 equ 12.22, p. 160):\n  \n    \n      \n        (\n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \u2212\n        \n          t\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        \u22c5\n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            \u03b8\n          \n        \n        ,\n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        \u2212\n        \n          t\n          \n            (\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n        \u22c5\n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            \u03b8\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\widehat {\\theta \\,}}-t_{(1-\\alpha /2)}^{*}\\cdot {\\widehat {\\text{se}}}_{\\theta },{\\widehat {\\theta \\,}}-t_{(\\alpha /2)}^{*}\\cdot {\\widehat {\\text{se}}}_{\\theta })}\n   where \n  \n    \n      \n        \n          t\n          \n            (\n            1\n            \u2212\n            \u03b1\n            \n              /\n            \n            2\n            )\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle t_{(1-\\alpha /2)}^{*}}\n   denotes the \n  \n    \n      \n        1\n        \u2212\n        \u03b1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 1-\\alpha /2}\n   percentile of the bootstrapped Student's t-test \n  \n    \n      \n        \n          t\n          \n            \u2217\n          \n        \n        =\n        (\n        \n          \n            \n              \n                \n                  \u03b8\n                  \n                \n                ^\n              \n            \n          \n          \n            \u2217\n          \n        \n        \u2212\n        \n          \n            \n              \n                \u03b8\n                \n              \n              ^\n            \n          \n        \n        )\n        \n          /\n        \n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \u03b8\n                      \n                    \n                    ^\n                  \n                \n              \n              \n                \u2217\n              \n            \n          \n        \n      \n    \n    {\\displaystyle t^{*}=({\\widehat {\\theta \\,}}^{*}-{\\widehat {\\theta \\,}})/{\\widehat {\\text{se}}}_{{\\widehat {\\theta \\,}}^{*}}}\n  , and \n  \n    \n      \n        \n          \n            \n              \n                se\n                ^\n              \n            \n          \n          \n            \u03b8\n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\text{se}}}_{\\theta }}\n   is the estimated standard error of the coefficient in the original model.The studentized test enjoys optimal properties as the statistic that is bootstrapped is pivotal (i.e. it does not depend on nuisance parameters as the t-test follows asymptotically a N(0,1) distribution), unlike the percentile bootstrap.Bias-corrected bootstrap \u2013 adjusts for bias in the bootstrap distribution.\nAccelerated bootstrap \u2013 The bias-corrected and accelerated (BCa) bootstrap, by Efron (1987), adjusts for both bias and skewness in the bootstrap distribution. This approach is accurate in a wide variety of settings, has reasonable computation requirements, and produces reasonably narrow intervals.\n\n\n== Bootstrap hypothesis testing ==\nEfron and Tibshirani suggest the following algorithm for comparing the means of two independent samples:\nLet \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n   be a random sample from distribution F with sample mean \n  \n    \n      \n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   and sample variance \n  \n    \n      \n        \n          \u03c3\n          \n            x\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{x}^{2}}\n  . Let \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle y_{1},\\ldots ,y_{m}}\n   be another, independent random sample from distribution G with mean \n  \n    \n      \n        \n          \n            \n              y\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {y}}}\n   and variance \n  \n    \n      \n        \n          \u03c3\n          \n            y\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{y}^{2}}\n  \n\nCalculate the test statistic \n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    \u00af\n                  \n                \n              \n              \u2212\n              \n                \n                  \n                    y\n                    \u00af\n                  \n                \n              \n            \n            \n              \n                \u03c3\n                \n                  x\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              n\n              +\n              \n                \u03c3\n                \n                  y\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {x}}-{\\bar {y}}}{\\sqrt {\\sigma _{x}^{2}/n+\\sigma _{y}^{2}/m}}}}\n  \nCreate two new data sets whose values are \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \u2032\n        \n        =\n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              x\n              \u00af\n            \n          \n        \n        +\n        \n          \n            \n              z\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i}'=x_{i}-{\\bar {x}}+{\\bar {z}}}\n   and \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \u2032\n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              y\n              \u00af\n            \n          \n        \n        +\n        \n          \n            \n              z\n              \u00af\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle y_{i}'=y_{i}-{\\bar {y}}+{\\bar {z}},}\n   where \n  \n    \n      \n        \n          \n            \n              z\n              \u00af\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {z}}}\n   is the mean of the combined sample.\nDraw a random sample (\n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle x_{i}^{*}}\n  ) of size \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   with replacement from \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle x_{i}'}\n   and another random sample (\n  \n    \n      \n        \n          y\n          \n            i\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle y_{i}^{*}}\n  ) of size \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   with replacement from \n  \n    \n      \n        \n          y\n          \n            i\n          \n          \u2032\n        \n      \n    \n    {\\displaystyle y_{i}'}\n  .\nCalculate the test statistic \n  \n    \n      \n        \n          t\n          \n            \u2217\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    \n                      x\n                      \n                        \u2217\n                      \n                    \n                    \u00af\n                  \n                \n              \n              \u2212\n              \n                \n                  \n                    \n                      y\n                      \n                        \u2217\n                      \n                    \n                    \u00af\n                  \n                \n              \n            \n            \n              \n                \u03c3\n                \n                  x\n                \n                \n                  \u2217\n                  2\n                \n              \n              \n                /\n              \n              n\n              +\n              \n                \u03c3\n                \n                  y\n                \n                \n                  \u2217\n                  2\n                \n              \n              \n                /\n              \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle t^{*}={\\frac {{\\bar {x^{*}}}-{\\bar {y^{*}}}}{\\sqrt {\\sigma _{x}^{*2}/n+\\sigma _{y}^{*2}/m}}}}\n  \nRepeat 3 and 4 \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   times (e.g. \n  \n    \n      \n        B\n        =\n        1000\n      \n    \n    {\\displaystyle B=1000}\n  ) to collect \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   values of the test statistic.\nEstimate the p-value as \n  \n    \n      \n        p\n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  B\n                \n              \n              I\n              {\n              \n                t\n                \n                  i\n                \n                \n                  \u2217\n                \n              \n              \u2265\n              t\n              }\n            \n            B\n          \n        \n      \n    \n    {\\displaystyle p={\\frac {\\sum _{i=1}^{B}I\\{t_{i}^{*}\\geq t\\}}{B}}}\n   where \n  \n    \n      \n        I\n        (\n        \n          condition\n        \n        )\n        =\n        1\n      \n    \n    {\\displaystyle I({\\text{condition}})=1}\n   when condition is true and 0 otherwise.\n\n\n== Example applications ==\n\n\n=== Smoothed bootstrap ===\nIn 1878, Simon Newcomb took observations on the speed of light.\nThe data set contains two outliers, which greatly influence the sample mean. (The sample mean need not be a consistent estimator for any population mean, because no mean needs to exist for a heavy-tailed distribution.) A well-defined and  robust statistic for the central tendency is the sample median, which is consistent and median-unbiased for the population median.\nThe bootstrap distribution for Newcomb's data appears below. We can reduce the discreteness of the bootstrap distribution by adding a small amount of random noise to each bootstrap sample. A conventional choice is to add noise with a standard deviation of \n  \n    \n      \n        \u03c3\n        \n          /\n        \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\sigma /{\\sqrt {n}}}\n   for a sample size n; this noise is often drawn from a Student-t distribution with n-1 degrees of freedom. This results in an approximately-unbiased estimator for the variance of the sample mean. This means that samples taken from the bootstrap distribution will have a variance which is, on average, equal to the variance of the total population.\nHistograms of the bootstrap distribution and the smooth bootstrap distribution appear below. The bootstrap distribution of the sample-median has only a small number of values. The smoothed bootstrap distribution has a richer support. However, note that whether the smoothed or standard bootstrap procedure is favorable is case-by-case and is shown to depend on both the underlying distribution function and on the quantity being estimated.\nIn this example, the bootstrapped 95% (percentile) confidence-interval for the population median is (26, 28.5), which is close to the interval for  (25.98, 28.46) for the smoothed bootstrap.\n\n\n== Relation to other approaches to inference ==\n\n\n=== Relationship to other resampling methods ===\nThe bootstrap is distinguished from:\n\nthe jackknife procedure, used to estimate biases of sample statistics and to estimate variances, and\ncross-validation, in which the parameters (e.g., regression weights, factor loadings) that are estimated in one subsample are applied to another subsample.For more details see resampling.\nBootstrap aggregating (bagging) is a meta-algorithm based on averaging model predictions obtained from models trained on multiple bootstrap samples.\n\n\n=== U-statistics ===\n\nIn situations where an obvious statistic can be devised to measure a required characteristic using only a small number, r, of data items, a corresponding statistic based on the entire sample can be formulated. Given an r-sample statistic, one can create an n-sample statistic by something similar to bootstrapping (taking the average of the statistic over all subsamples of size r). This procedure is known to have certain good properties and the result is a U-statistic. The sample mean and sample variance are of this form, for r = 1 and r = 2.\n\n\n== See also ==\nAccuracy and precision\nBootstrap aggregating\nBootstrapping\nEmpirical likelihood\nImputation (statistics)\nReliability (statistics)\nReproducibility\nResampling\n\n\n== References ==\n\n\n== Further reading ==\nDiaconis, P.; Efron, B. (May 1983). \"Computer-intensive methods in statistics\" (PDF). Scientific American. 248 (5): 116\u2013130. Bibcode:1983SciAm.248e.116D. doi:10.1038/scientificamerican0583-116. Archived from the original (PDF) on 2016-03-13. Retrieved 2016-01-19. popular-science\nEfron, B. (1981). \"Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods\". Biometrika. 68 (3): 589\u2013599. doi:10.1093/biomet/68.3.589.\nHesterberg, T. C.; D. S. Moore; S. Monaghan; A. Clipson & R. Epstein (2005). \"Bootstrap methods and permutation tests\" (PDF).  In David S. Moore & George McCabe (eds.). Introduction to the Practice of Statistics. software. Archived from the original (PDF) on 2006-02-15. Retrieved 2007-03-23.\nEfron, Bradley (1979). \"Bootstrap methods: Another look at the jackknife\". The Annals of Statistics. 7: 1\u201326. doi:10.1214/aos/1176344552.\nEfron, Bradley (1981). \"Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods\". Biometrika. 68 (3): 589\u2013599. doi:10.2307/2335441. JSTOR 2335441.\nEfron, Bradley (1982). The jackknife, the bootstrap, and other resampling plans, In Society of Industrial and Applied Mathematics CBMS-NSF Monographs, 38.\nDiaconis, P.; Efron, Bradley (1983), \"Computer-intensive methods in statistics,\" Scientific American, May, 116\u2013130.\nEfron, Bradley; Tibshirani, Robert J. (1993). An introduction to the bootstrap, New York: Chapman & Hall, software.\nDavison, A. C. and Hinkley, D. V. (1997): Bootstrap Methods and their Application, software.\nMooney, C Z & Duval, R D (1993). Bootstrapping. A Nonparametric Approach to Statistical Inference.  Sage University Paper series on Quantitative Applications in the Social Sciences, 07-095. Newbury Park, CA: Sage.\nSimon, J. L. (1997): Resampling: The New Statistics.\nWright, D.B., London, K., Field, A.P. Using Bootstrap Estimation and the Plug-in Principle for Clinical Psychology Data. 2011 Textrum Ltd. Online: https://www.researchgate.net/publication/236647074_Using_Bootstrap_Estimation_and_the_Plug-in_Principle_for_Clinical_Psychology_Data. Retrieved on 25/04/2016.\nAn Introduction to the Bootstrap. Monographs on Statistics and applied probability 57. Chapman&Hall/CHC. 1998. Online https://books.google.com/books?id=gLlpIUxRntoC&q=plug+in+principle.&pg=PA35 Retrieved on 25 04 2016.\n\n\n== External links ==\nBootstrap sampling tutorial using MS Excel\nBootstrap example to simulate stock prices using MS Excel\nbootstrapping tutorial \nWhat is the bootstrap?\n\n\n=== Software ===\nStatistics101: Resampling, Bootstrap, Monte Carlo Simulation program. Free program written in Java to run on any operating system.", "Accuracy and precision": "Accuracy and precision are two measures of observational error. \nAccuracy is how close a given set of measurements (observations or readings) are to their true value, while precision is how close the measurements are to each other.\nIn other words, precision is a description of random errors, a measure of statistical variability. Accuracy has two definitions:\n\nMore commonly, it is a description of only systematic errors, a measure of statistical bias of a given measure of central tendency; low accuracy causes a difference between a result and a true value; ISO calls this trueness.\nAlternatively, ISO defines accuracy as describing a combination of both types of observational error (random and systematic), so high accuracy requires both high precision and high trueness.In the first, more common definition of \"accuracy\" above, the concept is independent of \"precision\", so a particular set of data can be said to be accurate, precise, both, or neither.\nIn simpler terms, given a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small.\n\n\n== Common technical definition ==\n\nIn the fields of science and engineering, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's true value. The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results.  Although the two words precision and accuracy can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method.\nThe field of statistics, where the interpretation of measurements plays a central role, prefers to use the terms bias and variability instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision.\nA measurement system can be accurate but not precise, precise but not accurate, neither, or both. For example, if an experiment contains a systematic error, then increasing the sample size generally increases precision but does not improve accuracy. The result would be a consistent yet inaccurate string of results from the flawed experiment.  Eliminating the systematic error improves accuracy but does not change precision.\nA measurement system is considered valid if it is both accurate and precise. Related terms include bias (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and error (random variability).\nThe terminology is also applied to indirect measurements\u2014that is, values obtained by a computational procedure from observed data.\nIn addition to accuracy and precision, measurements may also have a measurement resolution, which is the smallest change in the underlying physical quantity that produces a response in the measurement.\nIn numerical analysis, accuracy is also the nearness of a calculation to the true value; while precision is the resolution of the representation, typically defined by the number of decimal or binary digits.\nIn military terms, accuracy refers primarily to the accuracy of fire (justesse de tir), the precision of fire expressed by the closeness of a grouping of shots at and around the centre of the target.\n\n\n=== Quantification ===\n\nIn industrial instrumentation, accuracy is the measurement tolerance, or transmission of the instrument and defines the limits of the errors made when the instrument is used in normal operating conditions.Ideally a measurement device is both accurate and precise, with measurements all close to and tightly clustered around the true value. The accuracy and precision of a measurement process is usually established by repeatedly measuring some traceable reference standard. Such standards are defined in the International System of Units (abbreviated SI from French: Syst\u00e8me international d'unit\u00e9s) and maintained by national standards organizations such as the National Institute of Standards and Technology in the United States.\nThis also applies when measurements are repeated and averaged. In that case, the term standard error is properly applied: the precision of the average is equal to the known standard deviation of the process divided by the square root of the number of measurements averaged. Further, the central limit theorem shows that the probability distribution of the averaged measurements will be closer to a normal distribution than that of individual measurements.\nWith regard to accuracy we can distinguish:\n\nthe difference between the mean of the measurements and the reference value, the bias. Establishing and correcting for bias is necessary for calibration.\nthe combined effect of that and precision.A common convention in science and engineering is to express accuracy and/or precision implicitly by means of significant figures. Where not explicitly stated, the margin of error is understood to be one-half the value of the last significant place. For instance, a recording of 843.6 m, or 843.0 m, or 800.0 m would imply a margin of 0.05 m (the last significant place is the tenths place), while a recording of 843 m would imply a margin of error of 0.5 m (the last significant digits are the units).\nA reading of 8,000 m, with trailing zeros and no decimal point, is ambiguous; the trailing zeros may or may not be intended as significant figures. To avoid this ambiguity, the number could be represented in scientific notation: 8.0 \u00d7 103 m indicates that the first zero is significant (hence a margin of 50 m) while 8.000 \u00d7 103 m indicates that all three zeros are significant, giving a margin of 0.5 m. Similarly, one can use a multiple of the basic measurement unit: 8.0 km is equivalent to 8.0 \u00d7 103 m. It indicates a margin of 0.05 km (50 m). However, reliance on this convention can lead to false precision errors when accepting data from sources that do not obey it. For example, a source reporting a number like 153,753 with precision +/- 5,000 looks like it has precision +/- 0.5. Under the convention it would have been rounded to 154,000.\nAlternatively, in a scientific context, if it is desired to indicate the margin of error with more precision, one can use a notation such as 7.54398(23) \u00d7 10\u221210 m, meaning a range of between 7.54375 and 7.54421 \u00d7 10\u221210 m.\nPrecision includes:\n\nrepeatability \u2014 the variation arising when all efforts are made to keep conditions constant by using the same instrument and operator, and repeating during a short time period; and\nreproducibility \u2014 the variation arising using the same measurement process among different instruments and operators, and over longer time periods.In engineering, precision is often taken as three times Standard Deviation of measurements taken, representing the range that 99.73% of measurements can occur within. For example, an ergonomist measuring the human body can be confident that 99.73% of their extracted measurements fall within \u00b1 0.7 cm - if using the GRYPHON processing system - or \u00b1 13 cm - if using unprocessed data.\n\n\n== ISO definition (ISO 5725) ==\n\nA shift in the meaning of these terms appeared with the publication of the ISO 5725 series of standards in 1994, which is also reflected in the 2008 issue of the BIPM International Vocabulary of Metrology (VIM), items 2.13 and 2.14.According to ISO 5725-1, the general term \"accuracy\" is used to describe the closeness of a measurement to the true value. When the term is applied to sets of measurements of the same measurand, it involves a component of random error and a component of systematic error. In this case trueness is the closeness of the mean of a set of measurement results to the actual (true) value and precision is the closeness of agreement among a set of results.\nISO 5725-1 and VIM also avoid the use of the term \"bias\", previously specified in BS 5497-1, because it has different connotations outside the fields of science and engineering, as in medicine and law.\n\n\n== In classification ==\n\n\n=== In binary classification ===\n\nAccuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. As such, it compares estimates of pre- and post-test probability. To make the context clear by the semantics, it is often referred to as the \"Rand accuracy\" or \"Rand index\". It is a parameter of the test.\nThe formula for quantifying binary accuracy is:\n\nwhere TP = True positive; FP = False positive; TN = True negative; FN = False negative\nNote that, in this context, the concepts of trueness and precision as defined by ISO 5725-1 are not applicable. One reason is that there is not a single \u201ctrue value\u201d of a quantity, but rather two possible true values for every case, while accuracy is an average across all cases and therefore takes into account both values. However, the term precision is used in this context to mean a different metric originating from the field of information retrieval (see below).\n\n\n=== In multiclass classification ===\nWhen computing accuracy in multiclass classification, accuracy is simply the fraction of correct classifications:\nThis is usually expressed as a percentage.  For example, if a classifier makes ten predictions and nine of them are correct, the accuracy is 90%.\nAccuracy is also called top-1 accuracy to distinguish it from top-5 accuracy, common in convolutional neural network evaluation.  To evaluate top-5 accuracy, the classifier must provide relative likelihoods for each class.  When these are sorted, a classification is considered correct if the correct classification falls anywhere within the top 5 predictions made by the network.  Top-5 accuracy was popularized by the ImageNet challenge. It is usually higher than top-1 accuracy, as any correct predictions in the 2nd through 5th positions will not improve the top-1 score, but do improve the top-5 score.\n\n\n== In psychometrics and psychophysics ==\nIn psychometrics and psychophysics, the term accuracy is interchangeably used with validity and constant error. Precision is a synonym for reliability and variable error. The validity of a measurement instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal consistency test like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population.\n\n\n== In logic simulation ==\nIn logic simulation, a common mistake in evaluation of accurate models is to compare a logic simulation model to a transistor circuit simulation model. This is a comparison of differences in precision, not accuracy. Precision is measured with respect to detail and accuracy is measured with respect to reality.\n\n\n== In information systems ==\nInformation retrieval systems, such as databases and web search engines, are evaluated by many different metrics, some of which are derived from the confusion matrix, which divides results into true positives (documents correctly retrieved), true negatives (documents correctly not retrieved), false positives (documents incorrectly retrieved), and false negatives (documents incorrectly not retrieved). Commonly used metrics include the notions of precision and recall.  In this context, precision is defined as the fraction of retrieved documents which are relevant to the query (true positives divided by true+false positives), using a set of ground truth relevant results selected by humans.  Recall is defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives).  Less commonly, the metric of accuracy is used, is defined as the total number of correct classifications (true positives plus true negatives) divided by the total number of documents.\nNone of these metrics take into account the ranking of results. Ranking is very important for web search engines because readers seldom go past the first page of results, and there are too many documents on the web to manually classify all of them as to whether they should be included or excluded from a given search.  Adding a cutoff at a particular number of results takes ranking into account to some degree. The measure precision at k, for example, is a measure of precision looking only at the top ten (k=10) search results. More sophisticated metrics, such as discounted cumulative gain, take into account each individual ranking, and are more commonly used where this is important.\n\n\n== In cognitive systems ==\nIn cognitive systems, accuracy and precision is used to characterize and measure results of a cognitive process performed by biological or artificial entities where a cognitive process is a transformation of data, information, knowledge, or wisdom to a higher-valued form. (DIKW Pyramid) Sometimes, a cognitive process produces exactly the intended or desired output but sometimes produces output far from the intended or desired. Furthermore, repetitions of a cognitive process do not always produce the same output. Cognitive accuracy (CA) is the propensity of a cognitive process to produce the intended or desired output. Cognitive precision (CP) is the propensity of a cognitive process to produce only the intended or desired output. To measure augmented cognition in human/cog ensembles, where one or more humans work collaboratively with one or more cognitive systems (cogs), increases in cognitive accuracy and cognitive precision assist in measuring the degree of cognitive augmentation.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nBIPM - Guides in metrology, Guide to the Expression of Uncertainty in Measurement (GUM) and International Vocabulary of Metrology (VIM)\n\"Beyond NIST Traceability: What really creates accuracy\", Controlled Environments magazine\nPrecision and Accuracy with Three Psychophysical Methods\nAppendix D.1: Terminology, Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results\nAccuracy and Precision\nAccuracy vs Precision \u2014 a brief video by Matt Parker\nWhat's the difference between accuracy and precision? by Matt Anticole at TED-Ed", "Bootstrap aggregating": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bootstrap aggregation can be related to the posterior predictive distribution\n\n\n== Description of the technique ==\nGiven a standard training set \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   of size n, bagging generates m new training sets \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  , each of size n\u2032, by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  . If n\u2032=n, then for large n the set \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n   is expected to have the fraction (1 - 1/e) (\u224863.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling.   Then, m models  are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n\nBagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. Bagging was shown to improve preimage learning. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors.\n\n\n== Process of the algorithm ==\n\n\n=== Key Terms ===\nThere are three types of datasets in bootstrap aggregating. These are the original, bootstrap, and out-of-bag datasets. Each section below will explain how each dataset is made except for the original dataset. The original dataset is whatever information is given.\n\n\n=== Creating the bootstrap dataset ===\nThe bootstrap dataset is made by randomly picking objects from the original dataset. Also, it must be the same size as the original dataset. However, the difference is that the bootstrap dataset can have duplicate objects. Here is simple example to demonstrate how it works along with the illustration below:\n\nSuppose the original dataset is a group of 12 people. These guys are Emily, Jessie, George, Constantine, Lexi, Theodore, John, James, Rachel, Anthony, Ellie, and Jamal.\nBy randomly picking a group of names, let us say our bootstrap dataset had James, Ellie, Constantine, Lexi, John, Constantine, Theodore, Constantine, Anthony, Lexi, Constantine, and Theodore. In this case, the bootstrap sample contained four duplicates for Constantine, and two duplicates for Lexi, and Theodore.\n\n\n=== Creating the out-of-bag dataset ===\nThe out-of-bag dataset represents the remaining people who were not in the bootstrap dataset. It can be calculated by taking the difference between the original and the bootstrap datasets. In this case, the remaining samples who were not selected are Emily, Jessie, George, Rachel, and Jamal. Keep in mind that since both datasets are sets, when taking the difference the duplicate names are ignored in the bootstrap dataset. The illustration below shows how the math is done:\n\n\n=== Importance ===\nCreating the bootstrap and out-of-bag datasets is crucial since it is used to test the accuracy of a random forest algorithm. For example, a model that produces 50 trees using the bootstrap/out-of-bag datasets will have a better accuracy than if it produced 10 trees. Since the algorithm generates multiple trees and therefore multiple datasets the chance that an object is left out of the bootstrap dataset is low. The next few sections talk about how the random forest algorithm works in more detail.\n\n\n=== Creation of Decision Trees ===\nThe next step of the algorithm involves the generation of decision trees from the bootstrapped dataset. To achieve this, the process examines each gene/feature and determines for how many samples the feature's presence or absence yields a positive or negative result. This information is then used to compute a confusion matrix, which lists the true positives, false positives, true negatives, and false negatives of the feature when used as a classifier. These features are then ranked according to various classification metrics based on their confusion matrices. Some common metrics include estimate of positive correctness (calculated by subtracting false positives from true positives), measure of \"goodness\", and information gain. These features are then used to partition the samples into two sets: those who possess the top feature, and those who do not.\nThe diagram below shows a decision tree of depth two being used to classify data. For example, a data point that exhibits Feature 1, but not Feature 2, will be given a \"No\". Another point that does not exhibit Feature 1, but does exhibit Feature 3, will be given a \"Yes\".\n\nThis process is repeated recursively for successive levels of the tree until the desired depth is reached. At the very bottom of the tree, samples that test positive for the final feature are generally classified as positive, while those that lack the feature are classified as negative. These trees are then used as predictors to classify new data.\n\n\n=== Random Forests ===\nThe next part of the algorithm involves introducing yet another element of variability amongst the bootstrapped trees. In addition to each tree only examining a bootstrapped set of samples, only a small but consistent number of unique features are considered when ranking them as classifiers. This means that each tree only knows about the data pertaining to a small constant number of features, and a variable number of samples that is less than or equal to that of the original dataset. Consequently, the trees are more likely to return a wider array of answers, derived from more diverse knowledge. This results in a random forest, which possesses numerous benefits over a single decision tree generated without randomness. In a random forest, each tree \"votes\" on whether or not to classify a sample as positive based on its features. The sample is then classified based on majority vote. An example of this is given in the diagram below, where the four trees in a random forest vote on whether or not a patient with mutations A, B, F, and G has cancer. Since three out of four trees vote yes, the patient is then classified as cancer positive.\n\nBecause of their properties, random forests are considered one of the most accurate data mining algorithms, are less likely to overfit their data, and run quickly and efficiently even for large datasets. They are primarily useful for classification as opposed to regression, which attempts to draw observed connections between statistical variables in a dataset. This makes random forests particularly useful in such fields as banking, healthcare, the stock market, and e-commerce where it is important to be able to predict future results based on past data. One of their applications would be as a useful tool for predicting cancer based on genetic factors, as seen in the above example.\nThere are several important factors to consider when designing a random forest. If the trees in the random forests are too deep, overfitting can still occur due to over-specificity. If the forest is too large, the algorithm may become less efficient due to an increased runtime. Random forests also do not generally perform well when given sparse data with little variability. However, they still have numerous advantages over similar data classification algorithms such as neural networks, as they are much easier to interpret and generally require less data for training. As an integral component of random forests, bootstrap aggregating is very important to classification algorithms, and provides a critical element of variability that allows for increased accuracy when analyzing new data, as discussed below.\n\n\n== Improving Random Forests and Bagging ==\nWhile the techniques described above utilize random forests and bagging (otherwise known as bootstrapping), there are certain techniques that can be used in order to improve their execution and voting time, their prediction accuracy, and their overall performance. The following are key steps in creating an efficient random forest:\n\nSpecify the maximum depth of trees: Instead of allowing your random forest to continue until all nodes are pure, it is better to cut it off at a certain point in order to further decrease chances of overfitting.\nPrune the dataset: Using an extremely large dataset may prove to create results that is less indicative of the data provided than a smaller set that more accurately represents what is being focused on.\nContinue pruning the data at each node split rather than just in the original bagging process.\nDecide on accuracy or speed: Depending on the desired results, increasing or decreasing the number of trees within the forest can help. Increasing the number of trees generally provides more accurate results while decreasing the number of trees will provide quicker results.\n\n\n== Algorithm (classification) ==\n\nFor classification, use a training set \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  , Inducer \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   and the number of bootstrap samples \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   as input. Generate a classifier \n  \n    \n      \n        \n          C\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle C^{*}}\n   as output\nCreate \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n    new training sets  \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  , from \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   with replacement\nClassifier \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle C_{i}}\n   is built from each set \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n   using \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   to determine the classification of set \n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  \nFinally classifier \n  \n    \n      \n        \n          C\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle C^{*}}\n   is generated by using the previously created set of classifiers \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle C_{i}}\n   on the original data set \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  , the classification predicted most often by the sub-classifiers \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle C_{i}}\n   is the final classificationfor i = 1 to m {\n    D' = bootstrap sample from D    (sample with replacement)\n    Ci = I(D')\n}\nC*(x) = argmax #{i:Ci(x)=y}         (most often predicted label y)\n         y\u2208Y   \n\n\n== Example: ozone data ==\nTo illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (1986), analysis done in R).\nThe relationship between temperature and ozone appears to be nonlinear in this data set, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. Rather than building a single smoother for the complete data set, 100 bootstrap samples were drawn. Each sample is composed of a random subset of the original data and maintains a semblance of the master set\u2019s distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The black lines represent these initial predictions. The lines lack agreement in their predictions and tend to overfit their data points: evident by the wobbly flow of the lines.\n\nBy taking the average of 100 smoothers, each corresponding to a subset of the original data set, we arrive at one bagged predictor (red line). The red line's flow is stable and does not overly conform to any data point(s).\n\n\n== Advantages and disadvantages ==\nAdvantages:\n\nMany weak learners aggregated typically outperform a single learner over the entire set, and has less overfit\nRemoves variance in high-variance low-bias weak learner \nCan be performed in parallel, as each separate bootstrap can be processed on its own before combinationDisadvantages:\n\nFor weak learner with high bias, bagging will also carry high bias into its aggregate\nLoss of interpretability of a model.\nCan be computationally expensive depending on the data set\n\n\n== History ==\nThe concept of bootstrap aggregating is derived from the concept of bootstrapping which was developed by Bradley Efron.\nBootstrap aggregating was proposed by Leo Breiman who also coined the abbreviated term \"bagging\" (bootstrap aggregating). Breiman developed the concept of bagging in 1994 to improve classification by combining classifications of randomly generated training sets. He argued, \"If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy\".\n\n\n== See also ==\nBoosting (meta-algorithm)\nBootstrapping (statistics)\nCross-validation (statistics)\nOut-of-bag error\nRandom forest\nRandom subspace method (attribute bagging)\nResampled efficient frontier\nPredictive analysis: Classification and regression trees\n\n\n== References ==\n\n\n== Further reading ==\nBreiman, Leo (1996). \"Bagging predictors\". Machine Learning. 24 (2): 123\u2013140. CiteSeerX 10.1.1.32.9399. doi:10.1007/BF00058655. S2CID 47328136.\nAlfaro, E., G\u00e1mez, M. and Garc\u00eda, N. (2012). \"adabag: An R package for classification with AdaBoost.M1, AdaBoost-SAMME and Bagging\". {{cite journal}}: Cite journal requires |journal= (help)\nKotsiantis, Sotiris (2014). \"Bagging and boosting variants for handling classifications problems: a survey\". Knowledge Eng. Review. 29 (1): 78\u2013100. doi:10.1017/S0269888913000313. S2CID 27301684.\nBoehmke, Bradley; Greenwell, Brandon (2019). \"Bagging\". Hands-On Machine Learning with R. Chapman & Hall. pp. 191\u2013202. ISBN 978-1-138-49568-5.", "Hill climbing": "In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.\nFor example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will likely be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.\nHill climbing finds optimal solutions for convex problems \u2013 for other problems it will find only local optima (solutions that cannot be improved upon by any neighboring configurations), which are not necessarily the best possible solution (the global optimum) out of all possible solutions (the search space). Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.:\u200a253\u200a To attempt to avoid getting stuck in local optima, one could use restarts (i.e. repeated local search), or more complex schemes based on iterations (like iterated local search), or on memory (like reactive search optimization and tabu search), or on memory-less stochastic modifications (like simulated annealing).\nThe relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Different choices for next nodes and starting nodes are used in related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems, so long as a small number of increments typically converges on a good solution (the optimal solution or a close approximation). At the other extreme, bubble sort can be viewed as a hill climbing algorithm (every adjacent element exchange decreases the number of disordered element pairs), yet this approach is far from efficient for even modest N, as the number of exchanges required grows quadratically. \nHill climbing is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends.\n\n\n== Mathematical description ==\nHill climbing attempts to maximize (or minimize) a target function \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n  , where \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   is a vector of continuous and/or discrete values. At each iteration, hill climbing will adjust a single element in \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   and determine whether the change improves the value of \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n  . (Note that this differs from gradient descent methods, which adjust all of the values in \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   at each iteration according to the gradient of the hill.) With hill climbing, any change that improves \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n   is accepted, and the process continues until no change can be found to improve the value of \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n  . Then \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   is said to be \"locally optimal\".\nIn discrete vector spaces, each possible value for \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   may be visualized as a vertex in a graph. Hill climbing will follow the graph from vertex to vertex, always locally increasing  (or decreasing) the value of \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n  , until a local maximum (or local minimum) \n  \n    \n      \n        \n          x\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x_{m}}\n   is reached.\n\n\n== Variants ==\nIn simple hill climbing, the first closer node is chosen, whereas in steepest ascent hill climbing all successors are compared and the closest to the solution is chosen. Both forms fail if there is no closer node, which may happen if there are local maxima in the search space which are not solutions. Steepest ascent hill climbing is similar to best-first search, which tries all possible extensions of the current path instead of only one.\nStochastic hill climbing does not examine all neighbors before deciding how to move. Rather, it selects a neighbor at random, and decides (based on the amount of improvement in that neighbor) whether to move to that neighbor or to examine another.\nCoordinate descent does a line search along one coordinate direction at the current point in each iteration. Some versions of coordinate descent randomly pick a different coordinate direction each iteration.\nRandom-restart hill climbing is a meta-algorithm built on top of the hill climbing algorithm. It is also known as Shotgun hill climbing. It iteratively does hill-climbing, each time with a random initial condition \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  . The best \n  \n    \n      \n        \n          x\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x_{m}}\n   is kept: if a new run of hill climbing produces a better \n  \n    \n      \n        \n          x\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x_{m}}\n   than the stored state, it replaces the stored state.\nRandom-restart hill climbing is a surprisingly effective algorithm in many cases. It turns out that it is often better to spend CPU time exploring the space, than carefully optimizing from an initial condition.\n\n\n== Problems ==\n\n\n=== Local maxima ===\n\nHill climbing will not necessarily find the global maximum, but may instead converge on a local maximum. This problem does not occur if the heuristic is convex. However, as many functions are not convex hill climbing may often fail to reach a global maximum. Other local search algorithms try to overcome this problem such as stochastic hill climbing, random walks and simulated annealing.\n\n\n=== Ridges and alleys ===\n\nRidges are a challenging problem for hill climbers that optimize in continuous spaces. Because hill climbers only adjust one element in the vector at a time, each step will move in an axis-aligned direction. If the target function creates a narrow ridge that ascends in a non-axis-aligned direction (or if the goal is to minimize, a narrow alley that descends in a non-axis-aligned direction), then the hill climber can only ascend the ridge (or descend the alley) by zig-zagging. If the sides of the ridge (or alley) are very steep, then the hill climber may be forced to take very tiny steps as it zig-zags toward a better position. Thus, it may take an unreasonable length of time for it to ascend the ridge (or descend the alley).\nBy contrast, gradient descent methods can move in any direction that the ridge or alley may ascend or descend. Hence, gradient descent or the conjugate gradient method is generally preferred over hill climbing when the target function is differentiable. Hill climbers, however, have the advantage of not requiring the target function to be differentiable, so hill climbers may be preferred when the target function is complex.\n\n\n=== Plateau ===\nAnother problem that sometimes occurs with hill climbing is that of a plateau. A plateau is encountered when the search space is flat, or sufficiently flat that the value returned by the target function is indistinguishable from the value returned for nearby regions due to the precision used by the machine to represent its value. In such cases, the hill climber may not be able to determine in which direction it should step, and may wander in a direction that never leads to improvement.\n\nPseudocode\nalgorithm Discrete Space Hill Climbing is\n    currentNode := startNode\n    loop do\n        L := NEIGHBORS(currentNode)\n        nextEval := \u2212INF\n        nextNode := NULL\n        for all x in L do\n            if EVAL(x) > nextEval then\n                nextNode := x\n                nextEval := EVAL(x)\n        if nextEval \u2264 EVAL(currentNode) then\n            // Return current node since no better neighbors exist\n            return currentNode\n        currentNode := nextNode\n\nalgorithm Continuous Space Hill Climbing is\n    currentPoint := initialPoint    // the zero-magnitude vector is common\n    stepSize := initialStepSizes    // a vector of all 1's is common\n    acceleration := someAcceleration // a value such as 1.2 is common\n    candidate[0] := \u2212acceleration\n    candidate[1] := \u22121 / acceleration\n    candidate[2] := 1 / acceleration\n    candidate[3] := acceleration\n    bestScore := EVAL(currentPoint)\n    loop do\n        beforeScore := bestScore\n        for each element i in currentPoint do\n            beforePoint := currentPoint[i]\n            bestStep := 0\n            for j from 0 to 3 do      // try each of 4 candidate locations\n                step := stepSize[i] \u00d7 candidate[j]\n                currentPoint[i] := beforePoint + step\n                score := EVAL(currentPoint)\n                if score > bestScore then\n                    bestScore := score\n                    bestStep := step\n            if bestStep is 0 then\n                currentPoint[i] := beforePoint\n                stepSize[i] := stepSize[i] / acceleration\n            else\n                currentPoint[i] := beforePoint + bestStep\n                stepSize[i] := bestStep // acceleration\n        if (bestScore \u2212 beforeScore) < epsilon then\n            return currentPoint\n\nContrast genetic algorithm; random optimization.\n\n\n== See also ==\nGradient descent\nGreedy algorithm\nT\u00e2tonnement\nMean-shift\nA* search algorithm\n\n\n== References ==\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, pp. 111\u2013114, ISBN 0-13-790395-2\n\n\n== Further reading ==\nLasry, George (2018). A Methodology for the Cryptanalysis of Classical Ciphers with Search Metaheuristics (PDF). Kassel University Press. ISBN 978-3-7376-0459-8.\n\n\n== External links ==\n Hill climbing at Wikibooks", "Least squares": "The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.\nThe most important application is in data fitting. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares.\nLeast squares problems fall into two categories: linear or ordinary least squares and nonlinear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\nPolynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.\nWhen the observations come from an exponential family with identity as its natural sufficient statistics and mild-conditions are satisfied (e.g. for normal, exponential, Poisson and binomial distributions), standardized least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator.\nThe following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model.\nThe least-squares method was officially discovered and published by Adrien-Marie Legendre (1805), though it is usually also co-credited to Carl Friedrich Gauss (1795) who contributed significant theoretical advances to the method and may have previously used it in his work.\n\n\n== History ==\n\n\n=== Founding ===\nThe method of least squares grew out of the fields of astronomy and geodesy, as scientists and mathematicians sought to provide solutions to the challenges of navigating the Earth's oceans during the Age of Discovery. The accurate description of the behavior of celestial bodies was the key to enabling ships to sail in open seas, where sailors could no longer rely on land sightings for navigation.\nThe method was the culmination of several advances that took place during the course of the eighteenth century:\nThe combination of different observations as being the best estimate of the true value; errors decrease with aggregation rather than increase, perhaps first expressed by Roger Cotes in 1722.\nThe combination of different observations taken under the same conditions contrary to simply trying one's best to observe and record a single observation accurately. The approach was known as the method of averages. This approach was notably used by Tobias Mayer while studying the librations of the moon in 1750, and by Pierre-Simon Laplace in his work in explaining the differences in motion of Jupiter and Saturn in 1788.\nThe combination of different observations taken under different conditions. The method came to be known as the method of least absolute deviation. It was notably performed by Roger Joseph Boscovich in his work on the shape of the earth in 1757 and by Pierre-Simon Laplace for the same problem in 1799.\nThe development of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the probability density for the errors and define a method of estimation that minimizes the error of estimation. For this purpose, Laplace used a symmetric two-sided exponential distribution we now call Laplace distribution to model the error distribution, and used the sum of absolute deviation as error of estimation. He felt these to be the simplest assumptions he could make, and he had hoped to obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.\n\n\n=== The method ===\n\nThe first clear and concise exposition of the method of least squares was published by Legendre in 1805. The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for the shape of the earth. Within ten years after Legendre's publication, the method of least squares had been adopted as a standard tool in astronomy and geodesy in France, Italy, and Prussia, which constitutes an extraordinarily rapid acceptance of a scientific technique.In 1809 Carl Friedrich Gauss published his method of calculating the orbits of celestial bodies. In that work he claimed to have been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the error of estimation. Gauss showed that the arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.\nAn early demonstration of the strength of Gauss's method came when it was used to predict the future location of the newly discovered asteroid Ceres. On 1 January 1801, the Italian astronomer Giuseppe Piazzi discovered Ceres and was able to track its path for 40 days before it was lost in the glare of the sun. Based on these data, astronomers desired to determine the location of Ceres after it emerged from behind the sun without solving Kepler's complicated nonlinear equations of planetary motion. The only predictions that successfully allowed Hungarian astronomer Franz Xaver von Zach to relocate Ceres were those performed by the 24-year-old Gauss using least-squares analysis.\nIn 1810, after reading Gauss's work, Laplace, after proving the central limit theorem, used it to give a large sample justification for the method of least squares and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal in the sense that in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the Gauss\u2013Markov theorem.\nThe idea of least-squares analysis was also independently formulated by the American Robert Adrain in 1808. In the next two centuries workers in the theory of errors and in statistics found many different ways of implementing least squares.\n\n\n== Problem statement ==\nThe objective consists of adjusting the parameters of a model function to best fit a data set. A simple data set consists of n points (data pairs) \n  \n    \n      \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle (x_{i},y_{i})\\!}\n  , i = 1, \u2026, n, where \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        \n      \n    \n    {\\displaystyle x_{i}\\!}\n   is an independent variable and \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        \n      \n    \n    {\\displaystyle y_{i}\\!}\n   is a dependent variable whose value is found by observation. The model function has the form \n  \n    \n      \n        f\n        (\n        x\n        ,\n        \n          \u03b2\n        \n        )\n      \n    \n    {\\displaystyle f(x,{\\boldsymbol {\\beta }})}\n  , where m adjustable parameters are held in the vector \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n  . The goal is to find the parameter values for the model that \"best\" fits the data. The fit of a model to a data point is measured by its residual, defined as the difference between the observed value of the dependent variable and the value predicted by the model: \n\n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          \u03b2\n        \n        )\n        .\n      \n    \n    {\\displaystyle r_{i}=y_{i}-f(x_{i},{\\boldsymbol {\\beta }}).}\n  The least-squares method finds the optimal parameter values by minimizing the sum of squared residuals, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  :\n\n  \n    \n      \n        S\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          r\n          \n            i\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle S=\\sum _{i=1}^{n}r_{i}^{2}.}\n  In the simplest case \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          \u03b2\n        \n        )\n        =\n        \u03b2\n      \n    \n    {\\displaystyle f(x_{i},{\\boldsymbol {\\beta }})=\\beta }\n   and the result of the least-squares method is the arithmetic mean of the input data.\nAn example of a model in two dimensions is that of the straight line. Denoting the y-intercept as \n  \n    \n      \n        \n          \u03b2\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0}}\n   and the slope as \n  \n    \n      \n        \n          \u03b2\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  , the model function is given by \n  \n    \n      \n        f\n        (\n        x\n        ,\n        \n          \u03b2\n        \n        )\n        =\n        \n          \u03b2\n          \n            0\n          \n        \n        +\n        \n          \u03b2\n          \n            1\n          \n        \n        x\n      \n    \n    {\\displaystyle f(x,{\\boldsymbol {\\beta }})=\\beta _{0}+\\beta _{1}x}\n  . See linear least squares for a fully worked out example of this model.\nA data point may consist of more than one independent variable. For example, when fitting a plane to a set of height measurements, the plane is a function of two independent variables, x and z, say. In the most general case there may be one or more independent variables and one or more dependent variables at each data point.  \nTo the right is a residual plot illustrating random fluctuations about \n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle r_{i}=0}\n  , indicating that a linear model\n  \n    \n      \n        (\n        \n          Y\n          \n            i\n          \n        \n        =\n        \u03b1\n        +\n        \u03b2\n        \n          x\n          \n            i\n          \n        \n        +\n        \n          U\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (Y_{i}=\\alpha +\\beta x_{i}+U_{i})}\n   is appropriate. \n  \n    \n      \n        \n          U\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle U_{i}}\n   is an independent, random variable.   \n\n \nIf the residual points had some sort of a shape and were not randomly fluctuating, a linear model would not be appropriate. For example, if the residual plot had a parabolic shape as seen to the right, a parabolic model \n  \n    \n      \n        (\n        \n          Y\n          \n            i\n          \n        \n        =\n        \u03b1\n        +\n        \u03b2\n        \n          x\n          \n            i\n          \n        \n        +\n        \u03b3\n        \n          x\n          \n            i\n          \n          \n            2\n          \n        \n        +\n        \n          U\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (Y_{i}=\\alpha +\\beta x_{i}+\\gamma x_{i}^{2}+U_{i})}\n   would be appropriate for the data. The residuals for a parabolic model can be calculated via \n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              \u03b1\n              ^\n            \n          \n        \n        \u2212\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        \n          x\n          \n            i\n          \n        \n        \u2212\n        \n          \n            \n              \u03b3\n              ^\n            \n          \n        \n        \n          x\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle r_{i}=y_{i}-{\\hat {\\alpha }}-{\\hat {\\beta }}x_{i}-{\\widehat {\\gamma }}x_{i}^{2}}\n  .\n\n\n== Limitations ==\nThis regression formulation considers only observational errors in the dependent variable (but the alternative total least squares regression can account for errors in both variables). There are two rather different contexts with  different implications:\n\nRegression for prediction. Here a model is fitted to provide a prediction rule for application in a similar situation to which the data used for fitting apply. Here the dependent variables corresponding to such future application would be subject to the same types of observation error as those in the data used for fitting. It is therefore logically consistent to use the least-squares prediction rule for such data.\nRegression for fitting a \"true relationship\". In standard regression analysis that leads to fitting by least squares there is an implicit assumption that errors in the independent variable are zero or strictly controlled so as to be negligible. When errors in the independent variable are non-negligible, models of measurement error can be used; such methods can lead to parameter estimates, hypothesis testing and confidence intervals that take into account the presence of observation errors in the independent variables. An alternative approach is to fit a model by total least squares; this can be viewed as taking a pragmatic approach to balancing the effects of the different sources of error in formulating an objective function for use in model-fitting.\n\n\n== Solving the least squares problem ==\nThe minimum of the sum of squares is found by setting the gradient to zero. Since the model contains m parameters, there are m gradient equations:\n\nand since \n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        f\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          \u03b2\n        \n        )\n      \n    \n    {\\displaystyle r_{i}=y_{i}-f(x_{i},{\\boldsymbol {\\beta }})}\n  , the gradient equations become\n\nThe gradient equations apply to all least squares problems. Each particular problem requires particular expressions for the model and its partial derivatives.\n\n\n=== Linear least squares ===\n\nA regression model is a linear one when the model comprises a linear combination of the parameters, i.e.,\n\nwhere the function \n  \n    \n      \n        \n          \u03d5\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\phi _{j}}\n   is a function of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .Letting \n  \n    \n      \n        \n          X\n          \n            i\n            j\n          \n        \n        =\n        \n          \u03d5\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle X_{ij}=\\phi _{j}(x_{i})}\n   and putting the independent and dependent variables in matrices \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , respectively, we can compute the least squares in the following way. Note that \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   is the set of all data.\nThe gradient of the loss is:\n\nSetting the gradient of the loss to zero and solving for \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n  , we get:\n\n\n=== Non-linear least squares ===\n\nThere is, in some cases, a closed-form solution to a non-linear least squares problem \u2013 but in general there is not. In  the case of no closed-form solution, numerical algorithms are used to find the value of the parameters \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n   that minimizes the objective.  Most algorithms involve choosing initial values for the parameters.  Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation:\n\nwhere a superscript k is an iteration number, and the vector of increments \n  \n    \n      \n        \u0394\n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta \\beta _{j}}\n   is called the shift vector.  In some commonly used algorithms, at each iteration the model may be linearized by approximation to a first-order Taylor series expansion about \n  \n    \n      \n        \n          \n            \u03b2\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}^{k}}\n  :\n\nThe Jacobian J is a function of constants, the independent variable and the parameters, so it changes from one iteration to the next. The residuals are given by\n\nTo minimize the sum of squares of \n  \n    \n      \n        \n          r\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle r_{i}}\n  , the gradient equation is set to zero and solved for \n  \n    \n      \n        \u0394\n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Delta \\beta _{j}}\n  :\n\nwhich, on rearrangement, become m simultaneous linear equations, the normal equations:\n\nThe normal equations are written in matrix notation as\n\nThese are the defining equations of the Gauss\u2013Newton algorithm.\n\n\n=== Differences between linear and nonlinear least squares ===\nThe model function, f, in LLSQ (linear least squares) is a linear combination of parameters of the form \n  \n    \n      \n        f\n        =\n        \n          X\n          \n            i\n            1\n          \n        \n        \n          \u03b2\n          \n            1\n          \n        \n        +\n        \n          X\n          \n            i\n            2\n          \n        \n        \n          \u03b2\n          \n            2\n          \n        \n        +\n        \u22ef\n      \n    \n    {\\displaystyle f=X_{i1}\\beta _{1}+X_{i2}\\beta _{2}+\\cdots }\n   The model may represent a straight line, a parabola or any other linear combination of functions. In NLLSQ  (nonlinear least squares) the parameters appear as functions, such as \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n        ,\n        \n          e\n          \n            \u03b2\n            x\n          \n        \n      \n    \n    {\\displaystyle \\beta ^{2},e^{\\beta x}}\n   and so forth. If the derivatives \n  \n    \n      \n        \u2202\n        f\n        \n          /\n        \n        \u2202\n        \n          \u03b2\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\partial f/\\partial \\beta _{j}}\n   are either constant or depend only on the values of the independent variable, the model is linear in the parameters. Otherwise the model is nonlinear.\nNeed initial values for the parameters to find the solution to a NLLSQ problem; LLSQ does not require them.\nSolution algorithms for NLLSQ often require that the Jacobian can be calculated similar to LLSQ. Analytical expressions for the partial derivatives can be complicated. If analytical expressions are impossible to obtain either the partial derivatives must be calculated by numerical approximation or an estimate must be made of the Jacobian, often via finite differences.\nNon-convergence (failure of the algorithm to find a minimum) is a common phenomenon in NLLSQ.\nLLSQ is globally concave so non-convergence is not an issue.\nSolving NLLSQ is usually an iterative process which has to be terminated when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems with large numbers of parameters are typically solved with iterative methods, such as the Gauss\u2013Seidel method.\nIn LLSQ the solution is unique, but in NLLSQ there may be multiple minima in the sum of squares.\nUnder the condition that the errors are uncorrelated with the predictor variables, LLSQ yields unbiased estimates, but even under that condition NLLSQ estimates are generally biased.These differences must be considered whenever the solution to a nonlinear least squares problem is being sought.\n\n\n== Example ==\nConsider a simple example drawn from physics. A spring should obey Hooke's law which states that the extension of a spring y is proportional to the force, F, applied to it.\n\n  \n    \n      \n        y\n        =\n        f\n        (\n        F\n        ,\n        k\n        )\n        =\n        k\n        F\n        \n      \n    \n    {\\displaystyle y=f(F,k)=kF\\!}\n  constitutes the model, where F is the independent variable. In order to estimate the force constant, k, we conduct a series of n measurements with different forces to produce a set of data, \n  \n    \n      \n        (\n        \n          F\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        ,\n         \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n        \n      \n    \n    {\\displaystyle (F_{i},y_{i}),\\ i=1,\\dots ,n\\!}\n  , where yi is a measured spring extension. Each experimental observation will contain some error, \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  , and so we may specify an empirical model for our observations,\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        k\n        \n          F\n          \n            i\n          \n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle y_{i}=kF_{i}+\\varepsilon _{i}.\\,}\n  There are many methods we might use to estimate the unknown parameter k. Since the n equations in the m variables in our data comprise an overdetermined system with one unknown and n equations, we estimate k using least squares.  The sum of squares to be minimized is\n\n  \n    \n      \n        S\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        \u2212\n        k\n        \n          F\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle S=\\sum _{i=1}^{n}(y_{i}-kF_{i})^{2}.}\n  The least squares estimate of the force constant, k, is given by\n\n  \n    \n      \n        \n          \n            \n              k\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                F\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n            \n            \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                F\n                \n                  i\n                \n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {k}}={\\frac {\\sum _{i}F_{i}y_{i}}{\\sum _{i}F_{i}^{2}}}.}\n  We assume that applying force causes the spring to expand. After having derived the force constant by least squares fitting, we predict the extension from Hooke's law. \n\n\n== Uncertainty quantification ==\nIn a least squares calculation with unit weights, or in linear regression, the variance on the jth parameter,\ndenoted \n  \n    \n      \n        var\n        \u2061\n        (\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {var} ({\\hat {\\beta }}_{j})}\n  , is usually estimated with\n\n  \n    \n      \n        var\n        \u2061\n        (\n        \n          \n            \n              \n                \u03b2\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        )\n        =\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          \n            (\n            \n              \n                [\n                \n                  \n                    X\n                    \n                      \n                        T\n                      \n                    \n                  \n                  X\n                \n                ]\n              \n              \n                \u2212\n                1\n              \n            \n            )\n          \n          \n            j\n            j\n          \n        \n        \u2248\n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \n          C\n          \n            j\n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {var} ({\\hat {\\beta }}_{j})=\\sigma ^{2}\\left(\\left[X^{\\mathsf {T}}X\\right]^{-1}\\right)_{jj}\\approx {\\hat {\\sigma }}^{2}C_{jj},}\n  \n\n  \n    \n      \n        \n          \n            \n              \n                \u03c3\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        \u2248\n        \n          \n            S\n            \n              n\n              \u2212\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}^{2}\\approx {\\frac {S}{n-m}}}\n  \n\n  \n    \n      \n        C\n        =\n        \n          \n            (\n            \n              \n                X\n                \n                  \n                    T\n                  \n                \n              \n              X\n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle C=\\left(X^{\\mathsf {T}}X\\right)^{-1},}\n  where the true error variance \u03c32 is replaced by an estimate, the reduced chi-squared statistic, based on the minimized value of the residual sum of squares (objective function), S. The denominator, n \u2212 m, is the statistical degrees of freedom; see effective degrees of freedom for generalizations. C is the  covariance matrix.\n\n\n== Statistical testing ==\nIf the probability distribution of the parameters is known or an asymptotic approximation is made, confidence limits can be found. Similarly, statistical tests on the residuals can be conducted if the probability distribution of the residuals is known or assumed. We can derive the probability distribution of any linear combination of the dependent variables if the probability distribution of experimental errors is known or assumed. Inferring is easy when assuming that the errors follow a normal distribution, consequently implying that the parameter estimates and residuals will also be normally distributed conditional on the values of the independent variables.It is necessary to make assumptions about the nature of the experimental errors to test the results statistically. A common assumption is that the errors belong to a normal distribution. The central limit theorem supports the idea that this is a good approximation in many cases.\n\nThe Gauss\u2013Markov theorem. In a linear model in which the errors have expectation zero conditional on the independent variables, are uncorrelated and have equal variances, the best linear unbiased estimator of any linear combination of the observations, is its least-squares estimator. \"Best\" means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all belong to the same distribution.\nIf the errors belong to a normal distribution, the least-squares estimators are also the maximum likelihood estimators in a linear model.However, suppose the errors are not normally distributed. In that case, a central limit theorem often nonetheless implies that the parameter estimates will be approximately normally distributed so long as the sample is reasonably large.  For this reason, given the important property that the error mean is independent of the independent variables, the distribution of the error term is not an important issue in regression analysis.  Specifically, it is not typically important whether the error term follows a normal distribution.\n\n\n== Weighted least squares ==\n\nA special case of generalized least squares called weighted least squares occurs when all the off-diagonal entries of \u03a9 (the correlation matrix of the residuals) are null; the variances of the observations (along the covariance matrix diagonal) may still be unequal (heteroscedasticity). In simpler terms, heteroscedasticity is when the variance of \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   depends on the value of \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   which causes the residual plot to create a \"fanning out\" effect towards larger \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   values as seen in the residual plot to the right. On the other hand, homoscedasticity is assuming that the variance of \n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n   and variance of \n  \n    \n      \n        \n          U\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle U_{i}}\n   are equal.  \n\n\n== Relationship to principal components ==\nThe first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points (as measured by squared distance of closest approach, i.e. perpendicular to the line).  In contrast, linear least squares tries to minimize the distance in the \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   direction only.  Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.\n\n\n== Relationship to measure theory ==\nNotable statistician Sara van de Geer used empirical process theory and the Vapnik\u2013Chervonenkis dimension to prove a least-squares estimator can be interpreted as a measure on the space of square-integrable functions.\n\n\n== Regularization ==\n\n\n=== Tikhonov regularization ===\n\nIn some contexts a regularized version of the least squares solution may be preferable. Tikhonov regularization (or ridge regression) adds a constraint that \n  \n    \n      \n        \u2016\n        \u03b2\n        \n          \u2016\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|\\beta \\|_{2}^{2}}\n  , the squared \n  \n    \n      \n        \n          \u2113\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\ell _{2}}\n  -norm of the parameter vector, is not greater than a given value to the least squares formulation, leading to a constrained minimization problem. This is equivalent to the unconstrained minimization problem where the objective function is the residual sum of squares plus a penalty term \n  \n    \n      \n        \u03b1\n        \u2016\n        \u03b2\n        \n          \u2016\n          \n            2\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\alpha \\|\\beta \\|_{2}^{2}}\n   and \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is a tuning parameter (this is the Lagrangian form of the constrained minimization problem).In a Bayesian context, this is equivalent to placing a zero-mean normally distributed prior on the parameter vector.\n\n\n=== Lasso method ===\nAn alternative regularized version of least squares is Lasso (least absolute shrinkage and selection operator), which uses the constraint that \n  \n    \n      \n        \u2016\n        \u03b2\n        \n          \u2016\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\|\\beta \\|_{1}}\n  , the L1-norm of the parameter vector, is no greater than a given value. (One can show like above using Lagrange multipliers that this is equivalent to an unconstrained minimization of the least-squares penalty with \n  \n    \n      \n        \u03b1\n        \u2016\n        \u03b2\n        \n          \u2016\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\alpha \\|\\beta \\|_{1}}\n   added.) In a Bayesian context, this is equivalent to placing a zero-mean Laplace prior distribution on the parameter vector. The optimization problem may be solved using quadratic programming or more general convex optimization methods, as well as by specific algorithms such as the least angle regression algorithm.\nOne of the prime differences between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause more and more of the parameters to be driven to zero. This is an advantage of Lasso over ridge regression, as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas Ridge regression never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples,  and FeaLect which analyzes the regression coefficients corresponding to different values of \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   to score all the features.The L1-regularized formulation is useful in some contexts due to its tendency to prefer solutions where more parameters are zero, which gives solutions that depend on fewer variables. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. An extension of this approach is elastic net regularization.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nBj\u00f6rck, \u00c5. (1996). Numerical Methods for Least Squares Problems. SIAM. ISBN 978-0-89871-360-2.\nKariya, T.; Kurata, H. (2004). Generalized Least Squares. Hoboken: Wiley. ISBN 978-0-470-86697-9.\nLuenberger, D. G. (1997) [1969]. \"Least-Squares Estimation\". Optimization by Vector Space Methods. New York: John Wiley & Sons. pp. 78\u2013102. ISBN 978-0-471-18117-0.\nRao, C. R.; Toutenburg, H.;  et al. (2008). Linear Models: Least Squares and Alternatives. Springer Series in Statistics (3rd ed.). Berlin: Springer. ISBN 978-3-540-74226-5.\nVan de moortel, Koen (April 2021). \"Multidirectional regression analysis\".\nWolberg, J. (2005). Data Analysis Using the Method of Least Squares: Extracting the Most Information from Experiments. Berlin: Springer. ISBN 978-3-540-25674-8.\n\n\n== External links ==\n Media related to Least squares at Wikimedia Commons", "Alpha\u2013beta pruning": "Alpha\u2013beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Connect 4, etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.\n\n\n== History ==\nAllen Newell and Herbert A. Simon who used what John McCarthy calls an \"approximation\" in 1958 wrote that alpha\u2013beta \"appears to have been reinvented a number of times\". Arthur Samuel had an early version for a checkers simulation. Richards, Timothy Hart, Michael Levin and/or Daniel Edwards also invented alpha\u2013beta independently in the United States. McCarthy proposed similar ideas during the Dartmouth workshop in 1956 and suggested it to a group of his students including Alan Kotok at MIT in 1961. Alexander Brudno independently conceived the alpha\u2013beta algorithm, publishing his results in 1963. Donald Knuth and Ronald W. Moore refined the algorithm in 1975. Judea Pearl proved its optimality in terms of the expected running time for trees with randomly assigned leaf values in two papers. The optimality of the randomized version of alpha\u2013beta was shown by Michael Saks and Avi Wigderson in 1986.\n\n\n== Core idea ==\nA game tree can represent many two-player zero-sum games, such as chess, checkers, and reversi. Each node in the tree represents a possible situation in the game. Each terminal node (outcome) of a branch is assigned a numeric score that determines the value of the outcome to the player with the next move.The algorithm maintains two values, alpha and beta, which respectively represent the minimum score that the maximizing player is assured of and the maximum score that the minimizing player is assured of. Initially, alpha is negative infinity and beta is positive infinity, i.e. both players start with their worst possible score. Whenever the maximum score that the minimizing player (i.e. the \"beta\" player) is assured of becomes less than the minimum score that the maximizing player (i.e., the \"alpha\" player) is assured of (i.e. beta < alpha), the maximizing player need not consider further descendants of this node, as they will never be reached in the actual play.\nTo illustrate this with a real-life example, suppose somebody is playing chess, and it is their turn. Move \"A\" will improve the player's position.  The player continues to look for moves to make sure a better one hasn't been missed.  Move \"B\" is also a good move, but the player then realizes that it will allow the opponent to force checkmate in two moves. Thus, other outcomes from playing move B no longer need to be considered since the opponent can force a win. The maximum score that the opponent could force after move \"B\" is negative infinity: a loss for the player. This is less than the minimum position that was previously found; move \"A\" does not result in a forced loss in two moves.\n\n\n== Improvements over naive minimax ==\n\nThe benefit of alpha\u2013beta pruning lies in the fact that branches of the search tree can be eliminated. This way, the search time can be limited to the 'more promising' subtree, and a deeper search can be performed in the same time. Like its predecessor, it belongs to the branch and bound class of algorithms. The optimization reduces the effective depth to slightly more than half that of simple minimax if the nodes are evaluated in an optimal or near optimal order (best choice for side on move ordered first at each node).\nWith an (average or constant) branching factor of b, and a search depth of d plies, the maximum number of leaf node positions evaluated (when the move ordering is pessimal) is O(b\u00d7b\u00d7...\u00d7b) = O(bd) \u2013 the same as a simple minimax search. If the move ordering for the search is optimal (meaning the best moves are always searched first), the number of leaf node positions evaluated is about O(b\u00d71\u00d7b\u00d71\u00d7...\u00d7b) for odd depth and O(b\u00d71\u00d7b\u00d71\u00d7...\u00d71) for even depth, or \n  \n    \n      \n        O\n        (\n        \n          b\n          \n            d\n            \n              /\n            \n            2\n          \n        \n        )\n        =\n        O\n        (\n        \n          \n            \n              b\n              \n                d\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O(b^{d/2})=O({\\sqrt {b^{d}}})}\n  . In the latter case, where the ply of a search is even, the effective branching factor is reduced to its square root, or, equivalently, the search can go twice as deep with the same amount of computation. The explanation of b\u00d71\u00d7b\u00d71\u00d7... is that all the first player's moves must be studied to find the best one, but for each, only the second player's best move is needed to refute all but the first (and best) first player move\u2014alpha\u2013beta ensures no other second player moves need be considered. When nodes are considered in a random order (i.e., the algorithm randomizes), asymptotically,\nthe expected number of nodes evaluated in uniform trees with binary leaf-values is \n  \n    \n      \n        \u0398\n        (\n        (\n        (\n        b\n        \u2212\n        1\n        +\n        \n          \n            \n              b\n              \n                2\n              \n            \n            +\n            14\n            b\n            +\n            1\n          \n        \n        )\n        \n          /\n        \n        4\n        \n          )\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Theta (((b-1+{\\sqrt {b^{2}+14b+1}})/4)^{d})}\n  \n.\nFor the same trees, when the values are assigned to the leaf values independently of each other and say zero and one are both equally probable, the expected number of nodes evaluated is \n  \n    \n      \n        \u0398\n        (\n        (\n        b\n        \n          /\n        \n        2\n        \n          )\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Theta ((b/2)^{d})}\n  , which is much smaller than the work done by the randomized algorithm, mentioned above, and is again optimal for such random trees. When the leaf values are chosen independently of each other but from the \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n   interval uniformly at random, the expected number of nodes evaluated increases to \n  \n    \n      \n        \u0398\n        (\n        \n          b\n          \n            d\n            \n              /\n            \n            l\n            o\n            g\n            (\n            d\n            )\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Theta (b^{d/log(d)})}\n   in the \n  \n    \n      \n        d\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle d\\to \\infty }\n   limit, which is again optimal for these kind random trees. Note that the actual work for \"small\" values of \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   is better approximated using \n  \n    \n      \n        0.925\n        \n          d\n          \n            0.747\n          \n        \n      \n    \n    {\\displaystyle 0.925d^{0.747}}\n  .A chess program that searches four plies with an average of 36 branches per node evaluates more than one million terminal nodes. An optimal alpha-beta prune would eliminate all but about 2,000 terminal nodes, a reduction of 99.8%.\n\nNormally during alpha\u2013beta, the subtrees are temporarily dominated by either a first player advantage (when many first player moves are good, and at each search depth the first move checked by the first player is adequate, but all second player responses are required to try to find a refutation), or vice versa. This advantage can switch sides many times during the search if the move ordering is incorrect, each time leading to inefficiency. As the number of positions searched decreases exponentially each move nearer the current position, it is worth spending considerable effort on sorting early moves. An improved sort at any depth will exponentially reduce the total number of positions searched, but sorting all positions at depths near the root node is relatively cheap as there are so few of them.  In practice, the move ordering is often determined by the results of earlier, smaller searches, such as through iterative deepening.\nAdditionally, this algorithm can be trivially modified to return an entire principal variation in addition to the score. Some more aggressive algorithms such as MTD(f) do not easily permit such a modification.\n\n\n== Pseudocode ==\nThe pseudo-code for depth limited minimax with alpha\u2013beta pruning is as follows:Implementations of alpha\u2013beta pruning can often be delineated by whether they are \"fail-soft,\" or \"fail-hard\".  With fail-soft alpha\u2013beta, the alphabeta function may return values (v) that exceed (v < \u03b1 or v > \u03b2) the \u03b1 and \u03b2 bounds set by its function call arguments. In comparison, fail-hard alpha\u2013beta limits its function return value into the inclusive range of \u03b1 and \u03b2. The main difference between fail-soft and fail-hard implementations is whether \u03b1 and \u03b2 are updated before or after the cutoff check. If they are updated before the check, then they can exceed initial bounds and the algorithm is fail-soft.\nThe following pseudo-code illustrates the fail-hard variation.\nfunction alphabeta(node, depth, \u03b1, \u03b2, maximizingPlayer) is\n    if depth == 0 or node is terminal then\n        return the heuristic value of node\n    if maximizingPlayer then\n        value := \u2212\u221e\n        for each child of node do\n            value := max(value, alphabeta(child, depth \u2212 1, \u03b1, \u03b2, FALSE))\n            if value > \u03b2 then\n                break (* \u03b2 cutoff *)\n            \u03b1 := max(\u03b1, value)\n        return value\n    else\n        value := +\u221e\n        for each child of node do\n            value := min(value, alphabeta(child, depth \u2212 1, \u03b1, \u03b2, TRUE))\n            if value < \u03b1 then\n                break (* \u03b1 cutoff *)\n            \u03b2 := min(\u03b2, value)\n        return value\n\n(* Initial call *)\nalphabeta(origin, depth, \u2212\u221e, +\u221e, TRUE)\n\nThe following pseudocode illustrates fail-soft alpha-beta.\n\nfunction alphabeta(node, depth, \u03b1, \u03b2, maximizingPlayer) is\n    if depth = 0 or node is a terminal node then\n        return the heuristic value of node\n    if maximizingPlayer then\n        value := \u2212\u221e\n        for each child of node do\n            value := max(value, alphabeta(child, depth \u2212 1, \u03b1, \u03b2, FALSE))\n            \u03b1 := max(\u03b1, value)\n            if value \u2265 \u03b2 then\n                break (* \u03b2 cutoff *)\n        return value\n    else\n        value := +\u221e\n        for each child of node do\n            value := min(value, alphabeta(child, depth \u2212 1, \u03b1, \u03b2, TRUE))\n            \u03b2 := min(\u03b2, value)\n            if value \u2264 \u03b1 then\n                break (* \u03b1 cutoff *)\n        return value\n\n(* Initial call *)\nalphabeta(origin, depth, \u2212\u221e, +\u221e, TRUE)\n\n\n== Heuristic improvements ==\nFurther improvement can be achieved without sacrificing accuracy by using ordering heuristics to search earlier parts of the tree that are likely to force alpha\u2013beta cutoffs. For example, in chess, moves that capture pieces may be examined before moves that do not, and moves that have scored highly in earlier passes through the game-tree analysis may be evaluated before others. Another common, and very cheap, heuristic is the killer heuristic, where the last move that caused a beta-cutoff at the same tree level in the tree search is always examined first. This idea can also be generalized into a set of refutation tables.\nAlpha\u2013beta search can be made even faster by considering only a narrow search window (generally determined by guesswork based on experience). This is known as aspiration search. In the extreme case, the search is performed with alpha and beta equal; a technique known as zero-window search, null-window search, or scout search. This is particularly useful for win/loss searches near the end of a game where the extra depth gained from the narrow window and a simple win/loss evaluation function may lead to a conclusive result. If an aspiration search fails, it is straightforward to detect whether it failed high (high edge of window was too low) or low (lower edge of window was too high). This gives information about what window values might be useful in a re-search of the position.\nOver time, other improvements have been suggested, and indeed the Falphabeta (fail-soft alpha\u2013beta) idea of John Fishburn is nearly universal and is already incorporated above in a slightly modified form. Fishburn also suggested a combination of the killer heuristic and zero-window search under the name Lalphabeta (\"last move with minimal window alpha\u2013beta search\").\n\n\n== Other algorithms ==\nSince the minimax algorithm and its variants are inherently depth-first, a strategy such as iterative deepening is usually used in conjunction with alpha\u2013beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using iterative deepening is that searches at shallower depths give move-ordering hints, as well as shallow alpha and beta estimates, that both can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.\nAlgorithms like SSS*, on the other hand, use the best-first strategy.  This can potentially make them more time-efficient, but typically at a heavy cost in space-efficiency.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nHeineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"7. Path Finding in AI\". Algorithms in a Nutshell. Oreilly Media. pp. 217\u2013223. ISBN 978-0-596-51624-6.\nPearl, Judea (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley. ISBN 978-0-201-05594-8. OCLC 1035596197.\nFishburn, John P. (1984). \"Appendix A: Some Optimizations of \u03b1-\u03b2 Search\". Analysis of Speedup in Distributed Algorithms (revision of 1981 PhD thesis). UMI Research Press. pp. 107\u2013111. ISBN 0-8357-1527-2.", "Histogram": "A histogram is an approximate representation of the distribution of numerical data. The term was first introduced by Karl Pearson.  To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent and are often (but not required to be) of equal size.If the bins are of equal size, a bar is drawn over the bin with height proportional to the frequency\u2014the number of cases in each bin.  A histogram may also be normalized to display \"relative\" frequencies showing the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1.\nHowever, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below.\nAs the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous.Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.\nThe histogram is one of the seven basic tools of quality control.Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.A bar graph and a histogram are two common types of graphical representations of data. While they may look similar, there are some key differences between the two that are important to understand.\nA bar graph is a chart that uses bars to represent the frequency or quantity of different categories of data. The bars can be either vertical or horizontal, and they are typically arranged either horizontally or vertically to make it easy to compare the different categories. Bar graphs are useful for displaying data that can be divided into discrete categories, such as the number of students in different grade levels at a school.\nA histogram, on the other hand, is a graph that shows the distribution of numerical data. It is a type of bar chart that shows the frequency or number of observations within different numerical ranges, called bins. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The histogram provides a visual representation of the distribution of the data, showing the number of observations that fall within each bin. This can be useful for identifying patterns and trends in the data, and for making comparisons between different datasets.\n\n\n== Examples ==\nThis is the data for the histogram to the right, using 500 items:\n\nThe words used to describe the patterns in a histogram are: \"symmetric\", \"skewed left\" or \"right\", \"unimodal\", \"bimodal\" or \"multimodal\".\n\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\nIt is a good idea to plot the data using several different bin widths to learn more about it. Here is an example on tips given in a restaurant.\n\n\t\t\nThe U.S. Census Bureau found that there were 124 million people who work outside of their homes.  Using their data on the time occupied by travel to work, the table below shows the absolute number of people who responded with travel times \"at least 30 but less than 35 minutes\" is higher than the numbers for the categories above and below it. This is likely due to people rounding their reported journey time. The problem of reporting values as somewhat arbitrarily rounded numbers is a common phenomenon when collecting data from people.\n\nThis histogram shows the number of cases per unit interval as the height of each block, so that the area of each block is equal to the number of people in the survey who fall into its category. The area under the curve represents the total number of cases (124 million). This type of histogram shows absolute numbers, with Q in thousands.\n\nThis histogram differs from the first only in the vertical scale.  The area of each block is the fraction of the total that each category represents, and the total area of all the bars is equal to 1 (the fraction meaning \"all\"). The curve displayed is a simple density estimate. This version shows proportions, and is also known as a unit area histogram.\n\nIn other words, a histogram represents a frequency distribution by means of rectangles whose widths represent class intervals and whose areas are proportional to the corresponding frequencies: the height of each is the average frequency density for the interval. The intervals are placed together in order to show that the data represented by the histogram, while exclusive, is also contiguous. (E.g., in a histogram it is possible to have two connecting intervals of 10.5\u201320.5 and 20.5\u201333.5, but not two connecting intervals of 10.5\u201320.5 and 22.5\u201332.5.  Empty intervals are represented as empty and not skipped.)\n\n\n== Mathematical definitions ==\n\nThe data used to construct a histogram are generated via a function mi that counts the number of observations that fall into each of the disjoint categories (known as bins). Thus, if we let n be the total number of observations and k be the total number of bins, the histogram data mi meet the following conditions:\n\n  \n    \n      \n        n\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            k\n          \n        \n        \n          \n            m\n            \n              i\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle n=\\sum _{i=1}^{k}{m_{i}}.}\n  A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently.\nAn alternative to kernel density estimation is the average shifted histogram,\nwhich is fast to compute and gives a smooth curve estimate of the density without using kernels.\n\n\n=== Cumulative histogram ===\nA cumulative histogram is a mapping that counts the cumulative number of observations in all of the bins up to the specified bin. That is, the cumulative histogram Mi of a histogram mj is defined as:\n\n  \n    \n      \n        \n          M\n          \n            i\n          \n        \n        =\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            i\n          \n        \n        \n          \n            m\n            \n              j\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle M_{i}=\\sum _{j=1}^{i}{m_{j}}.}\n  \n\n\n=== Number of bins and width ===\nThere is no \"best\" number of bins, and different bin sizes can reveal different features of the data.  Grouping data is at least as old as Graunt's work in the 17th century, but no systematic guidelines were given until Sturges' work in 1926.Using wider bins where the density of the underlying data points is low reduces noise due to sampling randomness; using narrower bins where the density is high (so the signal drowns the noise) gives greater precision to the density estimation.  Thus varying the bin-width within a histogram can be beneficial.  Nonetheless, equal-width bins are widely used.\nSome theoreticians have attempted to determine an optimal number of bins, but these methods generally make strong assumptions about the shape of the distribution.  Depending on the actual data distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.The number of bins k can be assigned directly or can be calculated from a suggested bin width h as:\n\n  \n    \n      \n        k\n        =\n        \n          \u2308\n          \n            \n              \n                max\n                x\n                \u2212\n                min\n                x\n              \n              h\n            \n          \n          \u2309\n        \n        .\n      \n    \n    {\\displaystyle k=\\left\\lceil {\\frac {\\max x-\\min x}{h}}\\right\\rceil .}\n  The braces indicate the ceiling function.\n\n\n==== Square-root choice ====\n\n  \n    \n      \n        k\n        =\n        \u2308\n        \n          \n            n\n          \n        \n        \u2309\n        \n      \n    \n    {\\displaystyle k=\\lceil {\\sqrt {n}}\\rceil \\,}\n  which takes the square root of the number  of  data  points  in  the  sample (used by Excel's Analysis Toolpak histograms and many other) and rounds to the next integer.\n\n\n==== Sturges' formula ====\nSturges' formula is derived from a binomial distribution and implicitly assumes an approximately normal distribution.\n\n  \n    \n      \n        k\n        =\n        \u2308\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        n\n        \u2309\n        +\n        1\n        ,\n        \n      \n    \n    {\\displaystyle k=\\lceil \\log _{2}n\\rceil +1,\\,}\n  Sturges' formula implicitly bases bin sizes on the range of the data, and can perform poorly if n < 30, because the number of bins will be small\u2014less than seven\u2014and unlikely to show trends in the data well. On the other extreme, Sturges' formula may overestimate bin width for very large datasets, resulting in oversmoothed histograms. It may also perform poorly if the data are not normally distributed.\nWhen compared to Scott's rule and the Terrell-Scott rule, two other widely accepted formulas for histogram bins, the output of Sturges' formula is closest when n \u2248 100.\n\n\n==== Rice rule ====\n\n  \n    \n      \n        k\n        =\n        \u2308\n        2\n        \n          \n            n\n            \n              3\n            \n          \n        \n        \u2309\n        ,\n      \n    \n    {\\displaystyle k=\\lceil 2{\\sqrt[{3}]{n}}\\rceil ,}\n  The Rice Rule  is presented as a simple alternative to Sturges' rule.\n\n\n==== Doane's formula ====\nDoane's formula is a modification of Sturges' formula which attempts to improve its performance with non-normal data.\n\n  \n    \n      \n        k\n        =\n        1\n        +\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        n\n        )\n        +\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        \n          (\n          \n            1\n            +\n            \n              \n                \n                  \n                    |\n                  \n                  \n                    g\n                    \n                      1\n                    \n                  \n                  \n                    |\n                  \n                \n                \n                  \u03c3\n                  \n                    \n                      g\n                      \n                        1\n                      \n                    \n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle k=1+\\log _{2}(n)+\\log _{2}\\left(1+{\\frac {|g_{1}|}{\\sigma _{g_{1}}}}\\right)}\n  where \n  \n    \n      \n        \n          g\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle g_{1}}\n   is the estimated 3rd-moment-skewness of the distribution and\n\n  \n    \n      \n        \n          \u03c3\n          \n            \n              g\n              \n                1\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                6\n                (\n                n\n                \u2212\n                2\n                )\n              \n              \n                (\n                n\n                +\n                1\n                )\n                (\n                n\n                +\n                3\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma _{g_{1}}={\\sqrt {\\frac {6(n-2)}{(n+1)(n+3)}}}}\n  \n\n\n==== Scott's normal reference rule ====\nBin width \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is given by\n\n  \n    \n      \n        h\n        =\n        \n          \n            \n              3.49\n              \n                \n                  \n                    \u03c3\n                    ^\n                  \n                \n              \n            \n            \n              n\n              \n                3\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle h={\\frac {3.49{\\hat {\\sigma }}}{\\sqrt[{3}]{n}}},}\n  where \n  \n    \n      \n        \n          \n            \n              \u03c3\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}}\n   is the sample standard deviation. Scott's normal reference rule is optimal for random samples of normally distributed data, in the sense that it minimizes the integrated mean squared error of the density estimate.\n\n\n==== Freedman\u2013Diaconis' choice ====\nThe Freedman\u2013Diaconis rule gives bin width \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   as:\n\n  \n    \n      \n        h\n        =\n        2\n        \n          \n            \n              IQR\n              \u2061\n              (\n              x\n              )\n            \n            \n              n\n              \n                3\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle h=2{\\frac {\\operatorname {IQR} (x)}{\\sqrt[{3}]{n}}},}\n  which is based on the interquartile range, denoted by IQR. It replaces 3.5\u03c3 of Scott's rule with 2 IQR, which is less sensitive than the standard deviation to outliers in data.\n\n\n==== Minimizing cross-validation estimated squared error ====\nThis approach of minimizing integrated mean squared error from Scott's rule can be generalized beyond normal distributions, by using leave-one out cross validation:\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            h\n          \n        \n        \n          \n            \n              J\n              ^\n            \n          \n        \n        (\n        h\n        )\n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            h\n          \n        \n        \n          (\n          \n            \n              \n                2\n                \n                  (\n                  n\n                  \u2212\n                  1\n                  )\n                  h\n                \n              \n            \n            \u2212\n            \n              \n                \n                  n\n                  +\n                  1\n                \n                \n                  \n                    n\n                    \n                      2\n                    \n                  \n                  (\n                  n\n                  \u2212\n                  1\n                  )\n                  h\n                \n              \n            \n            \n              \u2211\n              \n                k\n              \n            \n            \n              N\n              \n                k\n              \n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\underset {h}{\\operatorname {arg\\,min} }}{\\hat {J}}(h)={\\underset {h}{\\operatorname {arg\\,min} }}\\left({\\frac {2}{(n-1)h}}-{\\frac {n+1}{n^{2}(n-1)h}}\\sum _{k}N_{k}^{2}\\right)}\n  Here, \n  \n    \n      \n        \n          N\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle N_{k}}\n   is the number of datapoints in the kth bin, and choosing the value of h that minimizes J will minimize integrated mean squared error.\n\n\n==== Shimazaki and Shinomoto's choice ====\nThe choice is based on minimization of an estimated L2 risk function\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            h\n          \n        \n        \n          \n            \n              2\n              \n                \n                  \n                    m\n                    \u00af\n                  \n                \n              \n              \u2212\n              v\n            \n            \n              h\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\underset {h}{\\operatorname {arg\\,min} }}{\\frac {2{\\bar {m}}-v}{h^{2}}}}\n  where \n  \n    \n      \n        \n          \n            \n              \n                m\n                \u00af\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle {\\bar {m}}}\n   and \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\textstyle v}\n   are mean and biased variance of a histogram with bin-width \n  \n    \n      \n        \n          h\n        \n      \n    \n    {\\displaystyle \\textstyle h}\n  , \n  \n    \n      \n        \n          \n            \n              \n                m\n                \u00af\n              \n            \n          \n          =\n          \n            \n              1\n              k\n            \n          \n          \n            \u2211\n            \n              i\n              =\n              1\n            \n            \n              k\n            \n          \n          \n            m\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle {\\bar {m}}={\\frac {1}{k}}\\sum _{i=1}^{k}m_{i}}\n   and \n  \n    \n      \n        \n          v\n          =\n          \n            \n              1\n              k\n            \n          \n          \n            \u2211\n            \n              i\n              =\n              1\n            \n            \n              k\n            \n          \n          (\n          \n            m\n            \n              i\n            \n          \n          \u2212\n          \n            \n              \n                m\n                \u00af\n              \n            \n          \n          \n            )\n            \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle v={\\frac {1}{k}}\\sum _{i=1}^{k}(m_{i}-{\\bar {m}})^{2}}\n  .\n\n\n==== Variable bin widths ====\nRather than choosing evenly spaced bins, for some applications it is preferable to vary the bin width. This avoids bins with low counts. A common case is to choose equiprobable bins, where the number of samples in each bin is expected to be approximately equal. The bins may be chosen according to some known distribution or may be chosen based on the data so that each bin has \n  \n    \n      \n        \u2248\n        n\n        \n          /\n        \n        k\n      \n    \n    {\\displaystyle \\approx n/k}\n   samples. When plotting the histogram, the frequency density is used for the dependent axis. While all bins have approximately equal area, the heights of the histogram approximate the density distribution.\nFor equiprobable bins, the following rule for the number of bins is suggested:\n\n  \n    \n      \n        k\n        =\n        2\n        \n          n\n          \n            2\n            \n              /\n            \n            5\n          \n        \n      \n    \n    {\\displaystyle k=2n^{2/5}}\n  This choice of bins is motivated by maximizing the power of a Pearson chi-squared test testing whether the bins do contain equal numbers of samples. More specifically, for a given confidence interval \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   it is recommended to choose between 1/2 and 1 times the following equation:\n\n  \n    \n      \n        k\n        =\n        4\n        \n          \n            (\n            \n              \n                \n                  2\n                  \n                    n\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    \u03a6\n                    \n                      \u2212\n                      1\n                    \n                  \n                  (\n                  \u03b1\n                  )\n                \n              \n            \n            )\n          \n          \n            \n              1\n              5\n            \n          \n        \n      \n    \n    {\\displaystyle k=4\\left({\\frac {2n^{2}}{\\Phi ^{-1}(\\alpha )}}\\right)^{\\frac {1}{5}}}\n  Where \n  \n    \n      \n        \n          \u03a6\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle \\Phi ^{-1}}\n   is the probit function. Following this rule for \n  \n    \n      \n        \u03b1\n        =\n        0.05\n      \n    \n    {\\displaystyle \\alpha =0.05}\n   would give between \n  \n    \n      \n        1.88\n        \n          n\n          \n            2\n            \n              /\n            \n            5\n          \n        \n      \n    \n    {\\displaystyle 1.88n^{2/5}}\n   and \n  \n    \n      \n        3.77\n        \n          n\n          \n            2\n            \n              /\n            \n            5\n          \n        \n      \n    \n    {\\displaystyle 3.77n^{2/5}}\n  ; the coefficient of 2 is chosen as an easy-to-remember value from this broad optimum.\n\n\n==== Remark ====\nA good reason why the number of bins should be proportional to \n  \n    \n      \n        \n          \n            n\n            \n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt[{3}]{n}}}\n   is the following: suppose that the data are obtained as \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally \"rugged\" as \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   tends to infinity. If \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   is the \"width\" of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order \n  \n    \n      \n        n\n        h\n        \n          /\n        \n        s\n      \n    \n    {\\displaystyle nh/s}\n   and the relative standard error is of order \n  \n    \n      \n        \n          \n            s\n            \n              /\n            \n            (\n            n\n            h\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {s/(nh)}}}\n  . Compared to the next bin, the relative change of the frequency is of order \n  \n    \n      \n        h\n        \n          /\n        \n        s\n      \n    \n    {\\displaystyle h/s}\n   provided that the derivative of the density is non-zero. These two are of the same order if \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is of order \n  \n    \n      \n        s\n        \n          /\n        \n        \n          \n            n\n            \n              3\n            \n          \n        \n      \n    \n    {\\displaystyle s/{\\sqrt[{3}]{n}}}\n  , so that \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is of order \n  \n    \n      \n        \n          \n            n\n            \n              3\n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt[{3}]{n}}}\n  . This simple cubic root choice can also be applied to bins with non-constant widths.\n\n\n== Applications ==\nIn hydrology the histogram and estimated density function of rainfall and river discharge data, analysed with a probability distribution, are used to gain insight in their behaviour and frequency of occurrence. An example is shown in the blue figure.\nIn many Digital image processing programs there is an histogram tool, which show you the distribution of the contrast / brightness of the pixels. \n\n\n== See also ==\n\nData and information visualization\nData binning\nDensity estimation\nKernel density estimation, a smoother but more complex method of density estimation\nEntropy estimation\nFreedman\u2013Diaconis rule\nImage histogram\nPareto chart\nSeven basic tools of quality\nV-optimal histograms\n\n\n== References ==\n\n\n== Further reading ==\nLancaster, H.O. An Introduction to Medical Statistics. John Wiley and Sons. 1974. ISBN 0-471-51250-8\n\n\n== External links ==\n\nExploring Histograms, an essay by Aran Lunzer and Amelia McNamara\nJourney To Work and Place Of Work (location of census document cited in example)\nSmooth histogram for signals and images from a few samples\nHistograms: Construction, Analysis and Understanding with external links and an application to particle Physics.\nA Method for Selecting the Bin Size of a Histogram\nHistograms: Theory and Practice, some great illustrations of some of the Bin Width concepts derived above.\nHistograms the Right Way\nInteractive histogram generator\nMatlab function to plot nice histograms\nDynamic Histogram in MS Excel\nHistogram construction and manipulation using Java applets, and charts on SOCR\nToolbox for constructing the best histograms", "Discretization": "In applied mathematics, discretization is the process of transferring continuous functions, models, variables, and equations into discrete counterparts.  This process is usually carried out as a first step toward making them suitable for numerical evaluation and implementation on digital computers. Dichotomization is the special case of discretization in which the number of discrete classes is 2, which can approximate a continuous variable as a binary variable (creating a dichotomy for modeling purposes, as in binary classification).\nDiscretization is also related to discrete mathematics, and is an important component of granular computing.  In this context, discretization may also refer to modification of variable or category granularity, as when multiple discrete variables are aggregated or multiple discrete categories fused.\nWhenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand.\n The terms discretization  and quantization often have the same denotation but not always identical connotations. (Specifically, the two terms share a semantic field.) The same is true of discretization error and quantization error.\nMathematical methods relating to discretization include the Euler\u2013Maruyama method and the zero-order hold.\n\n\n== Discretization of linear state space models ==\nDiscretization is also concerned with the transformation of continuous differential equations into discrete difference equations, suitable for numerical computing.\nThe following continuous-time state space model\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              \u02d9\n            \n          \n        \n        (\n        t\n        )\n        =\n        \n          A\n        \n        \n          x\n        \n        (\n        t\n        )\n        +\n        \n          B\n        \n        \n          u\n        \n        (\n        t\n        )\n        +\n        \n          w\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle {\\dot {\\mathbf {x} }}(t)=\\mathbf {A} \\mathbf {x} (t)+\\mathbf {B} \\mathbf {u} (t)+\\mathbf {w} (t)}\n  \n\n  \n    \n      \n        \n          y\n        \n        (\n        t\n        )\n        =\n        \n          C\n        \n        \n          x\n        \n        (\n        t\n        )\n        +\n        \n          D\n        \n        \n          u\n        \n        (\n        t\n        )\n        +\n        \n          v\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {y} (t)=\\mathbf {C} \\mathbf {x} (t)+\\mathbf {D} \\mathbf {u} (t)+\\mathbf {v} (t)}\n  where v and w are continuous zero-mean white noise sources with power spectral densities\n\n  \n    \n      \n        \n          w\n        \n        (\n        t\n        )\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          Q\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {w} (t)\\sim N(0,\\mathbf {Q} )}\n  \n\n  \n    \n      \n        \n          v\n        \n        (\n        t\n        )\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          R\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {v} (t)\\sim N(0,\\mathbf {R} )}\n  can be discretized, assuming zero-order hold for the input u and continuous integration for the noise v, to\n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        +\n        1\n        ]\n        =\n        \n          \n            A\n          \n          \n            d\n          \n        \n        \n          x\n        \n        [\n        k\n        ]\n        +\n        \n          \n            B\n          \n          \n            d\n          \n        \n        \n          u\n        \n        [\n        k\n        ]\n        +\n        \n          w\n        \n        [\n        k\n        ]\n      \n    \n    {\\displaystyle \\mathbf {x} [k+1]=\\mathbf {A} _{d}\\mathbf {x} [k]+\\mathbf {B} _{d}\\mathbf {u} [k]+\\mathbf {w} [k]}\n  \n\n  \n    \n      \n        \n          y\n        \n        [\n        k\n        ]\n        =\n        \n          \n            C\n          \n          \n            d\n          \n        \n        \n          x\n        \n        [\n        k\n        ]\n        +\n        \n          \n            D\n          \n          \n            d\n          \n        \n        \n          u\n        \n        [\n        k\n        ]\n        +\n        \n          v\n        \n        [\n        k\n        ]\n      \n    \n    {\\displaystyle \\mathbf {y} [k]=\\mathbf {C} _{d}\\mathbf {x} [k]+\\mathbf {D} _{d}\\mathbf {u} [k]+\\mathbf {v} [k]}\n  with covariances\n\n  \n    \n      \n        \n          w\n        \n        [\n        k\n        ]\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \n            Q\n          \n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {w} [k]\\sim N(0,\\mathbf {Q} _{d})}\n  \n\n  \n    \n      \n        \n          v\n        \n        [\n        k\n        ]\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \n            R\n          \n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {v} [k]\\sim N(0,\\mathbf {R} _{d})}\n  where\n\n  \n    \n      \n        \n          \n            A\n          \n          \n            d\n          \n        \n        =\n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n        =\n        \n          \n            \n              L\n            \n          \n          \n            \u2212\n            1\n          \n        \n        {\n        (\n        s\n        \n          I\n        \n        \u2212\n        \n          A\n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        \n          }\n          \n            t\n            =\n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} _{d}=e^{\\mathbf {A} T}={\\mathcal {L}}^{-1}\\{(s\\mathbf {I} -\\mathbf {A} )^{-1}\\}_{t=T}}\n  \n\n  \n    \n      \n        \n          \n            B\n          \n          \n            d\n          \n        \n        =\n        \n          (\n          \n            \n              \u222b\n              \n                \u03c4\n                =\n                0\n              \n              \n                T\n              \n            \n            \n              e\n              \n                \n                  A\n                \n                \u03c4\n              \n            \n            d\n            \u03c4\n          \n          )\n        \n        \n          B\n        \n        =\n        \n          \n            A\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            A\n          \n          \n            d\n          \n        \n        \u2212\n        I\n        )\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} _{d}=\\left(\\int _{\\tau =0}^{T}e^{\\mathbf {A} \\tau }d\\tau \\right)\\mathbf {B} =\\mathbf {A} ^{-1}(\\mathbf {A} _{d}-I)\\mathbf {B} }\n  , if \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n   is nonsingular\n\n  \n    \n      \n        \n          \n            C\n          \n          \n            d\n          \n        \n        =\n        \n          C\n        \n      \n    \n    {\\displaystyle \\mathbf {C} _{d}=\\mathbf {C} }\n  \n\n  \n    \n      \n        \n          \n            D\n          \n          \n            d\n          \n        \n        =\n        \n          D\n        \n      \n    \n    {\\displaystyle \\mathbf {D} _{d}=\\mathbf {D} }\n  \n\n  \n    \n      \n        \n          \n            Q\n          \n          \n            d\n          \n        \n        =\n        \n          \u222b\n          \n            \u03c4\n            =\n            0\n          \n          \n            T\n          \n        \n        \n          e\n          \n            \n              A\n            \n            \u03c4\n          \n        \n        \n          Q\n        \n        \n          e\n          \n            \n              \n                A\n              \n              \n                \u22a4\n              \n            \n            \u03c4\n          \n        \n        d\n        \u03c4\n      \n    \n    {\\displaystyle \\mathbf {Q} _{d}=\\int _{\\tau =0}^{T}e^{\\mathbf {A} \\tau }\\mathbf {Q} e^{\\mathbf {A} ^{\\top }\\tau }d\\tau }\n  \n\n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n        =\n        \n          R\n        \n        \n          \n            1\n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {R} _{d}=\\mathbf {R} {\\frac {1}{T}}}\n  and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is the sample time, although \n  \n    \n      \n        \n          \n            A\n          \n          \n            \u22a4\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} ^{\\top }}\n   is the transposed matrix of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  . The equation for the discretized measurement noise is a consequence of the continuous measurement noise being defined with a power spectral density.A clever trick to compute Ad and Bd in one step is by utilizing the following property::\u200ap. 215\u200a\n\n  \n    \n      \n        \n          e\n          \n            \n              \n                [\n                \n                  \n                    \n                      \n                        A\n                      \n                    \n                    \n                      \n                        B\n                      \n                    \n                  \n                  \n                    \n                      \n                        0\n                      \n                    \n                    \n                      \n                        0\n                      \n                    \n                  \n                \n                ]\n              \n            \n            T\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \n                      A\n                      \n                        d\n                      \n                    \n                  \n                \n                \n                  \n                    \n                      B\n                      \n                        d\n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    I\n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle e^{{\\begin{bmatrix}\\mathbf {A} &\\mathbf {B} \\\\\\mathbf {0} &\\mathbf {0} \\end{bmatrix}}T}={\\begin{bmatrix}\\mathbf {A_{d}} &\\mathbf {B_{d}} \\\\\\mathbf {0} &\\mathbf {I} \\end{bmatrix}}}\n  Where \n  \n    \n      \n        \n          \n            A\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} _{d}}\n  and \n  \n    \n      \n        \n          \n            B\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} _{d}}\n  are the discretized state-space matrices.\n\n\n=== Discretization of process noise ===\nNumerical evaluation of \n  \n    \n      \n        \n          \n            Q\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Q} _{d}}\n   is a bit trickier due to the matrix exponential integral. It can, however, be computed by first constructing a matrix, and computing the exponential of it\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  \n                    A\n                  \n                \n                \n                  \n                    Q\n                  \n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    \n                      A\n                    \n                    \n                      \u22a4\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        T\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\begin{bmatrix}-\\mathbf {A} &\\mathbf {Q} \\\\\\mathbf {0} &\\mathbf {A} ^{\\top }\\end{bmatrix}}T}\n  \n\n  \n    \n      \n        \n          G\n        \n        =\n        \n          e\n          \n            \n              F\n            \n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \u2026\n                \n                \n                  \n                    \n                      A\n                    \n                    \n                      d\n                    \n                    \n                      \u2212\n                      1\n                    \n                  \n                  \n                    \n                      Q\n                    \n                    \n                      d\n                    \n                  \n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    \n                      A\n                    \n                    \n                      d\n                    \n                    \n                      \u22a4\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {G} =e^{\\mathbf {F} }={\\begin{bmatrix}\\dots &\\mathbf {A} _{d}^{-1}\\mathbf {Q} _{d}\\\\\\mathbf {0} &\\mathbf {A} _{d}^{\\top }\\end{bmatrix}}.}\n  The discretized process noise is then evaluated by multiplying the transpose of the lower-right partition of G with the upper-right partition of G:\n\n  \n    \n      \n        \n          \n            Q\n          \n          \n            d\n          \n        \n        =\n        (\n        \n          \n            A\n          \n          \n            d\n          \n          \n            \u22a4\n          \n        \n        \n          )\n          \n            \u22a4\n          \n        \n        (\n        \n          \n            A\n          \n          \n            d\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            Q\n          \n          \n            d\n          \n        \n        )\n        =\n        \n          \n            A\n          \n          \n            d\n          \n        \n        (\n        \n          \n            A\n          \n          \n            d\n          \n          \n            \u2212\n            1\n          \n        \n        \n          \n            Q\n          \n          \n            d\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\mathbf {Q} _{d}=(\\mathbf {A} _{d}^{\\top })^{\\top }(\\mathbf {A} _{d}^{-1}\\mathbf {Q} _{d})=\\mathbf {A} _{d}(\\mathbf {A} _{d}^{-1}\\mathbf {Q} _{d}).}\n  \n\n\n=== Derivation ===\nStarting with the continuous model\n\n  \n    \n      \n        \n          \n            \n              x\n              \u02d9\n            \n          \n        \n        (\n        t\n        )\n        =\n        \n          A\n        \n        \n          x\n        \n        (\n        t\n        )\n        +\n        \n          B\n        \n        \n          u\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\mathbf {\\dot {x}} (t)=\\mathbf {A} \\mathbf {x} (t)+\\mathbf {B} \\mathbf {u} (t)}\n  we know that the matrix exponential is\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \n          e\n          \n            \n              A\n            \n            t\n          \n        \n        =\n        \n          A\n        \n        \n          e\n          \n            \n              A\n            \n            t\n          \n        \n        =\n        \n          e\n          \n            \n              A\n            \n            t\n          \n        \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\frac {d}{dt}}e^{\\mathbf {A} t}=\\mathbf {A} e^{\\mathbf {A} t}=e^{\\mathbf {A} t}\\mathbf {A} }\n  and by premultiplying the model we get\n\n  \n    \n      \n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          \n            \n              x\n              \u02d9\n            \n          \n        \n        (\n        t\n        )\n        =\n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          A\n        \n        \n          x\n        \n        (\n        t\n        )\n        +\n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle e^{-\\mathbf {A} t}\\mathbf {\\dot {x}} (t)=e^{-\\mathbf {A} t}\\mathbf {A} \\mathbf {x} (t)+e^{-\\mathbf {A} t}\\mathbf {B} \\mathbf {u} (t)}\n  which we recognize as\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        (\n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          x\n        \n        (\n        t\n        )\n        )\n        =\n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}(e^{-\\mathbf {A} t}\\mathbf {x} (t))=e^{-\\mathbf {A} t}\\mathbf {B} \\mathbf {u} (t)}\n  and by integrating..\n\n  \n    \n      \n        \n          e\n          \n            \u2212\n            \n              A\n            \n            t\n          \n        \n        \n          x\n        \n        (\n        t\n        )\n        \u2212\n        \n          e\n          \n            0\n          \n        \n        \n          x\n        \n        (\n        0\n        )\n        =\n        \n          \u222b\n          \n            0\n          \n          \n            t\n          \n        \n        \n          e\n          \n            \u2212\n            \n              A\n            \n            \u03c4\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        \u03c4\n        )\n        d\n        \u03c4\n      \n    \n    {\\displaystyle e^{-\\mathbf {A} t}\\mathbf {x} (t)-e^{0}\\mathbf {x} (0)=\\int _{0}^{t}e^{-\\mathbf {A} \\tau }\\mathbf {B} \\mathbf {u} (\\tau )d\\tau }\n  \n\n  \n    \n      \n        \n          x\n        \n        (\n        t\n        )\n        =\n        \n          e\n          \n            \n              A\n            \n            t\n          \n        \n        \n          x\n        \n        (\n        0\n        )\n        +\n        \n          \u222b\n          \n            0\n          \n          \n            t\n          \n        \n        \n          e\n          \n            \n              A\n            \n            (\n            t\n            \u2212\n            \u03c4\n            )\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        \u03c4\n        )\n        d\n        \u03c4\n      \n    \n    {\\displaystyle \\mathbf {x} (t)=e^{\\mathbf {A} t}\\mathbf {x} (0)+\\int _{0}^{t}e^{\\mathbf {A} (t-\\tau )}\\mathbf {B} \\mathbf {u} (\\tau )d\\tau }\n  which is an analytical solution to the continuous model.\nNow we want to discretise the above expression. We assume that u is constant during each timestep.\n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        ]\n         \n        \n          \n            \n              \n                =\n              \n              \n                \n                  d\n                  e\n                  f\n                \n              \n            \n          \n        \n         \n        \n          x\n        \n        (\n        k\n        T\n        )\n      \n    \n    {\\displaystyle \\mathbf {x} [k]\\ {\\stackrel {\\mathrm {def} }{=}}\\ \\mathbf {x} (kT)}\n  \n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        ]\n        =\n        \n          e\n          \n            \n              A\n            \n            k\n            T\n          \n        \n        \n          x\n        \n        (\n        0\n        )\n        +\n        \n          \u222b\n          \n            0\n          \n          \n            k\n            T\n          \n        \n        \n          e\n          \n            \n              A\n            \n            (\n            k\n            T\n            \u2212\n            \u03c4\n            )\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        \u03c4\n        )\n        d\n        \u03c4\n      \n    \n    {\\displaystyle \\mathbf {x} [k]=e^{\\mathbf {A} kT}\\mathbf {x} (0)+\\int _{0}^{kT}e^{\\mathbf {A} (kT-\\tau )}\\mathbf {B} \\mathbf {u} (\\tau )d\\tau }\n  \n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        +\n        1\n        ]\n        =\n        \n          e\n          \n            \n              A\n            \n            (\n            k\n            +\n            1\n            )\n            T\n          \n        \n        \n          x\n        \n        (\n        0\n        )\n        +\n        \n          \u222b\n          \n            0\n          \n          \n            (\n            k\n            +\n            1\n            )\n            T\n          \n        \n        \n          e\n          \n            \n              A\n            \n            (\n            (\n            k\n            +\n            1\n            )\n            T\n            \u2212\n            \u03c4\n            )\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        \u03c4\n        )\n        d\n        \u03c4\n      \n    \n    {\\displaystyle \\mathbf {x} [k+1]=e^{\\mathbf {A} (k+1)T}\\mathbf {x} (0)+\\int _{0}^{(k+1)T}e^{\\mathbf {A} ((k+1)T-\\tau )}\\mathbf {B} \\mathbf {u} (\\tau )d\\tau }\n  \n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        +\n        1\n        ]\n        =\n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n        \n          [\n          \n            \n              e\n              \n                \n                  A\n                \n                k\n                T\n              \n            \n            \n              x\n            \n            (\n            0\n            )\n            +\n            \n              \u222b\n              \n                0\n              \n              \n                k\n                T\n              \n            \n            \n              e\n              \n                \n                  A\n                \n                (\n                k\n                T\n                \u2212\n                \u03c4\n                )\n              \n            \n            \n              B\n            \n            \n              u\n            \n            (\n            \u03c4\n            )\n            d\n            \u03c4\n          \n          ]\n        \n        +\n        \n          \u222b\n          \n            k\n            T\n          \n          \n            (\n            k\n            +\n            1\n            )\n            T\n          \n        \n        \n          e\n          \n            \n              A\n            \n            (\n            k\n            T\n            +\n            T\n            \u2212\n            \u03c4\n            )\n          \n        \n        \n          B\n        \n        \n          u\n        \n        (\n        \u03c4\n        )\n        d\n        \u03c4\n      \n    \n    {\\displaystyle \\mathbf {x} [k+1]=e^{\\mathbf {A} T}\\left[e^{\\mathbf {A} kT}\\mathbf {x} (0)+\\int _{0}^{kT}e^{\\mathbf {A} (kT-\\tau )}\\mathbf {B} \\mathbf {u} (\\tau )d\\tau \\right]+\\int _{kT}^{(k+1)T}e^{\\mathbf {A} (kT+T-\\tau )}\\mathbf {B} \\mathbf {u} (\\tau )d\\tau }\n  We recognize the bracketed expression as \n  \n    \n      \n        \n          x\n        \n        [\n        k\n        ]\n      \n    \n    {\\displaystyle \\mathbf {x} [k]}\n  , and the second term can be simplified by substituting with the function \n  \n    \n      \n        v\n        (\n        \u03c4\n        )\n        =\n        k\n        T\n        +\n        T\n        \u2212\n        \u03c4\n      \n    \n    {\\displaystyle v(\\tau )=kT+T-\\tau }\n  . Note that \n  \n    \n      \n        d\n        \u03c4\n        =\n        \u2212\n        d\n        v\n      \n    \n    {\\displaystyle d\\tau =-dv}\n  . We also assume that \n  \n    \n      \n        \n          u\n        \n      \n    \n    {\\displaystyle \\mathbf {u} }\n   is constant during the integral, which in turn yields\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                \n                [\n                k\n                +\n                1\n                ]\n              \n              \n                =\n              \n              \n                \n                  e\n                  \n                    \n                      A\n                    \n                    T\n                  \n                \n                \n                  x\n                \n                [\n                k\n                ]\n                \u2212\n                \n                  (\n                  \n                    \n                      \u222b\n                      \n                        v\n                        (\n                        k\n                        T\n                        )\n                      \n                      \n                        v\n                        (\n                        (\n                        k\n                        +\n                        1\n                        )\n                        T\n                        )\n                      \n                    \n                    \n                      e\n                      \n                        \n                          A\n                        \n                        v\n                      \n                    \n                    d\n                    v\n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n              \n            \n            \n              \n              \n                =\n              \n              \n                \n                  e\n                  \n                    \n                      A\n                    \n                    T\n                  \n                \n                \n                  x\n                \n                [\n                k\n                ]\n                \u2212\n                \n                  (\n                  \n                    \n                      \u222b\n                      \n                        T\n                      \n                      \n                        0\n                      \n                    \n                    \n                      e\n                      \n                        \n                          A\n                        \n                        v\n                      \n                    \n                    d\n                    v\n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n              \n            \n            \n              \n              \n                =\n              \n              \n                \n                  e\n                  \n                    \n                      A\n                    \n                    T\n                  \n                \n                \n                  x\n                \n                [\n                k\n                ]\n                +\n                \n                  (\n                  \n                    \n                      \u222b\n                      \n                        0\n                      \n                      \n                        T\n                      \n                    \n                    \n                      e\n                      \n                        \n                          A\n                        \n                        v\n                      \n                    \n                    d\n                    v\n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n              \n            \n            \n              \n              \n                =\n              \n              \n                \n                  e\n                  \n                    \n                      A\n                    \n                    T\n                  \n                \n                \n                  x\n                \n                [\n                k\n                ]\n                +\n                \n                  \n                    A\n                  \n                  \n                    \u2212\n                    1\n                  \n                \n                \n                  (\n                  \n                    \n                      e\n                      \n                        \n                          A\n                        \n                        T\n                      \n                    \n                    \u2212\n                    \n                      I\n                    \n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{matrix}\\mathbf {x} [k+1]&=&e^{\\mathbf {A} T}\\mathbf {x} [k]-\\left(\\int _{v(kT)}^{v((k+1)T)}e^{\\mathbf {A} v}dv\\right)\\mathbf {B} \\mathbf {u} [k]\\\\&=&e^{\\mathbf {A} T}\\mathbf {x} [k]-\\left(\\int _{T}^{0}e^{\\mathbf {A} v}dv\\right)\\mathbf {B} \\mathbf {u} [k]\\\\&=&e^{\\mathbf {A} T}\\mathbf {x} [k]+\\left(\\int _{0}^{T}e^{\\mathbf {A} v}dv\\right)\\mathbf {B} \\mathbf {u} [k]\\\\&=&e^{\\mathbf {A} T}\\mathbf {x} [k]+\\mathbf {A} ^{-1}\\left(e^{\\mathbf {A} T}-\\mathbf {I} \\right)\\mathbf {B} \\mathbf {u} [k]\\end{matrix}}}\n  which is an exact solution to the discretization problem.\nWhen \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n   is singular, the latter expression can still be used by replacing \n  \n    \n      \n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n      \n    \n    {\\displaystyle e^{\\mathbf {A} T}}\n   by its Taylor expansion,\n\n  \n    \n      \n        \n          e\n          \n            \n              \n                A\n              \n            \n            T\n          \n        \n        =\n        \n          \u2211\n          \n            k\n            =\n            0\n          \n          \n            \u221e\n          \n        \n        \n          \n            1\n            \n              k\n              !\n            \n          \n        \n        (\n        \n          \n            A\n          \n        \n        T\n        \n          )\n          \n            k\n          \n        \n        .\n      \n    \n    {\\displaystyle e^{{\\mathbf {A} }T}=\\sum _{k=0}^{\\infty }{\\frac {1}{k!}}({\\mathbf {A} }T)^{k}.}\n  This yields\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                \n                [\n                k\n                +\n                1\n                ]\n              \n              \n                =\n              \n              \n                \n                  e\n                  \n                    \n                      \n                        A\n                      \n                    \n                    T\n                  \n                \n                \n                  x\n                \n                [\n                k\n                ]\n                +\n                \n                  (\n                  \n                    \n                      \u222b\n                      \n                        0\n                      \n                      \n                        T\n                      \n                    \n                    \n                      e\n                      \n                        \n                          \n                            A\n                          \n                        \n                        v\n                      \n                    \n                    d\n                    v\n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n              \n            \n            \n              \n              \n                =\n              \n              \n                \n                  (\n                  \n                    \n                      \u2211\n                      \n                        k\n                        =\n                        0\n                      \n                      \n                        \u221e\n                      \n                    \n                    \n                      \n                        1\n                        \n                          k\n                          !\n                        \n                      \n                    \n                    (\n                    \n                      \n                        A\n                      \n                    \n                    T\n                    \n                      )\n                      \n                        k\n                      \n                    \n                  \n                  )\n                \n                \n                  x\n                \n                [\n                k\n                ]\n                +\n                \n                  (\n                  \n                    \n                      \u2211\n                      \n                        k\n                        =\n                        1\n                      \n                      \n                        \u221e\n                      \n                    \n                    \n                      \n                        1\n                        \n                          k\n                          !\n                        \n                      \n                    \n                    \n                      \n                        \n                          A\n                        \n                      \n                      \n                        k\n                        \u2212\n                        1\n                      \n                    \n                    \n                      T\n                      \n                        k\n                      \n                    \n                  \n                  )\n                \n                \n                  B\n                \n                \n                  u\n                \n                [\n                k\n                ]\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{matrix}\\mathbf {x} [k+1]&=&e^{{\\mathbf {A} }T}\\mathbf {x} [k]+\\left(\\int _{0}^{T}e^{{\\mathbf {A} }v}dv\\right)\\mathbf {B} \\mathbf {u} [k]\\\\&=&\\left(\\sum _{k=0}^{\\infty }{\\frac {1}{k!}}({\\mathbf {A} }T)^{k}\\right)\\mathbf {x} [k]+\\left(\\sum _{k=1}^{\\infty }{\\frac {1}{k!}}{\\mathbf {A} }^{k-1}T^{k}\\right)\\mathbf {B} \\mathbf {u} [k],\\end{matrix}}}\n  which is the form used in practice.\n\n\n=== Approximations ===\nExact discretization may sometimes be intractable due to the heavy matrix exponential and integral operations involved. It is much easier to calculate an approximate discrete model, based on that for small timesteps \n  \n    \n      \n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n        \u2248\n        \n          I\n        \n        +\n        \n          A\n        \n        T\n      \n    \n    {\\displaystyle e^{\\mathbf {A} T}\\approx \\mathbf {I} +\\mathbf {A} T}\n  . The approximate solution then becomes:\n\n  \n    \n      \n        \n          x\n        \n        [\n        k\n        +\n        1\n        ]\n        \u2248\n        (\n        \n          I\n        \n        +\n        \n          A\n        \n        T\n        )\n        \n          x\n        \n        [\n        k\n        ]\n        +\n        T\n        \n          B\n        \n        \n          u\n        \n        [\n        k\n        ]\n      \n    \n    {\\displaystyle \\mathbf {x} [k+1]\\approx (\\mathbf {I} +\\mathbf {A} T)\\mathbf {x} [k]+T\\mathbf {B} \\mathbf {u} [k]}\n  This is also known as the Euler method, which is also known as the forward Euler method. Other possible approximations are \n  \n    \n      \n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n        \u2248\n        \n          \n            (\n            \n              \n                I\n              \n              \u2212\n              \n                A\n              \n              T\n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle e^{\\mathbf {A} T}\\approx \\left(\\mathbf {I} -\\mathbf {A} T\\right)^{-1}}\n  , otherwise known as the backward Euler method and \n  \n    \n      \n        \n          e\n          \n            \n              A\n            \n            T\n          \n        \n        \u2248\n        \n          (\n          \n            \n              I\n            \n            +\n            \n              \n                1\n                2\n              \n            \n            \n              A\n            \n            T\n          \n          )\n        \n        \n          \n            (\n            \n              \n                I\n              \n              \u2212\n              \n                \n                  1\n                  2\n                \n              \n              \n                A\n              \n              T\n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle e^{\\mathbf {A} T}\\approx \\left(\\mathbf {I} +{\\frac {1}{2}}\\mathbf {A} T\\right)\\left(\\mathbf {I} -{\\frac {1}{2}}\\mathbf {A} T\\right)^{-1}}\n  , which is known as the bilinear transform, or Tustin transform. Each of these approximations has different stability properties. The bilinear transform preserves the instability of the continuous-time system.\n\n\n== Discretization of continuous features ==\n\nIn statistics and machine learning, discretization refers to the process of converting continuous features or variables to discretized or nominal features. This can be useful when creating probability mass functions.\n\n\n== Discretization of smooth functions ==\n\nIn generalized functions theory, discretization \narises as a particular case of the Convolution Theorem \non tempered distributions\n\n  \n    \n      \n        \n          \n            F\n          \n        \n        {\n        f\n        \u2217\n        III\n        }\n        =\n        \n          \n            F\n          \n        \n        {\n        f\n        }\n        \u22c5\n        III\n      \n    \n    {\\displaystyle {\\mathcal {F}}\\{f*\\operatorname {III} \\}={\\mathcal {F}}\\{f\\}\\cdot \\operatorname {III} }\n  \n\n  \n    \n      \n        \n          \n            F\n          \n        \n        {\n        \u03b1\n        \u22c5\n        III\n        }\n        =\n        \n          \n            F\n          \n        \n        {\n        \u03b1\n        }\n        \u2217\n        III\n      \n    \n    {\\displaystyle {\\mathcal {F}}\\{\\alpha \\cdot \\operatorname {III} \\}={\\mathcal {F}}\\{\\alpha \\}*\\operatorname {III} }\n  where \n  \n    \n      \n        III\n      \n    \n    {\\displaystyle \\operatorname {III} }\n   is the Dirac comb,\n\n  \n    \n      \n        \u22c5\n        III\n      \n    \n    {\\displaystyle \\cdot \\operatorname {III} }\n   is discretization, \n  \n    \n      \n        \u2217\n        III\n      \n    \n    {\\displaystyle *\\operatorname {III} }\n   is \nperiodization, \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is a rapidly decreasing tempered distribution \n(e.g. a Dirac delta function \n  \n    \n      \n        \u03b4\n      \n    \n    {\\displaystyle \\delta }\n   or any other \ncompactly supported function), \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is a smooth, \nslowly growing\nordinary function (e.g. the function that is constantly \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  \nor any other band-limited function)\nand \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n   is the (unitary, ordinary frequency) Fourier transform.\nFunctions \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   which are not smooth can be made smooth using a mollifier prior to discretization.\nAs an example, discretization of the function that is constantly \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   yields the sequence \n  \n    \n      \n        [\n        .\n        .\n        ,\n        1\n        ,\n        1\n        ,\n        1\n        ,\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [..,1,1,1,..]}\n   which, interpreted as the coefficients of a linear combination of Dirac delta functions, forms a Dirac comb. If additionally truncation is applied, one obtains finite sequences, e.g. \n  \n    \n      \n        [\n        1\n        ,\n        1\n        ,\n        1\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [1,1,1,1]}\n  . They are discrete in both, time and frequency.\n\n\n== See also ==\nDiscrete event simulation\nDiscrete space\nDiscrete time and continuous time\nFinite difference method\nFinite volume method for unsteady flow\nSmoothing\nStochastic simulation\nTime-scale calculus\n\n\n== References ==\n\n\n== Further reading ==\nRobert Grover Brown & Patrick Y. C. Hwang (1997). Introduction to random signals and applied Kalman filtering (3rd ed.). ISBN 978-0471128397.\nChi-Tsong Chen (1984). Linear System Theory and Design. Philadelphia, PA, USA: Saunders College Publishing. ISBN 978-0030716911.\nC. Van Loan (Jun 1978). \"Computing integrals involving the matrix exponential\" (PDF). IEEE Transactions on Automatic Control. 23 (3): 395\u2013404. doi:10.1109/TAC.1978.1101743. hdl:1813/7095.\nR.H. Middleton & G.C. Goodwin (1990). Digital control and estimation: a unified approach. p. 33f. ISBN 978-0132116657.\n\n\n== External links ==", "Model selection": "Model selection is the task of selecting a model from among various candidates on the basis of performance criterion to choose the best one.\nIn the context of learning, this may be the selection of a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice (Occam's razor).\nKonishi & Kitagawa (2008, p. 75) state, \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling\".  Relatedly, Cox (2006, p. 197) has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".\nModel selection may also refer to the problem of selecting a few representative models from a large set of computational models for the purpose of decision making or optimization under uncertainty.In machine learning, algorithmic approaches to model selection include feature selection, hyperparameter optimization, and statistical learning theory.\n\n\n== Introduction ==\n\nIn its most basic forms, model selection is one of the fundamental tasks of scientific inquiry.  Determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations.  For example, when Galileo performed his inclined plane experiments, he demonstrated that the motion of the balls fitted the parabola predicted by his model.\nOf the countless number of possible mechanisms and processes that could have produced the data, how can one even begin to choose the best model?  The mathematical approach commonly taken decides among a set of candidate models; this set must be chosen by the researcher. Often simple models such as polynomials are used, at least initially. Burnham & Anderson (2002) emphasize throughout their book the importance of choosing models based on sound scientific principles, such as understanding of the phenomenological processes or mechanisms (e.g., chemical reactions) underlying the data.\nOnce the set of candidate models has been chosen, the statistical analysis allows us to select the best of these models.  What is meant by best is controversial.  A good model selection technique will balance goodness of fit with simplicity.  More complex models will be better able to adapt their shape to fit the data (for example, a fifth-order polynomial can exactly fit six points), but the additional parameters may not represent anything useful. (Perhaps those six points are really just randomly distributed about a straight line.)  Goodness of fit is generally determined using a likelihood ratio approach, or an approximation of this, leading to a chi-squared test.  The complexity is generally measured by counting the number of parameters in the model.\nModel selection techniques can be considered as estimators of some physical quantity, such as the probability of the model producing the given data. The bias and variance are both important measures of the quality of this estimator; efficiency is also often considered.\nA standard example of model selection is that of curve fitting, where, given a set of points and other background knowledge (e.g. points are a result of i.i.d. samples), we must select a curve that describes the function that generated the points.\n\n\n== Two directions of model selection ==\nThere are two main objectives in inference and learning from data. One is for scientific discovery, also called statistical inference, understanding of the underlying data-generating mechanism and interpretation of the nature of the data. Another objective of learning from data is for predicting future or unseen observations, also called Statistical Prediction. In the second objective, the data scientist does not necessarily concern an accurate probabilistic description of the data. Of course, one may also be interested in both directions.\nIn line with the two different objectives, model selection can also have two directions: model selection for inference and model selection for prediction. The first direction is to identify the best model for the data, which will preferably provide a reliable characterization of the sources of uncertainty for scientific interpretation. For this goal, it is significantly important that the selected model is not too sensitive to the sample size. Accordingly, an appropriate notion for evaluating model selection is the selection consistency, meaning that the most robust candidate will be consistently selected given sufficiently many data samples.\nThe second direction is to choose a model as machinery to offer excellent predictive performance.  For the latter, however, the selected model may simply be the lucky winner among a few close competitors, yet the predictive performance can still be the best possible. If so, the model selection is fine for the second goal (prediction), but the use of the selected model for insight and interpretation may be severely unreliable and misleading. Moreover, for very complex models selected this way, even predictions may be unreasonable for data only slightly different from those on which the selection was made.\n\n\n== Methods to assist in choosing the set of candidate models ==\nData transformation (statistics)\nExploratory data analysis\nModel specification\nScientific method\n\n\n== Criteria ==\nBelow is a list of criteria for model selection. The most commonly used criteria are (i) the Akaike information criterion and (ii) the Bayes factor and/or the Bayesian information criterion (which to some extent approximates the Bayes factor), see \nStoica & Selen (2004) for a review.\n\nAkaike information criterion (AIC), a measure of the goodness fit of an estimated statistical model\nBayes factor\nBayesian information criterion (BIC), also known as the Schwarz information criterion, a statistical criterion for model selection\nBridge criterion (BC), a statistical criterion that can attain the better performance of AIC and BIC despite the appropriateness of model specification.\nCross-validation\nDeviance information criterion (DIC), another Bayesian oriented model selection criterion\nFalse discovery rate\nFocused information criterion (FIC), a selection criterion sorting statistical models by their effectiveness for a given focus parameter\nHannan\u2013Quinn information criterion, an alternative to the Akaike and Bayesian criteria\nKashyap information criterion (KIC) is a powerful alternative to AIC and BIC, because KIC uses Fisher information matrix\nLikelihood-ratio test\nMallows's Cp\nMinimum description length\nMinimum message length (MML)\nPRESS statistic, also known as the PRESS criterion\nStructural risk minimization\nStepwise regression\nWatanabe\u2013Akaike information criterion (WAIC), also called the widely applicable information criterion\nExtended Bayesian Information Criterion (EBIC) is an extension of ordinary Bayesian information criterion (BIC) for models with high parameter spaces.\nExtended Fisher Information Criterion (EFIC) is a model selection criterion for linear regression models.\nConstrained Minimum Criterion (CMC) is a frequentist criterion for selecting regression models with a geometric underpinning.Among these criteria, cross-validation is typically the most accurate, and computationally the most expensive, for supervised learning problems.Burnham & Anderson (2002, \u00a76.3) say the following:\n\nThere is a variety of model selection methods. However, from the point of view of statistical performance of a method, and intended context of its use, there are only two distinct classes of methods: These have been labeled efficient and consistent. (...) Under the frequentist paradigm for model selection one generally has three main approaches: (I) optimization of some selection criteria, (II) tests of hypotheses, and (III) ad hoc methods.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nAho, K.; Derryberry, D.; Peterson, T. (2014), \"Model selection for ecologists: the worldviews of AIC and BIC\", Ecology, 95 (3): 631\u2013636, doi:10.1890/13-1452.1, PMID 24804445\nAkaike, H. (1994), \"Implications of informational point of view on the development of statistical science\",  in Bozdogan, H. (ed.), Proceedings of the First US/JAPAN Conference on The Frontiers of Statistical Modeling: An Informational Approach\u2014Volume 3, Kluwer Academic Publishers, pp. 27\u201338\nAnderson, D.R. (2008), Model Based Inference in the Life Sciences, Springer, ISBN 9780387740751\nAndo, T. (2010), Bayesian Model Selection and Statistical Modeling, CRC Press, ISBN 9781439836156\nBreiman, L. (2001), \"Statistical modeling: the two cultures\", Statistical Science, 16: 199\u2013231, doi:10.1214/ss/1009213726\nBurnham, K.P.; Anderson, D.R. (2002), Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach (2nd ed.), Springer-Verlag, ISBN 0-387-95364-7 [this has over 38000 citations on Google Scholar]\nChamberlin, T.C. (1890), \"The method of multiple working hypotheses\", Science, 15 (366): 92\u20136, Bibcode:1890Sci....15R..92., doi:10.1126/science.ns-15.366.92, PMID 17782687 (reprinted 1965, Science 148: 754\u2013759 [1] doi:10.1126/science.148.3671.754)\nClaeskens, G. (2016), \"Statistical model choice\" (PDF), Annual Review of Statistics and Its Application, 3 (1): 233\u2013256, Bibcode:2016AnRSA...3..233C, doi:10.1146/annurev-statistics-041715-033413\nClaeskens, G.; Hjort, N.L. (2008), Model Selection and Model Averaging, Cambridge University Press, ISBN 9781139471800\nCox, D.R. (2006), Principles of Statistical Inference, Cambridge University Press\nDing, J.; Tarokh, V.; Yang, Y. (2018), \"Model Selection Techniques - An Overview\", IEEE Signal Processing Magazine, 35 (6): 16\u201334, arXiv:1810.09583, Bibcode:2018ISPM...35f..16D, doi:10.1109/MSP.2018.2867638, S2CID 53035396\nKashyap, R.L. (1982), \"Optimal choice of AR and MA parts in autoregressive moving average models\", IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE, PAMI-4 (2): 99\u2013104, doi:10.1109/TPAMI.1982.4767213, PMID 21869012, S2CID 18484243\nKonishi, S.; Kitagawa, G. (2008), Information Criteria and Statistical Modeling, Springer, Bibcode:2007icsm.book.....K, ISBN 9780387718866\nLahiri, P. (2001), Model Selection, Institute of Mathematical Statistics\nLeeb, H.; P\u00f6tscher, B. M. (2009), \"Model selection\",  in Anderson, T. G. (ed.), Handbook of Financial Time Series, Springer, pp. 889\u2013925, doi:10.1007/978-3-540-71297-8_39, ISBN 978-3-540-71296-1\nLukacs, P. M.; Thompson, W. L.; Kendall, W. L.; Gould, W. R.; Doherty, P. F. Jr.; Burnham, K. P.; Anderson, D. R. (2007), \"Concerns regarding a call for pluralism of information theory and hypothesis testing\", Journal of Applied Ecology, 44 (2): 456\u2013460, doi:10.1111/j.1365-2664.2006.01267.x, S2CID 83816981\nMcQuarrie, Allan D. R.; Tsai, Chih-Ling (1998), Regression and Time Series Model Selection, Singapore: World Scientific, ISBN 981-02-3242-X\nMassart, P. (2007), Concentration Inequalities and Model Selection, Springer\nMassart, P. (2014), \"A non-asymptotic walk in probability and statistics\",  in Lin, Xihong (ed.), Past, Present, and Future of Statistical Science, Chapman & Hall, pp. 309\u2013321, ISBN 9781482204988\nNavarro, D. J. (2019), \"Between the Devil and the Deep Blue Sea: Tensions between scientific judgement and statistical model selection\", Computational Brain & Behavior, 2: 28\u201334, doi:10.1007/s42113-018-0019-z\nResende, Paulo Angelo Alves; Dorea, Chang Chung Yu (2016), \"Model identification using the Efficient Determination Criterion\", Journal of Multivariate Analysis, 150: 229\u2013244, arXiv:1409.7441, doi:10.1016/j.jmva.2016.06.002, S2CID 5469654\nShmueli, G. (2010), \"To explain or to predict?\", Statistical Science, 25 (3): 289\u2013310, arXiv:1101.0891, doi:10.1214/10-STS330, MR 2791669, S2CID 15900983\nStoica, P.; Selen, Y. (2004), \"Model-order selection: a review of information criterion rules\" (PDF), IEEE Signal Processing Magazine, 21 (4): 36\u201347, doi:10.1109/MSP.2004.1311138, S2CID 17338979\nWit, E.; van den Heuvel, E.; Romeijn, J.-W. (2012), \"'All models are wrong...': an introduction to model uncertainty\" (PDF), Statistica Neerlandica, 66 (3): 217\u2013236, doi:10.1111/j.1467-9574.2012.00530.x, S2CID 7793470\nWit, E.; McCullagh, P. (2001),  Viana, M. A. G.; Richards, D. St. P. (eds.), \"The extendibility of statistical models\", Algebraic Methods in Statistics and Probability, pp. 327\u2013340\nW\u00f3jtowicz, Anna; Bigaj, Tomasz (2016), \"Justification, confirmation, and the problem of mutually exclusive hypotheses\",  in Ku\u017aniar, Adrian; Odrow\u0105\u017c-Sypniewska, Joanna (eds.), Uncovering Facts and Values, Brill Publishers, pp. 122\u2013143, doi:10.1163/9789004312654_009, ISBN 9789004312654\nOwrang, Arash; Jansson, Magnus (2018), \"A Model Selection Criterion for High-Dimensional Linear Regression\", IEEE Transactions on Signal Processing , 66 (13): 3436\u20133446, Bibcode:2018ITSP...66.3436O, doi:10.1109/TSP.2018.2821628, ISSN 1941-0476, S2CID 46931136\nB. Gohain, Prakash; Jansson, Magnus (2022), \"Scale-Invariant and consistent Bayesian information criterion for order selection in linear regression models\", Signal Processing, 196: 108499, doi:10.1016/j.sigpro.2022.108499, ISSN 0165-1684, S2CID 246759677", "Knowledge base": "A knowledge base (KB) is a set of sentences, each sentence given in a knowledge representation language, with  interfaces to tell new sentences and to ask questions about what is known, where either of these interfaces might use inference. It is a technology used to store complex structured data  used by a computer system. The initial use of the term was in connection with expert systems, which were the first knowledge-based systems. \n\n\n== Original usage of the term ==\nThe original use of the term knowledge base was to describe one of the two sub-systems of an expert system. A knowledge-based system consists of a knowledge-base representing facts about the world and ways of reasoning about those facts to deduce new facts or highlight inconsistencies.\n\n\n== Properties ==\nThe term \"knowledge-base\" was coined to distinguish this form of knowledge store from the more common and widely used term database. During the 1970s, virtually all large management information systems stored their data in some type of hierarchical or relational database. At this point in the history of information technology, the distinction between a database and a knowledge-base was clear and unambiguous.\nA database had the following properties:\n\nFlat data: Data was usually represented in a tabular format with strings or numbers in each field.\nMultiple users: A conventional database needed to support more than one user or system logged into the same data at the same time.\nTransactions: An essential requirement for a database was to maintain integrity and consistency among data accessed by concurrent users. These are the so-called ACID properties: Atomicity, Consistency, Isolation, and Durability.\nLarge, long-lived data: A corporate database needed to support not just thousands but hundreds of thousands or more rows of data. Such a database usually needed to persist past the specific uses of any individual program; it needed to store data for years and decades rather than for the life of a program.The first knowledge-based systems had data needs that were the opposite of these database requirements. An expert system requires structured data. Not just tables with numbers and strings, but pointers to other objects that in turn have additional pointers. The ideal representation for a knowledge base is an object model (often called an ontology in artificial intelligence literature) with classes, subclasses and instances.\nEarly expert systems also had little need for multiple users or the complexity that comes with requiring transactional properties on data. The data for the early expert systems was used to arrive at a specific answer, such as a medical diagnosis, the design of a molecule, or a response to an emergency. Once the solution to the problem was known, there was not a critical demand to store large amounts of data back to a permanent memory store. A more precise statement would be that given the technologies available, researchers compromised and did without these capabilities because they realized they were beyond what could be expected, and they could develop useful solutions to non-trivial problems without them. Even from the beginning, the more astute researchers realized the potential benefits of being able to store, analyze, and reuse knowledge. For example, see the discussion of Corporate Memory in the earliest work of the Knowledge-Based Software Assistant program by Cordell Green et al.The volume requirements were also different for a knowledge-base compared to a conventional database. The knowledge-base needed to know facts about the world. For example, to represent the statement that \"All humans are mortal\", a database typically could not represent this general knowledge but instead would need to store information about thousands of tables that represented information about specific humans. Representing that all humans are mortal and being able to reason about any given human that they are mortal is the work of a knowledge-base. Representing that George, Mary, Sam, Jenna, Mike,... and hundreds of thousands of other customers are all humans with specific ages, sex, address, etc. is the work for a database.As expert systems moved from being prototypes to systems deployed in corporate environments the requirements for their data storage rapidly started to overlap with the standard database requirements for multiple, distributed users with support for transactions. Initially, the demand could be seen in two different but competitive markets. From the AI and Object-Oriented communities, object-oriented databases such as Versant emerged. These were systems designed from the ground up to have support for object-oriented capabilities but also to support standard database services as well. On the other hand, the large database vendors such as Oracle added capabilities to their products that provided support for knowledge-base requirements such as class-subclass relations and rules.\n\n\n== Internet as a knowledge base ==\nThe next evolution for the term \u201cknowledge-base\u201d was the Internet. With the rise of the Internet, documents, hypertext, and multimedia support were now critical for any corporate database. It was no longer enough to support large tables of data or relatively small objects that lived primarily in computer memory. Support for corporate web sites required persistence and transactions for documents. This created a whole new discipline known as Web Content Management.\nThe other driver for document support was the rise of knowledge management vendors such as Lotus Notes. Knowledge Management actually predated the Internet but with the Internet there was great synergy between the two areas. Knowledge management products adopted the term \"knowledge-base\" to describe their repositories but the meaning had a big difference. In the case of previous knowledge-based systems, the knowledge was primarily for the use of an automated system, to reason about and draw conclusions about the world. With knowledge management products, the knowledge was primarily meant for humans, for example to serve as a repository of manuals, procedures, policies, best practices, reusable designs and code, etc. In both cases the distinctions between the uses and kinds of systems were ill-defined. As the technology scaled up it was rare to find a system that could really be cleanly classified as knowledge-based in the sense of an expert system that performed automated reasoning and knowledge-based in the sense of knowledge management that provided knowledge in the form of documents and media that could be leveraged by humans.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==", "Cross-validation (statistics)": "Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\nOne round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\nIn summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.\n\n\n== Motivation ==\nAssume a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set).  The fitting process optimizes the model parameters to make the model fit the training data as well as possible.  If an independent sample of validation data is taken from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large.  Cross-validation is a way to estimate the size of this effect.\nIn linear regression, there exist real response values y1, ..., yn, and n p-dimensional vector covariates x1, ..., xn.  The components of the vector xi are denoted xi1, ..., xip. If least squares is used to fit a function in the form of a hyperplane \u0177 = a + \u03b2Tx to the data (xi, yi) 1 \u2264 i \u2264 n, then the fit can be assessed using the mean squared error (MSE). The MSE for given estimated parameter values a and \u03b2 on the training set (xi, yi) 1 \u2264 i \u2264 n is defined as:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  MSE\n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    n\n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                  y\n                  \n                    i\n                  \n                \n                \u2212\n                \n                  \n                    \n                      \n                        y\n                        ^\n                      \n                    \n                  \n                  \n                    i\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n                =\n                \n                  \n                    1\n                    n\n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                  y\n                  \n                    i\n                  \n                \n                \u2212\n                a\n                \u2212\n                \n                  \n                    \u03b2\n                  \n                  \n                    T\n                  \n                \n                \n                  \n                    x\n                  \n                  \n                    i\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    n\n                  \n                \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                  y\n                  \n                    i\n                  \n                \n                \u2212\n                a\n                \u2212\n                \n                  \u03b2\n                  \n                    1\n                  \n                \n                \n                  x\n                  \n                    i\n                    1\n                  \n                \n                \u2212\n                \u22ef\n                \u2212\n                \n                  \u03b2\n                  \n                    p\n                  \n                \n                \n                  x\n                  \n                    i\n                    p\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MSE}}&={\\frac {1}{n}}\\sum _{i=1}^{n}(y_{i}-{\\hat {y}}_{i})^{2}={\\frac {1}{n}}\\sum _{i=1}^{n}(y_{i}-a-{\\boldsymbol {\\beta }}^{T}\\mathbf {x} _{i})^{2}\\\\&={\\frac {1}{n}}\\sum _{i=1}^{n}(y_{i}-a-\\beta _{1}x_{i1}-\\dots -\\beta _{p}x_{ip})^{2}\\end{aligned}}}\n  If the model is correctly specified, it can be shown under mild assumptions that the expected value of the MSE for the training set is (n \u2212 p \u2212 1)/(n + p + 1) < 1 times the expected value of the MSE for the validation set (the expected value is taken over the distribution of training sets).  Thus, a fitted model and computed MSE on the training set will result in an optimistically biased assessment of how well the model will fit an independent data set.  This biased estimate is called the in-sample estimate of the fit, whereas the cross-validation estimate is an out-of-sample estimate.\nSince in linear regression it is possible to directly compute the factor (n \u2212 p \u2212 1)/(n + p + 1) by which the training MSE underestimates the validation MSE under the assumption that the model specification is valid, cross-validation can be used for checking whether the model has been overfitted, in which case the MSE in the validation set will substantially exceed its anticipated value. (Cross-validation in the context of linear regression is also useful in that it can be used to select an optimally regularized cost function.)\nIn most other regression procedures (e.g. logistic regression), there is no simple formula to compute the expected out-of-sample fit. Cross-validation is, thus, a generally applicable way to predict the performance of a model on unavailable data using numerical computation in place of theoretical analysis.\n\n\n== Types ==\nTwo types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation.\n\n\n=== Exhaustive cross-validation ===\nExhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.\n\n\n==== Leave-p-out cross-validation ====\nLeave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set.LpO cross-validation require training and validating the model \n  \n    \n      \n        \n          C\n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle C_{p}^{n}}\n   times, where n is the number of observations in the original sample, and where \n  \n    \n      \n        \n          C\n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle C_{p}^{n}}\n   is the binomial coefficient.  For p > 1 and for even moderately large n, LpO CV can become computationally infeasible.  For example, with n = 100 and p = 30, \n  \n    \n      \n        \n          C\n          \n            30\n          \n          \n            100\n          \n        \n        \u2248\n        3\n        \u00d7\n        \n          10\n          \n            25\n          \n        \n        .\n      \n    \n    {\\displaystyle C_{30}^{100}\\approx 3\\times 10^{25}.}\n  \nA variant of LpO cross-validation with p=2 known as leave-pair-out cross-validation has been recommended as a nearly unbiased method for estimating the area under ROC curve of binary classifiers.\n\n\n==== Leave-one-out cross-validation ====\n\nLeave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with p = 1. The process looks similar to jackknife; however, with cross-validation one computes a statistic on the left-out sample(s), while with jackknifing one computes a statistic from the kept samples only.\nLOO cross-validation requires less computation time than LpO cross-validation because there are only \n  \n    \n      \n        \n          C\n          \n            1\n          \n          \n            n\n          \n        \n        =\n        n\n      \n    \n    {\\displaystyle C_{1}^{n}=n}\n   passes rather than \n  \n    \n      \n        \n          C\n          \n            p\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle C_{p}^{n}}\n  . However, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate.Pseudo-code algorithm:\nInput:\nx, {vector of length N with x-values of incoming points}\ny, {vector of length N with y-values of the expected result}\ninterpolate( x_in, y_in, x_out ), { returns the estimation for point x_out after the model is trained with x_in-y_in pairs}\nOutput:\nerr, {estimate for the prediction error}\nSteps:\n\n err \u2190 0\n for i \u2190 1, ..., N do\n   // define the cross-validation subsets\n   x_in \u2190 (x[1], ..., x[i \u2212 1], x[i + 1], ..., x[N])\n   y_in \u2190 (y[1], ..., y[i \u2212 1], y[i + 1], ..., y[N])\n   x_out \u2190 x[i]\n   y_out \u2190 interpolate(x_in, y_in, x_out)\n   err \u2190 err + (y[i] \u2212 y_out)^2\n end for\n err \u2190 err/N\n\n\n=== Non-exhaustive cross-validation ===\nNon-exhaustive cross validation methods do not compute all ways of splitting the original sample. These methods are approximations of leave-p-out cross-validation.\n\n\n==== k-fold cross-validation ====\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k \u2212 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data.  The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling  (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.\nFor example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and validate on d1, followed by training on d1 and validating on d0.\nWhen k = n (the number of observations), k-fold cross-validation is equivalent to leave-one-out cross-validation.In stratified k-fold cross-validation, the partitions are selected so that the mean response value is approximately equal in all the partitions.  In the case of binary classification, this means that each partition contains roughly the same proportions of the two types of class labels.\nIn repeated cross-validation the data is randomly split into k partitions several times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice.When many different statistical or machine learning models are being considered, greedy k-fold cross-validation can be used to quickly identify the most promising candidate models.\n\n\n==== Holdout method ====\nIn the holdout method, we randomly assign data points to two sets d0 and d1, usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train (build a model) on d0 and test (evaluate its performance) on d1.\nIn typical cross-validation, results of multiple runs of model-testing are averaged together; in contrast, the holdout method, in isolation, involves a single run. It should be used with caution because without such averaging of multiple runs, one may achieve highly misleading results. One's indicator of predictive accuracy (F*) will tend to be unstable since it will not be smoothed out by multiple iterations (see below).  Similarly, indicators of the specific role played by various predictor variables (e.g., values of regression coefficients) will tend to be unstable.\nWhile the holdout method can be framed as \"the simplest kind of cross-validation\", many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation.\n\n\n==== Repeated random sub-sampling validation ====\nThis method, also known as Monte Carlo cross-validation, creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (i.e., the number of partitions). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.\nAs the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation.\nIn a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data.\nA method that applies repeated random sub-sampling is RANSAC.\n\n\n== Nested cross-validation ==\nWhen cross-validation is used simultaneously for selection of the best set of hyperparameters and for error estimation (and assessment of generalization capacity), a nested cross-validation is required. Many variants exist. At least two variants can be distinguished:\n\n\n=== k*l-fold cross-validation ===\nThis is a truly nested variant which contains an outer loop of k sets and an inner loop of l sets. The total data set is split into k sets. One by one, a set is selected as the (outer) test set and the k - 1 other sets are combined into the corresponding outer training set. This is repeated for each of the k sets. Each outer training set is further sub-divided into l sets.  One by one, a set is selected as inner test (validation) set and the l - 1 other sets are combined into the corresponding inner training set. This is repeated for each of the l sets. The inner training sets are used to fit model parameters, while the outer test set is used as a validation set to provide an unbiased evaluation of the model fit. Typically, this is repeated for many different hyperparameters (or even different model types) and the validation set is used to determine the best hyperparameter set (and model type) for this inner training set. After this, a new model is fit on the entire outer training set, using the best set of hyperparameters from the inner cross-validation. The performance of this model is then evaluated using the outer test set.\n\n\n=== k-fold cross-validation with validation and test set ===\nThis is a type of k*l-fold cross-validation when l = k - 1. A single k-fold cross-validation is used with both a validation and test set. The total data set is split into k sets. One by one, a set is selected as test set. Then, one by one, one of the remaining sets is used as a validation set and the other k - 2 sets are used as training sets until all possible combinations have been evaluated. Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. Finally, for the selected parameter set, the test set is used to evaluate the model with the best parameter set. Here, two variants are possible: either evaluating the model that was trained on the training set or evaluating a new model that was fit on the combination of the training and the validation set.\n\n\n== Measures of fit ==\nThe goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model.  It can be used to estimate any quantitative measure of fit that is appropriate for the data and model.  For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures like positive predictive value could also be used.  When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors.\n\n\n== Using prior information ==\nWhen users apply cross-validation to select a good configuration \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  , then they might want to balance the cross-validated choice with their own estimate of the configuration. In this way, they can attempt to counter the volatility of cross-validation when the sample size is small and include relevant information from previous research. In a forecasting combination exercise, for instance, cross-validation can be applied to estimate the weights that are assigned to each forecast. Since a simple equal-weighted forecast is difficult to beat, a penalty can be added for deviating from equal weights.  Or, if cross-validation is applied to assign individual weights to observations, then one can penalize deviations from equal weights to avoid wasting potentially relevant information.  Hoornweg (2018) shows how a tuning parameter \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   can be defined so that a user can intuitively balance between the accuracy of cross-validation and the simplicity of sticking to a reference parameter \n  \n    \n      \n        \n          \u03bb\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{R}}\n   that is defined by the user.\nIf \n  \n    \n      \n        \n          \u03bb\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{i}}\n   denotes the \n  \n    \n      \n        \n          i\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle i^{th}}\n   candidate configuration that might be selected, then the loss function that is to be minimized can be defined as\n\n  \n    \n      \n        \n          L\n          \n            \n              \u03bb\n              \n                i\n              \n            \n          \n        \n        =\n        (\n        1\n        \u2212\n        \u03b3\n        )\n        \n          \n            \n               Relative Accuracy\n            \n          \n          \n            i\n          \n        \n        +\n        \u03b3\n        \n          \n            \n               Relative Simplicity\n            \n          \n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle L_{\\lambda _{i}}=(1-\\gamma ){\\mbox{ Relative Accuracy}}_{i}+\\gamma {\\mbox{ Relative Simplicity}}_{i}.}\n  Relative accuracy can be quantified as \n  \n    \n      \n        \n          \n            MSE\n          \n        \n        (\n        \n          \u03bb\n          \n            i\n          \n        \n        )\n        \n          /\n        \n        \n          \n            MSE\n          \n        \n        (\n        \n          \u03bb\n          \n            R\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mbox{MSE}}(\\lambda _{i})/{\\mbox{MSE}}(\\lambda _{R})}\n  , so that the mean squared error of a candidate  \n  \n    \n      \n        \n          \u03bb\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{i}}\n   is made relative to that of a user-specified \n  \n    \n      \n        \n          \u03bb\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{R}}\n  . The relative simplicity term measures the amount that \n  \n    \n      \n        \n          \u03bb\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{i}}\n   deviates from \n  \n    \n      \n        \n          \u03bb\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{R}}\n   relative to the maximum amount of deviation from \n  \n    \n      \n        \n          \u03bb\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{R}}\n  . Accordingly, relative simplicity can be specified as \n  \n    \n      \n        \n          \n            \n              (\n              \n                \u03bb\n                \n                  i\n                \n              \n              \u2212\n              \n                \u03bb\n                \n                  R\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n            \n              (\n              \n                \u03bb\n                \n                  max\n                \n              \n              \u2212\n              \n                \u03bb\n                \n                  R\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {(\\lambda _{i}-\\lambda _{R})^{2}}{(\\lambda _{\\max }-\\lambda _{R})^{2}}}}\n  , where \n  \n    \n      \n        \n          \u03bb\n          \n            max\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{\\max }}\n   corresponds to the \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   value with the highest permissible deviation from \n  \n    \n      \n        \n          \u03bb\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{R}}\n  . With \n  \n    \n      \n        \u03b3\n        \u2208\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\gamma \\in [0,1]}\n  , the user determines how high the influence of the reference parameter is relative to cross-validation.\nOne can add relative simplicity terms for multiple configurations \n  \n    \n      \n        c\n        =\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        C\n      \n    \n    {\\displaystyle c=1,2,...,C}\n   by specifying the loss function as\n\n  \n    \n      \n        \n          L\n          \n            \n              \u03bb\n              \n                i\n              \n            \n          \n        \n        =\n        \n          \n            \n               Relative Accuracy\n            \n          \n          \n            i\n          \n        \n        +\n        \n          \u2211\n          \n            c\n            =\n            1\n          \n          \n            C\n          \n        \n        \n          \n            \n              \u03b3\n              \n                c\n              \n            \n            \n              1\n              \u2212\n              \n                \u03b3\n                \n                  c\n                \n              \n            \n          \n        \n        \n          \n            \n               Relative Simplicity\n            \n          \n          \n            i\n            ,\n            c\n          \n        \n        .\n      \n    \n    {\\displaystyle L_{\\lambda _{i}}={\\mbox{ Relative Accuracy}}_{i}+\\sum _{c=1}^{C}{\\frac {\\gamma _{c}}{1-\\gamma _{c}}}{\\mbox{ Relative Simplicity}}_{i,c}.}\n  Hoornweg (2018) shows that a loss function with such an accuracy-simplicity tradeoff can also be used to intuitively define shrinkage estimators like the (adaptive) lasso and Bayesian / ridge regression. Click on the lasso for an example.\n\n\n== Statistical properties ==\nSuppose we choose a measure of fit F, and use cross-validation to produce an estimate F* of the expected fit EF of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for F* will vary. The statistical properties of F* result from this variation.\nThe cross-validation estimator F* is very nearly unbiased for EF. The reason that it is slightly biased is that the training set in cross-validation is slightly smaller than the actual data set (e.g. for LOOCV the training set size is n \u2212 1 when there are n observed cases). In nearly all situations, the effect of this bias will be conservative in that the estimated fit will be slightly biased in the direction suggesting a poorer fit. In practice, this bias is rarely a concern.\nThe variance of F* can be large. For this reason, if two statistical procedures are compared based on the results of cross-validation, the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of EF).  Some progress has been made on constructing confidence intervals around cross-validation estimates, but this is considered a difficult problem.\n\n\n== Computational issues ==\nMost forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available.  In particular, the prediction method can be a \"black box\" \u2013 there is no need to have access to the internals of its implementation.  If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly.  In some cases such as least squares and kernel regression, cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast \"updating rules\" such as the Sherman\u2013Morrison formula.  However one must be careful to preserve the \"total blinding\" of the validation set from the training procedure, otherwise bias may result.  An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the prediction residual error sum of squares (PRESS).\n\n\n== Limitations and misuse ==\nCross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled.\nIn many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is \"non-stationary\").  Both of these can introduce systematic differences between the training and validation sets.  For example, if a model for predicting stock values is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population.  As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year.  If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance.\nIn many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor.   New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity.  As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another.  When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation.\nThe reason for the success of the swapped sampling is a built-in control for human biases in model building.  In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused:\n\nBy performing an initial analysis to identify the most informative features using the entire data set \u2013 if feature selection or model tuning is required by the modeling procedure, this must be repeated on every training set. Otherwise, predictions will certainly be upwardly biased.  If cross-validation is used to decide which features to use, an inner cross-validation to carry out the feature selection on every training set must be performed.\nPerforming mean-centering, rescaling, dimensionality reduction, outlier removal or any other data-dependent preprocessing using the entire data set. While very common in practice, this has been shown to introduce biases into the cross-validation estimates. By allowing some of the training data to also be included in the test set \u2013 this can happen due to \"twinning\" in the data set, whereby some exactly identical or nearly identical samples are present in the data set. To some extent twinning always takes place even in perfectly independent training and validation samples. This is because some of the training sample observations will have nearly identical values of predictors as validation sample observations. And some of these will correlate with a target at better than chance levels in the same direction in both training and validation when they are actually driven by confounded predictors with poor external validity.  If such a cross-validated model is selected from a k-fold set, human confirmation bias will be at work and determine that such a model has been validated. This is why traditional cross-validation needs to be supplemented with controls for human bias and confounded model specification like swap sampling and prospective studies.\n\n\n== Cross validation for time-series models ==\nSince the order of the data is important, cross-validation might be problematic for time-series models. A more appropriate approach might be to use rolling cross-validation.However, if performance is described by a single summary statistic, it is possible that the approach described by Politis and Romano as a stationary bootstrap will work. The statistic of the bootstrap needs to accept an interval of the time series and return the summary statistic on it. The call to the stationary bootstrap needs to specify an appropriate mean interval length.\n\n\n== Applications ==\nCross-validation can be used to compare the performances of different predictive modeling procedures.  For example, suppose we are interested in optical character recognition, and we are considering using either a Support Vector Machine (SVM) or k-nearest neighbors (KNN) to predict the true character from an image of a handwritten character.  Using cross-validation, we could objectively compare these two methods in terms of their respective fractions of misclassified characters.  If we simply compared the methods based on their in-sample error rates, one method would likely appear to perform better, since it is more flexible and hence more prone to overfitting compared to the other method.\nCross-validation can also be used in variable selection. Suppose we are using the expression levels of 20 proteins to predict whether a cancer patient will respond to a drug. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative.\nA recent development in medical statistics is its use in meta-analysis. It forms the basis of the validation statistic, Vn which is used to test the statistical validity of meta-analysis summary estimates.  It has also been used in a more conventional sense in meta-analysis to estimate the likely prediction error of meta-analysis results.\n\n\n== See also ==\n\n\n== Notes and references ==", "Statistical model": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is \"a formal representation of a theory\" (Herman Ad\u00e8r quoting Kenneth Bollen).All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\n\n== Introduction ==\nInformally, a statistical model can be thought of as a statistical assumption (or set of statistical assumptions) with a certain property: that the assumption allows us to calculate the probability of any event. As an example, consider a pair of ordinary six-sided dice. We will study two different statistical assumptions about the dice.\nThe first statistical assumption is this: for each of the dice, the probability of each face (1, 2, 3, 4, 5, and 6) coming up is 1/6. From that assumption, we can calculate the probability of both dice coming up 5:\u2009 1/6 \u00d7 1/6\u2002=\u20021/36.\u2009 More generally, we can calculate the probability of any event: e.g. (1 and 2) or (3 and 3) or (5 and 6). \nThe alternative statistical assumption is this: for each of the dice, the probability of the face 5 coming up is 1/8 (because the dice are weighted). From that assumption, we can calculate the probability of both dice coming up 5:\u2009 1/8 \u00d7 1/8\u2002=\u20021/64.\u2009 We cannot, however, calculate the probability of any other nontrivial event, as the probabilities of the other faces are unknown.\nThe first statistical assumption constitutes a statistical model: because with the assumption alone, we can calculate the probability of any event. The alternative statistical assumption does not constitute a statistical model: because with the assumption alone, we cannot calculate the probability of every event.\nIn the example above, with the first assumption, calculating the probability of an event is easy. With some other examples, though, the calculation can be difficult, or even impractical (e.g. it might require millions of years of computation). For an assumption to constitute a statistical model, such difficulty is acceptable: doing the calculation does not need to be practicable, just theoretically possible.\n\n\n== Formal definition ==\nIn mathematical terms, a statistical model is usually thought of as a pair (\n  \n    \n      \n        S\n        ,\n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle S,{\\mathcal {P}}}\n  ), where \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   is the set of possible observations, i.e. the sample space, and \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n   is a set of probability distributions on \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  .The intuition behind this definition is as follows.  It is assumed that there is a \"true\" probability distribution induced by the process that generates the observed data.  We choose \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n   to represent a set (of distributions) which contains a distribution that adequately approximates the true distribution. \nNote that we do not require that \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n   contains the true distribution, and in practice that is rarely the case. Indeed, as Burnham & Anderson state, \"A model is a simplification or approximation of reality and hence will not reflect all of reality\"\u2014hence the saying \"all models are wrong\".\nThe set \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n   is almost always parameterized: \n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          P\n          \n            \u03b8\n          \n        \n        :\n        \u03b8\n        \u2208\n        \u0398\n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}}\n  . The set \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   defines the parameters of the model. A parameterization is generally required to have distinct parameter values give rise to distinct distributions, i.e. \n  \n    \n      \n        \n          P\n          \n            \n              \u03b8\n              \n                1\n              \n            \n          \n        \n        =\n        \n          P\n          \n            \n              \u03b8\n              \n                2\n              \n            \n          \n        \n        \u21d2\n        \n          \u03b8\n          \n            1\n          \n        \n        =\n        \n          \u03b8\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{\\theta _{1}}=P_{\\theta _{2}}\\Rightarrow \\theta _{1}=\\theta _{2}}\n   must hold (in other words, it must be injective). A parameterization that meets the requirement is said to be identifiable.\n\n\n== An example ==\nSuppose that we have a population of children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 1.5 meters tall. We could formalize that relationship in a linear regression model, like this: \nheighti = b0 + b1agei + \u03b5i, where b0 is the intercept, b1 is a parameter that age is multiplied by to obtain a prediction of height, \u03b5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.\nAn admissible model must be consistent with all the data points.  Thus, a straight line (heighti = b0 + b1agei) cannot be the equation for a model of the data\u2014unless it exactly fits all the data points, i.e. all the data points lie perfectly on the line.  The error term, \u03b5i, must be included in the equation, so that the model is consistent with all the data points.\nTo do statistical inference, we would first need to assume some probability distributions for the \u03b5i.  For instance, we might assume that the \u03b5i distributions are i.i.d. Gaussian, with zero mean.  In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.\nWe can formally specify the model in the form (\n  \n    \n      \n        S\n        ,\n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle S,{\\mathcal {P}}}\n  ) as follows.  The sample space, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  , of our model comprises the set of all possible pairs (age, height).  Each possible value of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   = (b0, b1, \u03c32) determines a distribution on \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  ; denote that distribution by \n  \n    \n      \n        \n          P\n          \n            \u03b8\n          \n        \n      \n    \n    {\\displaystyle P_{\\theta }}\n  .  If \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   is the set of all possible values of \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  , then \n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          P\n          \n            \u03b8\n          \n        \n        :\n        \u03b8\n        \u2208\n        \u0398\n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}}\n  . (The parameterization is identifiable, and this is easy to check.)\nIn this example, the model is determined by (1) specifying \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   and (2) making some assumptions relevant to \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n  . There are two assumptions: that height can be approximated by a linear function of age; that errors in the approximation are distributed as i.i.d. Gaussian.  The assumptions are sufficient to specify \n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n  \u2014as they are required to do.\n\n\n== General remarks ==\nA statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the above example with children's heights, \u03b5 is a stochastic variable; without that stochastic variable, the model would be deterministic.\nStatistical models are often used even when the data-generating process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).\nChoosing an appropriate statistical model to represent a given data-generating process is sometimes extremely difficult, and may require knowledge of both the process and relevant statistical analyses. Relatedly, the statistician Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".There are three purposes for a statistical model, according to Konishi & Kitagawa.\nPredictions\nExtraction of information\nDescription of stochastic structuresThose three purposes are essentially the same as the three purposes indicated by Friendly & Meyer: prediction, estimation, description. The three purposes correspond with the three kinds of logical reasoning: deductive reasoning, inductive reasoning, abductive reasoning.\n\n\n== Dimension of a model ==\nSuppose that we have a statistical model (\n  \n    \n      \n        S\n        ,\n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle S,{\\mathcal {P}}}\n  ) with \n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          P\n          \n            \u03b8\n          \n        \n        :\n        \u03b8\n        \u2208\n        \u0398\n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}}\n  .  The model is said to be parametric if \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   has a finite dimension. In notation, we write that \n  \n    \n      \n        \u0398\n        \u2286\n        \n          \n            R\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\Theta \\subseteq \\mathbb {R} ^{k}}\n   where k is a positive integer (\n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n   denotes the real numbers; other sets can be used, in principle).  Here, k is called the dimension of the model.\nAs an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that  \n\n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        \n          {\n          \n            \n              P\n              \n                \u03bc\n                ,\n                \u03c3\n              \n            \n            (\n            x\n            )\n            \u2261\n            \n              \n                1\n                \n                  \n                    \n                      2\n                      \u03c0\n                    \n                  \n                  \u03c3\n                \n              \n            \n            exp\n            \u2061\n            \n              (\n              \n                \u2212\n                \n                  \n                    \n                      (\n                      x\n                      \u2212\n                      \u03bc\n                      \n                        )\n                        \n                          2\n                        \n                      \n                    \n                    \n                      2\n                      \n                        \u03c3\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n              )\n            \n            :\n            \u03bc\n            \u2208\n            \n              R\n            \n            ,\n            \u03c3\n            >\n            0\n          \n          }\n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\left\\{P_{\\mu ,\\sigma }(x)\\equiv {\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right):\\mu \\in \\mathbb {R} ,\\sigma >0\\right\\}}\n  .In this example, the dimension, k, equals 2.\nAs another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean): this leads to the same statistical model as was used in the example with children's heights.  The dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)\nAlthough formally \n  \n    \n      \n        \u03b8\n        \u2208\n        \u0398\n      \n    \n    {\\displaystyle \\theta \\in \\Theta }\n   is a single parameter that has dimension k, it is sometimes regarded as comprising k separate parameters. For example, with the univariate Gaussian distribution, \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is formally a single parameter with dimension 2, but it is sometimes regarded as comprising 2 separate parameters\u2014the mean and the standard deviation.\nA statistical model is nonparametric if the parameter set \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   is infinite dimensional.  A statistical model is semiparametric if it has both finite-dimensional and infinite-dimensional parameters.  Formally, if k is the dimension of \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n   and n is the number of samples, both semiparametric and nonparametric models have \n  \n    \n      \n        k\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle k\\rightarrow \\infty }\n   as \n  \n    \n      \n        n\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle n\\rightarrow \\infty }\n  .  If \n  \n    \n      \n        k\n        \n          /\n        \n        n\n        \u2192\n        0\n      \n    \n    {\\displaystyle k/n\\rightarrow 0}\n   as \n  \n    \n      \n        n\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle n\\rightarrow \\infty }\n  , then the model is semiparametric; otherwise, the model is nonparametric.\nParametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, \"These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\".\n\n\n== Nested models ==\n\nTwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model \n\ny = b0 + b1x + b2x2 + \u03b5,    \u03b5 ~ \ud835\udca9(0, \u03c32)has, nested within it, the linear model \n\ny = b0 + b1x + \u03b5,    \u03b5 ~ \ud835\udca9(0, \u03c32)\u2014we constrain the parameter b2 to equal 0.\nIn both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension 1).  Such is often, but not always, the case.  As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.\n\n\n== Comparing models ==\n\nComparing statistical models is fundamental for much of statistical inference. Indeed, Konishi & Kitagawa (2008, p. 75) state this: \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models.\"\nCommon criteria for comparing models include the following: R2, Bayes factor, Akaike information criterion, and the likelihood-ratio test together with its generalization, the relative likelihood.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nDavison, A. C. (2008), Statistical Models, Cambridge University Press\nDrton, M.; Sullivant, S. (2007), \"Algebraic statistical models\" (PDF), Statistica Sinica, 17: 1273\u20131297\nFreedman, D. A. (2009), Statistical Models, Cambridge University Press\nHelland, I. S. (2010), Steps Towards a Unified Basis for Scientific Models and Methods, World Scientific\nKroese, D. P.; Chan, J. C. C. (2014), Statistical Modeling and Computation, Springer\nShmueli, G. (2010), \"To explain or to predict?\", Statistical Science, 25 (3): 289\u2013310, arXiv:1101.0891, doi:10.1214/10-STS330, S2CID 15900983", "Minkowski distance": "The Minkowski distance or Minkowski metric is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance. It is named after the German mathematician Hermann Minkowski.\n\n\n== Definition ==\nThe Minkowski distance of order \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   (where \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is an integer) between two points\n\nis defined as:\n\nFor \n  \n    \n      \n        p\n        \u2265\n        1\n        ,\n      \n    \n    {\\displaystyle p\\geq 1,}\n   the Minkowski distance is a metric as a result of the Minkowski inequality. When \n  \n    \n      \n        p\n        <\n        1\n        ,\n      \n    \n    {\\displaystyle p<1,}\n   the distance between \n  \n    \n      \n        (\n        0\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (0,0)}\n   and \n  \n    \n      \n        (\n        1\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (1,1)}\n   is \n  \n    \n      \n        \n          2\n          \n            1\n            \n              /\n            \n            p\n          \n        \n        >\n        2\n        ,\n      \n    \n    {\\displaystyle 2^{1/p}>2,}\n   but the point \n  \n    \n      \n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (0,1)}\n   is at a distance \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n   from both of these points.  Since this violates the triangle inequality, for \n  \n    \n      \n        p\n        <\n        1\n      \n    \n    {\\displaystyle p<1}\n   it is not a metric. However, a metric can be obtained for these values by simply removing the exponent of \n  \n    \n      \n        1\n        \n          /\n        \n        p\n        .\n      \n    \n    {\\displaystyle 1/p.}\n   The resulting metric is also an F-norm.\nMinkowski distance is typically used with \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   being 1 or 2, which correspond to the Manhattan distance and the Euclidean distance, respectively. In the limiting case of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   reaching infinity, we obtain the Chebyshev distance:\n\nSimilarly, for \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   reaching negative infinity, we have:\n\nThe Minkowski distance can also be viewed as a multiple of the power mean of the component-wise differences between \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   and \n  \n    \n      \n        Q\n        .\n      \n    \n    {\\displaystyle Q.}\n  \nThe following figure shows unit circles (the level set of the distance function where all points are at the unit distance from the center) with various values of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  :\n\n\n== See also ==\nGeneralized mean \u2013 N-th root of the arithmetic mean of the given numbers raised to the power n\n\n  \n    \n      \n        \n          L\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle L^{p}}\n   space \u2013 Function spaces generalizing finite-dimensional p norm spaces\nNorm (mathematics) \u2013 Length in a vector space\n\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  -norm \u2013 Function spaces generalizing finite-dimensional p norm spacesPages displaying short descriptions of redirect targets\n\n\n== External links ==\nSimple IEEE 754 implementation in C++\nNPM JavaScript Package/Module", "Data type": "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types. A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take. On literal data, it tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.\n\n\n== Concept ==\nA data type may be specified for many reasons: similarity, convenience, or to focus the attention. It is frequently a matter of good organization\nthat aids the understanding of complex definitions. Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity. An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted.Different languages may use different data types or similar types with different semantics. For example, in the Python programming language, int represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication. However in the Java programming language, the type int represents the set of 32-bit integers ranging in value from \u22122,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow. In Rust this 32-bit integer type is denoted i32 and panics on overflow in debug mode.Most programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.  For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name.\nData types are used within type systems, which offer various ways of defining, implementing, and using them. In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data. A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\nMost data types in statistics have comparable types in computer programming, and vice versa, as shown in the following table:\n\n\n== Definition ==\nParnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used\u2014sometimes implicitly\u2014in the literature:\n\nSyntactic\nA type is a purely syntactic label associated with a variable when it is declared. Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types.\nRepresentation\nA type is defined in terms of a composition of more primitive types\u2014often machine types.\nRepresentation and behaviour\nA type is defined as its representation and a set of operators manipulating these representations.\nValue space\nA type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.\nValue space and behaviour\nA type is a set of values which a variable can possess and a set of functions that one can apply to these values.The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.\n\n\n== Classification ==\nData types may be categorized according to several factors:\n\nPrimitive data types or built-in data types are types that are built-in to a language implementation. User-defined data types are non-primitive types. For example, Java's numeric types are primitive, while classes are user-defined.\nA value of an atomic type is a single data item that cannot be broken into component parts. A value of a composite type  or aggregate type is a collection of data items that can be accessed individually. For example, an integer is generally considered atomic, although it consists of a sequence of bits, while an array of integers is certainly composite.\nBasic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements. Generated data types or derived data types are specified, and partly defined, in terms of other data types. All basic types are atomic. For example, integers are a basic type defined in mathematics, while an array of integers is the result of applying an array type generator to the integer type.The terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably.\n\n\n== Examples ==\n\n\n=== Machine data types ===\nAll data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2011, typically 32 or 64 bits).\nMachine data types expose or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable. Hence machine types are mainly used in systems programming or low-level programming languages. In higher-level languages most data types are abstracted in that they do not have a language-defined machine representation. The C programming language, for instance, supplies types such as booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined. The only C type with a precise machine representation is the char type that represents a byte.\n\n\n=== Boolean type ===\nThe Boolean type represents the values true and false. Although only two values are possible, they are more often represented as a word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit. Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true.\nBoolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.\n\n\n=== Numeric types ===\nAlmost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\nFloating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form a \u00d7 2b (where a and b are integers), but displayed in familiar decimal form.\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\nFor independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.\n\n\n=== Enumerations ===\nThe enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.  If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.\n\n\n=== String and text types ===\nStrings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size.\nSince most character sets include the digits, it is possible to have a numeric string, such as \"1234\". These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them.\n\n\n=== Union types ===\n\nA union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g. \"float or long integer\". In contrast with a record, which could be defined to contain a float and an integer, a union may only contain one subtype at a time.\nA tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.\n\n\n=== Algebraic data types ===\n\nAn algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.\nIf there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type.\nOne common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a.\n\n\n=== Data structures ===\nSome types are very useful for storing and retrieving data and are called data structures. Common data structures include:\n\nAn array (also called vector, list, or sequence) stores a number of elements and provides random access to individual elements. The elements of an array are typically (but not in all contexts) required to be of the same type. Arrays may be fixed-length or expandable. Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array).\nRecord (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.\nAn object contains a number of data fields, like a record, and also offers a number of subroutines for accessing or modifying them, called methods.\nthe singly linked list, which can be used to implement a queue and is defined in Haskell as the ADT data List a = Nil | Cons a (List a), and\nthe binary tree, which allows fast searching, and can be defined in Haskell as the ADT data BTree a = Nil | Node (BTree a) a (BTree a)\n\n\n=== Abstract data types ===\n\nAn abstract data type is a data type that does not specify the concrete representation of the data.  Instead, a formal specification based on the data type's operations is used to describe it. Any implementation of a specification must fulfill the rules given. For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array. Another example is a set which stores values, without any particular order, and no repeated values. Values themselves are not retrieved from sets, rather one tests a value for membership to obtain a boolean \"in\" or \"not in\".\nAbstract data types are used in formal semantics and program verification and, less strictly, in design. Beyond verification, a specification might immediately be turned into an implementation. The OBJ family of programming languages for instance bases on this option using equations for specification and rewriting to run them. Algebraic specification was an important subject of research in CS around 1980 and almost a synonym for abstract data types at that time. It has a mathematical foundation in universal algebra. The specification language can be made more expressive by allowing other formulas than only equations.\nA more involved example is the Boom hierarchy of the binary tree, list, bag and set abstract data types. All these data types can be declared by three operations: null, which constructs the empty container, single, which constructs a container from a single element and append, which combines two containers of the same type. The complete specification for the four data types can then be given by successively adding the following rules over these operations:\n\nAccess to the data can be specified by pattern-matching over the three operations, e.g. a member function for these containers by:\n\nCare must be taken to ensure that the function is invariant under the relevant rules for the data type. Within each of the equivalence classes implied by the chosen subset of equations, it has to yield the same result for all of its members.\n\n\n=== Pointers and references ===\n\nThe main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, pointers are considered a separate type to the type of data they point to, even if the underlying representation is the same.\n\n\n=== Function types ===\n\nFunctional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions. Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data. Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type Int -> Bool denoting functions taking an integer and returning a boolean. In C, a function is not a first-class data type but function pointers can be manipulated by the program. Java and C++ originally did not have function values but have added them in C++11 and Java 8.\n\n\n=== Type constructors ===\n\nA type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type. Product types, function types, power types and list types can be made into type constructors.\n\n\n=== Quantified types ===\nUniversally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as \n  \n    \n      \n        \u2200\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\forall x.f(x)}\n   or forall x. f x and is the intersection over all types x of the body f x, i.e. the value is of type f x for every x. Existential quantification written as \n  \n    \n      \n        \u2203\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\exists x.f(x)}\n   or exists x. f x and is the union over all types x of the body f x, i.e. the value is of type f x for some x.\nIn Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type.\n\n\n=== Refinement types ===\n\nA refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. For instance, the type of natural numbers greater than 5 may be written as \n  \n    \n      \n        {\n        n\n        \u2208\n        \n          N\n        \n        \n        \n          |\n        \n        \n        n\n        >\n        5\n        }\n      \n    \n    {\\displaystyle \\{n\\in \\mathbb {N} \\,|\\,n>5\\}}\n  \n\n\n=== Dependent types ===\n\nA dependent type is a type whose definition depends on a value. Two common examples of dependent types are dependent functions and dependent pairs. The return type of a dependent function may depend on the value (not just type) of one of its arguments. A dependent pair may have a second value of which the type depends on the first value.\n\n\n=== Intersection types ===\n\nAn intersection type is a type containing those values that are members of two specified types. For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces. Therefore, an object of type Boolean is a member of the type Serializable & Comparable. Considering types as sets of values, the intersection type \n  \n    \n      \n        \u03c3\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle \\sigma \\cap \\tau }\n   is the set-theoretic intersection of \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   and \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  . It is also possible to define a dependent intersection type, denoted \n  \n    \n      \n        (\n        x\n        :\n        \u03c3\n        )\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle (x:\\sigma )\\cap \\tau }\n  , where the type \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n   may depend on the term variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .\n\n\n=== Meta types ===\n\nSome programming languages represent the type information as data, enabling type introspection and reflection. In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.\n\n\n=== Convenience types ===\nFor convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency). These may be built-in to the language or implemented as composite types in a library.\n\n\n== See also ==\nC data types\nData dictionary\nFunctional programming\nKind\nType (model theory)\nType theory for the mathematical models of types\nType system for different choices in programming language typing\nType conversion\nISO/IEC 11404, General Purpose Datatypes\n\n\n== References ==\n\n\n== Further reading ==\nParnas, David L.; Shore, John E.; Weiss, David (1976). \"Abstract types defined as classes of variables\". Proceedings of the 1976 Conference on Data: Abstraction, Definition and Structure: 149\u2013154. doi:10.1145/800237.807133. S2CID 14448258.\nCardelli, Luca; Wegner, Peter (December 1985). \"On Understanding Types, Data Abstraction, and Polymorphism\" (PDF). ACM Computing Surveys. 17 (4): 471\u2013523. CiteSeerX 10.1.1.117.695. doi:10.1145/6041.6042. ISSN 0360-0300. S2CID 2921816. Archived (PDF) from the original on 2008-12-03.\nCleaveland, J. Craig (1986). An Introduction to Data Types. Addison-Wesley. ISBN 978-0201119404.\n\n\n== External links ==\n Media related to Data types at Wikimedia Commons", "Active learning (machine learning)": "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning.\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\n\n\n== Definitions ==\nLet T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\nDuring each iteration, i, T is broken up into three subsets\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            K\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{K,i}}\n  : Data points where the label is known.\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            U\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{U,i}}\n  : Data points where the label is unknown.\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            C\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{C,i}}\n  : A subset of TU,i that is chosen to be labeled.Most of the current research in active learning involves the best method to choose the data points for TC,i.\n\n\n== Scenarios ==\nMembership Query Synthesis: This is where the learner generates its own instance from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small.\nPool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner \u201cunderstands\u201d the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels.\nStream-Based Selective Sampling: Here, each unlabeled data point is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint.\n\n\n== Query strategies ==\nAlgorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:\nBalance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.\nExpected model change: label those points that would most change the current model.\nExpected error reduction: label those points that would most reduce the model's generalization error.\nExponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.\nRandom Sampling: a sample is randomly selected.\nUncertainty sampling: label those points for which the current model is least certain as to what the correct output should be.\nEntropy Sampling: The entropy formula is used on each sample, and the sample with the highest entropy is considered to be the least certain.\nMargin Sampling: The sample with the smallest difference between the two highest class probabilities is considered to be the most uncertain.\nLeast Confident Sampling: The sample with the smallest best probability is considered to be the most uncertain.\nQuery by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the \"committee\" disagrees the most\nQuerying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.\nVariance reduction: label those points that would minimize output variance, which is one of the components of error.\nConformal predictors: This method predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.\nMismatch-first farthest-traversal: The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data.\nUser Centered Labeling Strategies: Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances.A wide variety of algorithms have been studied that fall into these categories.\n\n\n== Minimum marginal hyperplane ==\nSome active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, W, of each unlabeled datum in TU,i and treat W as an n-dimensional distance from that datum to the separating hyperplane.\nMinimum Marginal Hyperplane methods assume that the data with the smallest W are those that the SVM is most uncertain about and therefore should be placed in TC,i to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest W. Tradeoff methods choose a mix of the smallest and largest Ws.\n\n\n== See also ==\nList of datasets for machine learning research\n\n\n== Notes ==", "Learning rate": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". In the adaptive control literature, the learning rate is commonly referred to as gain.In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms.\n\n\n== Learning rate schedule ==\nInitial rate can be left as system default or can be selected using a range of techniques. A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: decay and momentum . There are many different learning rate schedules but the most common are time-based, step-based and exponential.Decay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minimum, and is controlled by a hyperparameter.\nMomentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps. Momentum is controlled by a hyper parameter analogous to a ball's mass which must be chosen manually\u2014too high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras.\nTime-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:\n\n  \n    \n      \n        \n          \u03b7\n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            \n              \u03b7\n              \n                n\n              \n            \n            \n              1\n              +\n              d\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle \\eta _{n+1}={\\frac {\\eta _{n}}{1+dn}}}\n  \nwhere \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n   is the learning rate, \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   is a decay parameter and \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the iteration step.\nStep-based learning schedules changes the learning rate according to some pre defined steps. The decay application formula is here defined as:\n\n  \n    \n      \n        \n          \u03b7\n          \n            n\n          \n        \n        =\n        \n          \u03b7\n          \n            0\n          \n        \n        \n          d\n          \n            \n              \u230a\n              \n                \n                  \n                    1\n                    +\n                    n\n                  \n                  r\n                \n              \n              \u230b\n            \n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}=\\eta _{0}d^{\\left\\lfloor {\\frac {1+n}{r}}\\right\\rfloor }}\n  \nwhere \n  \n    \n      \n        \n          \u03b7\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}}\n   is the learning rate at iteration \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , \n  \n    \n      \n        \n          \u03b7\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\eta _{0}}\n   is the initial learning rate, \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   is how much the learning rate should change at each drop (0.5 corresponds to a halving) and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   corresponds to the droprate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations). The floor function (\n  \n    \n      \n        \u230a\n        \u2026\n        \u230b\n      \n    \n    {\\displaystyle \\lfloor \\dots \\rfloor }\n  ) here drops the value of its input to 0 for all values smaller than 1.\nExponential learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:\n\n  \n    \n      \n        \n          \u03b7\n          \n            n\n          \n        \n        =\n        \n          \u03b7\n          \n            0\n          \n        \n        \n          e\n          \n            \u2212\n            d\n            n\n          \n        \n      \n    \n    {\\displaystyle \\eta _{n}=\\eta _{0}e^{-dn}}\n  \nwhere \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   is a decay parameter.\n\n\n== Adaptive learning rate ==\nThe issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, and Adam which are generally built into deep learning libraries such as Keras.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nG\u00e9ron, Aur\u00e9lien (2017). \"Gradient Descent\". Hands-On Machine Learning with Scikit-Learn and TensorFlow. O'Reilly. pp. 113\u2013124. ISBN 978-1-4919-6229-9.\nPlagianakos, V. P.; Magoulas, G. D.; Vrahatis, M. N. (2001). \"Learning Rate Adaptation in Stochastic Gradient Descent\". Advances in Convex Analysis and Global Optimization. Kluwer. pp. 433\u2013444. ISBN 0-7923-6942-4.\n\n\n== External links ==\nde Freitas, Nando (February 12, 2015). \"Optimization\". Deep Learning Lecture 6. University of Oxford \u2013 via YouTube.", "Missing data": "In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.  Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (\"subject\"). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies\u2014for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing.\nData often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher\u2014for example, when data collection is done improperly or mistakes are made in data entry.These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random.  Missing data can be handled similarly as censored data.\n\n\n== Types ==\nUnderstanding the reasons why data are missing is important for handling the remaining data correctly. If values are missing completely at random, the data sample is likely still representative of the population. But if the values are missing systematically, analysis may be biased.  For example, in a study of the relation between IQ and income, if participants with an above-average IQ tend to skip the question \u2018What is your salary?\u2019, analyses that do not take into account this missing at random (MAR pattern (see below)) may falsely fail to find a positive association between IQ and salary.  Because of these problems, methodologists routinely advise researchers to design studies to minimize the occurrence of missing values. Graphical models can be used to describe the missing data mechanism in detail.\n\n\n=== Missing completely at random ===\nValues in a data set are missing completely at random (MCAR) if the events that lead to any particular data-item being missing are independent both of observable variables and of unobservable parameters of interest, and occur entirely at random. When data are MCAR, the analysis performed on the data is unbiased; however, data are rarely MCAR.\nIn the case of MCAR, the missingness of data is unrelated to any study variable: thus, the participants with completely observed data are in effect a random sample of all the participants assigned a particular intervention. With MCAR, the random assignment of treatments is assumed to be preserved, but that is usually an unrealistically strong assumption in practice.\n\n\n=== Missing at random ===\nMissing at random (MAR) occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information. Since MAR is an assumption that is impossible to verify statistically, we must rely on its substantive reasonableness. An example is that males are less likely to fill in a depression survey but this has nothing to do with their level of depression, after accounting for maleness. Depending on the analysis method, these data can still induce parameter bias in analyses due to the contingent emptiness of cells (male, very high depression may have zero entries). However, if the parameter is estimated with Full Information Maximum Likelihood, MAR will provide asymptotically unbiased estimates.\n\n\n=== Missing not at random ===\nMissing not at random (MNAR) (also known as nonignorable nonresponse) is data that is neither MAR nor MCAR (i.e. the value of the variable that's missing is related to the reason it's missing). To extend the previous example, this would occur if men failed to fill in a depression survey because of their level of depression.\nSamuelson and Spirer (1992) discussed how missing and/or distorted data about demographics, law enforcement, and health could be indicators of patterns of human rights violations.  They gave several fairly well documented examples.\n\n\n== Techniques of dealing with missing data ==\nMissing data reduces the representativeness of the sample and can therefore distort inferences about the population. Generally speaking, there are three main approaches to handle missing data: (1) Imputation\u2014where values are filled in the place of missing data, (2) omission\u2014where samples with invalid data are discarded from further analysis and (3) analysis\u2014by directly applying methods unaffected by the missing values. One systematic review addressing the prevention and handling of missing data for patient-centered outcomes research identified 10 standards as necessary for the prevention and handling of missing data. These include standards for study design, study conduct, analysis, and reporting.In some practical application, the experimenters can control the level of missingness, and prevent missing values before gathering the data. For example, in computer questionnaires, it is often not possible to skip a question. A question has to be answered, otherwise one cannot continue to the next. So missing values due to the participant are eliminated by this type of questionnaire, though this method may not be permitted by an ethics board overseeing the research. In survey research, it is common to make multiple efforts to contact each individual in the sample, often sending letters to attempt to persuade those who have decided not to participate to change their minds.:\u200a161\u2013187\u200a However, such techniques can either help or hurt in terms of reducing the negative inferential effects of missing data, because the kind of people who are willing to be persuaded to participate after initially refusing or not being home are likely to be significantly different from the kinds of people who will still refuse or remain unreachable after additional effort.:\u200a188\u2013198\u200aIn situations where missing values are likely to occur, the researcher is often advised on planning to use methods of data analysis methods that are robust to missingness. An analysis is robust when we are confident that mild to moderate violations of the technique's key assumptions will produce little or no bias, or distortion in the conclusions drawn about the population.\n\n\n=== Imputation ===\n\nSome data analysis techniques are not robust to missingness, and require to \"fill in\", or impute the missing data. Rubin (1987) argued that repeating imputation even a few times (5 or less) enormously improves the quality of estimation. For many practical purposes, 2 or 3 imputations capture most of the relative efficiency that could be captured with a larger number of imputations.  However, a too-small number of imputations can lead to a substantial loss of statistical power, and some scholars now recommend 20 to 100 or more.  Any multiply-imputed data analysis must be repeated for each of the imputed data sets and, in some cases, the relevant statistics must be combined in a relatively complicated way. Multiple imputation is not conducted in specific disciplines, as there is a lack of training or misconceptions about them. Methods such as listwise deletion have been used to impute data but it has been found to introduce additional bias. There is a beginner guide that provides a step-by-step instruction how to impute data.  \nThe expectation-maximization algorithm is an approach in which values of the statistics which would be computed if a complete dataset were available are estimated (imputed), taking into account the pattern of missing data. In this approach, values for individual missing data-items are not usually imputed.\n\n\n==== Interpolation ====\n\nIn the mathematical field of numerical analysis, interpolation is a method of constructing new data points within the range of a discrete set of known data points.\nIn the comparison of two paired samples with missing data, a test statistic that uses all available data without the need for imputation is the partially overlapping samples t-test. This is valid under normality and assuming MCAR\n\n\n=== Partial deletion ===\nMethods which involve reducing the data available to a dataset having no missing values include:\n\nListwise deletion/casewise deletion\nPairwise deletion\n\n\n=== Full analysis ===\nMethods which take full account of all information available, without the distortion resulting from using imputed values as if they were actually observed:\n\nGenerative approaches:\nThe expectation-maximization algorithm\nfull information maximum likelihood estimation\nDiscriminative approaches:\nMax-margin classification of data with absent features Partial identification methods may also be used.\n\n\n== Model-based techniques ==\nModel based techniques, often using graphs, offer\nadditional tools for testing missing data types (MCAR, MAR, MNAR) and for estimating parameters under missing data conditions. For example, a test for refuting MAR/MCAR reads as follows:\nFor any three variables X,Y, and Z where Z is fully observed and X and Y partially observed, the data should satisfy:\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        \n          R\n          \n            y\n          \n        \n        \n          |\n        \n        (\n        \n          R\n          \n            x\n          \n        \n        ,\n        Z\n        )\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp R_{y}|(R_{x},Z)}\n  .\nIn words, the observed portion of X should be independent on the missingness status of Y, conditional on every value of Z.\nFailure to satisfy this condition indicates that the problem belongs to the MNAR category.(Remark:\nThese tests are necessary for variable-based MAR which is a slight variation of event-based MAR.)\nWhen data falls into MNAR category techniques are available for consistently estimating parameters when certain conditions hold in the model.\nFor example, if Y explains the reason for missingness in X and Y itself has missing values, the joint probability distribution of X and Y can still be estimated if the\nmissingness of Y is random.\nThe estimand in this case will be:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                X\n                ,\n                Y\n                )\n              \n              \n                \n                =\n                P\n                (\n                X\n                \n                  |\n                \n                Y\n                )\n                P\n                (\n                Y\n                )\n              \n            \n            \n              \n              \n                \n                =\n                P\n                (\n                X\n                \n                  |\n                \n                Y\n                ,\n                \n                  R\n                  \n                    x\n                  \n                \n                =\n                0\n                ,\n                \n                  R\n                  \n                    y\n                  \n                \n                =\n                0\n                )\n                P\n                (\n                Y\n                \n                  |\n                \n                \n                  R\n                  \n                    y\n                  \n                \n                =\n                0\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(X,Y)&=P(X|Y)P(Y)\\\\&=P(X|Y,R_{x}=0,R_{y}=0)P(Y|R_{y}=0)\\end{aligned}}}\n  where \n  \n    \n      \n        \n          R\n          \n            x\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle R_{x}=0}\n   and \n  \n    \n      \n        \n          R\n          \n            y\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle R_{y}=0}\n   denote the observed portions of their respective variables.\nDifferent model structures may yield different estimands and different procedures of estimation whenever consistent estimation is possible.  The preceding estimand calls for first\nestimating \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Y\n        )\n      \n    \n    {\\displaystyle P(X|Y)}\n   from complete data and multiplying it by \n  \n    \n      \n        P\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle P(Y)}\n   estimated from cases in which Y is observed  regardless of the status of X. Moreover, in order to\nobtain a consistent estimate it is crucial that the first term be \n  \n    \n      \n        P\n        (\n        X\n        \n          |\n        \n        Y\n        )\n      \n    \n    {\\displaystyle P(X|Y)}\n   as opposed to \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        )\n      \n    \n    {\\displaystyle P(Y|X)}\n  .\nIn many cases model based techniques permit the model structure to undergo refutation tests.\nAny model which implies the independence between a partially observed variable X and the missingness indicator of another variable Y (i.e. \n  \n    \n      \n        \n          R\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle R_{y}}\n  ), conditional\non \n  \n    \n      \n        \n          R\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle R_{x}}\n   can be submitted to  the following refutation test:\n\n  \n    \n      \n        X\n        \u22a5\n        \n        \n        \n        \u22a5\n        \n          R\n          \n            y\n          \n        \n        \n          |\n        \n        \n          R\n          \n            x\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp R_{y}|R_{x}=0}\n  .\nFinally, the estimands that emerge from these techniques are derived in closed form and do not require iterative procedures such as Expectation Maximization that\nare susceptible to local optima.A special class of problems appears when the probability of the missingness depends on time. For example, in the trauma databases the probability to lose data about the trauma outcome depends on the day after trauma. In these cases various non-stationary Markov chain models are applied.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nAcock AC (2005), \"Working with missing values\", Journal of Marriage and Family, 67 (4): 1012\u201328, doi:10.1111/j.1741-3737.2005.00191.x, archived from the original on 2013-01-05\nAllison, Paul D. (2001), Missing Data, SAGE Publishing\nBouza-Herrera, Carlos N. (2013), Handling Missing Data in Ranked Set Sampling, Springer\nEnders, Craig K. (2010), Applied Missing Data Analysis, Guilford Press\nGraham, John W. (2012), Missing Data, Springer\nMolenberghs, Geert; Fitzmaurice, Garrett; Kenward, Michael G.; Tsiatis, Anastasios; Verbeke, Geert, eds. (2015), Handbook of Missing Data Methodology, Chapman & Hall\nRaghunathan, Trivellore (2016), Missing Data Analysis in Practice, Chapman & Hall\nLittle, Roderick J. A.; Rubin, Donald B. (2002), Statistical Analysis with Missing Data (2nd ed.), Wiley\nTsiatis, Anastasios A. (2006), Semiparametric Theory and Missing Data, Springer\nVan den Broeck J, Cunningham SA, Eeckels R, Herbst K (2005), \"Data cleaning: detecting, diagnosing, and editing data abnormalities\", PLOS Medicine, 2 (10): e267, doi:10.1371/journal.pmed.0020267, PMC 1198040, PMID 16138788, S2CID 5667073\nZarate LE, Nogueira BM, Santos TR, Song MA (2006). \"Techniques for Missing Value Recovering in Imbalanced Databases: Application in a marketing database with massive missing data\". IEEE International Conference on Systems, Man and Cybernetics, 2006. SMC '06. Vol. 3. pp. 2658\u20132664. doi:10.1109/ICSMC.2006.385265.\n\n\n== External links ==\n\n\n=== Background ===\nMissing Data, Department of Medical Statistics, London School of Hygiene & Tropical Medicine\nSpatial and temporal Trend Analysis of Long Term rainfall records in data-poor catchments with  missing data, a case study of Lower Shire floodplain in Malawi for the period 1953\u20132010.\nR-miss-tastic, A unified platform for missing values methods and workflows.\nMissing values-envision\n\n\n=== Software ===\nMplus\nPROC MI and PROC MIANALYZE - SAS\nSPSS", "Data modeling": "Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques.\n\n\n== Overview ==\nData modeling is a process  used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.  The data requirements are initially recorded as a conceptual data model which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into a logical data model, which documents structures of the data that can be implemented in databases. Implementation of one conceptual data model may require multiple logical data models. The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details. Data modeling defines not just data elements, but also their structures and the relationships between them.Data modeling techniques and methodologies are used to model data in a standard, consistent, predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using data modeling:\n\nto assist business analysts, programmers, testers, manual writers, IT package selectors, engineers, managers, related organizations and clients to understand and use an agreed upon semi-formal model that encompasses the concepts of the organization and how they relate to one another\nto manage data as a resource\nto integrate information systems\nto design databases/data warehouses (aka data repositories)Data modeling may be performed during various types of projects and in multiple phases of projects. Data models are progressive; there is no such thing as the final data model for a business or application. Instead a data model should be considered a living document that will change in response to a changing business. The data models should ideally be stored in a repository so that they can be retrieved, expanded, and edited over time. Whitten et al. (2004) determined two types of data modeling:\nStrategic data modeling: This is part of the creation of an information systems strategy, which defines an overall vision and architecture for information systems. Information technology engineering is a methodology that embraces this approach.\nData modeling during systems analysis: In systems analysis logical data models are created as part of the development of new databases.Data modeling is also used as a technique for detailing business requirements for specific databases. It is sometimes called database modeling because a data model is eventually implemented in a database.\n\n\n== Topics ==\n\n\n=== Data models ===\n\nData models provide a framework for data to be used within information systems by providing specific definition and format. If a data model is used consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data seamlessly. The results of this are indicated in the diagram. However, systems and interfaces are often expensive to build, operate, and maintain. They may also constrain the business rather than support it. This may occur when the quality of the data models implemented in systems and interfaces is poor.Some common problems found in data models are:\n\nBusiness rules, specific to how things are done in a particular place, are often fixed in the structure of a data model. This means that small changes in the way business is conducted lead to large changes in computer systems and interfaces. So, business rules need to be implemented in a flexible way that does not result in complicated dependencies, rather the data model should be flexible enough so that changes in the business can be implemented within the data model in a relatively quick and efficient way.\nEntity types are often not identified, or are identified incorrectly.  This can lead to replication of data, data structure and functionality, together with the attendant costs of that duplication in development and maintenance. Therefore, data definitions should be made as explicit and easy to understand as possible to minimize misinterpretation and duplication.\nData models for different systems are arbitrarily different. The result of this is that complex interfaces are required between systems that share data. These interfaces can account for between 25 and 70% of the cost of current systems. Required interfaces should be considered inherently while designing a data model, as a data model on its own would not be usable without interfaces within different systems.\nData cannot be shared electronically with customers and suppliers, because the structure and meaning of data has not been standardised. To obtain optimal value from an implemented data model, it is very important to define standards that will ensure that data models will both meet business needs and be consistent.\n\n\n=== Conceptual, logical and physical schemas ===\n\nIn 1975 ANSI described three kinds of data-model instance:\nConceptual schema: describes the semantics of a domain (the scope of the model). For example, it may be a model of the interest area of an organization or of an industry. This consists of entity classes, representing kinds of things of significance in the domain, and relationships assertions about associations between pairs of entity classes. A conceptual schema specifies the kinds of facts or propositions that can be expressed using the model. In that sense, it defines the allowed expressions in an artificial \"language\" with a scope that is limited by the scope of the model. Simply described, a conceptual schema is the first step in organizing the data requirements.\nLogical schema: describes the structure of some domain of information.  This consists of descriptions of (for example) tables, columns, object-oriented classes, and XML tags. The logical schema and conceptual schema are sometimes implemented as one and the same.\nPhysical schema: describes the physical means used to store data.  This is concerned with partitions, CPUs, tablespaces, and the like.According to ANSI, this approach allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual schema. The table/column structure can change without (necessarily) affecting the conceptual schema.  In each case, of course, the structures must remain consistent across all schemas of the same data model.\n\n\n=== Data modeling process ===\n\nIn the context of business process integration (see figure), data modeling complements business process modeling, and ultimately results in database generation.The process of designing a database involves producing the previously described three types of schemas - conceptual, logical, and physical. The database design documented in these schemas are converted through a Data Definition Language, which can then be used to generate a database.  A fully attributed data model contains detailed attributes (descriptions) for every entity within it. The term \"database design\" can describe many different parts of the design of an overall database system. Principally, and most correctly, it can be thought of as the logical design of the base data structures used to store the data. In the relational model these are the tables and views. In an object database the entities and relationships map directly to object classes and named relationships. However, the term \"database design\" could also be used to apply to the overall process of designing, not just the base data structures, but also the forms and queries used as part of the overall database application within the Database Management System or DBMS.\nIn the process, system interfaces account for 25% to 70% of the development and support costs of current systems. The primary reason for this cost is that these systems do not share a common data model. If data models are developed on a system by system basis, then not only is the same analysis repeated in overlapping areas, but further analysis must be performed to create the interfaces between them. Most systems within an organization contain the same basic data, redeveloped for a specific purpose. Therefore, an efficiently designed basic data model can minimize rework with minimal modifications for the purposes of different systems within the organization\n\n\n=== Modeling methodologies ===\nData models represent information areas of interest. While there are many ways to create data models, according to Len Silverston (1997) only two modeling methodologies stand out, top-down and bottom-up:  \n\nBottom-up models or View Integration models are often the result of a reengineering effort. They usually start with existing data structures forms, fields on application screens, or reports. These models are usually physical, application-specific, and incomplete from an enterprise perspective. They may not promote data sharing, especially if they are built without reference to other parts of the organization.\nTop-down logical data models, on the other hand, are created in an abstract way by getting information from people who know the subject area. A system may not implement all the entities in a logical model, but the model serves as a reference point or template.Sometimes models are created in a mixture of the two methods: by considering the data needs and structure of an application and by consistently referencing a subject-area model. Unfortunately, in many environments the distinction between a logical data model and a physical data model is blurred. In addition, some CASE tools don't make a distinction between logical and physical data models.\n\n\n=== Entity\u2013relationship diagrams ===\n\nThere are several notations for data modeling. The actual model is frequently called \"entity\u2013relationship model\", because it depicts data in terms of the entities and relationships described in the data. An entity\u2013relationship model (ERM) is an abstract conceptual representation of structured data. Entity\u2013relationship modeling is a relational schema database modeling method, used in software engineering to produce a type of conceptual data model (or semantic data model) of a system, often a relational database, and its requirements in a top-down fashion.\nThese models are being used in the first stage of information system design during the requirements analysis to describe information needs or the type of information that is to be stored in a database. The data modeling technique can be used to describe any ontology (i.e. an overview and classifications of used terms and their relationships) for a certain universe of discourse i.e. area of interest.\nSeveral techniques have been developed for the design of data models. While these methodologies guide data modelers in their work, two different people using the same methodology will often come up with very different results. Most notable are:\n\nBachman diagrams\nBarker's notation\nChen's notation\nData Vault Modeling\nExtended Backus\u2013Naur form\nIDEF1X\nObject-relational mapping\nObject-Role Modeling and Fully Communication Oriented Information Modeling\nRelational Model\nRelational Model/Tasmania\n\n\n=== Generic data modeling ===\n\nGeneric data models are generalizations of conventional data models. They define standardized general relation types, together with the kinds of things that may be related by such a relation type. \nThe definition of generic data model is similar to the definition of a natural language. For example, a generic data model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related.\nGiven an extensible list of classes, this allows the classification of any individual thing and to specify part-whole relations for any individual object. By standardization of an extensible list of relation types, a generic data model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages. Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model.\n\n\n=== Semantic data modeling ===\n\nThe logical data structure of a DBMS, whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. That is unless the semantic data model is implemented in the database on purpose, a choice which may slightly impact performance but generally vastly improves productivity.\n\nTherefore, the need to define data from a conceptual view has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure the real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.The purpose of semantic data modeling is to create a structural model of a piece of the real world, called \"universe of discourse\". For this, four fundamental structural relations are considered:\n\nClassification/instantiation: Objects with some structural similarity are described as instances of classes\nAggregation/decomposition: Composed objects are obtained joining its parts\nGeneralization/specialization: Distinct classes with some common properties are reconsidered in a more generic class with the common attributesA semantic data model can be used to serve many purposes, such as:\nPlanning of data resources\nBuilding of shareable databases\nEvaluation of vendor software\nIntegration of existing databasesThe overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the artificial intelligence field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations.\n\n\n== See also ==\nArchitectural pattern\nComparison of data modeling tools\nData (computer science)\nData dictionary\nDocument modeling\nEnterprise data modelling\nEntity Data Model\nInformation management\nInformation model\nBuilding information modeling\nMetadata modeling\nThree-schema approach\nZachman Framework\n\n\n== References ==\n\n This article incorporates public domain material from the National Institute of Standards and Technology.\n\n\n== Further reading ==\nJ.H. ter Bekke (1991). Semantic Data Modeling in Relational Environments\nJohn Vincent Carlis, Joseph D. Maguire (2001). Mastering Data Modeling: A User-driven Approach.\nAlan Chmura, J. Mark Heumann (2005). Logical Data Modeling: What it is and how to Do it.\nMartin E. Modell (1992). Data Analysis, Data Modeling, and Classification.\nM. Papazoglou, Stefano Spaccapietra, Zahir Tari (2000). Advances in Object-oriented Data Modeling.\nG. Lawrence Sanders (1995). Data Modeling\nGraeme C. Simsion, Graham C. Witt (2005). Data Modeling Essentials'\nMatthew West (2011) Developing High Quality Data Models\n\n\n== External links ==\n\nAgile/Evolutionary Data Modeling\nData modeling articles Archived March 7, 2010, at the Wayback Machine\nDatabase Modelling in UML\nData Modeling 101\nSemantic data modeling\nSystem Development, Methodologies and Modeling Notes on by Tony Drewry\nRequest For Proposal - Information Management Metamodel (IMM) of the Object Management Group\nData Modeling is NOT just for DBMS's Part 1 Chris Bradley\nData Modeling is NOT just for DBMS's Part 2 Chris Bradley", "Classification rule": "Given a population whose members each belong to one of a number of different sets or classes, a classification rule or classifier is a procedure by which the elements of the population set are each predicted to belong to one of the classes. A perfect classification is one for which every element in the population is assigned to the class it really belongs to. An imperfect classification is one in which some errors appear, and then statistical analysis must be applied to analyse the classification.\nA special kind of classification rule is binary classification, for problems in which there are only two classes.\n\n\n== Testing classification rules ==\nGiven a data set consisting of pairs x and y, where x denotes an element of the population and y the class it belongs to, a classification rule h(x) is a function that assigns each element x to a predicted class \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        h\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle {\\hat {y}}=h(x).}\n   A binary classification is such that the label y can take only one of two values.\nThe true labels yi can be known but will not necessarily match their approximations \n  \n    \n      \n        \n          \n            \n              \n                y\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        =\n        h\n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {y_{i}}}=h(x_{i})}\n  . In a binary classification, the elements that are not correctly classified are named false positives and false negatives.\nSome classification rules are static functions. Others can be computer programs. A computer classifier can be able to learn or can implement static classification rules. For a training data-set, the true labels yj are unknown, but it is a prime target for the classification procedure that the approximation \n  \n    \n      \n        \n          \n            \n              \n                y\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        =\n        h\n        (\n        \n          x\n          \n            j\n          \n        \n        )\n        \u2248\n        \n          y\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y_{j}}}=h(x_{j})\\approx y_{j}}\n   as well as possible, where the quality of this approximation needs to be judged on the basis of the statistical or probabilistic properties of the overall population from which future observations will be drawn.\nGiven a classification rule, a classification test is the result of applying the rule to a finite sample of the initial data set.\n\n\n== Binary and multiclass classification ==\nClassification can be thought of as two separate problems \u2013 binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers. An important point is that in many practical binary classification problems, the two groups are not symmetric \u2013 rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present). In multiclass classifications, the classes may be considered symmetrically (all errors are equivalent), or asymmetrically, which is considerably more complicated.\nBinary classification methods include probit regression and logistic regression. Multiclass classification methods include multinomial probit and multinomial logit.\n\n\n== Confusion Matrix and Classifiers ==\n\nWhen the classification function is not perfect, false results will appear. In the example in the image to the right. There are 20 dots on the left side of the line (true side) while only 8 of those 20 were actually true. In a similar situation for the right side of the line (false side) where there are 16 dots on the right side and 4 of those 16 dots were inaccurately marked as true. Using the dot locations, we can build a confusion matrix to express the values. We can use 4 different metrics to express the 4 different possible outcomes. There is true positive (TP), false positive (FP), false negative (FN), and true negative (TN).\n\n\n=== False positives ===\nFalse positives result when a test falsely (incorrectly) reports a positive result.  For example, a medical test for a disease may return a positive result indicating that the patient has the disease even if the patient does not have the disease.  False positive is commonly denoted as the top right (Condition negative X test outcome positive) unit in a Confusion matrix.\n\n\n=== False negatives ===\nOn the other hand, false negatives result when a test falsely or incorrectly reports a negative result.  For example, a medical test for a disease may return a negative result indicating that patient does not have a disease even though the patient actually has the disease.  False negative is commonly denoted as the bottom left (Condition positive X test outcome negative) unit in a Confusion matrix. \n\n\n=== True positives ===\nTrue positives result when a test correctly reports a positive result. As an example, a medical test for a disease may return a positive result indicating that the patient has the disease.  This is shown to be true when the patient test confirms the existence of the disease.  True positive is commonly denoted as the top left (Condition positive X test outcome positive) unit in a Confusion matrix.  \n\n\n=== True negatives ===\nTrue negative result when a test correctly reports a negative result. As an example, a medical test for a disease may return a positive result indicating that the patient does not have the disease. This is shown to be true when the patients test also reports not having the disease. True negative is commonly denoted as the bottom right (Condition negative X test outcome negative) unit in a Confusion matrix.\n\n\n== Application with Bayes\u2019 Theorem ==\nWe can also calculate true positives, false positive, true negative, and false negatives using Bayes' theorem. Using Bayes' theorem will help describe the Probability of an Event (probability theory), based on prior knowledge of conditions that might be related to the event. Expressed are the four classifications using the example below.\n\nIf a tested patient does not have the disease, the test returns a positive result 5% of the time, or with a probability of 0.05.\nSuppose that only 0.1% of the population has that disease, so that a randomly selected patient has a 0.001 prior probability of having the disease.\nLet A represent the condition in which the patient has the disease\nLet \\neg A represent the condition in which the patient does not have the disease\nLet B represent the evidence of a positive test result.\nLet \\neg B represent the evidence of a negative test result.In terms of true positive, false positive, false negative, and true negative:\n\nFalse positive is the probability P that \\neg A (The patient does not have the disease) then B (The patient tests positive for the disease) also expressed as P(\\neg A|B)\nFalse negative is the probability P that A (The patient has the disease) then \\neg B (The patient tests negative for the disease) also expressed as P( A|\\neg B)\nTrue positive is the probability P that A (The patient has the disease) then B (The patient tests positive for the disease) also expressed as P(A|B)\nTrue negative is the probability P that \\neg A (The patient does not have the disease) then \\neg B (The patient tests negative for the disease) also expressed as P(\\neg A|\\neg B)\n\n\n=== False positives ===\nWe can use Bayes' theorem to determine the probability that a positive result is in fact a false positive.  We find that if a disease is rare, then the majority of positive results may be false positives, even if the test is relatively accurate.\nNaively, one might think that only 5% of positive test results are false, but that is quite wrong, as we shall see.\nSuppose that only 0.1% of the population has that disease, so that a randomly selected patient has a 0.001 prior probability of having the disease.\nWe can use Bayes' theorem to calculate the probability that a positive test result is a false positive.\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                A\n                \n                  |\n                \n                B\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                      +\n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      \u00ac\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.99\n                      \u00d7\n                      0.001\n                    \n                    \n                      0.99\n                      \u00d7\n                      0.001\n                      +\n                      0.05\n                      \u00d7\n                      0.999\n                    \n                  \n                \n              \n            \n            \n              \n                 \n              \n            \n            \n              \n              \n                \n                \u2248\n                0.019.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(A|B)&={\\frac {P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)}}\\\\\\\\&={\\frac {0.99\\times 0.001}{0.99\\times 0.001+0.05\\times 0.999}}\\\\~\\\\&\\approx 0.019.\\end{aligned}}}\n  and hence the probability that a positive result is a false positive is about 1 \u2212 0.019 = 0.98, or 98%.\nDespite the apparent high accuracy of the test, the incidence of the disease is so low that the vast majority of patients who test positive do not have the disease. Nonetheless, the fraction of patients who test positive who do have the disease (0.019) is 19 times the fraction of people who have not yet taken the test who have the disease (0.001). Thus the test is not useless, and re-testing may improve the reliability of the result.\nIn order to reduce the problem of false positives, a test should be very accurate in reporting a negative result when the patient does not have the disease.  If the test reported a negative result in patients without the disease with probability 0.999, then\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              0.99\n              \u00d7\n              0.001\n            \n            \n              0.99\n              \u00d7\n              0.001\n              +\n              0.001\n              \u00d7\n              0.999\n            \n          \n        \n        \u2248\n        0.5\n        ,\n      \n    \n    {\\displaystyle P(A|B)={\\frac {0.99\\times 0.001}{0.99\\times 0.001+0.001\\times 0.999}}\\approx 0.5,}\n  so that 1 \u2212 0.5 = 0.5 now is the probability of a false positive.\n\n\n=== False negatives ===\nWe can use Bayes' theorem to determine the probability that the negative result is in fact a false negative using the example from above:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                A\n                \n                  |\n                \n                \u00ac\n                B\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                      +\n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      \u00ac\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.01\n                      \u00d7\n                      0.001\n                    \n                    \n                      0.01\n                      \u00d7\n                      0.001\n                      +\n                      0.95\n                      \u00d7\n                      0.999\n                    \n                  \n                \n              \n            \n            \n              \n                 \n              \n            \n            \n              \n              \n                \n                \u2248\n                0.0000105.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(A|\\neg B)&={\\frac {P(\\neg B|A)P(A)}{P(\\neg B|A)P(A)+P(\\neg B|\\neg A)P(\\neg A)}}\\\\\\\\&={\\frac {0.01\\times 0.001}{0.01\\times 0.001+0.95\\times 0.999}}\\\\~\\\\&\\approx 0.0000105.\\end{aligned}}}\n  The probability that a negative result is a false negative is about 0.0000105 or 0.00105%.  When a disease is rare, false negatives will not be a major problem with the test.\nBut if 60% of the population had the disease, then the probability of a false negative would be greater.  With the above test, the probability of a false negative would be\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                A\n                \n                  |\n                \n                \u00ac\n                B\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                      +\n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      \u00ac\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.01\n                      \u00d7\n                      0.6\n                    \n                    \n                      0.01\n                      \u00d7\n                      0.6\n                      +\n                      0.95\n                      \u00d7\n                      0.4\n                    \n                  \n                \n              \n            \n            \n              \n                 \n              \n            \n            \n              \n              \n                \n                \u2248\n                0.0155.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(A|\\neg B)&={\\frac {P(\\neg B|A)P(A)}{P(\\neg B|A)P(A)+P(\\neg B|\\neg A)P(\\neg A)}}\\\\\\\\&={\\frac {0.01\\times 0.6}{0.01\\times 0.6+0.95\\times 0.4}}\\\\~\\\\&\\approx 0.0155.\\end{aligned}}}\n  The probability that a negative result is a false negative rises to 0.0155 or 1.55%.\n\n\n=== True positives ===\nWe can use Bayes' theorem to determine the probability that the positive result is in fact a true positive using the example from above:\n\nIf a tested patient has the disease, the test returns a positive result 99% of the time, or with a probability of 0.99.\nIf a tested patient does not have the disease, the test returns a positive result 5% of the time, or with a probability of 0.05.\nSuppose that only 0.1% of the population has that disease, so that a randomly selected patient has a 0.001 prior probability of having the disease.Let A represent the condition in which the patient has the disease, and B represent the evidence of a positive test result. Then, the probability that the patient actually has the disease given a positive test result is:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                A\n                \n                  |\n                \n                B\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                      +\n                      P\n                      (\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      \u00ac\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.99\n                      \u00d7\n                      0.001\n                    \n                    \n                      0.99\n                      \u00d7\n                      0.001\n                      +\n                      0.05\n                      \u00d7\n                      0.999\n                    \n                  \n                \n              \n            \n            \n              \n                 \n              \n            \n            \n              \n              \n                \n                \u2248\n                0.019.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(A|B)&={\\frac {P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)}}\\\\\\\\&={\\frac {0.99\\times 0.001}{0.99\\times 0.001+0.05\\times 0.999}}\\\\~\\\\&\\approx 0.019.\\end{aligned}}}\n  The probability that a positive result is a true positive is about 0.019%\n\n\n=== True negatives ===\nWe can also use Bayes' theorem to calculate the probability of true negative. Using the examples above:\n\nIf a tested patient has the disease, the test returns a positive result 99% of the time, or with a probability of 0.99.\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                \u00ac\n                A\n                \n                  |\n                \n                \u00ac\n                B\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                    \n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      \u00ac\n                      A\n                      )\n                      P\n                      (\n                      \u00ac\n                      A\n                      )\n                      +\n                      P\n                      (\n                      \u00ac\n                      B\n                      \n                        |\n                      \n                      A\n                      )\n                      P\n                      (\n                      A\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.99\n                      \u00d7\n                      0.999\n                    \n                    \n                      0.99\n                      \u00d7\n                      0.999\n                      +\n                      0.05\n                      \u00d7\n                      0.001\n                    \n                  \n                \n              \n            \n            \n              \n                 \n              \n            \n            \n              \n              \n                \n                \u2248\n                0.0000105.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(\\neg A|\\neg B)&={\\frac {P(\\neg B|\\neg A)P(A)}{P(\\neg B|\\neg A)P(\\neg A)+P(\\neg B|A)P(A)}}\\\\\\\\&={\\frac {0.99\\times 0.999}{0.99\\times 0.999+0.05\\times 0.001}}\\\\~\\\\&\\approx 0.0000105.\\end{aligned}}}\n  The probability that a negative result is a true negative is 0.9999494 or 99.99%.  Since the disease is rare and the positive to positive rate is high and the negative to negative rate is also high, this will produce a large True Negative rate.\n\n\n== Measuring a classifier with sensitivity and specificity ==\nIn training a classifier, one may wish to measure its performance using the well-accepted metrics of sensitivity and specificity.  It may be instructive to compare the classifier to a random classifier that flips a coin based on the prevalence of a disease.  Suppose that the probability a person has the disease is \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and the probability that they do not is \n  \n    \n      \n        q\n        =\n        1\n        \u2212\n        p\n      \n    \n    {\\displaystyle q=1-p}\n  .  Suppose then that we have a random classifier that guesses that the patient has the disease with that same probability \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and guesses that he does not with the same probability \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  .\nThe probability of a true positive is the probability that the patient has the disease times the probability that the random classifier guesses this correctly, or \n  \n    \n      \n        \n          p\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle p^{2}}\n  .  With similar reasoning, the probability of a false negative is \n  \n    \n      \n        p\n        q\n      \n    \n    {\\displaystyle pq}\n  . From the definitions above, the sensitivity of this classifier is \n  \n    \n      \n        \n          p\n          \n            2\n          \n        \n        \n          /\n        \n        (\n        \n          p\n          \n            2\n          \n        \n        +\n        p\n        q\n        )\n        =\n        p\n      \n    \n    {\\displaystyle p^{2}/(p^{2}+pq)=p}\n  .  With similar reasoning, we can calculate the specificity as \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n        \n          /\n        \n        (\n        \n          q\n          \n            2\n          \n        \n        +\n        p\n        q\n        )\n        =\n        q\n      \n    \n    {\\displaystyle q^{2}/(q^{2}+pq)=q}\n  .\nSo, while the measure itself is independent of disease prevalence, the performance of this random classifier depends on disease prevalence.  The classifier may have performance that is like this random classifier, but with a better-weighted coin (higher sensitivity and specificity).  So, these measures may be influenced by disease prevalence. An alternative measure of performance is the Matthews correlation coefficient, for which any random classifier will get an average score of 0.\nThe extension of this concept to non-binary classifications yields the confusion matrix.\n\n\n== See also ==\nBayes classifier\nBayesian inference\nBinary classification\nDecision rule\nDiagnostic test\nGold standard (test)\nLoss functions for classification\nMedical test\nSensitivity and specificity\nStatistical classification\n\n\n== Notes ==\n\n\n== References ==", "Nonlinear system": "In mathematics and science, a nonlinear system (or a non-linear system) is a system in which the change of the output is not proportional to the change of the input. Nonlinear problems are of interest to engineers, biologists, physicists, mathematicians, and many other scientists since most systems are inherently nonlinear in nature. Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.\nTypically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one.\nIn other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.\nAs nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos, and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.\nSome authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others:\n\nUsing a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.\n\n\n== Definition ==\nIn mathematics, a linear map (or linear function) \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is one which satisfies both of the following properties:\n\nAdditivity or superposition principle: \n  \n    \n      \n        \n          f\n          (\n          x\n          +\n          y\n          )\n          =\n          f\n          (\n          x\n          )\n          +\n          f\n          (\n          y\n          )\n          ;\n        \n      \n    \n    {\\displaystyle \\textstyle f(x+y)=f(x)+f(y);}\n  \nHomogeneity: \n  \n    \n      \n        \n          f\n          (\n          \u03b1\n          x\n          )\n          =\n          \u03b1\n          f\n          (\n          x\n          )\n          .\n        \n      \n    \n    {\\displaystyle \\textstyle f(\\alpha x)=\\alpha f(x).}\n  Additivity implies homogeneity for any rational \u03b1, and, for continuous functions, for any real \u03b1. For a complex \u03b1, homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principle\n\n  \n    \n      \n        f\n        (\n        \u03b1\n        x\n        +\n        \u03b2\n        y\n        )\n        =\n        \u03b1\n        f\n        (\n        x\n        )\n        +\n        \u03b2\n        f\n        (\n        y\n        )\n      \n    \n    {\\displaystyle f(\\alpha x+\\beta y)=\\alpha f(x)+\\beta f(y)}\n  An equation written as\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        C\n      \n    \n    {\\displaystyle f(x)=C}\n  is called linear if \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is a linear map (as defined above) and nonlinear otherwise. The equation is called homogeneous if \n  \n    \n      \n        C\n        =\n        0\n      \n    \n    {\\displaystyle C=0}\n   and \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is a homogeneous function.\nThe definition \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        C\n      \n    \n    {\\displaystyle f(x)=C}\n   is very general in that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   can be any sensible mathematical object (number, vector, function, etc.), and the function \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   can literally be any mapping, including integration or differentiation with associated constraints (such as boundary values). If \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   contains differentiation with respect to \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , the result will be a differential equation.\n\n\n== Nonlinear algebraic equations ==\n\nNonlinear algebraic equations, which are also called polynomial equations, are defined by equating polynomials (of degree greater than one) to zero. For example,\n\n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        x\n        \u2212\n        1\n        =\n        0\n        \n        .\n      \n    \n    {\\displaystyle x^{2}+x-1=0\\,.}\n  For a single polynomial equation, root-finding algorithms can be used to find solutions to the equation (i.e., sets of values for the variables that satisfy the equation). However, systems of algebraic equations are more complicated; their study is one motivation for the field of algebraic geometry, a difficult branch of modern mathematics. It is even difficult to decide whether a given algebraic system has complex solutions (see Hilbert's Nullstellensatz). Nevertheless, in the case of the systems with a finite number of complex solutions, these systems of polynomial equations are now well understood and efficient methods exist for solving them.\n\n\n== Nonlinear recurrence relations ==\nA nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures. These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains.\n\n\n== Nonlinear differential equations ==\nA system of differential equations is said to be nonlinear if it is not a system of linear equations. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier\u2013Stokes equations in fluid dynamics and the Lotka\u2013Volterra equations in biology.\nOne of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions.\n\n\n=== Ordinary differential equations ===\nFirst order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equation\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        \u2212\n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {du}{dx}}=-u^{2}}\n  has \n  \n    \n      \n        u\n        =\n        \n          \n            1\n            \n              x\n              +\n              C\n            \n          \n        \n      \n    \n    {\\displaystyle u={\\frac {1}{x+C}}}\n   as a general solution (and also the special solution \n  \n    \n      \n        u\n        =\n        0\n        ,\n      \n    \n    {\\displaystyle u=0,}\n   corresponding to the limit of the general solution when C tends to infinity). The equation is nonlinear because it may be written as\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              d\n              x\n            \n          \n        \n        +\n        \n          u\n          \n            2\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {du}{dx}}+u^{2}=0}\n  and the left-hand side of the equation is not a linear function of \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n   and its derivatives. Note that if the \n  \n    \n      \n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle u^{2}}\n   term were replaced with \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  , the problem would be linear (the exponential decay problem).\nSecond and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed-form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered.\nCommon methods for the qualitative analysis of nonlinear ordinary differential equations include:\n\nExamination of any conserved quantities, especially in Hamiltonian systems\nExamination of dissipative quantities (see Lyapunov function) analogous to conserved quantities\nLinearization via Taylor expansion\nChange of variables into something easier to study\nBifurcation theory\nPerturbation methods (can be applied to algebraic equations too)\nExistence of solutions of Finite-Duration, which can happen under specific conditions for some non-linear ordinary differential equations.\n\n\n=== Partial differential equations ===\n\nThe most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable.\nAnother common (though less mathematical) tactic, often exploited in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation.\nOther methods include examining the characteristics and using the methods outlined above for ordinary differential equations.\n\n\n=== Pendula ===\n\nA classic, extensively studied nonlinear problem is the dynamics of a frictionless pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shown that the motion of a pendulum can be described by the dimensionless nonlinear equation\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03b8\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\sin(\\theta )=0}\n  where gravity points \"downwards\" and \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is the angle the pendulum forms with its rest position, as shown in the figure at right. One approach to \"solving\" this equation is to use \n  \n    \n      \n        d\n        \u03b8\n        \n          /\n        \n        d\n        t\n      \n    \n    {\\displaystyle d\\theta /dt}\n   as an integrating factor, which would eventually yield\n\n  \n    \n      \n        \u222b\n        \n          \n            \n              d\n              \u03b8\n            \n            \n              \n                C\n                \n                  0\n                \n              \n              +\n              2\n              cos\n              \u2061\n              (\n              \u03b8\n              )\n            \n          \n        \n        =\n        t\n        +\n        \n          C\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\int {\\frac {d\\theta }{\\sqrt {C_{0}+2\\cos(\\theta )}}}=t+C_{1}}\n  which is an implicit solution involving an elliptic integral. This \"solution\" generally does not have many uses because most of the nature of the solution is hidden in the nonelementary integral (nonelementary unless \n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        2\n      \n    \n    {\\displaystyle C_{0}=2}\n  ).\nAnother way to approach the problem is to linearize any nonlinearity (the sine function term in this case) at the various points of interest through Taylor expansions. For example, the linearization at \n  \n    \n      \n        \u03b8\n        =\n        0\n      \n    \n    {\\displaystyle \\theta =0}\n  , called the small angle approximation, is\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03b8\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \u03b8\n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\theta =0}\n  since \n  \n    \n      \n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        \u2248\n        \u03b8\n      \n    \n    {\\displaystyle \\sin(\\theta )\\approx \\theta }\n   for \n  \n    \n      \n        \u03b8\n        \u2248\n        0\n      \n    \n    {\\displaystyle \\theta \\approx 0}\n  . This is a simple harmonic oscillator corresponding to oscillations of the pendulum near the bottom of its path. Another linearization would be at \n  \n    \n      \n        \u03b8\n        =\n        \u03c0\n      \n    \n    {\\displaystyle \\theta =\\pi }\n  , corresponding to the pendulum being straight up:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03b8\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \u03c0\n        \u2212\n        \u03b8\n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\pi -\\theta =0}\n  since \n  \n    \n      \n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        \u2248\n        \u03c0\n        \u2212\n        \u03b8\n      \n    \n    {\\displaystyle \\sin(\\theta )\\approx \\pi -\\theta }\n   for \n  \n    \n      \n        \u03b8\n        \u2248\n        \u03c0\n      \n    \n    {\\displaystyle \\theta \\approx \\pi }\n  . The solution to this problem involves hyperbolic sinusoids, and note that unlike the small angle approximation, this approximation is unstable, meaning that \n  \n    \n      \n        \n          |\n        \n        \u03b8\n        \n          |\n        \n      \n    \n    {\\displaystyle |\\theta |}\n   will usually grow without limit, though bounded solutions are possible. This corresponds to the difficulty of balancing a pendulum upright, it is literally an unstable state.\nOne more interesting linearization is possible around \n  \n    \n      \n        \u03b8\n        =\n        \u03c0\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle \\theta =\\pi /2}\n  , around which \n  \n    \n      \n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        \u2248\n        1\n      \n    \n    {\\displaystyle \\sin(\\theta )\\approx 1}\n  :\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03b8\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        1\n        =\n        0.\n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+1=0.}\n  This corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods.\n\n\n== Types of nonlinear dynamic behaviors ==\nAmplitude death \u2013 any oscillations present in the system cease due to some kind of interaction with other system or feedback by the same system\nChaos \u2013 values of a system cannot be predicted indefinitely far into the future, and fluctuations are aperiodic\nMultistability \u2013 the presence of two or more stable states\nSolitons \u2013 self-reinforcing solitary waves\nLimit cycles \u2013 asymptotic periodic orbits to which destabilized fixed points are attracted.\nSelf-oscillations \u2013 feedback oscillations taking place in open dissipative physical systems.\n\n\n== Examples of nonlinear equations ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nCommand and Control Research Program (CCRP)\nNew England Complex Systems Institute: Concepts in Complex Systems\nNonlinear Dynamics I: Chaos at MIT's OpenCourseWare\nNonlinear Model Library \u2013  (in MATLAB) a Database of Physical Systems\nThe Center for Nonlinear Studies at Los Alamos National Laboratory", "Local optimum": "In applied mathematics and computer science, a local optimum of an optimization problem is a solution that is optimal (either maximal or minimal) within a neighboring set of candidate solutions. This is in contrast to a global optimum, which is the optimal solution among all possible solutions, not just those in a particular neighborhood of values. Importantly, a global optimum is necessarily a local optimum, but a local optimum is not necessarily a global optimum. \n\n\n== Continuous domain ==\nWhen the function to be optimized is continuous, it may be possible to employ calculus to find local optima. If the first derivative exists everywhere, it can be equated to zero; if the function has an unbounded domain, for a point to be a local optimum it is necessary that it satisfy this equation. Then the second derivative test provides a sufficient condition for the point to be a local maximum or local minimum.\n\n\n== Search techniques ==\nLocal search or hill climbing methods for solving optimization problems start from an initial configuration and repeatedly move to an improving neighboring configuration. A trajectory in search space is generated, which maps an initial point to a local optimum, where local search is stuck (no improving neighbors are available). The search space is therefore subdivided into basins of attraction, each consisting of all initial points which have a given local optimum as the final point of the local search trajectory.\nA local optimum can be isolated (surrounded by non-locally-optimal points) or part of a plateau, a locally optimal region with more than one point of equal value.\nIf the problem to be solved has all locally optimal points with the same value of the function to be\noptimized, local search effectively solves the global problem: finding a local optimum delivers\na globally optimal solution.\nThe locality of the optimum is dependent on the neighborhood structure as defined by the local search method that is used for optimizing the function.\nIn many cases, local optima deliver sub-optimal solutions to the global problem, and\na local search method needs to be modified to continue the search\nbeyond local optimality; see for example iterated local search, tabu search, reactive search optimization, and\nsimulated annealing.\n\n\n== See also ==\n\nMaxima and minima", "Data processing": "Data processing is the collection and manipulation of digital data to produce meaningful information.  \nData processing is a form of information processing, which is the modification (processing) of information in any manner detectable by an observer.The term \"Data Processing\", or \"DP\" has also been used to refer to a department within an organization responsible for the operation of data processing programs.\n\n\n== Data processing functions ==\nData processing may involve various processes, including:\n\nValidation \u2013 Ensuring that supplied data is correct and relevant.\nSorting \u2013 \"arranging items in some sequence and/or in different sets.\"\nSummarization(statistical) or (automatic) \u2013 reducing detailed data to its main points.\nAggregation \u2013 combining multiple pieces of data.\nAnalysis \u2013 the \"collection, organization, analysis, interpretation and presentation of data.\"\nReporting \u2013 list detail or summary data or computed information.\nClassification \u2013 separation of data into various categories.\n\n\n== History ==\nThe United States Census Bureau history illustrates the evolution of data processing from manual through electronic procedures.\n\n\n=== Manual data processing ===\nAlthough widespread use of the term data processing dates only from the 1950's,  data processing functions have been performed manually for millennia. For example, bookkeeping involves functions such as posting transactions and producing reports like the balance sheet and the cash flow statement. Completely manual methods were augmented by the application of mechanical or electronic calculators. A person whose job was to perform calculations manually or using a calculator was called a \"computer.\"\nThe 1890 United States Census schedule was the first to gather data by individual rather than household. A number of questions could be answered by making a check in the appropriate box on the form. From 1850 to 1880 the Census Bureau employed \"a system of tallying, which, by reason of the increasing number of combinations of classifications required, became increasingly complex. Only a limited number of combinations could be recorded in one tally, so it was necessary to handle the schedules 5 or 6 times, for as many independent tallies.\"  \"It took over 7 years to publish the results of the 1880 census\" using manual processing methods.\n\n\n=== Automatic data processing ===\nThe term automatic data processing was applied to operations performed by means of unit record equipment, such as Herman Hollerith's application of punched card equipment for the 1890 United States Census. \"Using Hollerith's punchcard equipment, the Census Office was able to complete tabulating most of the 1890 census data in 2 to 3 years, compared with 7 to 8 years for the 1880 census. It is estimated that using Hollerith's system saved some $5 million in processing costs\" in 1890 dollars even though there were twice as many questions as in 1880.\n\n\n=== Electronic data processing ===\nComputerized data processing, or Electronic data processing represents a later development, with a computer used instead of several independent pieces of equipment.  The Census Bureau first made limited use of electronic computers for the 1950 United States Census, using a UNIVAC I system, delivered in 1952.\n\n\n=== Other developments ===\nThe term data processing has mostly been subsumed by the more general term information technology (IT). The older term \"data processing\" is suggestive of older technologies. For example, in 1996 the Data Processing Management Association (DPMA) changed its name to the Association of Information Technology Professionals. Nevertheless, the terms are approximately synonymous.\n\n\n== Applications ==\n\n\n=== Commercial data processing ===\n\nCommercial data processing involves a large volume of input data, relatively few computational operations, and a large volume of output. For example, an insurance company needs to keep records on tens or hundreds of thousands of policies, print and mail bills, and receive and post payments.\n\n\n=== Data analysis ===\n\nIn science and engineering, the terms data processing and information systems are considered too broad, and the term data processing is typically used for the initial stage followed by a data analysis in the second stage of the overall data handling.\nData analysis uses specialized algorithms and statistical calculations that are less often observed in a typical general business environment. For data analysis, software suites like SPSS or SAS, or their free counterparts such as DAP, gretl or PSPP are often used.\n\n\n== Systems ==\nA data processing system is a combination of machines, people, and processes that for a set of inputs produces a defined set of outputs. The inputs and outputs are interpreted as data, facts, information etc. depending on the interpreter's relation to the system.\nA term commonly used synonymously with data or storage (codes) processing system is information system. With regard particularly to electronic data processing, the corresponding concept is referred to as electronic data processing system.\n\n\n=== Examples ===\n\n\n==== Simple example ====\nA very simple example of a data processing system is the process of maintaining a check register.  Transactions\u2014 checks and deposits\u2014 are recorded as they occur and the transactions are summarized to determine a current balance.  Monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank.\nA more sophisticated record keeping system might further identify the transactions\u2014 for example deposits by source or checks by type, such as charitable contributions.  This information might be used to obtain information like the total of all contributions for the year.\nThe important thing about this example is that it is a system, in which, all transactions are recorded consistently, and the same method of bank reconciliation is used each time.\n\n\n==== Real-world example ====\nThis is a flowchart of a data processing system combining manual and computerized processing to handle accounts receivable, billing, and general ledger\n\n\n== See also ==\nScientific computing\nBig data\nComputation\nDecision-making software\nInformation and communications technology\nInformation technology\nComputer science\n\n\n== Notes ==\n\n\n== External links ==\n\n\n== References ==\n\n\n== Further reading ==\nBourque, Linda B.; Clark, Virginia A. (1992) Processing Data: The Survey Example. (Quantitative Applications in the Social Sciences, no. 07-085). Sage Publications. ISBN 0-8039-4741-0\nLevy, Joseph (1967) Punched Card Data Processing. McGraw-Hill Book Company.", "Semi-supervised learning": "Weak supervision, also called semi-supervised learning, is a branch of machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). Semi-supervised learning aims to alleviate the issue of having limited amounts of labeled data available for training. \n\nSemi-supervised learning is motivated by problem settings where unlabeled data is abundant and obtaining labeled data is expensive. Other branches of machine learning that share the same motivation but follow different assumptions and methodologies are active learning and weak supervision. Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.\nMore formally, semi-supervised learning assumes a set of \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   independently identically distributed examples \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{1},\\dots ,x_{l}\\in X}\n   with corresponding labels \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            l\n          \n        \n        \u2208\n        Y\n      \n    \n    {\\displaystyle y_{1},\\dots ,y_{l}\\in Y}\n   and \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n   unlabeled examples \n  \n    \n      \n        \n          x\n          \n            l\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{l+1},\\dots ,x_{l+u}\\in X}\n   are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.\nSemi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data \n  \n    \n      \n        \n          x\n          \n            l\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n      \n    \n    {\\displaystyle x_{l+1},\\dots ,x_{l+u}}\n   only. The goal of inductive learning is to infer the correct mapping from \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   to \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\nIntuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.\nIt is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.\n\n\n== Assumptions ==\nIn order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:\n\n\n=== Continuity / smoothness assumption ===\nPoints that are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.\n\n\n=== Cluster assumption ===\nThe data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.\n\n\n=== Manifold assumption ===\n\nThe data lie approximately on a manifold of much lower dimension than the input space. In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.\nThe manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds, and images of various facial expressions are controlled by a few muscles. In these cases, it is better to consider distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images, respectively.\n\n\n== History ==\nThe heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning, with examples of applications starting in the 1960s.The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s. Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available\u2014e.g. text on websites, protein sequences, or images.\n\n\n== Methods ==\n\n\n=== Generative models ===\nGenerative approaches to statistical learning first seek to estimate \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle p(x|y)}\n  , the distribution of data points belonging to each class. The probability \n  \n    \n      \n        p\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle p(y|x)}\n   that a given point \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   has label \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is then proportional to \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        )\n        p\n        (\n        y\n        )\n      \n    \n    {\\displaystyle p(x|y)p(y)}\n   by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about \n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n  ) or as an extension of unsupervised learning (clustering plus some labels).\nGenerative models assume that the distributions take some particular form \n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        y\n        ,\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(x|y,\\theta )}\n   parameterized by the vector \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone. \nHowever, if the assumptions are correct, then the unlabeled data necessarily improves performance.The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.\nThe parameterized joint distribution can be written as \n  \n    \n      \n        p\n        (\n        x\n        ,\n        y\n        \n          |\n        \n        \u03b8\n        )\n        =\n        p\n        (\n        y\n        \n          |\n        \n        \u03b8\n        )\n        p\n        (\n        x\n        \n          |\n        \n        y\n        ,\n        \u03b8\n        )\n      \n    \n    {\\displaystyle p(x,y|\\theta )=p(y|\\theta )p(x|y,\\theta )}\n   by using the chain rule. Each parameter vector \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is associated with a decision function \n  \n    \n      \n        \n          f\n          \n            \u03b8\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            argmax\n            y\n          \n        \n         \n        p\n        (\n        y\n        \n          |\n        \n        x\n        ,\n        \u03b8\n        )\n      \n    \n    {\\displaystyle f_{\\theta }(x)={\\underset {y}{\\operatorname {argmax} }}\\ p(y|x,\\theta )}\n  . \nThe parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  :\n\n  \n    \n      \n        \n          \n            argmax\n            \u0398\n          \n        \n        \n          (\n          \n            log\n            \u2061\n            p\n            (\n            {\n            \n              x\n              \n                i\n              \n            \n            ,\n            \n              y\n              \n                i\n              \n            \n            \n              }\n              \n                i\n                =\n                1\n              \n              \n                l\n              \n            \n            \n              |\n            \n            \u03b8\n            )\n            +\n            \u03bb\n            log\n            \u2061\n            p\n            (\n            {\n            \n              x\n              \n                i\n              \n            \n            \n              }\n              \n                i\n                =\n                l\n                +\n                1\n              \n              \n                l\n                +\n                u\n              \n            \n            \n              |\n            \n            \u03b8\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\underset {\\Theta }{\\operatorname {argmax} }}\\left(\\log p(\\{x_{i},y_{i}\\}_{i=1}^{l}|\\theta )+\\lambda \\log p(\\{x_{i}\\}_{i=l+1}^{l+u}|\\theta )\\right)}\n  \n\n\n=== Low-density separation ===\nAnother major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss \n  \n    \n      \n        (\n        1\n        \u2212\n        y\n        f\n        (\n        x\n        )\n        \n          )\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle (1-yf(x))_{+}}\n   for labeled data, a loss function \n  \n    \n      \n        (\n        1\n        \u2212\n        \n          |\n        \n        f\n        (\n        x\n        )\n        \n          |\n        \n        \n          )\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle (1-|f(x)|)_{+}}\n   is introduced over the unlabeled data by letting \n  \n    \n      \n        y\n        =\n        sign\n        \u2061\n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle y=\\operatorname {sign} {f(x)}}\n  . TSVM then selects \n  \n    \n      \n        \n          f\n          \n            \u2217\n          \n        \n        (\n        x\n        )\n        =\n        \n          h\n          \n            \u2217\n          \n        \n        (\n        x\n        )\n        +\n        b\n      \n    \n    {\\displaystyle f^{*}(x)=h^{*}(x)+b}\n   from a reproducing kernel Hilbert space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n   by minimizing the regularized empirical risk:\n\n  \n    \n      \n        \n          f\n          \n            \u2217\n          \n        \n        =\n        \n          \n            argmin\n            f\n          \n        \n        \n          (\n          \n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                l\n              \n            \n            (\n            1\n            \u2212\n            \n              y\n              \n                i\n              \n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n            \n              )\n              \n                +\n              \n            \n            +\n            \n              \u03bb\n              \n                1\n              \n            \n            \u2016\n            h\n            \n              \u2016\n              \n                \n                  H\n                \n              \n              \n                2\n              \n            \n            +\n            \n              \u03bb\n              \n                2\n              \n            \n            \n              \u2211\n              \n                i\n                =\n                l\n                +\n                1\n              \n              \n                l\n                +\n                u\n              \n            \n            (\n            1\n            \u2212\n            \n              |\n            \n            f\n            (\n            \n              x\n              \n                i\n              \n            \n            )\n            \n              |\n            \n            \n              )\n              \n                +\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f^{*}={\\underset {f}{\\operatorname {argmin} }}\\left(\\displaystyle \\sum _{i=1}^{l}(1-y_{i}f(x_{i}))_{+}+\\lambda _{1}\\|h\\|_{\\mathcal {H}}^{2}+\\lambda _{2}\\sum _{i=l+1}^{l+u}(1-|f(x_{i})|)_{+}\\right)}\n  An exact solution is intractable due to the non-convex term \n  \n    \n      \n        (\n        1\n        \u2212\n        \n          |\n        \n        f\n        (\n        x\n        )\n        \n          |\n        \n        \n          )\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle (1-|f(x)|)_{+}}\n  , so research focuses on useful approximations.Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).\n\n\n=== Laplacian regularization ===\nLaplacian regularization has been historically approached through graph-Laplacian.\nGraph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   nearest neighbors or to examples within some distance \n  \n    \n      \n        \u03f5\n      \n    \n    {\\displaystyle \\epsilon }\n  . The weight \n  \n    \n      \n        \n          W\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle W_{ij}}\n   of an edge between \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   and \n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle x_{j}}\n   is then set to \n  \n    \n      \n        \n          e\n          \n            \n              \n                \u2212\n                \u2016\n                \n                  x\n                  \n                    i\n                  \n                \n                \u2212\n                \n                  x\n                  \n                    j\n                  \n                \n                \n                  \u2016\n                  \n                    2\n                  \n                \n              \n              \u03f5\n            \n          \n        \n      \n    \n    {\\displaystyle e^{\\frac {-\\|x_{i}-x_{j}\\|^{2}}{\\epsilon }}}\n  .\nWithin the framework of manifold regularization, the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes\n\n  \n    \n      \n        \n          \n            argmin\n            \n              f\n              \u2208\n              \n                \n                  H\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              \n                1\n                l\n              \n            \n            \n              \n                \u2211\n                \n                  i\n                  =\n                  1\n                \n                \n                  l\n                \n              \n              V\n              (\n              f\n              (\n              \n                x\n                \n                  i\n                \n              \n              )\n              ,\n              \n                y\n                \n                  i\n                \n              \n              )\n              +\n              \n                \u03bb\n                \n                  A\n                \n              \n              \u2016\n              f\n              \n                \u2016\n                \n                  \n                    H\n                  \n                \n                \n                  2\n                \n              \n              +\n              \n                \u03bb\n                \n                  I\n                \n              \n              \n                \u222b\n                \n                  \n                    M\n                  \n                \n              \n              \u2016\n              \n                \u2207\n                \n                  \n                    M\n                  \n                \n              \n              f\n              (\n              x\n              )\n              \n                \u2016\n                \n                  2\n                \n              \n              d\n              p\n              (\n              x\n              )\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\underset {f\\in {\\mathcal {H}}}{\\operatorname {argmin} }}\\left({\\frac {1}{l}}\\displaystyle \\sum _{i=1}^{l}V(f(x_{i}),y_{i})+\\lambda _{A}\\|f\\|_{\\mathcal {H}}^{2}+\\lambda _{I}\\int _{\\mathcal {M}}\\|\\nabla _{\\mathcal {M}}f(x)\\|^{2}dp(x)\\right)}\n  where \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n   is a reproducing kernel Hilbert space and \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n   is the manifold on which the data lie. The regularization parameters \n  \n    \n      \n        \n          \u03bb\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{A}}\n   and \n  \n    \n      \n        \n          \u03bb\n          \n            I\n          \n        \n      \n    \n    {\\displaystyle \\lambda _{I}}\n   control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian \n  \n    \n      \n        L\n        =\n        D\n        \u2212\n        W\n      \n    \n    {\\displaystyle L=D-W}\n   where \n  \n    \n      \n        \n          D\n          \n            i\n            i\n          \n        \n        =\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            l\n            +\n            u\n          \n        \n        \n          W\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle D_{ii}=\\sum _{j=1}^{l+u}W_{ij}}\n   and \n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle \\mathbf {f} }\n   is the vector \n  \n    \n      \n        [\n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        \u2026\n        f\n        (\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle [f(x_{1})\\dots f(x_{l+u})]}\n  , we have\n\n  \n    \n      \n        \n          \n            f\n          \n          \n            T\n          \n        \n        L\n        \n          f\n        \n        =\n        \n          \n            \u2211\n            \n              i\n              ,\n              j\n              =\n              1\n            \n            \n              l\n              +\n              u\n            \n          \n          \n            W\n            \n              i\n              j\n            \n          \n          (\n          \n            f\n            \n              i\n            \n          \n          \u2212\n          \n            f\n            \n              j\n            \n          \n          \n            )\n            \n              2\n            \n          \n          \u2248\n          \n            \u222b\n            \n              \n                M\n              \n            \n          \n          \u2016\n          \n            \u2207\n            \n              \n                M\n              \n            \n          \n          f\n          (\n          x\n          )\n          \n            \u2016\n            \n              2\n            \n          \n          d\n          p\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {f} ^{T}L\\mathbf {f} =\\displaystyle \\sum _{i,j=1}^{l+u}W_{ij}(f_{i}-f_{j})^{2}\\approx \\int _{\\mathcal {M}}\\|\\nabla _{\\mathcal {M}}f(x)\\|^{2}dp(x)}\n  .The graph-based approach to Laplacian regularization is to put in relation with finite difference method.The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.\n\n\n=== Heuristic approaches ===\nSome methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\dots ,x_{l+u}}\n   may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation. Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.\nSelf-training is a wrapper method for semi-supervised learning. First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.\n\n\n== In human cognition ==\nHuman responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data. More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).\nHuman infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces. Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.\n\n\n== See also ==\nPU learning\n\n\n== References ==\n\n\n== Sources ==\nChapelle, Olivier; Sch\u00f6lkopf, Bernhard; Zien, Alexander (2006). Semi-supervised learning. Cambridge, Mass.: MIT Press. ISBN 978-0-262-03358-9.\n\n\n== External links ==\nManifold Regularization A freely available MATLAB implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.\nKEEL: A software tool to assess evolutionary algorithms for Data Mining problems (regression, classification, clustering, pattern mining and so on) KEEL module for semi-supervised learning.\nSemi-Supervised Learning Software Semi-Supervised Learning Software\nSemi-Supervised learning \u2014 scikit-learn documentation Semi-supervised learning in scikit-learn.", "Data reduction": "Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications.When information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. The data reduction is often undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined.\nAn example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating dozens of megabytes of data per second, which is orders-of-magnitudes more than the downlink bandwidth of 550 kB/s. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further.\nResearch has also been carried out on the use of data reduction in wearable (wireless) devices for health monitoring and diagnosis applications. For example, in the context of epilepsy diagnosis, data reduction has been used to increase the battery lifetime of a wearable EEG device by selecting and only transmitting, EEG data that is relevant for diagnosis and discarding background activity.\n\n\n== Types of Data Reduction ==\n\n\n=== Dimensionality Reduction ===\nWhen dimensionality increases, data becomes increasingly sparse while density and distance between points, critical to clustering and outlier analysis, becomes less meaningful. Dimensionality reduction helps reduce noise in the data and allows for easier visualization, such as the example below where 3-dimensional data is transformed into 2 dimensions to show hidden parts. One method of dimensionality reduction is wavelet transform, in which data is transformed to preserver relative distance between objects at different levels of resolution, and is often used for image compression.\n\n\n=== Numerosity Reduction ===\nThis method of data reduction reduces the data volume by choosing alternate, smaller forms of data representation. Numerosity reduction can be split into 2 groups: parametric and non-parametric methods. Parametric methods (regression, for example) assume the data fits some model, estimate model parameters, store only the parameters, and discard the data. One example of this is in the image below, where the volume of data to be processed is reduced based on more specific criteria. Another example would be a log-linear model, obtaining a value at a point in m-D space as the product on appropriate marginal subspaces. Non-parametric methods do not assume models, some examples being histograms, clustering, sampling, etc.\n\n\n=== Statistical modelling ===\nData reduction can be obtained by assuming a statistical model for the data. Classical principles of data reduction include sufficiency, likelihood, conditionality and equivariance.\n\n\n== See also ==\nData cleansing\nData editing\nData pre-processing\nData wrangling\n\n\n== References ==\n\n\n== Further reading ==\nEhrenberg, Andrew S. C. (1982). A Primer in Data Reduction: An Introductory Statistics Textbook. New York: Wiley. ISBN 0-471-10134-6."}