{"DBSCAN": "DBSCAN（英語：Density-based spatial clustering of applications with noise），是1996年由Martin Ester, Hans-Peter Kriegel, Jörg Sander及Xiaowei Xu提出的聚類分析算法， 這個算法是以密度為本的：給定某空間裡的一個點集合，這算法能把附近的點分成一組（有很多相鄰點的點），並標記出位於低密度區域的局外點（最接近它的點也十分遠），DBSCAN是其中一個最常用的聚類分析算法，也是其中一個科學文章中最常引用的。\n在2014年，這個算法在領頭數據挖掘會議KDD上獲頒發了Test of Time award，該獎項是頒發給一些於理論及實際層面均獲得持續性的關注的算法。\n\n\n== 基礎知識 ==\n考慮在某空間裡將被聚類的點集合，為了進行 DBSCAN 聚類，所有的點被分為核心點、(密度)可達點及局外點，詳請如下：\n\n如果一個點 p 在距離 ε 範圍內有至少 minPts 個點(包括自己)，則這個點被稱為核心點，那些 ε 範圍內的則被稱為由 p 直接可達的。根據定義，沒有任何點是由非核心點直接可達的。\n如果存在一條道路 p1, ..., pn ，有 p1 = p和pn = q， 且每個 pi+1 都是由 pi 直接可達的(道路上除了 q 以外所有點都一定是核心點)，則稱 q 是由 p 可達的。\n所有不由任何點可達的點都被稱為局外點。如果 p 是核心點，則它與所有由它可達的點(包括核心點和非核心點)形成一個聚類，每個聚類擁有最少一個核心點，非核心點也可以是聚類的一部分，但它是在聚類的「邊緣」位置，因為它不能達至更多的點。\n\n「可達性」（英文：Reachability ）不是一個對稱關係，因為根據定義，沒有點是由非核心點可達的，但非核心點可以是由其他點可達的。所以為了正式地界定 DBSCAN 找出的聚類，進一步定義兩點之間的「連結性」（英文：Connectedness） ：如果存在一個點 o 使得點 p 和點 q 都是由 o 可達的，則點 p 和點 q 被稱為(密度)連結的，而連結性是一個對稱關係。\n定義了連結性之後，每個聚類都符合兩個性質：\n\n一個聚類裡的每兩個點都是互相連結的；\n如果一個點 p 是由一個在聚類裡的點 q 可達的，那麼 p 也在 q 所屬的聚類裡。\n\n\n== 算法 ==\nDBSCAN 需要兩個參數：ε (eps) 和形成高密度區域所需要的最少點數 (minPts)，它由一個任意未被訪問的點開始，然後探索這個點的 ε-鄰域，如果 ε-鄰域裡有足夠的點，則建立一個新的聚類，否則這個點被標籤為雜音。注意這個點之後可能被發現在其它點的 ε-鄰域裡，而該 ε-鄰域可能有足夠的點，屆時這個點會被加入該聚類中。\n如果一個點位於一個聚類的密集區域裡，它的 ε-鄰域裡的點也屬於該聚類，當這些新的點被加進聚類後，如果它(們)也在密集區域裡，它(們)的 ε-鄰域裡的點也會被加進聚類裡。這個過程將一直重覆，直至不能再加進更多的點為止，這樣，一個密度連結的聚類被完整地找出來。然後，一個未曾被訪問的點將被探索，從而發現一個新的聚類或雜音。\n算法可以以下偽代碼表達，當中變數根據原本刊登時的命名：\n\nDBSCAN(D, eps, MinPts) {\n   C = 0\n   for each point P in dataset D {\n      if P is visited\n         continue next point\n      mark P as visited\n      NeighborPts = regionQuery(P, eps)\n      if sizeof(NeighborPts) < MinPts\n         mark P as NOISE\n      else {\n         C = next cluster\n         expandCluster(P, NeighborPts, C, eps, MinPts)\n      }\n   }\n}\n\nexpandCluster(P, NeighborPts, C, eps, MinPts) {\n   add P to cluster C\n   for each point P' in NeighborPts { \n      if P' is not visited {\n         mark P' as visited\n         NeighborPts' = regionQuery(P', eps)\n         if sizeof(NeighborPts') >= MinPts\n            NeighborPts = NeighborPts joined with NeighborPts'\n      }\n      if P' is not yet member of any cluster\n         add P' to cluster C\n   }\n}\n\nregionQuery(P, eps)\n   return all points within P's eps-neighborhood (including P)\n\n注意這個算法可以以下方式簡化：其一，\"has been visited\" 和 \"belongs to cluster C\" 可被結合起來，另外 \"expandCluster\" 副程式不必被抽出來，因為它只在一個位置被调用。以上算法沒有以簡化方式呈現，以反映原本出版的版本。另外，regionQuery 是否包含 P 並不重要，它等價於改變 MinPts 的值。\n\n\n== 複雜度 ==\nDBSCAN對資料庫裡的每一點進行訪問，可能多於一次（例如作為不同聚類的候選者），但在現實的考慮中，時間複雜度主要受regionQuery的调用次數影響，DBSCAN對每點都進行剛好一次调用，且如果使用了特別的編號結構，則總平均時間複雜度為\n  \n    \n      \n        O\n        (\n        n\n        log\n        ⁡\n        \n          n\n        \n        )\n      \n    \n    {\\displaystyle O(n\\log {n})}\n  ，最差時間複雜度則為\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  。可以使用\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  空間複雜度的距離矩陣以避免重複計算距離，但若不使用距離矩陣，DBSCAN的空間複雜度為\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  。\n\n\n== 優點 ==\n相比 K-平均算法，DBSCAN 不需要預先聲明聚類數量。\nDBSCAN 可以找出任何形狀的聚類，甚至能找出一個聚類，它包圍但不連接另一個聚類，另外，由於 MinPts 參數，single-link effect （不同聚類以一點或極幼的線相連而被當成一個聚類）能有效地被避免。\nDBSCAN 能分辨噪音（局外點）。\nDBSCAN 只需兩個參數，且對資料庫內的點的次序幾乎不敏感（兩個聚類之間邊緣的點有機會受次序的影響被分到不同的聚類，另外聚類的次序會受點的次序的影響）。\nDBSCAN 被設計成能配合可加速範圍訪問的資料庫結構，例如 R*樹。\n如果對資料有足夠的了解，可以選擇適當的參數以獲得最佳的分類。\n\n\n== 缺點 ==\nDBSCAN 不是完全決定性的：在兩個聚類交界邊緣的點會視乎它在資料庫的次序決定加入哪個聚類，幸運地，這種情況並不常見，而且對整體的聚類結果影響不大——DBSCAN 對核心點和噪音都是決定性的。DBSCAN* 是一種變化了的演算法，把交界點視為噪音，達到完全決定性的結果。\nDBSCAN 聚類分析的質素受函數 regionQuery(P,ε) 裡所使用的度量影響，最常用的度量是歐幾里得距離，尤其在高維度資料中，由於受所謂「維數災難」影響，很難找出一個合適的 ε ，但事實上所有使用歐幾里得距離的演算法都受維數災難影響。\n如果資料庫裡的點有不同的密度，而該差異很大，DBSCAN 將不能提供一個好的聚類結果，因為不能選擇一個適用於所有聚類的 minPts-ε 參數組合。\n如果沒有對資料和比例的足夠理解，將很難選擇適合的 ε 參數。\n\n\n== 有關文章 ==\nOPTICS算法：一個DBSCAN的一般化，有效地以「最大搜尋半徑」代替ε參數。\nConnected component\n並查集\n\n\n== 注意 ==\n\n\n== 參考文獻 ==\n\n\n=== 延伸閱讀 ===\nArlia, Domenica; Coppola, Massimo. \"Experiments in Parallel Clustering with DBSCAN\". Euro-Par 2001: Parallel Processing: 7th International Euro-Par Conference Manchester, UK August 28–31, 2001, Proceedings. Springer Berlin. \nKriegel, Hans-Peter; Kröger, Peer; Sander, Jörg; Zimek, Arthur (2011). \"Density-based Clustering\"（页面存档备份，存于互联网档案馆）. WIREs Data Mining and Knowledge Discovery. 1 (3): 231–240. doi:10.1002/widm.30.", "OPTICS": "OPTICS（英語：Ordering points to identify the clustering structure）是由Mihael Ankerst，Markus M. Breunig，Hans-Peter Kriegel和Jörg Sander提出的基于密度的聚类分析算法。OPTICS并不依赖全局变量来确定聚类，而是将空间上最接近的点相邻排列，以得到数据集合中的对象的线性排序。排序后生成的序列存储了与相邻点之间的距离，并最终生成了一个 dendrogram 。OPTICS算法的思路与DBSCAN类似，但是解决了DBSCAN的一个主要弱点，即如何在密度变化的数据中取得有效的聚类。同时 OPTICS也避免了多数聚类算法中对输入参数敏感的问题。\n\n\n== 复杂度 ==\n类似于DBSCAN，OPTICS处理数据集中的每个点，在这个过程中进行\n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  -邻域查询。如果保证给定空间坐标时候，邻域查询可以以\n  \n    \n      \n        O\n        (\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n  的复杂度完成，可以得到总时间复杂度为\n  \n    \n      \n        O\n        (\n        n\n        ⋅\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\displaystyle O(n\\cdot \\log n)}\n  。OPTICS原始论文的作者表明OPTICS算法比DBSCAN算法慢常数1.6倍。由于值过大可能会使邻域查询的的时间复杂度降至线性，这个数值可能会显著变化。\n实践中，选择\n  \n    \n      \n        ε\n        >\n        \n          max\n          \n            x\n            ,\n            y\n          \n        \n        d\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle \\varepsilon >\\max _{x,y}d(x,y)}\n  （大于数据集中的最大距离）是可能的，但由于每此领域查询会在整个数据集中进行，时间复杂度会降至平方。即使没有可用的空间索引，也会产生额外的堆管理成本。 因此\n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  应当被仔细选择。\n\n\n== 软件实现 ==\nELKI数据挖掘框架提供了OPTICS、OPTICS-OF、DeLi-Clu、HiSC、HiCO和DiSH的Java实现。\nR语言中，dbscan包提供了OPTICS的C++实现。\nPython中，PyClustering库和Scikit-learn库实现了OPTICS；hdbscan库提供了HDBSCAN*实现。\n\n\n== 参考资料 ==", "信賴區間": "在统计学中，一个概率样本的置信区间（英語：confidence interval，CI），是对产生这个样本的总体的参数分布（parametric distribution）中的某一个未知母數值，以区间形式给出的估计。相对于点估计（point estimation）用一个样本统计量来估计参数值，置信区间还蕴含了估计的精确度的信息。在现代机器学习中越来越常用的置信集合（confidence set）概念是置信区间在多维分析的推广。\n置信区间在频率学派中间使用，其在贝叶斯统计中的对应概念是可信区间（credible interval）。两者建立在不同的概念基础上的，贝叶斯统计将分布的位置参数视为随机变量，并对给定观测到的数据之后未知参数的后验分布进行描述，故无论对随机样本还是已观测数据，构造出来的可信区间，其可信水平都是一个合法的概率；而置信区间的置信水平，只在考虑随机样本时可以被理解为一个概率。\n\n\n== 定义 ==\n\n\n=== 对随机样本的定义 ===\n定义置信区间最清晰的方式是从一个随机样本出发。考虑一个一维随机变量\n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\cal {X}}}\n  服从分布\n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\cal {F}}}\n  ，又假设\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  是\n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\cal {F}}}\n  的参数之一。假设我们的数据采集计划将要独立地抽样\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  次，得到一个随机样本\n  \n    \n      \n        {\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{X_{1},\\ldots ,X_{n}\\}}\n  ，注意这里所有的\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  都是随机的，我们是在讨论一个尚未被观测的数据集。如果存在统计量（统计量定义为样本\n  \n    \n      \n        X\n        =\n        {\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle X=\\{X_{1},\\ldots ,X_{n}\\}}\n  的一个函数，且不得依赖于任何未知参数）\n  \n    \n      \n        u\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n        ,\n        v\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle u(X_{1},\\ldots ,X_{n}),v(X_{1},\\ldots ,X_{n})}\n  满足\n  \n    \n      \n        u\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n        <\n        v\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle u(X_{1},\\ldots ,X_{n})<v(X_{1},\\ldots ,X_{n})}\n  使得：\n\n  \n    \n      \n        \n          P\n        \n        \n          (\n          \n            θ\n            ∈\n            \n              (\n              \n                u\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  X\n                  \n                    n\n                  \n                \n                )\n                ,\n                v\n                (\n                \n                  X\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  X\n                  \n                    n\n                  \n                \n                )\n              \n              )\n            \n          \n          )\n        \n        =\n        1\n        −\n        α\n      \n    \n    {\\displaystyle \\mathbb {P} \\left(\\theta \\in \\left(u(X_{1},\\ldots ,X_{n}),v(X_{1},\\ldots ,X_{n})\\right)\\right)=1-\\alpha }\n  则称\n  \n    \n      \n        \n          (\n          \n            u\n            (\n            \n              X\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n              \n            \n            )\n            ,\n            v\n            (\n            \n              X\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(u(X_{1},\\ldots ,X_{n}),v(X_{1},\\ldots ,X_{n})\\right)}\n  为一个用于估计参数\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  的\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  置信区间，其中的，\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  称为置信水平，\n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  在假设检验中也称为显著性水平。\n\n\n=== 对观测到的数据的定义 ===\n接续随机样本版本的定义，现在，对于随机变量\n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\cal {X}}}\n  的一个已经观测到的样本\n  \n    \n      \n        {\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{x_{1},\\ldots ,x_{n}\\}}\n  ，注意这里用小写x表记的\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  都是已经观测到的数字，没有随机性了，定义基于数据的\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  置信区间为：\n\n  \n    \n      \n        \n          (\n          \n            u\n            (\n            \n              x\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              x\n              \n                n\n              \n            \n            )\n            ,\n            v\n            (\n            \n              x\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              x\n              \n                n\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(u(x_{1},\\ldots ,x_{n}),v(x_{1},\\ldots ,x_{n})\\right)}\n  注意，置信区间可以是单尾或者双尾的，单尾的置信区间中设定\n  \n    \n      \n        u\n        =\n        −\n        ∞\n      \n    \n    {\\displaystyle u=-\\infty }\n  或者\n  \n    \n      \n        v\n        =\n        +\n        ∞\n      \n    \n    {\\displaystyle v=+\\infty }\n  ，具体前者还是后者取决于所构造的置信区间的方向。\n初学者常犯一个概念性错误，是将基于观测到的数据所同样构造的置信区间的置信水平，误认为是它包含真实未知参数的真实值的概率。正确的理解是：置信水平只有在描述这个同样构造置信区间的过程（或称方法）的意义下才能被视为一个概率。一个基于已经观测到的数据所构造出来的置信区间，其两个端点已经不再具有随机性，因此，类似的构造的间隔将会包含真正的值的比例在所有值中，其包含未知参数的真实值的概率是0或者1，但我们不能知道是前者还是后者。\n\n\n== 例子 ==\n\n\n=== 例1：正态分布，已知总体方差 ===\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  \n\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平的正态置信区间为：\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            −\n            \n              z\n              \n                α\n                \n                  /\n                \n                2\n              \n            \n            \n              \n                σ\n                \n                  n\n                \n              \n            \n            ,\n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            +\n            \n              z\n              \n                α\n                \n                  /\n                \n                2\n              \n            \n            \n              \n                σ\n                \n                  n\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\bar {x}}-z_{\\alpha /2}{\\frac {\\sigma }{\\sqrt {n}}},{\\bar {x}}+z_{\\alpha /2}{\\frac {\\sigma }{\\sqrt {n}}}\\right)}\n    (双尾)\n\n  \n    \n      \n        \n          (\n          \n            −\n            ∞\n            ,\n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            +\n            \n              z\n              \n                α\n              \n            \n            \n              \n                σ\n                \n                  n\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(-\\infty ,{\\bar {x}}+z_{\\alpha }{\\frac {\\sigma }{\\sqrt {n}}}\\right)}\n    (单尾)\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            −\n            \n              z\n              \n                α\n              \n            \n            \n              \n                σ\n                \n                  n\n                \n              \n            \n            ,\n            +\n            ∞\n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\bar {x}}-z_{\\alpha }{\\frac {\\sigma }{\\sqrt {n}}},+\\infty \\right)}\n    (单尾)以下为方便起见，只列出双尾置信区间的例子，且区间中用\"\n  \n    \n      \n        ±\n      \n    \n    {\\displaystyle \\pm }\n  \"进行简记：\n\n\n=== 例2：正态分布，未知总体方差 ===\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  \n\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平的双尾正态置信区间为：\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            ±\n            \n              t\n              \n                n\n                −\n                1\n                ;\n                α\n                \n                  /\n                \n                2\n              \n            \n            \n              \n                s\n                \n                  n\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\bar {x}}\\pm t_{n-1;\\alpha /2}{\\frac {s}{\\sqrt {n}}}\\right)}\n  \n\n\n=== 例3：两个独立正态样本 ===\n設有兩個獨立正態樣本\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  和\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ，样本大小为\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  和\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  ，估计总体均值之差\n  \n    \n      \n        \n          μ\n          \n            1\n          \n        \n        −\n        \n          μ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu _{1}-\\mu _{2}}\n  ，假设总体方差未知但相等：\n  \n    \n      \n        \n          σ\n          \n            1\n          \n        \n        =\n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{1}=\\sigma _{2}}\n  (如果未知且不等就要应用Welch公式来确定t分布的自由度)\n\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平的双尾正态置信区间为：\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            −\n            \n              \n                \n                  y\n                  ¯\n                \n              \n            \n            ±\n            \n              t\n              \n                m\n                +\n                n\n                −\n                2\n                ;\n                α\n                \n                  /\n                \n                2\n              \n            \n            ⋅\n            \n              s\n              \n                p\n              \n            \n            ⋅\n            \n              \n                \n                  \n                    1\n                    m\n                  \n                \n                +\n                \n                  \n                    1\n                    n\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\bar {x}}-{\\bar {y}}\\pm t_{m+n-2;\\alpha /2}\\cdot s_{p}\\cdot {\\sqrt {{\\frac {1}{m}}+{\\frac {1}{n}}}}\\right)}\n  ，其中\n  \n    \n      \n        \n          s\n          \n            p\n          \n        \n        =\n        \n          \n            \n              \n                (\n                m\n                −\n                1\n                )\n                \n                  s\n                  \n                    x\n                  \n                  \n                    2\n                  \n                \n                +\n                (\n                n\n                −\n                1\n                )\n                \n                  s\n                  \n                    y\n                  \n                  \n                    2\n                  \n                \n              \n              \n                m\n                +\n                n\n                −\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s_{p}={\\sqrt {\\frac {(m-1)s_{x}^{2}+(n-1)s_{y}^{2}}{m+n-2}}}}\n  且\n  \n    \n      \n        \n          s\n          \n            x\n          \n        \n        ,\n        \n          s\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle s_{x},s_{y}}\n  分别表示\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  和\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  的样本标准差。\n\n\n== 常見誤解 ==\n\n信賴區間及信心水準常被誤解，出版的研究也顯示出既使是專業的科學家也常做出錯誤的詮釋。\n以95%的信賴區間來說，建構出一個信賴區間，不代表分佈的參數有95%的機率會落在該信賴區間內（也就是說該區間有95%的機率涵蓋了分佈參數）。 依照嚴格的頻率學派詮釋，一旦信賴區間被建構完全，此區間不是涵蓋了參數就是沒涵蓋參數，已經沒有機率可言。95%機率指的是建構信賴區間步驟的可靠性，不是針對一個特定的區間。內曼本人（信賴區間的原始提倡者）在他的原始論文提出此點：「在上面的敘述中可以注意到，機率是指統計學家在未來關心的估計問題。事實上，我已多次說明，正確結果的頻率會趨向於α。考慮到一個樣本已被抽取，[特定端點]也已被計算完成。我們能說在這個特定的例子裡真值[落到端點中]的機率等於α嗎？答案明顯是否定的。參數是未知的常數，無法做出對其值的機率敘述……」Deborah Mayo針對此點進一步說道：「無論如何必須強調，在看到[資料的]數值後，Neyman–Pearson理論從不允許做出以下結論，特定產生的信賴區間涵蓋了真值的機率或信心為(1 − α)100%。Seidenfeld的評論似乎源於一種（並非不尋常的）期望，Neyman–Pearson信賴區間能提供他們無法合理提供的，也就是未知參數落入特定區間的機率大小、信心高低或支持程度的測度。隨著Savage (1962)之後，參數落入特定區間的機率可能是指最終精密度的測度。最終精密度的測度令人嚮往而且信賴區間又常被(錯誤地)解釋成可提供此測度，然而此解釋是不被保證的。無可否認的，『信賴』二字助長了此誤解。」95%信賴區間不代表有95%的樣本資料落在此信賴區間。\n信賴區間不是樣本參數的可能值的確定範圍，雖然它常被啟發為可能值的範圍。\n從一個實驗中算出的一個95%信賴區間，不代表從不同實驗得到的樣本參數有95%落在該區間中 \n\n\n== 构造法 ==\n一般来说，置信区间的构造需要先找到一个枢轴变量（pivotal quantity，或称pivot），其表达式依赖于样本以及待估计的未知参数(但不能依赖于总体的其它未知参数)，其分布不依赖于任何未知参数。\n下面以上述例2为例，说明如何利用枢轴变量构造置信区间。对于一个正态分布的随机样本\n  \n    \n      \n        \n          \n            X\n            \n              1\n            \n          \n          ,\n          …\n          ,\n          \n            X\n            \n              n\n            \n          \n        \n      \n    \n    {\\displaystyle {X_{1},\\ldots ,X_{n}}}\n  ，可以证明(此证明对初学者并不容易)如下统计量互相独立：\n\n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}={\\frac {1}{n}}\\sum _{i=1}^{n}X_{i}}\n    和   \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                \n                  (\n                  \n                    \n                      X\n                      \n                        i\n                      \n                    \n                    −\n                    \n                      \n                        \n                          X\n                          ¯\n                        \n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n            \n              n\n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle S^{2}={\\frac {\\sum _{i=1}^{n}\\left(X_{i}-{\\bar {X}}\\right)^{2}}{n-1}}}\n  它们的分布是：\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    X\n                    ¯\n                  \n                \n              \n              −\n              μ\n            \n            \n              σ\n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n        ∼\n        N\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle {\\frac {{\\bar {X}}-\\mu }{\\sigma /{\\sqrt {n}}}}\\sim N(0,1)}\n    和  \n  \n    \n      \n        (\n        n\n        −\n        1\n        )\n        \n          \n            \n              S\n              \n                2\n              \n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        ∼\n        \n          χ\n          \n            n\n            −\n            1\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (n-1){\\frac {S^{2}}{\\sigma ^{2}}}\\sim \\chi _{n-1}^{2}}\n  所以根据t分布的定义，有\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    X\n                    ¯\n                  \n                \n              \n              −\n              μ\n            \n            \n              S\n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n        ∼\n        \n          t\n          \n            n\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\bar {X}}-\\mu }{S/{\\sqrt {n}}}}\\sim t_{n-1}}\n  于是反解如下等式左边括号中的不等式\n\n  \n    \n      \n        \n          P\n        \n        \n          (\n          \n            −\n            \n              t\n              \n                n\n                −\n                1\n                ;\n                α\n                \n                  /\n                \n                2\n              \n            \n            <\n            t\n            =\n            \n              \n                \n                  \n                    \n                      \n                        X\n                        ¯\n                      \n                    \n                  \n                  −\n                  μ\n                \n                \n                  S\n                  \n                    \n                      n\n                    \n                  \n                \n              \n            \n            <\n            \n              t\n              \n                n\n                −\n                1\n                ;\n                α\n                \n                  /\n                \n                2\n              \n            \n          \n          )\n        \n        =\n        1\n        −\n        α\n      \n    \n    {\\displaystyle \\mathbb {P} \\left(-t_{n-1;\\alpha /2}<t={\\frac {{\\bar {X}}-\\mu }{S{\\sqrt {n}}}}<t_{n-1;\\alpha /2}\\right)=1-\\alpha }\n  就得到了例2中双尾置信区间的表达式。\n\n\n== 与参数检验的联系 ==\n有时，置信区间可以用来进行母數检验。例如在上面的例1中构造的双尾\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平置信区间，可以用来检验具有相应的显著水平为\n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  的双尾對立假說，具体地说是如下检验：\n正态分布总体，知道总体方差\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  ，在\n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  显著水平下检验：\n\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n        :\n        μ\n        =\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}:\\mu =\\mu _{0}}\n   vs \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n        :\n        μ\n        ≠\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{1}:\\mu \\neq \\mu _{0}}\n  检验方法是：当（且仅当）相应的\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平置信区间不包含\n  \n    \n      \n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  时拒绝零假设\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  \n例1中构造的双尾\n  \n    \n      \n        1\n        −\n        α\n      \n    \n    {\\displaystyle 1-\\alpha }\n  水平置信区间也可以用来检验如下两个显著水平为\n  \n    \n      \n        α\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle \\alpha /2}\n  的单尾对立假设：\n\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n        :\n        μ\n        ≤\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}:\\mu \\leq \\mu _{0}}\n   vs \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n        :\n        μ\n        >\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{1}:\\mu >\\mu _{0}}\n  和\n\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n        :\n        μ\n        ≥\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}:\\mu \\geq \\mu _{0}}\n   vs \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n        :\n        μ\n        <\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{1}:\\mu <\\mu _{0}}\n  检验方法是完全类似的，比如对于上述第一个单尾检验\n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n        :\n        μ\n        >\n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{1}:\\mu >\\mu _{0}}\n  ，当且仅当双尾置信区间的左端点大于\n  \n    \n      \n        \n          μ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n  时拒绝零假设。\n\n\n== 参考文献 ==\n\n\n== 参考书目 ==", "先验概率": "在贝叶斯统计中，某一不确定量p的先验概率（Prior probability）分布是在考虑「观测数据」前，能表达p不确定性的概率分布。它旨在描述这个不确定量的不确定程度，而不是这个不确定量的随机性。这个不确定量可以是一个参数，或者是一个隐含变量（英語：latent variable）。依據應用領域的不同，事前機率又叫做先驗機率、先驗概率、事前先驗機率、居先機率。\n在使用贝叶斯定理时，我们通过将先验概率与似然函数相乘，随后标准化，来得到后验概率分布，也就是给出某数据，该不确定量的条件分布。\n先验概率通常是主观的猜测，为了使计算后验概率方便，有时候会选择共轭先验。如果后验概率和先验概率是同一族的，则认为它们是共轭分布，这个先验概率就是对应于似然函数的共轭先验。\n\n\n== 參考資料 ==", "提升方法": "提升方法（Boosting）是一种机器学习中的集成学习元启发算法，主要用来减小監督式學習中偏差并且也减小方差，以及一系列将弱学习器转换为强学习器的机器学习算法。面對的问题是邁可·肯斯（Michael Kearns）和莱斯利·瓦利安特(Leslie Valiant)提出的：一組“弱学习者”的集合能否生成一个“强学习者”？弱学习者一般是指一个分类器，它的结果只比随机分类好一点点；强学习者指分类器的结果非常接近真值。\nRobert Schapire在1990年的一篇论文中对肯斯和瓦利安特的问题的肯定回答在机器学习和统计方面产生了重大影响，最显着的是导致了提升方法的发展 。\n\n\n== 提升算法 ==\n大多数提升算法包括由迭代使用弱学习分類器組成，並將其結果加入一個最終的成强学习分類器。加入的过程中，通常根据它们的分类准确率给予不同的权重。加和弱学习者之后，数据通常会被重新加权，来强化对之前分类错误数据点的分类。\n\n一个经典的提升算法例子是AdaBoost。一些最近的例子包括LPBoost、TotalBoost、BrownBoost、MadaBoost及LogitBoost。许多提升方法可以在AnyBoost框架下解释为在函数空间利用一个凸的误差函数作梯度下降。\n\n\n== 批评 ==\n2008年，谷歌的菲利普·隆（Phillip Long）與哥倫比亞大學的羅可·A·瑟維迪歐（Rocco A. Servedio）发表论文指出这些方法是有缺陷的：在训练集有错误的标记的情况下，一些提升算法雖會尝试提升这种样本点的正确率，但卻無法产生一个正确率大于1/2的模型。\n\n\n== 相關條目 ==\nAdaBoost\n随机森林\nLogit模型\n人工神经网络\n支持向量機\n机器学习\n\n\n== 实现 ==\nOrange, a free data mining software suite, module Orange.ensemble （页面存档备份，存于互联网档案馆）\nWeka is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost\nR package GBM （页面存档备份，存于互联网档案馆） (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.\njboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees\n\n\n== 参考文献 ==\n\n\n=== 腳註 ===\n\n\n=== 其他參考資料 ===\nYoav Freund and Robert E. Schapire (1997); A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting （页面存档备份，存于互联网档案馆）, Journal of Computer and System Sciences, 55(1):119-139\nRobert E. Schapire and Yoram Singer (1999); Improved Boosting Algorithms Using Confidence-Rated Predictors （页面存档备份，存于互联网档案馆）, Machine Learning, 37(3):297-336\n\n\n== 外部链接 ==\nRobert E. Schapire (2003); The Boosting Approach to Machine Learning: An Overview （页面存档备份，存于互联网档案馆）, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification\nAn up-to-date collection of papers on boosting", "朴素贝叶斯分类器": "朴素贝叶斯分类器（英語：Naive Bayes classifier，台湾稱為單純貝氏分類器），在机器学习中是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单概率分类器。\n單純貝氏自1950年代已广泛研究，在1960年代初就以另外一个名称引入到文本信息检索界中， 并仍然是文本分类的一种热门（基准）方法，文本分类是以词频为特征判断文件所属类别或其他（如垃圾邮件、合法性、体育或政治等等）的问题。通过适当的预处理，它可以与这个领域更先进的方法（包括支持向量机）相竞争。 它在自动医疗诊断中也有应用。單純貝氏分类器是高度可扩展的，因此需要数量与学习问题中的变量（特征/预测器）成线性关系的参数。最大似然训练可以通过评估一个封闭形式的表达式来完成， 只需花费线性时间，而不需要其他很多类型的分类器所使用的费时的迭代逼近。\n在统计学和计算机科学文献中，單純貝氏模型有各种名称，包括简单贝叶斯和独立贝叶斯。 所有这些名称都参考了贝叶斯定理在该分类器的决策规则中的使用，但單純貝氏不（一定）用到贝叶斯方法； 《Russell和Norvig》提到“『單純貝氏』有时被称为贝叶斯分类器，这个马虎的使用促使真正的贝叶斯论者称之为傻瓜贝叶斯模型。”\n\n\n== 简介 ==\n單純貝氏是一种建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有單純貝氏分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而單純貝氏分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。\n对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，單純貝氏模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，單純貝氏模型也能奏效。\n尽管是带着这些朴素思想和过于简单化的假设，但單純貝氏分类器在很多复杂的现实情形中仍能够取得相当好的效果。2004年，一篇分析贝叶斯分类器问题的文章揭示了單純貝氏分类器取得看上去不可思议的分类效果的若干理论上的原因。 尽管如此，2006年有一篇文章详细比较了各种分类方法，发现更新的方法（如决策树和随机森林）的性能超过了贝叶斯分类器。單純貝氏分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量的方法，而不需要确定整个协方差矩阵。\n\n\n== 單純貝氏概率模型 ==\n理论上，概率模型分类器是一个条件概率模型。\n\n  \n    \n      \n        p\n        (\n        C\n        |\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle p(C\\vert F_{1},\\dots ,F_{n})\\,}\n  独立的类别变量\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  有若干类别，条件依赖于若干特征变量\n\n  \n    \n      \n        \n          F\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle F_{1}}\n  ,\n  \n    \n      \n        \n          F\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle F_{2}}\n  ,...,\n  \n    \n      \n        \n          F\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle F_{n}}\n  。但问题在于如果特征数量\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  较大或者每个特征能取大量值时，基于概率模型列出概率表变得不现实。所以我们修改这个模型使之变得可行。\n贝叶斯定理有以下式子：\n\n  \n    \n      \n        p\n        (\n        C\n        |\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            \n              p\n              (\n              C\n              )\n               \n              p\n              (\n              \n                F\n                \n                  1\n                \n              \n              ,\n              …\n              ,\n              \n                F\n                \n                  n\n                \n              \n              |\n              C\n              )\n            \n            \n              p\n              (\n              \n                F\n                \n                  1\n                \n              \n              ,\n              …\n              ,\n              \n                F\n                \n                  n\n                \n              \n              )\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle p(C\\vert F_{1},\\dots ,F_{n})={\\frac {p(C)\\ p(F_{1},\\dots ,F_{n}\\vert C)}{p(F_{1},\\dots ,F_{n})}}.\\,}\n  用朴素的语言可以表达为：\n\n  \n    \n      \n        \n          \n            posterior\n          \n        \n        =\n        \n          \n            \n              \n                \n                  prior\n                \n              \n              ×\n              \n                \n                  likelihood\n                \n              \n            \n            \n              evidence\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle {\\mbox{posterior}}={\\frac {{\\mbox{prior}}\\times {\\mbox{likelihood}}}{\\mbox{evidence}}}.\\,}\n  实际中，我们只关心分式中的分子部分，因为分母不依赖于\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  而且特征\n  \n    \n      \n        \n          F\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle F_{i}}\n  的值是给定的，于是分母可以认为是一个常数。这样分子就等价于联合分布模型。\n\n  \n    \n      \n        p\n        (\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle p(C,F_{1},\\dots ,F_{n})\\,}\n  重复使用链式法则，可将该式写成条件概率的形式，如下所示：\n\n  \n    \n      \n        p\n        (\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle p(C,F_{1},\\dots ,F_{n})\\,}\n  \n\n  \n    \n      \n        ∝\n        p\n        (\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        |\n        C\n        )\n      \n    \n    {\\displaystyle \\varpropto p(C)\\ p(F_{1},\\dots ,F_{n}\\vert C)}\n  \n\n  \n    \n      \n        ∝\n        p\n        (\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            1\n          \n        \n        |\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varpropto p(C)\\ p(F_{1}\\vert C)\\ p(F_{2},\\dots ,F_{n}\\vert C,F_{1})}\n  \n\n  \n    \n      \n        ∝\n        p\n        (\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            1\n          \n        \n        |\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            2\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        )\n         \n        p\n        (\n        \n          F\n          \n            3\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        \n          F\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varpropto p(C)\\ p(F_{1}\\vert C)\\ p(F_{2}\\vert C,F_{1})\\ p(F_{3},\\dots ,F_{n}\\vert C,F_{1},F_{2})}\n  \n\n  \n    \n      \n        ∝\n        p\n        (\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            1\n          \n        \n        |\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            2\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        )\n         \n        p\n        (\n        \n          F\n          \n            3\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        \n          F\n          \n            2\n          \n        \n        )\n         \n        p\n        (\n        \n          F\n          \n            4\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        \n          F\n          \n            2\n          \n        \n        ,\n        \n          F\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle \\varpropto p(C)\\ p(F_{1}\\vert C)\\ p(F_{2}\\vert C,F_{1})\\ p(F_{3}\\vert C,F_{1},F_{2})\\ p(F_{4},\\dots ,F_{n}\\vert C,F_{1},F_{2},F_{3})}\n  \n\n  \n    \n      \n        ∝\n        p\n        (\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            1\n          \n        \n        |\n        C\n        )\n         \n        p\n        (\n        \n          F\n          \n            2\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        )\n         \n        p\n        (\n        \n          F\n          \n            3\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        \n          F\n          \n            2\n          \n        \n        )\n         \n        …\n        p\n        (\n        \n          F\n          \n            n\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            1\n          \n        \n        ,\n        \n          F\n          \n            2\n          \n        \n        ,\n        \n          F\n          \n            3\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n            −\n            1\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\varpropto p(C)\\ p(F_{1}\\vert C)\\ p(F_{2}\\vert C,F_{1})\\ p(F_{3}\\vert C,F_{1},F_{2})\\ \\dots p(F_{n}\\vert C,F_{1},F_{2},F_{3},\\dots ,F_{n-1}).}\n  现在“朴素”的条件独立假设开始发挥作用：假设每个特征\n  \n    \n      \n        \n          F\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle F_{i}}\n  对于其他特征\n  \n    \n      \n        \n          F\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle F_{j}}\n  ,\n  \n    \n      \n        j\n        ≠\n        i\n      \n    \n    {\\displaystyle j\\neq i}\n  是条件独立的。这就意味着\n\n  \n    \n      \n        p\n        (\n        \n          F\n          \n            i\n          \n        \n        |\n        C\n        ,\n        \n          F\n          \n            j\n          \n        \n        )\n        =\n        p\n        (\n        \n          F\n          \n            i\n          \n        \n        |\n        C\n        )\n        \n      \n    \n    {\\displaystyle p(F_{i}\\vert C,F_{j})=p(F_{i}\\vert C)\\,}\n  对于\n  \n    \n      \n        i\n        ≠\n        j\n      \n    \n    {\\displaystyle i\\neq j}\n  ，所以联合分布模型可以表达为\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                (\n                C\n                |\n                \n                  F\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  F\n                  \n                    n\n                  \n                \n                )\n              \n              \n                \n                ∝\n                p\n                (\n                C\n                ,\n                \n                  F\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  F\n                  \n                    n\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                ∝\n                p\n                (\n                C\n                )\n                 \n                p\n                (\n                \n                  F\n                  \n                    1\n                  \n                \n                |\n                C\n                )\n                 \n                p\n                (\n                \n                  F\n                  \n                    2\n                  \n                \n                |\n                C\n                )\n                 \n                p\n                (\n                \n                  F\n                  \n                    3\n                  \n                \n                |\n                C\n                )\n                 \n                ⋯\n                \n              \n            \n            \n              \n              \n                \n                ∝\n                p\n                (\n                C\n                )\n                \n                  ∏\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                p\n                (\n                \n                  F\n                  \n                    i\n                  \n                \n                |\n                C\n                )\n                .\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}p(C\\vert F_{1},\\dots ,F_{n})&\\varpropto p(C,F_{1},\\dots ,F_{n})\\\\&\\varpropto p(C)\\ p(F_{1}\\vert C)\\ p(F_{2}\\vert C)\\ p(F_{3}\\vert C)\\ \\cdots \\,\\\\&\\varpropto p(C)\\prod _{i=1}^{n}p(F_{i}\\vert C).\\,\\end{aligned}}}\n  这意味着上述假设下，类变量\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  的条件分布可以表达为：\n\n  \n    \n      \n        p\n        (\n        C\n        |\n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            1\n            Z\n          \n        \n        p\n        (\n        C\n        )\n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        p\n        (\n        \n          F\n          \n            i\n          \n        \n        |\n        C\n        )\n      \n    \n    {\\displaystyle p(C\\vert F_{1},\\dots ,F_{n})={\\frac {1}{Z}}p(C)\\prod _{i=1}^{n}p(F_{i}\\vert C)}\n  其中\n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  （证据因子）是一个只依赖与\n  \n    \n      \n        \n          F\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          F\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle F_{1},\\dots ,F_{n}}\n  等的缩放因子，当特征变量的值已知时是一个常数。\n由于分解成所谓的类先验概率\n  \n    \n      \n        p\n        (\n        C\n        )\n      \n    \n    {\\displaystyle p(C)}\n  和独立概率分布\n  \n    \n      \n        p\n        (\n        \n          F\n          \n            i\n          \n        \n        |\n        C\n        )\n      \n    \n    {\\displaystyle p(F_{i}\\vert C)}\n  ，上述概率模型的可掌控性得到很大的提高。如果这是一个\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  分类问题，且每个\n  \n    \n      \n        p\n        (\n        \n          F\n          \n            i\n          \n        \n        |\n        C\n        =\n        c\n        )\n      \n    \n    {\\displaystyle p(F_{i}\\vert C=c)}\n  可以表达为\n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  个参数，于是相应的單純貝氏模型有(k − 1) + n r k个参数。实际应用中，通常取\n  \n    \n      \n        k\n        =\n        2\n      \n    \n    {\\displaystyle k=2}\n  （二分类问题），\n  \n    \n      \n        r\n        =\n        1\n      \n    \n    {\\displaystyle r=1}\n  （伯努利分布作为特征），因此模型的参数个数为\n  \n    \n      \n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2n+1}\n  ，其中\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  是二值分类特征的个数。\n\n\n=== 从概率模型中构造分类器 ===\n讨论至此为止我们导出了独立分布特征模型，也就是單純貝氏概率模型。單純貝氏分类器包括了这种模型和相应的决策规则。一个普通的规则就是选出最有可能的那个：这就是大家熟知的最大后验概率（MAP）决策准则。相应的分类器便是如下定义的\n  \n    \n      \n        \n          c\n          l\n          a\n          s\n          s\n          i\n          f\n          y\n        \n      \n    \n    {\\displaystyle \\mathrm {classify} }\n  公式：\n\n  \n    \n      \n        \n          c\n          l\n          a\n          s\n          s\n          i\n          f\n          y\n        \n        (\n        \n          f\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          f\n          \n            n\n          \n        \n        )\n        =\n        \n          \n            argmax\n            c\n          \n        \n         \n        p\n        (\n        C\n        =\n        c\n        )\n        \n          \n            ∏\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          p\n          (\n          \n            F\n            \n              i\n            \n          \n          =\n          \n            f\n            \n              i\n            \n          \n          |\n          C\n          =\n          c\n          )\n          .\n        \n      \n    \n    {\\displaystyle \\mathrm {classify} (f_{1},\\dots ,f_{n})={\\underset {c}{\\operatorname {argmax} }}\\ p(C=c)\\displaystyle \\prod _{i=1}^{n}p(F_{i}=f_{i}\\vert C=c).}\n  \n\n\n== 参数估计 ==\n所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计。类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。\n\n\n=== 高斯單純貝氏 ===\n如果要处理的是连续数据一种通常的假设是这些连续数值为高斯分布。\n例如，假设训练集中有一个连续属性，\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  。我们首先对数据根据类别分类，然后计算每个类别中\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  的均值和方差。令\n  \n    \n      \n        \n          μ\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle \\mu _{c}}\n   表示为\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  在c类上的均值，令\n  \n    \n      \n        \n          σ\n          \n            c\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{c}^{2}}\n  为 \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  在c类上的方差。在给定类中某个值的概率，\n  \n    \n      \n        P\n        (\n        x\n        =\n        v\n        \n          |\n        \n        c\n        )\n      \n    \n    {\\displaystyle P(x=v|c)}\n  ，可以通过将\n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  表示为均值为\n  \n    \n      \n        \n          μ\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle \\mu _{c}}\n  方差为\n  \n    \n      \n        \n          σ\n          \n            c\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{c}^{2}}\n  正态分布计算出来。如下，\n\n  \n    \n      \n        P\n        (\n        x\n        =\n        v\n        \n          |\n        \n        c\n        )\n        =\n        \n          \n            \n              1\n              \n                2\n                π\n                \n                  σ\n                  \n                    c\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  (\n                  v\n                  −\n                  \n                    μ\n                    \n                      c\n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      c\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(x=v|c)={\\tfrac {1}{\\sqrt {2\\pi \\sigma _{c}^{2}}}}\\,e^{-{\\frac {(v-\\mu _{c})^{2}}{2\\sigma _{c}^{2}}}}}\n  \n处理连续数值问题的另一种常用的技术是通过离散化连续数值的方法。通常，当训练样本数量较少或者是精确的分布已知时，通过概率分布的方法是一种更好的选择。在大量样本的情形下离散化的方法表现更优，因为大量的样本可以学习到数据的分布。由于單純貝氏是一种典型的用到大量样本的方法（越大计算量的模型可以产生越高的分类精确度），所以單純貝氏方法都用到离散化方法，而不是概率分布估计的方法。\n\n\n== 样本修正 ==\n如果一个给定的类和特征值在训练集中没有一起出现过，那么基于频率的估计下该概率将为0。这将是一个问题。因为与其他概率相乘时将会把其他概率的信息统统去除。所以常常要求要对每个小类样本的概率估计进行修正，以保证不会出现有为0的概率出现。\n\n\n== 讨论 ==\n尽管实际上独立假设常常是不准确的，但單純貝氏分类器的若干特性让其在实践中能够取得令人惊奇的效果。特别地，各类条件特征之间的解耦意味着每个特征的分布都可以独立地被当做一维分布来估计。这样减轻了由于维数灾带来的阻碍,当样本的特征个数增加时就不需要使样本规模呈指数增长。然而單純貝氏在大多数情况下不能对类概率做出非常准确的估计，但在许多应用中这一点并不要求。例如，單純貝氏分类器中，依据最大后验概率决策规则只要正确类的后验概率比其他类要高就可以得到正确的分类。所以不管概率估计轻度的甚至是严重的不精确都不影响正确的分类结果。在这种方式下，分类器可以有足够的鲁棒性去忽略單純貝氏概率模型上存在的缺陷。\n\n\n== 实例 ==\n\n\n=== 性别分类 ===\n问题描述:通过一些测量的特征，包括身高、体重、脚的尺寸，判定一个人是男性还是女性。\n\n\n==== 训练 ====\n训练数据如下：\n\n假设训练集样本的特征满足高斯分布，得到下表：\n\n我们认为两种类别是等概率的，也就是P(male)= P(female) = 0.5。在没有做辨识的情况下就做这样的假设并不是一个好的点子。但我们通过数据集中两类样本出现的频率来确定P(C)，我们得到的结果也是一样的。\n\n\n==== 测试 ====\n以下给出一个待分类是男性还是女性的样本。\n\n我们希望得到的是男性还是女性哪类的后验概率大。男性的后验概率通过下面式子来求取\n\n  \n    \n      \n        p\n        o\n        s\n        t\n        e\n        r\n        i\n        o\n        r\n        (\n        m\n        a\n        l\n        e\n        )\n        =\n        \n          \n            \n              P\n              (\n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              h\n              e\n              i\n              g\n              h\n              t\n              \n                |\n              \n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              w\n              e\n              i\n              g\n              h\n              t\n              \n                |\n              \n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              f\n              o\n              o\n              t\n              s\n              i\n              z\n              e\n              \n                |\n              \n              m\n              a\n              l\n              e\n              )\n            \n            \n              e\n              v\n              i\n              d\n              e\n              n\n              c\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle posterior(male)={\\frac {P(male)\\,p(height|male)\\,p(weight|male)\\,p(footsize|male)}{evidence}}}\n  女性的后验概率通过下面式子来求取\n\n  \n    \n      \n        p\n        o\n        s\n        t\n        e\n        r\n        i\n        o\n        r\n        (\n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        \n          \n            \n              P\n              (\n              f\n              e\n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              h\n              e\n              i\n              g\n              h\n              t\n              \n                |\n              \n              f\n              e\n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              w\n              e\n              i\n              g\n              h\n              t\n              \n                |\n              \n              f\n              e\n              m\n              a\n              l\n              e\n              )\n              \n              p\n              (\n              f\n              o\n              o\n              t\n              s\n              i\n              z\n              e\n              \n                |\n              \n              f\n              e\n              m\n              a\n              l\n              e\n              )\n            \n            \n              e\n              v\n              i\n              d\n              e\n              n\n              c\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle posterior(female)={\\frac {P(female)\\,p(height|female)\\,p(weight|female)\\,p(footsize|female)}{evidence}}}\n  证据因子（通常是常数）用来对各类的后验概率之和进行归一化.\n\n  \n    \n      \n        e\n        v\n        i\n        d\n        e\n        n\n        c\n        e\n        =\n        P\n        (\n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        h\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        w\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        f\n        o\n        o\n        t\n        s\n        i\n        z\n        e\n        \n          |\n        \n        m\n        a\n        l\n        e\n        )\n        +\n        P\n        (\n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        h\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        w\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        \n        p\n        (\n        f\n        o\n        o\n        t\n        s\n        i\n        z\n        e\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n      \n    \n    {\\displaystyle evidence=P(male)\\,p(height|male)\\,p(weight|male)\\,p(footsize|male)+P(female)\\,p(height|female)\\,p(weight|female)\\,p(footsize|female)}\n  证据因子是一个常数（在正态分布中通常是正数），所以可以忽略。接下来我们来判定这样样本的性别。\n\n  \n    \n      \n        P\n        (\n        m\n        a\n        l\n        e\n        )\n        =\n        0.5\n      \n    \n    {\\displaystyle P(male)=0.5}\n  \n  \n    \n      \n        p\n        (\n        \n          \n            height\n          \n        \n        \n          |\n        \n        \n          \n            male\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              2\n              π\n              \n                σ\n                \n                  2\n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              \n                −\n                (\n                6\n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                2\n                \n                  σ\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        ≈\n        1.5789\n      \n    \n    {\\displaystyle p({\\mbox{height}}|{\\mbox{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(6-\\mu )^{2}}{2\\sigma ^{2}}}\\right)\\approx 1.5789}\n  ,其中\n  \n    \n      \n        μ\n        =\n        5.855\n      \n    \n    {\\displaystyle \\mu =5.855}\n  ，\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n        =\n        3.5033\n        \n          e\n          \n            −\n            02\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}=3.5033e^{-02}}\n  是训练集样本的正态分布参数. 注意，这里的值大于1也是允许的 – 这里是概率密度而不是概率，因为身高是一个连续的变量.\n\n  \n    \n      \n        p\n        (\n        w\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        m\n        a\n        l\n        e\n        )\n        =\n        5.9881\n        \n          e\n          \n            −\n            06\n          \n        \n      \n    \n    {\\displaystyle p(weight|male)=5.9881e^{-06}}\n  \n  \n    \n      \n        p\n        (\n        f\n        o\n        o\n        t\n        s\n        i\n        z\n        e\n        \n          |\n        \n        m\n        a\n        l\n        e\n        )\n        =\n        1.3112\n        \n          e\n          \n            −\n            3\n          \n        \n      \n    \n    {\\displaystyle p(footsize|male)=1.3112e^{-3}}\n  \n  \n    \n      \n        p\n        o\n        s\n        t\n        e\n        r\n        i\n        o\n        r\n        n\n        u\n        m\n        e\n        r\n        a\n        t\n        o\n        r\n        (\n        m\n        a\n        l\n        e\n        )\n        =\n        6.1984\n        \n          e\n          \n            −\n            09\n          \n        \n      \n    \n    {\\displaystyle posteriornumerator(male)=6.1984e^{-09}}\n  \n  \n    \n      \n        P\n        (\n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        0.5\n      \n    \n    {\\displaystyle P(female)=0.5}\n  \n  \n    \n      \n        p\n        (\n        h\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        2.2346\n        \n          e\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle p(height|female)=2.2346e^{-1}}\n  \n  \n    \n      \n        p\n        (\n        w\n        e\n        i\n        g\n        h\n        t\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        1.6789\n        \n          e\n          \n            −\n            2\n          \n        \n      \n    \n    {\\displaystyle p(weight|female)=1.6789e^{-2}}\n  \n  \n    \n      \n        p\n        (\n        f\n        o\n        o\n        t\n        s\n        i\n        z\n        e\n        \n          |\n        \n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        2.8669\n        \n          e\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle p(footsize|female)=2.8669e^{-1}}\n  \n  \n    \n      \n        p\n        o\n        s\n        t\n        e\n        r\n        i\n        o\n        r\n        n\n        u\n        m\n        e\n        r\n        a\n        t\n        o\n        r\n        (\n        f\n        e\n        m\n        a\n        l\n        e\n        )\n        =\n        5.3778\n        \n          e\n          \n            −\n            04\n          \n        \n      \n    \n    {\\displaystyle posteriornumerator(female)=5.3778e^{-04}}\n  由于女性后验概率的分子比较大，所以我们预计这个样本是女性。\n\n\n=== 文本分类 ===\n这是一个用單純貝氏分类做的一个文本分类问题的例子。考虑一个基于内容的文本分类问题，例如判断邮件是否为垃圾邮件。想像文本可以分成若干的类别，首先文本可以被一些单词集标注，而这个单词集是独立分布的，在给定的C类文本中第i个单词出现的概率可以表示为：\n\n  \n    \n      \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        C\n        )\n        \n      \n    \n    {\\displaystyle p(w_{i}\\vert C)\\,}\n  （通过这种处理，我们进一步简化了工作，假设每个单词是在文中是随机分布的-也就是单词不依赖于文本的长度，与其他词出现在文中的位置，或者其他文本内容。）\n所以，对于一个给定类别C，文本D包含所有单词\n  \n    \n      \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i}}\n  的概率是:\n\n  \n    \n      \n        p\n        (\n        D\n        |\n        C\n        )\n        =\n        \n          ∏\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        C\n        )\n        \n      \n    \n    {\\displaystyle p(D\\vert C)=\\prod _{i}p(w_{i}\\vert C)\\,}\n  我们要回答的问题是「文档D属于类C的概率是多少？」换而言之\n  \n    \n      \n        p\n        (\n        C\n        |\n        D\n        )\n        \n      \n    \n    {\\displaystyle p(C\\vert D)\\,}\n  是多少？\n现在定义\n\n  \n    \n      \n        p\n        (\n        D\n        |\n        C\n        )\n        =\n        \n          \n            \n              p\n              (\n              D\n              ∩\n              C\n              )\n            \n            \n              p\n              (\n              C\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(D\\vert C)={p(D\\cap C) \\over p(C)}}\n  \n  \n    \n      \n        p\n        (\n        C\n        |\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              D\n              ∩\n              C\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(C\\vert D)={p(D\\cap C) \\over p(D)}}\n  通过贝叶斯定理将上述概率处理成似然度的形式\n\n  \n    \n      \n        p\n        (\n        C\n        |\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              C\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n        \n        p\n        (\n        D\n        |\n        C\n        )\n      \n    \n    {\\displaystyle p(C\\vert D)={p(C) \\over p(D)}\\,p(D\\vert C)}\n  假设现在只有两个相互独立的类别，S和¬S（垃圾邮件和非垃圾邮件），这里每个元素（邮件）要么是垃圾邮件，要么就不是。\n  \n    \n      \n        p\n        (\n        D\n        |\n        S\n        )\n        =\n        \n          ∏\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        S\n        )\n        \n      \n    \n    {\\displaystyle p(D\\vert S)=\\prod _{i}p(w_{i}\\vert S)\\,}\n  \n  \n    \n      \n        p\n        (\n        D\n        |\n        ¬\n        S\n        )\n        =\n        \n          ∏\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        ¬\n        S\n        )\n        \n      \n    \n    {\\displaystyle p(D\\vert \\neg S)=\\prod _{i}p(w_{i}\\vert \\neg S)\\,}\n  \n用上述贝叶斯的结果，可以写成\n\n  \n    \n      \n        p\n        (\n        S\n        |\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n        \n        \n          ∏\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        S\n        )\n      \n    \n    {\\displaystyle p(S\\vert D)={p(S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\vert S)}\n  \n  \n    \n      \n        p\n        (\n        ¬\n        S\n        |\n        D\n        )\n        =\n        \n          \n            \n              p\n              (\n              ¬\n              S\n              )\n            \n            \n              p\n              (\n              D\n              )\n            \n          \n        \n        \n        \n          ∏\n          \n            i\n          \n        \n        p\n        (\n        \n          w\n          \n            i\n          \n        \n        |\n        ¬\n        S\n        )\n      \n    \n    {\\displaystyle p(\\neg S\\vert D)={p(\\neg S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\vert \\neg S)}\n  两者相除:\n\n  \n    \n      \n        \n          \n            \n              p\n              (\n              S\n              |\n              D\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              |\n              D\n              )\n            \n          \n        \n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n              \n              \n                ∏\n                \n                  i\n                \n              \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              S\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              )\n              \n              \n                ∏\n                \n                  i\n                \n              \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              ¬\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {p(S\\vert D) \\over p(\\neg S\\vert D)}={p(S)\\,\\prod _{i}p(w_{i}\\vert S) \\over p(\\neg S)\\,\\prod _{i}p(w_{i}\\vert \\neg S)}}\n  整理得:\n\n  \n    \n      \n        \n          \n            \n              p\n              (\n              S\n              |\n              D\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              |\n              D\n              )\n            \n          \n        \n        =\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              )\n            \n          \n        \n        \n        \n          ∏\n          \n            i\n          \n        \n        \n          \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              S\n              )\n            \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              ¬\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {p(S\\vert D) \\over p(\\neg S\\vert D)}={p(S) \\over p(\\neg S)}\\,\\prod _{i}{p(w_{i}\\vert S) \\over p(w_{i}\\vert \\neg S)}}\n  这样概率比p(S | D) / p(¬S | D)可以表达为似然比。实际的概率p(S | D)可以很容易通过log (p(S | D) / p(¬S | D))计算出来，基于p(S | D) + p(¬S | D) = 1。\n结合上面所讨论的概率比，可以得到：\n\n  \n    \n      \n        ln\n        ⁡\n        \n          \n            \n              p\n              (\n              S\n              |\n              D\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              |\n              D\n              )\n            \n          \n        \n        =\n        ln\n        ⁡\n        \n          \n            \n              p\n              (\n              S\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              )\n            \n          \n        \n        +\n        \n          ∑\n          \n            i\n          \n        \n        ln\n        ⁡\n        \n          \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              S\n              )\n            \n            \n              p\n              (\n              \n                w\n                \n                  i\n                \n              \n              |\n              ¬\n              S\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\ln {p(S\\vert D) \\over p(\\neg S\\vert D)}=\\ln {p(S) \\over p(\\neg S)}+\\sum _{i}\\ln {p(w_{i}\\vert S) \\over p(w_{i}\\vert \\neg S)}}\n  (这种对数似然比的技术在统计中是一种常用的技术。在这种两个独立的分类情况下（如这个垃圾邮件的例子），把对数似然比转化为S曲线的形式)。\n最后文本可以分类，当\n  \n    \n      \n        p\n        (\n        S\n        |\n        D\n        )\n        >\n        p\n        (\n        ¬\n        S\n        |\n        D\n        )\n      \n    \n    {\\displaystyle p(S\\vert D)>p(\\neg S\\vert D)}\n  或者\n  \n    \n      \n        ln\n        ⁡\n        \n          \n            \n              p\n              (\n              S\n              |\n              D\n              )\n            \n            \n              p\n              (\n              ¬\n              S\n              |\n              D\n              )\n            \n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle \\ln {p(S\\vert D) \\over p(\\neg S\\vert D)}>0}\n  时判定为垃圾邮件，否则为正常邮件。\n\n\n== 参见 ==\n\n\n== 参考文献 ==\n\n\n== 延伸阅读 ==\nDomingos, Pedro; Pazzani, Michael. On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning. 1997, 29: 103–137  [2012-04-01]. （原始内容存档于2008-04-18）. \nWebb, G. I.; Boughton, J.; Wang, Z. Not So Naive Bayes: Aggregating One-Dependence Estimators. Machine Learning (Springer). 2005, 58 (1): 5–24. doi:10.1007/s10994-005-4258-6. \nMozina, M.; Demsar, J.; Kattan, M.; Zupan, B. Nomograms for Visualization of Naive Bayesian Classifier (PDF). Proc. PKDD-2004: 337–348. 2004. \nMaron, M. E. Automatic Indexing: An Experimental Inquiry. JACM. 1961, 8 (3): 404–417. doi:10.1145/321075.321084. \nMinsky, M. Steps toward Artificial Intelligence. Proc. IRE 49 (1): 8–30. 1961. \n\n\n== 外部链接 ==\nBook Chapter: Naive Bayes text classification, Introduction to Information Retrieval （页面存档备份，存于互联网档案馆）\nNaive Bayes for Text Classification with Unbalanced Classes （页面存档备份，存于互联网档案馆）\nBenchmark results of Naive Bayes implementations （页面存档备份，存于互联网档案馆）\nHierarchical Naive Bayes Classifiers for uncertain data （页面存档备份，存于互联网档案馆） (an extension of the Naive Bayes classifier).软件Naive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout, Mallet （页面存档备份，存于互联网档案馆）, NLTK, Orange, scikit-learn and Weka.\nIMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier.\nWinnow content recommendation Open source Naive Bayes text classifier works with very small training and unbalanced training sets. High performance, C, any Unix.\nAn interactive Microsoft Excel spreadsheet Naive Bayes implementation using VBA (requires enabled macros) with viewable source code.\njBNC - Bayesian Network Classifier Toolbox （页面存档备份，存于互联网档案馆）\nStatistical Pattern Recognition Toolbox for Matlab （页面存档备份，存于互联网档案馆）.\nifile （页面存档备份，存于互联网档案馆） - the first freely available (Naive) Bayesian mail/spam filter\nNClassifier （页面存档备份，存于互联网档案馆） - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J.\nClassifier4J （页面存档备份，存于互联网档案馆） - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier.", "前馈神经网络": "前馈神经网络（英文：Feedforward Neural Network），為人工智能領域中，最早发明的簡單人工神经网络类型。在它内部，参数从输入层向输出层单向传播。有異於循环神经网络，它的内部不会构成有向环。\n\n\n== 单层感知机 ==\n\n\n== 多层感知机 ==\n\n\n== 参考 ==", "四分位距": "四分位距（英語：interquartile range, IQR）。是描述統計學中的一种方法，以确定第三四分位数和第一四分位数的分别（即\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        ,\n         \n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{1},\\ Q_{3}}\n  的差距）。與變異數、標準差一樣，表示統計資料中各變量分散情形，但四分差更多为一种稳健统计（robust statistic）。\n四分位差（英語：Quartile Deviation, QD），是\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        ,\n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{1},Q_{3}}\n  的值差的一半，即\n  \n    \n      \n        \n          Q\n          D\n        \n        =\n        \n          \n            \n              \n                Q\n                \n                  3\n                \n              \n              −\n              \n                Q\n                \n                  1\n                \n              \n            \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {QD} ={\\frac {Q_{3}-Q_{1}}{2}}}\n  。\n\n\n== 定义 ==\n四分位距通常是用来构建箱形图，以及对概率分布的简要图表概述。对一个对称性分布数据（其中位数必然等于第三四分位数与第一四分位数的算术平均数），二分之一的四分差等于绝对中位差（MAD）。中位数是聚中趋势的反映。\n\n  \n    \n      \n        \n          I\n          Q\n          R\n        \n        =\n        \n          Q\n          \n            3\n          \n        \n        −\n        \n          Q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {IQR} =Q_{3}-Q_{1}}\n  \n\n\n== 举例 ==\n\n\n=== 图表中的数据 ===\n从这个图示中，我们可以算出四分差的距离为\n  \n    \n      \n        115\n        −\n        105\n        =\n        10\n      \n    \n    {\\displaystyle 115-105=10}\n  。\n\n\n=== 箱形图中的数据 ===\n                            +-----+-+    \n  o           *     |-------|     | |---|\n                            +-----+-+    \n                                         \n+---+---+---+---+---+---+---+---+---+---+---+---+   数列\n0   1   2   3   4   5   6   7   8   9  10  11  12\n\n从该图中我们可算出:\n\n第一四分位数\n  \n    \n      \n        (\n        \n          Q\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            0.25\n          \n        \n        )\n        =\n        7\n      \n    \n    {\\displaystyle (Q_{1},x_{0.25})=7}\n  \n中位数（第二四分位数）\n  \n    \n      \n        (\n        \n          M\n          e\n          d\n        \n        ,\n        \n          x\n          \n            0.5\n          \n        \n        )\n        =\n        8.5\n      \n    \n    {\\displaystyle (\\mathrm {Med} ,x_{0.5})=8.5}\n  \n第三四分位数\n  \n    \n      \n        (\n        \n          Q\n          \n            3\n          \n        \n        ,\n        \n          x\n          \n            0.75\n          \n        \n        )\n        =\n        9\n      \n    \n    {\\displaystyle (Q_{3},x_{0.75})=9}\n  \n四分位距\n  \n    \n      \n        \n          I\n          Q\n          R\n        \n        =\n        \n          Q\n          \n            3\n          \n        \n        −\n        \n          Q\n          \n            1\n          \n        \n        =\n        2\n      \n    \n    {\\displaystyle \\mathrm {IQR} =Q_{3}-Q_{1}=2}\n  \n四分位差\n  \n    \n      \n        \n          Q\n          D\n        \n        =\n        \n          \n            \n              \n                Q\n                \n                  3\n                \n              \n              −\n              \n                Q\n                \n                  1\n                \n              \n            \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\mathrm {QD} ={\\frac {Q_{3}-Q_{1}}{2}}=1}\n  \n\n\n== 相关条目 ==\n四分位数\n百分位数\n\n\n== 參考文獻 ==\n\n\n== 外部連結 ==\nInterquartile Range（页面存档备份，存于互联网档案馆）\nQuartileDeviation（页面存档备份，存于互联网档案馆）", "聚类分析": "聚类分析（英語：Cluster analysis）亦称为集群分析，是对于统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。\n一般把数据聚类归纳为一种非監督式學習。\n\n\n== 定义 ==\n“聚类(clustering)”的概念不能精确定义，这也是为什么聚类算法众多的原因之一。 聚类问题的共同点就是有一组数据对象。 然而，不同的研究人员采用不同的聚类模型，并且对于这些聚类模型中的每一个，可以再给出不同的算法。 而且不同算法发现的“类（簇）”在其属性上往往会有很大差异。 理解这些“聚类模型”是理解各种算法之间差异的关键。 典型的聚类模型包括以下几种：\n\n连通性模型：例如，层次聚类基于距离连通性构建模型。质心模型：例如，k-means 算法用单个均值向量表示每个类。分布模型：聚类过程使用统计分布建模，例如期望最大化算法使用的多元正态分布。密度模型：例如，DBSCAN 和 OPTICS 将聚类定义为数据空间中相连接的密集区域。子空间模型：在双聚类（也称为共聚类或双模式聚类）中，聚类是用聚类成员和相关属性建模的。组模型：这些算法不为其结果提供改进的模型，只提供分组信息。基于图的模型：团(clique)，即图中节点的子集，子集中的每两个节点都由一条边连接，可以被认为是聚类的原型形式。 与 HCS 聚类算法一样，放宽完全连通性的要求（一部分边可能会丢失）被称为”准团”。符号图模型：符号图（signed graph）中的每条路径都有一个符号，该符号来自边上符号的乘积。 在平衡理论的假设下，边可能会改变符号并导致分叉图。 较弱的“聚类性公理”（没有循环恰好有一个负边）产生的结果是两个以上的聚类，或只有正边的子图。神经模型：最著名的无监督神经网络是自组织映射，这些模型的特征通常与上述一个或多个模型相似，并且在神经网络实现主成分分析或独立分析形式时包括子空间模型 成分分析\n“聚类”的本质上是找到一组类, 这些类通常包含数据集中的所有对象。 此外，它可以指定各个类彼此之间的关系，例如，类相互嵌入的层次结构。 聚类可以大致区分为：\n\n硬聚类：每个对象是否属于一个类软聚类（又作：模糊聚类）：每个对象在一定程度上属于每个聚类（例如，属于该聚类的可能性）也可能有更细微的区别，例如：\n严格分区聚类：每个对象恰好属于一个类\n\n有异常值的严格分区聚类（Strict partitioning clustering with outliers）：对象也可以不属于任何簇，被认为是异常值重叠聚类（又作：替代聚类、多视图聚类）：对象可能属于多个聚类； 通常涉及硬簇层次聚类：属于子聚类的对象也属于父聚类子空间聚类：虽然是重叠聚类，但在唯一定义的子空间内，聚类不应重叠\n\n\n== 算法 ==\n没有客观上“正确”的聚类算法，但正如人们所指出的，“聚类在观察者的眼中”。除非你有喜欢一个聚类模型而不是另一个的数学原因，通常需要通过实验选择最适合特定问题的聚类算法。为一种模型设计的算法通常会在包含完全不同类型模型的数据集上失败。 例如，k-means 无法找到非凸簇。\n\n\n=== 连通性聚类 ===\n基于连通性的聚类，也称为层次聚类，其核心思想是对象与附近对象的相关性高于与较远对象的相关性。 这些算法根据它们的距离将“对象”连接起来形成“簇”。 集群可以主要通过连接集群各部分所需的最大距离来描述。 在不同的距离，会形成不同的聚类，这可以用树状图表示，这解释了通用名称“层次聚类”的来源：这些算法不提供数据集的单一分区，而是提供广泛的层次结构 在一定距离处相互合并的簇。 在树状图中，y 轴标记集群合并的距离，而对象沿 x 轴放置，这样集群就不会混合。\n基于连通性的聚类是一整套方法，它们因计算距离的方式不同而不同。 除了通常选择的距离函数外，用户还需要决定要使用的连接标准（因为一个类/簇由多个对象组成，所以有多个候选来计算距离）。 流行的选择被称为单链接聚类（对象距离的最小值）、完全链接聚类（对象距离的最大值）和 UPGMA 或 WPGMA（“具有算术平均值的未加权或加权对组方法”，也称为平均链接 聚类）。 此外，层次聚类可以是凝聚的（从单个元素开始并将它们聚合成簇）或分裂的（从完整的数据集开始并将其分成多个分区）。\n这些方法不会产生数据集的唯一分区，而是产生一个层次结构，用户仍然需要从中选择合适的集群。 它们对异常值不是很稳健，异常值要么显示为额外的集群，要么甚至导致其他集群合并（称为“链接现象”，特别是单链接集群）。 在一般情况下，复杂度是 O(n)3) 对于凝聚聚类和 O(2n-1)  用于分裂聚类，[7] 这使得它们对于大型数据集来说太慢了。 对于某些特殊情况，已知最优有效方法（复杂度 O(n)2) ：SLINK 用于单链接和 CLINK  用于完全链接聚类。\n\n\n=== 质心聚类 ===\n在基于质心的聚类算法中，每个聚类由一个中心向量表示，它不一定是数据集的成员。 当簇数固定为 k 时，k-means 聚类给出了优化问题的形式化定义：找到 k 个簇中心并将对象分配到最近的簇中心，使得与簇的平方距离最小。\n已知优化问题本身是 NP问题（NP困 难），因此常用的方法是只搜索近似解。 一个特别著名的近似方法是 Lloyd 算法， 通常简称为“k-means 算法”（尽管另一个算法引入了这个名称）。 然而，它确实只能找到局部最优值，并且通常会使用不同的随机初始化运行多次。 k-means 的变体通常包括这样的优化，例如选择多次运行中的最佳者，但也将质心限制为数据集的成员（k-medoids），选择中位数（k-medians 聚类），不太随机地选择初始中心（ k-means++) 或允许模糊聚类分配 (fuzzy c-means)。\n大多数 k-means 类型的算法都需要预先指定聚类的数量 - k，这被认为是这些算法的最大缺点之一。 此外，算法更喜欢大小大致相似的簇，因为它们总是会将对象分配给最近的质心。 这通常会导致错误地切割集群边界（这并不奇怪，因为该算法优化的是集群中心，而不是集群边界）。\nK-means 有许多有趣的理论特性。 首先，它将数据空间划分为一种称为 Voronoi 图的结构。 其次，它在概念上接近最近邻分类，因此在机器学习中很受欢迎。 第三，它可以看作是基于模型的聚类的变体，Lloyd 算法可以看作是下面讨论的该模型的期望最大化算法的变体。\n\n基于质心的聚类问题（如 k-means 和 k-medoids）是无容量、度量设施位置问题的特例，是运筹学和计算几何社区中的典型问题。 在一个基本的设施位置问题（其中有许多变体可以模拟更复杂的设置）中，任务是找到最佳仓库位置以最佳地服务一组给定的消费者。 人们可以将“仓库”视为聚类质心，将“消费者位置”视为要聚类的数据。 这使得将设施位置文献中完善的算法解决方案应用于目前考虑的基于质心的聚类问题成为可能。\n\n\n=== 分布模型聚类 ===\n与统计学关系最密切的聚类模型是基于分布模型的。 然后可以很容易地将聚类定义为最有可能属于同一分布的对象。 这种方法的一个方便的特性是它非常类似于生成人工数据集的方式：通过从分布中抽取随机对象。\n尽管这些方法的理论基础非常出色，但它们都存在一个称为过度拟合的关键问题，除非对模型的复杂性施加约束。 更复杂的模型通常能够更好地解释数据，这使得选择合适的模型复杂性本身就很困难。\n一种著名的方法被称为高斯混合模型（使用期望最大化算法）。 在这里，数据集通常使用固定数量（以避免过度拟合）的高斯分布建模，这些高斯分布随机初始化并且其参数经过迭代优化以更好地拟合数据集。 这将收敛到局部最优，因此多次运行可能会产生不同的结果。 为了获得硬聚类，通常会将对象分配给它们最有可能属于的高斯分布； 对于软聚类，这不是必需的。\n基于分布的聚类为聚类生成复杂模型，可以捕获属性之间的相关性和依赖性。 然而，这些算法给用户带来了额外的负担：对于许多真实数据集，可能没有简明定义的数学模型（例如，假设高斯分布是对数据的相当强的假设）。\n\n\n=== 密度聚类 ===\n在基于密度的聚类中，\n聚类被定义为密度高于数据集其余部分的区域。 稀疏区域中的对象——需要分离集群——通常被认为是噪声和边界点。\n最流行的基于密度的聚类方法是 DBSCAN,。\n与许多较新的方法相比，它具有一个定义明确的集群模型，称为“密度可达性”。 类似于基于链接的聚类，它基于一定距离阈值内的连接点。 然而，它只连接满足密度标准的点，在原始变体中定义为该半径内其他对象的最小数量。 簇由所有密度连接的对象（与许多其他方法相反，它可以形成任意形状的簇）加上这些对象范围内的所有对象组成。 DBSCAN 的另一个有趣的特性是它的复杂性相当低——它需要对数据库进行线性数量的范围查询——并且它会发现基本相同的结果（它对核心点和噪声点是确定性的，但对边界点不是） 在每次运行中，因此无需多次运行。 OPTICS是 DBSCAN 的推广，无需为范围参数  ε 选择合适的值，并产生与连锁聚类相关的分层结果。 DeLi-Clu， Density-Link-Clustering 结合了单链接聚类和 OPTICS 的思想，完全消除了 ε  参数，并通过使用 R 树索引提供了优于 OPTICS 的性能改进。\nDBSCAN 和 OPTICS 的主要缺点是它们期望某种密度下降来检测簇边界。 例如，在具有重叠高斯分布（人工数据中的常见用例）的数据集上，这些算法生成的聚类边界通常看起来很随意，因为聚类密度不断降低。 在由高斯混合组成的数据集上，这些算法几乎总是优于能够精确建模此类数据的 EM 聚类等方法。\n均值漂移(mean-shift)是一种聚类方法，其中基于核密度估计将每个对象移动到其附近最密集的区域。 最终，对象会聚到密度的局部最大值。 与 k-means 聚类类似，这些“密度吸引子”可以作为数据集的代表，但 mean-shift 可以检测类似于 DBSCAN 的任意形状的聚类。 由于昂贵的迭代过程和密度估计，均值漂移通常比 DBSCAN 或 k-Means 慢。 除此之外，均值漂移算法对多维数据的适用性受到核密度估计的不平滑行为的阻碍，这会导致聚类尾部过度碎片化。\n\n\n=== 网格聚类 ===\n基于网格的技术用于多维数据集。 在此技术中，我们创建一个网格结构，并在网格（也称为单元格）上执行比较。 基于网格的技术速度快，计算复杂度低。 有两种类型的基于网格的聚类方法：STING 和 CLIQUE。\n\n\n== 聚类类型 ==\n数据聚类算法可以分为结构性或者分散性。结构性算法利用以前成功使用过的聚类器进行分类，而分散型算法则是一次确定所有分类。结构性算法可以从上至下或者从下至上双向进行计算。从下至上算法从每个对象作为单独分类开始，不断融合其中相近的对象。而从上至下算法则是把所有对象作为一个整体分类，然后逐渐分小。\n分散式聚类算法，是一次性确定要产生的类别，这种算法也已应用于从下至上聚类算法。\n基于密度的聚类算法，是为了挖掘有任意形状特性的类别而发明的。此算法把一个类别视为数据集中大于某阈值的一个区域。DBSCAN和OPTICS是两个典型的算法。\n许多聚类算法在执行之前，需要指定从输入数据集中产生的分类个数。除非事先准备好一个合适的值，否则必须决定一个大概值，关于这个问题已经有一些现成的技术。\n\n\n== 距离测量 ==\n数据对象间的相似度度量一般是通过数据之间的相互关系来确定。\n在结构性聚类中，关键性的一步就是要选择测量的距离。一个简单的测量就是使用曼哈顿距离，它相当于每个变量的绝对差值之和。该名字的由来起源于在纽约市区测量街道之间的距离就是由人步行的步数来确定的。\n一个更为常见的测量方法是利用欧氏空间距离，可以认为数据分布一个多维空间中，并且计算每个空间中每个点到原点的距离，然后对所有距离进行换算。\n常用的几个距离计算方法：\n\n欧氏距离（2-norm距离）\n曼哈顿距离（Manhattan distance, 1-norm距离）\ninfinity norm\n马氏距离\n余弦相似性\n汉明距离\n\n\n== 结构性聚类 ==\n在已经得到距离值之后，元素间可以被联系起来。通过分离和融合可以构建一个结构。传统上，表示的方法是树形数据结构，\n然后对该结构进行修剪。树的根节点表示一个包含所有项目的类别，树叶表示与个别的项目相关的类别。\n层次聚类算法，要么是自底向上聚集型的，即从叶子节点开始，最终汇聚到根节点；要么是自顶向下分裂型的，即从根节点开始，递归的向下分裂。\n任意非负值的函数都可以用于衡量一对观测值之间的相似度。决定一个类别是否分裂或者合并的是一个连动的标准，它是两两观测值之间距离的函数。\n在一个指定高度上切割此树，可以得到一个相应精度的分类。\n\n\n=== 聚集型层次聚类 ===\n\n它的层次聚类树如下图\n\n\n=== 概念聚类 （页面存档备份，存于互联网档案馆） ===\n\n\n== 分散性聚类 ==\n\n\n=== K-均值法及衍生算法 ===\n\n\n==== K-均值法聚类 ====\nK-均值算法表示以空间中k个点为中心进行聚类，对最靠近他们的对象归类。\n\n例如：数据集合为三维，聚类以两点：X =(x1, x2, x3),Y =(y1, y2, y3)。中心点Z变为Z =(z1, z2, z3)，其中z1 = (x1 + y1)/2，z2 = (x2 + y2)/2，z3 = (x3 + y3)/2。算法归纳为（J. MacQueen, 1967）：\n\n选择聚类的个数k.\n任意产生k个聚类，然后确定聚类中心，或者直接生成k个中心。\n对每个点确定其聚类中心点。\n再计算其聚类新中心。\n重复以上步骤直到满足收敛要求。（通常就是确定的中心点不再改变。）该算法的最大优势在于简洁和快速。劣势在于对于一些结果并不能够满足需要，因为结果往往需要随机点的选择非常巧合。\n\n\n==== QT聚类算法 ====\n\n\n=== 图论方法 ===\n\n\n== 谱聚类 ==\n\n在多元变量统计中，谱聚类（英語：spectral clustering）技术利用数据相似矩阵的谱（特征值），在对数据进行降维后，以较少的维度进行聚类。相似矩阵作为输入，提供了对数据集中每一对点相对相似性的定量评估。\n在图像分割中，谱聚类被称为基于分割的物体分类。\n\n\n== 评估和验证 ==\n聚类结果的评估（或“验证”）与聚类本身一样困难 。 流行的方法包括“内部”评估，其中聚类被总结为一个单一的质量分数，“外部”评估，其中聚类与现有的“基准真相(ground truth)”分类进行比较，由人类专家进行“手动”评估，以及“间接”通过评估聚类在其预期应用中的效用来进行评估。\n内部评估措施存在一个问题，即它们代表的功能本身可以被视为聚类目标。 例如，可以通过 Silhouette 系数对数据集进行聚类； 除了没有已知的有效算法之外。 通过使用这种内部衡量指标进行评估，人们更愿意比较优化问题的相似性, 而不一定是聚类的有用程度 。\n外部评估也有类似的问题：如果我们有这样的“ground truth”标签，那么我们就不需要聚类了； 而在实际应用中我们通常没有这样的标签。 另一方面，标签仅反映了数据集的一种可能分区，这并不意味着不存在不同的甚至更好的聚类。\n因此，这两种方法都不能最终判断聚类的实际质量，但这需要人工评估， 这是非常主观的。 尽管如此，这样的统计数据在识别不良聚类方面可能非常有用， 但不应忽视主观的人为评估。 \n\n\n== 应用 ==\n\n\n=== 生物 ===\n\n\n=== 市场研究 ===\n\n\n=== 其他应用 ===\nAbdi, H. (1994). Additive-tree representations (with an application to face processing) Lecture Notes in Biomathematics, 84, 43-59.. 1990. \nClatworthy, J., Buick, D., Hankins, M., Weinman, J., & Horne, R. (2005). The use and reporting of cluster analysis in health psychology: A review. British Journal of Health Psychology 10: 329-358.\nCole, A. J. & Wishart, D. (1970). An improved algorithm for the Jardine-Sibson method of generating overlapping clusters. The Computer Journal 13(2):156-163.Ester, M., Kriegel, H.P., Sander, J., and Xu, X. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, Oregon, USA: AAAI Press, pp. 226–231.Heyer, L.J., Kruglyak, S. and Yooseph, S., Exploring Expression Data: Identification and Analysis of Coexpressed Genes, Genome Research 9:1106-1115.Huang, Z. (1998). Extensions to the K-means Algorithm for Clustering Large Datasets with ategorical Values. Data Mining and Knowledge Discovery, 2, p. 283-304.\nJardine, N. & Sibson, R. (1968). The construction of hierarchic and non-hierarchic classifications. The Computer Journal 11:177.\nThe on-line textbook: Information Theory, Inference, and Learning Algorithms （页面存档备份，存于互联网档案馆）, by David J.C. MacKay includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm.\nNg, R.T. and Han, J. 1994. Efficient and effective clustering methods for spatial data mining. Proceedings of the 20th VLDB Conference, Santiago, Chile, pp. 144–155.\nPrinzie A., D. Van den Poel (2006), Incorporating sequential information into traditional classification models by using an element/position-sensitive SAM （页面存档备份，存于互联网档案馆）. Decision Support Systems 42 (2): 508-526.\nRomesburg, H. Clarles, Cluster Analysis for Researchers, 2004, 340 pp. ISBN 1-4116-0617-5 or publisher （页面存档备份，存于互联网档案馆）, reprint of 1990 edition published by Krieger Pub. Co... A Japanese language translation is available from Uchida Rokakuho Publishing Co., Ltd., Tokyo, Japan.\nZhang, T., Ramakrishnan, R., and Livny, M. 1996. BIRCH: An efficient data clustering method for very large databases. Proceedings of ACM SIGMOD Conference, Montreal, Canada, pp. 103–114.For spectral clustering :\n\nJianbo Shi and Jitendra Malik, \"Normalized Cuts and Image Segmentation\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888-905, August 2000. Available on Jitendra Malik's homepage （页面存档备份，存于互联网档案馆）\nMarina Meila and Jianbo Shi, \"Learning Segmentation with Random Walk\", Neural Information Processing Systems, NIPS, 2001. Available from Jianbo Shi's homepage （页面存档备份，存于互联网档案馆）For estimating number of clusters:\n\nCan, F., Ozkarahan, E. A. (1990) \"Concepts and effectiveness of the cover coefficient-based clustering methodology for text databases.\" ACM Transactions on Database Systems. 15 (4) 483-517.For discussion of the elbow criterion:\n\nAldenderfer, M.S., Blashfield, R.K, Cluster Analysis, (1984), Newbury Park (CA): Sage.\n\n\n== 外部链接 ==\nP. Berkhin, Survey of Clustering Data Mining Techniques （页面存档备份，存于互联网档案馆）, Accrue Software, 2002.\nJain, Murty and Flynn: Data Clustering: A Review （页面存档备份，存于互联网档案馆）, ACM Comp. Surv., 1999.\nfor another presentation of hierarchical, k-means and fuzzy c-means see this introduction to clustering （页面存档备份，存于互联网档案馆）. Also has an explanation on mixture of Gaussians.\nDavid Dowe, Mixture Modelling page （页面存档备份，存于互联网档案馆） - other clustering and mixture model links.\na tutorial on clustering [1]\nThe on-line textbook: Information Theory, Inference, and Learning Algorithms （页面存档备份，存于互联网档案馆）, by David J.C. MacKay includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm.\n\n\n=== 相关软件 ===\n\n\n==== 免费类 ====\nThe flexclust package for R\nCOMPACT - Comparative Package for Clustering Assessment （页面存档备份，存于互联网档案馆）（in Matlab）\nYALE (Yet Another Learning Environment): freely available open-source software for data pre-processing, knowledge discovery, data mining, machine learning, visualization, etc. also including a plugin for clustering, fully integrating Weka, easily extendible, and featuring a graphical user interface as well as a XML-based scripting language for data mining;\nmixmod （页面存档备份，存于互联网档案馆） : Model Based Cluster And Discriminant Analysis. Code in C++, interface with Matlab and Scilab\nLingPipe Clustering Tutorial （页面存档备份，存于互联网档案馆） Tutorial for doing complete- and single-link clustering using LingPipe, a Java text data mining package distributed with source.\nWeka : Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.\nTanagra （页面存档备份，存于互联网档案馆） : a free data mining software including several clustering algorithms such as K-MEANS, SOM, Clustering Tree, HAC and more.\nCluster : Open source clustering software. The routines are available in the form of a C clustering library, an extension module to Python, a module to Perl.\npython-cluster （页面存档备份，存于互联网档案馆） Pure python implementation\n\n\n==== 商业类 ====\nClustan （页面存档备份，存于互联网档案馆）\nPeltarion Synapse（using self-organizing maps）[2] （页面存档备份，存于互联网档案馆）\n\n\n== 参考文献 ==", "貝氏網路": "貝氏網路（Bayesian network），又稱信念網絡（belief network）或是有向無環圖模型（directed acyclic graphical model），是一種機率圖型模型，藉由有向無環圖（directed acyclic graphs, or DAGs）中得知一組隨機變數\n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              X\n              \n                n\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},...,X_{n}\\right\\}}\n  及其n組條件機率分配（conditional probability distributions, or CPDs）的性質。舉例而言，貝氏網路可用來表示疾病和其相關症狀間的機率關係；倘若已知某種症狀下，貝氏網路就可用來計算各種可能罹患疾病之發生機率。\n一般而言，貝氏網路的有向無環圖中的節點表示隨機變數，它們可以是可觀察到的變量，抑或是潛在變量、未知參數等。連接兩個節點的箭頭代表此兩個隨機變數是具有因果關係或是非條件獨立的；而两个節點間若沒有箭頭相互連接一起的情況就稱其隨機變數彼此間為條件獨立。若兩個節點間以一個單箭頭連接在一起，表示其中一個節點是「因（parents）」，另一個是「果（descendants or children）」，兩節點就會產生一個條件機率值。比方說，我們以\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  表示第i個節點，而\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  的「因」以\n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  表示，\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  的「果」以\n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle C_{i}}\n  表示；圖一就是一種典型的貝氏網路結構圖，依照先前的定義，我們就可以輕易的從圖一可以得知：\n\n  \n    \n      \n        \n          P\n          \n            2\n          \n        \n        =\n        \n          {\n          \n            \n              X\n              \n                4\n              \n            \n            ,\n            \n              X\n              \n                5\n              \n            \n          \n          }\n        \n        ,\n        \n          C\n          \n            2\n          \n        \n        =\n        ∅\n        ,\n        \n          P\n          \n            4\n          \n        \n        =\n        ∅\n        ,\n        \n          C\n          \n            4\n          \n        \n        =\n        \n          {\n          \n            \n              X\n              \n                2\n              \n            \n            ,\n            \n              X\n              \n                5\n              \n            \n          \n          }\n        \n        ,\n        \n          P\n          \n            5\n          \n        \n        =\n        \n          {\n          \n            X\n            \n              4\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle P_{2}=\\left\\{X_{4},X_{5}\\right\\},C_{2}=\\emptyset ,P_{4}=\\emptyset ,C_{4}=\\left\\{X_{2},X_{5}\\right\\},P_{5}=\\left\\{X_{4}\\right\\}}\n  ，以及\n  \n    \n      \n        \n          C\n          \n            5\n          \n        \n        =\n        \n          {\n          \n            X\n            \n              2\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle C_{5}=\\left\\{X_{2}\\right\\}}\n  大部分的情況下，貝氏網路適用在節點的性質是屬於離散型的情況下，且依照\n  \n    \n      \n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        \n          |\n        \n        \n          P\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle P(X_{i}|P_{i})}\n  此條件機率寫出條件機率表（conditional probability table, or CPT），此條件機率表的每一列（row）列出所有可能發生的\n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  ，每一行（column）列出所有可能發生的\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  ，且任一列的機率總和必為1。寫出條件機率表後就很容易將事情給條理化，且輕易地得知此貝氏網路結構圖中各節點間之因果關係；但是條件機率表也有其缺點：若是節點\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  是由很多的「因」所造成的「果」，如此條件機率表就會變得在計算上既複雜又使用不便。下圖為圖一貝氏網路中某部分結構圖之條件機率表。\n\n\n== 數學定義 ==\n令 G = (I,E) 表示一個有向無環圖（DAG），其中 I 代表圖中所有的節點的集合，而 E 代表有向連接線段的集合，且令 X = (Xi)i∈I 為其有向無環圖中的某一節點 i 所代表之隨機變數，若節點 X 的聯合機率分配可以表示成：\n\n  \n    \n      \n        p\n        (\n        x\n        )\n        =\n        \n          ∏\n          \n            i\n            ∈\n            I\n          \n        \n        p\n        \n          \n            (\n          \n        \n        \n          x\n          \n            i\n          \n        \n        \n        \n          \n            |\n          \n        \n        \n        \n          x\n          \n            pa\n            ⁡\n            (\n            i\n            )\n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle p(x)=\\prod _{i\\in I}p{\\big (}x_{i}\\,{\\big |}\\,x_{\\operatorname {pa} (i)}{\\big )}}\n  則稱 X 為相對於一有向無環圖 G 的貝氏網路，其中\n  \n    \n      \n        p\n        a\n        (\n        i\n        )\n      \n    \n    {\\displaystyle pa(i)}\n  表示節點 i 之「因」。\n對任意的隨機變數，其聯合分配可由各自的局部條件機率分配相乘而得出：\n\n  \n    \n      \n        \n          P\n        \n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          P\n        \n        (\n        \n          X\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n          \n        \n        ∣\n        \n          X\n          \n            i\n            +\n            1\n          \n        \n        =\n        \n          x\n          \n            i\n            +\n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathrm {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{i=1}^{n}\\mathrm {P} (X_{i}=x_{i}\\mid X_{i+1}=x_{i+1},\\ldots ,X_{n}=x_{n})}\n  依照上式，我們可以將一貝氏網路的聯合機率分配寫成：\n\n  \n    \n      \n        \n          P\n        \n        (\n        \n          X\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          P\n        \n        (\n        \n          X\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n          \n        \n        ∣\n        \n          X\n          \n            j\n          \n        \n        =\n        \n          x\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathrm {P} (X_{1}=x_{1},\\ldots ,X_{n}=x_{n})=\\prod _{i=1}^{n}\\mathrm {P} (X_{i}=x_{i}\\mid X_{j}=x_{j})}\n  （對每個相對於Xi的「因」變數 Xj 而言)上面兩個表示式之差別在於條件機率的部分，在貝氏網路中，若已知其「因」變數下，某些節點會與其「因」變數條件獨立，只有與「因」變數有關的節點才會有條件機率的存在。\n如果聯合分配的相依數目很稀少時，使用貝氏函數的方法可以節省相當大的記憶體容量。舉例而言，若想將10個變數其值皆為0或1儲存成一條件機率表型式，一個直觀的想法可知我們總共必須要計算\n  \n    \n      \n        \n          2\n          \n            10\n          \n        \n        =\n        1024\n      \n    \n    {\\displaystyle 2^{10}=1024}\n  個值；但若這10個變數中無任何變數之相關「因」變數是超過三個以上的話，則貝氏網路的條件機率表最多只需計算\n  \n    \n      \n        10\n        ∗\n        \n          2\n          \n            3\n          \n        \n        =\n        80\n      \n    \n    {\\displaystyle 10*2^{3}=80}\n  個值即可。另一個貝式網路優點在於：對人類而言，它更能輕易地得知各變數間是否條件獨立或相依與其局部分配（local distribution）的型態來求得所有隨機變數之聯合分配。\n\n\n== 馬可夫毯（Markov blanket） ==\n定義一個節點之馬可夫毯為此節點的因節點、果節點與果節點的因節點所成之集合。一旦給定其馬可夫毯的值後，若網路內之任一節點X皆會與其他的節點條件獨立的話，就稱X為相對於一有向無環圖G的貝氏網路。\n\n\n== 舉例說明 ==\n假設有兩個伺服器\n  \n    \n      \n        (\n        \n          S\n          \n            1\n          \n        \n        ,\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (S_{1},S_{2})}\n  ，會傳送封包到使用者端（以U表示之），但是第二個伺服器的封包傳送成功率會與第一個伺服器傳送成功與否有關，因此此貝氏網路的結構圖可以表示成如圖二的型式。就每個封包傳送而言，只有兩種可能值：T（成功）或 F（失敗）。則此貝氏網路之聯合機率分配可以表示成：\n\n  \n    \n      \n        P\n        (\n        U\n        ,\n        \n          S\n          \n            1\n          \n        \n        ,\n        \n          S\n          \n            2\n          \n        \n        )\n        =\n        P\n        (\n        U\n        \n          |\n        \n        \n          S\n          \n            1\n          \n        \n        ,\n        \n          S\n          \n            2\n          \n        \n        )\n        ×\n        P\n        (\n        \n          S\n          \n            2\n          \n        \n        \n          |\n        \n        \n          S\n          \n            1\n          \n        \n        )\n        ×\n        P\n        (\n        \n          S\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P(U,S_{1},S_{2})=P(U|S_{1},S_{2})\\times P(S_{2}|S_{1})\\times P(S_{1})}\n  \n此模型亦可回答如：「假設已知使用者端成功接受到封包，求第一伺服器成功發送封包的機率?」諸如此類的問題，而此類型問題皆可用條件機率的方法來算出其所求之發生機率：\n\n  \n    \n      \n        \n          P\n        \n        (\n        \n          \n            \n              S\n              \n                1\n              \n            \n          \n        \n        =\n        T\n        ∣\n        \n          \n            U\n          \n        \n        =\n        T\n        )\n        =\n        \n          \n            \n              \n                P\n              \n              (\n              \n                \n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n              \n              =\n              T\n              ,\n              \n                \n                  U\n                \n              \n              =\n              T\n              )\n            \n            \n              \n                P\n              \n              (\n              \n                \n                  U\n                \n              \n              =\n              T\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {P} ({\\mathit {S_{1}}}=T\\mid {\\mathit {U}}=T)={\\frac {\\mathrm {P} ({\\mathit {S_{1}}}=T,{\\mathit {U}}=T)}{\\mathrm {P} ({\\mathit {U}}=T)}}}\n  \n\n  \n    \n      \n        =\n        \n          \n            \n              \n                ∑\n                \n                  \n                    \n                      \n                        S\n                        \n                          2\n                        \n                      \n                    \n                  \n                  ∈\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              \n                P\n              \n              (\n              \n                \n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n              \n              =\n              T\n              ,\n              \n                \n                  \n                    S\n                    \n                      2\n                    \n                  \n                \n              \n              ,\n              \n                \n                  U\n                \n              \n              =\n              T\n              )\n            \n            \n              \n                ∑\n                \n                  \n                    \n                      \n                        S\n                        \n                          1\n                        \n                      \n                    \n                  \n                  ,\n                  \n                    \n                      \n                        S\n                        \n                          2\n                        \n                      \n                    \n                  \n                  ∈\n                  {\n                  T\n                  ,\n                  F\n                  }\n                \n              \n              \n                P\n              \n              (\n              \n                \n                  U\n                \n              \n              =\n              T\n              ,\n              \n                \n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n              \n              ,\n              \n                \n                  \n                    S\n                    \n                      2\n                    \n                  \n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle ={\\frac {\\sum _{{\\mathit {S_{2}}}\\in \\{T,F\\}}\\mathrm {P} ({\\mathit {S_{1}}}=T,{\\mathit {S_{2}}},{\\mathit {U}}=T)}{\\sum _{{\\mathit {S_{1}}},{\\mathit {S_{2}}}\\in \\{T,F\\}}\\mathrm {P} ({\\mathit {U}}=T,{\\mathit {S_{1}}},{\\mathit {S_{2}}})}}}\n  \n\n  \n    \n      \n        =\n        \n          \n            \n              (\n              0.4\n              ×\n              0.3\n              ×\n              1\n              \n                )\n                \n                  U\n                  =\n                  T\n                  ,\n                  \n                    S\n                    \n                      1\n                    \n                  \n                  =\n                  T\n                  ,\n                  \n                    S\n                    \n                      2\n                    \n                  \n                  =\n                  F\n                \n              \n              +\n              (\n              0.4\n              ×\n              0.7\n              ×\n              1\n              \n                )\n                \n                  U\n                  =\n                  T\n                  ,\n                  \n                    S\n                    \n                      1\n                    \n                  \n                  =\n                  T\n                  ,\n                  \n                    S\n                    \n                      2\n                    \n                  \n                  =\n                  T\n                \n              \n            \n            \n              (\n              0.4\n              ×\n              0.3\n              ×\n              1\n              \n                )\n                \n                  T\n                  T\n                  F\n                \n              \n              +\n              (\n              0.4\n              ×\n              0.7\n              ×\n              1\n              \n                )\n                \n                  T\n                  T\n                  T\n                \n              \n              +\n              (\n              0.6\n              ×\n              0.3\n              ×\n              1\n              \n                )\n                \n                  F\n                  T\n                  T\n                \n              \n              +\n              \n                0\n                \n                  T\n                  F\n                  F\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle ={\\frac {(0.4\\times 0.3\\times 1)_{U=T,S_{1}=T,S_{2}=F}+(0.4\\times 0.7\\times 1)_{U=T,S_{1}=T,S_{2}=T}}{(0.4\\times 0.3\\times 1)_{TTF}+(0.4\\times 0.7\\times 1)_{TTT}+(0.6\\times 0.3\\times 1)_{FTT}+0_{TFF}}}}\n  \n\n  \n    \n      \n        ≈\n        68.96\n        %\n      \n    \n    {\\displaystyle \\approx 68.96\\%}\n  。\n\n\n== 求解方法 ==\n以上例子是一個很簡單的貝氏網路模型，但是如果當模型很複雜時，這時使用列舉式的方法來求解機率就會變得非常複雜且難以計算，因此必須使用其他的替代方法。一般來說，貝氏機率有以下幾種求法：\n\n\n=== 精確推論 ===\n列舉推理法（如上述例子）\n變數消元演算法（variable elimination）\n\n\n=== 隨機推論（蒙地卡羅方法） ===\n直接取樣演算法\n拒絕取樣演算法\n概似加權演算法\n馬可夫链蒙地卡羅演算法（Markov chain Monte Carlo algorithm）在此，以馬可夫链蒙地卡羅演算法為例，又馬可夫链蒙地卡羅演算法的類型很多，故在這裡只說明其中一種吉布斯采样的操作步驟：\n首先將已給定數值的變數固定，然後將未給定數值的其他變數隨意給定一個初始值，接著進入以下迭代步驟：\n\n（1）隨意挑選其中一個未給定數值的變數\n（2）從條件分配\n  \n    \n      \n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        \n          |\n        \n        \n          Markovblanket\n        \n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle P(X_{i}|{\\text{Markovblanket}}(X_{i}))}\n  抽樣出新的\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  的值，接著重新計算\n  \n    \n      \n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        \n          |\n        \n        \n          Markovblanket\n        \n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        )\n        =\n        α\n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        \n          |\n        \n        \n          parents\n        \n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        )\n        ×\n        \n          ∏\n          \n            \n              Y\n              \n                i\n              \n            \n            ∈\n            \n              children\n            \n            (\n            \n              X\n              \n                i\n              \n            \n            )\n          \n        \n        \n          parent\n        \n        (\n        \n          Y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle P(X_{i}|{\\text{Markovblanket}}(X_{i}))=\\alpha P(X_{i}|{\\text{parents}}(X_{i}))\\times \\prod _{Y_{i}\\in {\\text{children}}(X_{i})}{\\text{parent}}(Y_{i})}\n  當迭代結束後，刪除前面若干筆尚未穩定的數值，就可以求出的近似條件機率分配。馬可夫链蒙地卡羅演算法的優點是在計算很大的網路時效率很好，但缺點是所抽取出的樣本並不具獨立性。\n當貝氏網路上的結構跟參數皆已知時，我們可以透過以上方法來求得特定情況的機率，不過，如果當網路的結構或參數未知時，我們必須藉由所觀測到的資料去推估網路的結構或參數，一般而言，推估網路的結構會比推估節點上的參數來的困難。依照對貝氏網路結構的了解和觀測值的完整與否，我們可以分成下列四種情形：\n\n以下就結構已知的部分，作進一步的說明。\n\n\n==== 1.結構已知，觀測值完整： ====\n此時我們可以用最大概似估計法（MLE）來求得參數。其對數概似函數為\n\n  \n    \n      \n        L\n        =\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            s\n          \n        \n        log\n        ⁡\n        (\n        P\n        (\n        \n          X\n          \n            i\n          \n        \n        \n          |\n        \n        p\n        a\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        ,\n        \n          D\n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle L={\\frac {1}{N}}\\sum _{i=1}^{n}\\sum _{i=1}^{s}\\log(P(X_{i}|pa(X_{i}),D_{i}))}\n  其中\n  \n    \n      \n        p\n        a\n        (\n        \n          X\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle pa(X_{i})}\n  代表\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  的因變數，\n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  代表第\n  \n    \n      \n        \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {1}}}\n  個觀測值，N代表觀測值資料的總數。\n以圖二當例子，我們可以求出節點U的最大概似估計式為\n\n  \n    \n      \n        P\n        (\n        U\n        =\n        u\n        \n          |\n        \n        \n          S\n          \n            1\n          \n        \n        =\n        \n          s\n          \n            1\n          \n        \n        ,\n        \n          S\n          \n            2\n          \n        \n        =\n        \n          s\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            \n              n\n              (\n              U\n              =\n              u\n              ,\n              \n                S\n                \n                  1\n                \n              \n              =\n              \n                s\n                \n                  1\n                \n              \n              ,\n              \n                S\n                \n                  2\n                \n              \n              =\n              \n                s\n                \n                  2\n                \n              \n              )\n            \n            \n              n\n              (\n              \n                S\n                \n                  1\n                \n              \n              =\n              \n                s\n                \n                  1\n                \n              \n              ,\n              \n                S\n                \n                  2\n                \n              \n              =\n              \n                s\n                \n                  2\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(U=u|S_{1}=s_{1},S_{2}=s_{2})={\\frac {n(U=u,S_{1}=s_{1},S_{2}=s_{2})}{n(S_{1}=s_{1},S_{2}=s_{2})}}}\n  由上式就可以藉由觀測值來估計出節點U的條件分配。如果當模型很複雜時，這時可能就要利用數值分析或其它最佳化技巧來求出參數。\n\n\n==== 2.結構已知，觀測值不完整（有遺漏資料）： ====\n如果有些節點觀測不到的話，可以使用EM演算法（Expectation-Maximization algorithm）來決定出參數的區域最佳概似估計式。而EM演算法的的主要精神在於如果所有節點的值都已知下，在M階段就會很簡單，如同最大概似估計法。而EM演算法的步驟如下：\n\n（1）首先給定欲估計的參數一個起始值，然後利用此起始值和其他的觀測值，求出其他未觀測到節點的條件期望值，接著將所估計出的值視為觀測值，將此完整的觀測值帶入此模型的最大概似估計式中，如下所示（以圖二為例）：\n\n  \n    \n      \n        P\n        (\n        U\n        =\n        u\n        \n          |\n        \n        \n          S\n          \n            1\n          \n        \n        =\n        \n          s\n          \n            1\n          \n        \n        ,\n        \n          S\n          \n            2\n          \n        \n        =\n        \n          s\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            \n              E\n              N\n              (\n              U\n              =\n              u\n              ,\n              \n                S\n                \n                  1\n                \n              \n              =\n              \n                s\n                \n                  1\n                \n              \n              ,\n              \n                S\n                \n                  2\n                \n              \n              =\n              \n                s\n                \n                  2\n                \n              \n              )\n            \n            \n              E\n              N\n              (\n              \n                S\n                \n                  1\n                \n              \n              =\n              \n                s\n                \n                  1\n                \n              \n              ,\n              \n                S\n                \n                  2\n                \n              \n              =\n              \n                s\n                \n                  2\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(U=u|S_{1}=s_{1},S_{2}=s_{2})={\\frac {EN(U=u,S_{1}=s_{1},S_{2}=s_{2})}{EN(S_{1}=s_{1},S_{2}=s_{2})}}}\n  其中\n  \n    \n      \n        E\n        N\n        (\n        x\n        )\n      \n    \n    {\\displaystyle EN(x)}\n  代表在目前的估計參數下，事件x的條件機率期望值為\n\n  \n    \n      \n        E\n        N\n        (\n        x\n        )\n        =\n        E\n        \n          ∑\n          \n            k\n          \n        \n        I\n        (\n        x\n        \n          |\n        \n        D\n        (\n        k\n        )\n        )\n        =\n        \n          ∑\n          \n            k\n          \n        \n        P\n        (\n        x\n        \n          |\n        \n        D\n        (\n        k\n        )\n        )\n      \n    \n    {\\displaystyle EN(x)=E\\sum _{k}I(x|D(k))=\\sum _{k}P(x|D(k))}\n  \n（2）最大化此最大概似估計式，求出此參數之最有可能值，如此重複步驟（1）與（2），直到參數收斂為止，即可得到最佳的參數估計值。\n\n\n== 補充例子（列舉推理法） ==\n讓我們考慮一個應用在醫藥上的機率推論例子，在此病人會被診斷出是否有呼吸困難的症狀。表一代表一個我們所觀測到的資料集合，包含10筆觀測值，S代表的是吸菸與否（Smoker），C代表是否為罹癌者（Cancer），B代表是否罹患支氣管炎（bronchitis），D代表是否有呼吸困難及咳嗽（dyspnea and asthma）的症狀。『1』和『0』分別代表『是』和『否』。此醫藥網路結構顯示於圖三。\n\n表二代表的是整個網路的經驗聯合機率分配，是由所收集到的資料所建構而成，利用此表可建構出節點的聯合機率分配。見圖四。此貝氏公式\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(A,B)}{P(B)}}}\n  可利用節點的邊際機率和聯合機率去計算節點的條件機率，待會會應用在建立條件機率表格（Conditional probability Table; CPT）上。見圖五。貝氏網路的聯合機率可由下列式子計算：\n\n  \n    \n      \n        P\n        (\n        S\n        ,\n        C\n        ,\n        B\n        ,\n        D\n        )\n        =\n        P\n        (\n        S\n        )\n        ×\n        P\n        (\n        C\n        \n          |\n        \n        S\n        )\n        ×\n        P\n        (\n        B\n        \n          |\n        \n        S\n        ,\n        C\n        )\n        ×\n        P\n        (\n        D\n        \n          |\n        \n        B\n        ,\n        C\n        )\n      \n    \n    {\\displaystyle P(S,C,B,D)=P(S)\\times P(C|S)\\times P(B|S,C)\\times P(D|B,C)}\n  其值見表三。\n使用整個網路經驗聯合機率分配所計算出來的值會與使用CPT所計算出來的值不同，其差異可由表二和表三得知。其中差異不只是值的不同，也出現了新事件的機率（原本所沒觀察到的事件）。\n\n建立在觀測資料上的機率推論演算法：\n\n1.資料集合\n  \n    \n      \n        D\n        =\n        \n          {\n          \n            \n              d\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              d\n              \n                n\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle D=\\left\\{d_{1},\\ldots ,d_{n}\\right\\}}\n  ，其中\n  \n    \n      \n        \n          d\n          \n            i\n          \n        \n        =\n        \n          \n            x\n            \n              i\n            \n            \n              (\n              1\n              )\n            \n          \n          ,\n          …\n          ,\n          \n            x\n            \n              i\n            \n            \n              (\n              N\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle d_{i}={x_{i}^{(1)},\\ldots ,x_{i}^{(N)}}}\n  （下標代表第幾個觀測值，上標代表第幾個變數），且一共有n個觀測值。每一個觀測值包含\n  \n    \n      \n        N\n        (\n        N\n        ≥\n        2\n        )\n      \n    \n    {\\displaystyle N(N\\geq 2)}\n  個變數\n  \n    \n      \n        \n          \n            X\n            \n              i\n            \n            \n              (\n              1\n              )\n            \n          \n          ,\n          …\n          ,\n          \n            X\n            \n              i\n            \n            \n              (\n              N\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {X_{i}^{(1)},\\ldots ,X_{i}^{(N)}}}\n  ，第j個變數有\n  \n    \n      \n        \n          A\n          \n            (\n            j\n            )\n          \n        \n        =\n        \n          0\n          ,\n          1\n          ,\n          …\n          ,\n          \n            α\n            \n              (\n              j\n              )\n            \n          \n          −\n          1\n        \n        (\n        \n          α\n          \n            (\n            j\n            )\n          \n        \n        ≥\n        2\n        )\n      \n    \n    {\\displaystyle A^{(j)}={0,1,\\ldots ,\\alpha ^{(j)}-1}(\\alpha ^{(j)}\\geq 2)}\n  狀態。2.此貝氏網路的結構G代表N個前代（predecessors）節點集合\n  \n    \n      \n        \n          {\n          \n            \n              Π\n              \n                (\n                1\n                )\n              \n            \n            ,\n            …\n            ,\n            \n              Π\n              \n                (\n                N\n                )\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{\\Pi ^{(1)},\\ldots ,\\Pi ^{(N)}\\right\\}}\n  ，也就是對第j個節點，\n  \n    \n      \n        \n          Π\n          \n            (\n            j\n            )\n          \n        \n      \n    \n    {\\displaystyle \\Pi ^{(j)}}\n  為其親代節點的集合, j=1,2,…,N3.範例點（instantiated node）\n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                \n                  P\n                  \n                    1\n                  \n                \n              \n            \n            =\n            \n              x\n              \n                \n                  P\n                  \n                    1\n                  \n                \n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                \n                  P\n                  \n                    v\n                  \n                \n              \n            \n            =\n            \n              x\n              \n                \n                  P\n                  \n                    v\n                  \n                \n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X^{P_{1}}=x^{P_{1}},\\ldots ,X^{P_{v}}=x^{P_{v}}\\right\\}}\n  為節點在已知狀態，即在此狀態的機率為1。如果範例點為空集合，將使用古典機率推論使用表一的觀測值和圖一的貝氏網路結構，並且已知範例點（instantiated node）為\n  \n    \n      \n        \n          {\n          \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{S=0,C=0\\right\\}}\n  ，也就是病人為非吸菸者和罹癌者：\n  \n    \n      \n        P\n        \n          (\n          \n            S\n            =\n            0\n          \n          )\n        \n        =\n        1\n        ,\n        P\n        \n          (\n          \n            C\n            =\n            0\n          \n          )\n        \n        =\n        1\n      \n    \n    {\\displaystyle P\\left(S=0\\right)=1,P\\left(C=0\\right)=1}\n  問題：\n1.病人患有支氣管炎的機率\n  \n    \n      \n        P\n        \n          (\n          B\n          )\n        \n        =\n        ?\n      \n    \n    {\\displaystyle P\\left(B\\right)=?}\n  \n2.病人會有呼吸困難的機率\n  \n    \n      \n        P\n        \n          (\n          D\n          )\n        \n        =\n        ?\n      \n    \n    {\\displaystyle P\\left(D\\right)=?}\n  解答：\n1. \n  \n    \n      \n        P\n        \n          (\n          \n            B\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        =\n        0.8\n      \n    \n    {\\displaystyle P\\left(B=0|S=0,C=0\\right)=0.8}\n  \n\n  \n    \n      \n        P\n        \n          (\n          \n            B\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        =\n        0.2\n      \n    \n    {\\displaystyle P\\left(B=1|S=0,C=0\\right)=0.2}\n  故為0.22. \n  \n    \n      \n        P\n        \n          (\n          \n            D\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(D=0|S=0,C=0\\right)}\n  \n\n  \n    \n      \n        =\n        P\n        \n          (\n          \n            D\n            =\n            0\n            ,\n            B\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        +\n        P\n        \n          (\n          \n            D\n            =\n            0\n            ,\n            B\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle =P\\left(D=0,B=0|S=0,C=0\\right)+P\\left(D=0,B=1|S=0,C=0\\right)}\n  \n\n  \n    \n      \n        =\n        P\n        \n          (\n          \n            D\n            =\n            0\n            \n              |\n            \n            B\n            =\n            0\n            ,\n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        ×\n        P\n        \n          (\n          \n            B\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle =P\\left(D=0|B=0,S=0,C=0\\right)\\times P\\left(B=0|S=0,C=0\\right)}\n  \n  \n    \n      \n        +\n        P\n        \n          (\n          \n            D\n            =\n            0\n            \n              |\n            \n            B\n            =\n            1\n            ,\n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        ×\n        P\n        \n          (\n          \n            B\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle +P\\left(D=0|B=1,S=0,C=0\\right)\\times P\\left(B=1|S=0,C=0\\right)}\n  \n\n  \n    \n      \n        P\n        \n          (\n          \n            D\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(D=1|S=0,C=0\\right)}\n  \n  \n    \n      \n        =\n        P\n        \n          (\n          \n            D\n            =\n            1\n            ,\n            B\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        +\n        P\n        \n          (\n          \n            D\n            =\n            1\n            ,\n            B\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle =P\\left(D=1,B=1|S=0,C=0\\right)+P\\left(D=1,B=0|S=0,C=0\\right)}\n  \n\n  \n    \n      \n        =\n        P\n        \n          (\n          \n            D\n            =\n            1\n            \n              |\n            \n            B\n            =\n            0\n            ,\n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        ×\n        P\n        \n          (\n          \n            B\n            =\n            0\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle =P\\left(D=1|B=0,S=0,C=0\\right)\\times P\\left(B=0|S=0,C=0\\right)}\n  \n  \n    \n      \n        +\n        P\n        \n          (\n          \n            D\n            =\n            1\n            \n              |\n            \n            B\n            =\n            1\n            ,\n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n        ×\n        P\n        \n          (\n          \n            B\n            =\n            1\n            \n              |\n            \n            S\n            =\n            0\n            ,\n            C\n            =\n            0\n          \n          )\n        \n      \n    \n    {\\displaystyle +P\\left(D=1|B=1,S=0,C=0\\right)\\times P\\left(B=1|S=0,C=0\\right)}\n  \n\n\n== 應用 ==\n貝氏網路目前應用在類比計算生物學與生物資訊學基因調控網路、蛋白質結構、基因表達分析、醫學、文档分类、資訊檢索、決策支援系統、工程學、資料結合、圖像處理等。\n\n\n== 參考文獻 ==\n\n\n== 外部連結 ==\nA tutorial on learning with Bayesian Networks\nAn Introduction to Bayesian Networks and their Contemporary Applications （页面存档备份，存于互联网档案馆）\nOn-line Tutorial on Bayesian nets and probability （页面存档备份，存于互联网档案馆）\nWeb-App to create Bayesian nets and run it with a Monte Carlo method\nContinuous Time Bayesian Networks （页面存档备份，存于互联网档案馆）\nBayesian Networks: Explanation and Analogy\nA live tutorial on learning Bayesian networks （页面存档备份，存于互联网档案馆）\nA hierarchical Bayes Model for handling sample heterogeneity in classification problems （页面存档备份，存于互联网档案馆）, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples.\nHierarchical Naive Bayes Model for handling sample uncertainty （页面存档备份，存于互联网档案馆）, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.", "数据挖掘": "数据挖掘（英語：data mining）是一个跨学科的计算机科学分支 。它是用人工智能、机器学习、统计学和数据库的交叉方法在相對較大型的数据集中发现模式的计算过程。\n数据挖掘过程的总体目标是从一个数据集中提取信息，并将其转换成可理解的结构，以进一步使用。除了原始分析步骤，它还涉及到数据库和数据管理方面、数据预处理、模型与推断方面考量、兴趣度度量、复杂度的考虑，以及发现结构、可视化及在线更新等后处理。数据挖掘是“資料庫知識發現”（Knowledge-Discovery in Databases, KDD）的分析步骤 ，本质上属于机器学习的范畴。\n类似词语“資料採礦”、“数据捕鱼”和“数据探测”指用数据挖掘方法来采样（可能）过小以致无法可靠地统计推断出所发现任何模式的有效性的更大总体数据集的部分。不过这些方法可以建立新的假设来检验更大数据总体。\n\n\n== 歷史 ==\n資料探勘是因為海量有用資料快速增長的產物。使用計算機進行歷史資料分析，1960年代數字方式採集資料已經實現。1980年代，关系数据库隨著能夠適應動態按需分析資料的結構化查詢語言發展起來。数据仓库開始用來存儲大量的資料。\n因為面臨處理資料庫中大量資料的挑戰，於是資料探勘應運而生，對於這些問題，它的主要方法是資料統計分析和人工智能搜索技術。\n\n\n== 定義 ==\n資料探勘有以下這些不同的定義：\n\n「從資料中提取出隱含的過去未知的有價值的潛在信息」\n「一門從大量資料或資料庫中提取有用信息的科學」儘管通常資料探勘應用於資料分析，但是像人工智能一樣，它也是一個具有豐富含義的詞彙，可用於不同的領域。\n它与KDD(Knowledge discovery in databases)的关系是：KDD是从数据中辨别有效的、新颖的、潜在有用的、最终可理解的模式的过程；而数据挖掘是KDD通过特定的算法在可接受的计算效率限制内生成特定模式的一个步骤。\n事实上，在现今的文献中，这两个术语经常不加区分的使用。\n\n\n== 本质 ==\n数据挖掘本质上属于机器学习的内容。\n例如《数据挖掘：实用机器学习技术及Java实现》一书大部分是机器学习的内容。这本书最初只叫做“实用机器学习”，“数据挖掘”一词是后来为了营销才加入的。通常情况下，使用更为正式的术语，（大规模）数据分析和分析学，或者指出实际的研究方法（例如人工智能和机器学习）会更准确一些。\n\n\n== 过程 ==\n数据挖掘的实际工作是对大规模数据进行自动或半自动的分析，以提取过去未知的有价值的潜在信息，例如数据的分组（通过聚类分析）、数据的异常记录（通过异常检测）和数据之间的关系（通过关联式规则挖掘）。这通常涉及到数据库技术，例如空间索引。这些潜在信息可通过对输入数据处理之后的总结来呈现，之后可以用于进一步分析，比如机器学习和预测分析。举个例子，进行数据挖掘操作时可能要把数据分成多组，然后可以使用决策支持系统以获得更加精确的预测结果。不过数据收集、数据预处理、结果解释和撰写报告都不算数据挖掘的步骤，但是它们确实属于“資料庫知識發現”（KDD）过程，只不过是一些额外的环节。\n数据库知识发现（KDD）过程通常定义为以下阶段：\n\n(1) 选择\n(2) 预处理\n(3) 变换\n(4) 数据挖掘\n(5) 解释/评估。\n\n\n=== 预处理 ===\n在运用数据挖掘算法之前，必须收集目标数据集。由于数据挖掘只能发现实际存在于数据中的模式，目标数据集必须大到足以包含这些模式，而其余的足够简洁以在一个可接受的时间范围内挖掘。常见的数据源如資料超市或資料倉儲。在数据挖掘之前，有必要预处理来分析多变量数据。然后要清理目标集。数据清理移除包含噪声和含有缺失数据的观测量。\n\n\n=== 数据挖掘 ===\n数据挖掘涉及六类常见的任务： \n异常检测（异常/变化/偏差检测）– 识别不寻常的数据记录，错误数据需要进一步调查。\n关联规则学习（依赖建模）– 搜索变量之间的关系。例如，一个超市可能会收集顾客购买习惯的数据。运用关联规则学习，超市可以确定哪些产品经常一起买，并利用这些信息帮助营销。这有时被称为市场购物篮分析。\n聚类 – 是在未知数据的结构下，发现数据的类别与结构。\n分類 – 是对新的数据推广已知的结构的任务。例如，一个电子邮件程序可能试图将一个电子邮件分类为“正常郵件”或“垃圾邮件”。\n迴歸 – 试图找到能够以最小误差对该数据建模的函数。\n汇总 – 提供了一个更紧凑的数据集表示，包括生成可视化和报表。\n\n\n=== 结果验证 ===\n数据挖掘的价值一般带着一定的目的，而这目的是否得到实现一般可以通过结果验证来实现。验证是指“通过提供客观证据对规定要求已得到满足的认定”，而这个“认定”活动的策划、实施和完成，与“规定要求”的内容紧密相关。数据挖掘过程中的数据验证的“规定要求”的设定，往往与数据挖掘要达到的基本目标、过程目标和最终目标有关。验证的结果可能是“规定要求”得到完全满足，或者完全没有得到满足，以及其他介于两者之间的满足程度的状况。验证可以由数据挖掘的人自己完成，也可以通过其他人参与或完全通过他人的项目，以与数据挖掘者毫无关联的方式进行验证。一般验证过程中，数据挖掘者是不可能不参与的，但对于认定过程中的客观证据的收集、认定的评估等过程如果通过与验证提出者无关的人来实现，往往更具有客观性。通过结果验证，数据挖掘者可以得到对自己所挖掘的数据价值高低的评估。\n\n\n== 隐私问题及伦理 ==\n與資料探勘有關的，還牽扯到隐私問題，例如：一個僱主可以透過訪問醫療記錄來篩選出那些有糖尿病或者嚴重心臟病的人，從而意圖削減保險支出。然而，這種做法會導致倫理和法律問題。\n對於政府和商業資料的挖掘，可能會涉及到的，是國家安全或者商業機密之類的問題。這對於保密也是個不小的挑戰。資料探勘有很多合法的用途，例如可以在患者群的資料庫中查出某藥物和其副作用的關聯。這種關聯可能在1000人中也不會出現一例，但藥物學相關的項目就可以運用此方法減少對藥物有不良反應的病人數量，還有可能挽救生命；但这當中還是存在着資料庫可能被濫用的问题。\n資料探勘實現了用其他方法不可能實現的方法來發現資訊，但它必須受到規範，應當在適當的說明下使用。\n如果資料是收集自特定的個人，那麼就會出現一些涉及保密、法律和倫理的問題。2018年5月25日，歐盟一般資料保護規範(General Data Protection Regulation，GDPR)正式上路，保障個人資料蒐集的同意權與刪除要求，在進入網站時會進行個人資料蒐集、處理及利用之告知，並在當事人同意之下做蒐集。\n\n\n== 方法 ==\n数据挖掘的方法包括監督式學習、非監督式學習、半监督学习、增强学习。監督式學習包括：分類、估计、預測。非监督式学习包括：聚类，关联规则分析。\n\n\n== 例子 ==\n数据挖掘在零售行業中的應用：零售公司跟蹤客戶的購買情況，發現某個客戶購買了大量的真絲襯衣，這時資料探勘系統就在此客戶和真絲襯衣之間建立關聯。銷售部门就會看到此信息，直接發送真絲襯衣的當前行情，以及所有关于真丝衬衫的资料发給該客戶。這樣零售商店通過資料探勘系統就發現了以前未知的關於客戶的新信息，并且扩大经营范围。\n\n\n== 数据捕捞 ==\n通常作為與資料倉庫和分析相關的技術，資料探勘處於它們的中間。然而，有時還會出現十分可笑的應用，例如發掘出不存在但看起來振奮人心的模式（特別的因果關係），這些根本不相關的、甚至引人誤入歧途的、或是毫無價值的關聯，在統計學文獻裡通常被戲稱為「資料挖泥」（Data dredging, data fishing, or data snooping）。\n資料探勘意味著掃瞄可能存在任何關係的資料，然後篩選出符合的模式，（這也叫作「過度匹配模式」）。大量的數據集中總會有碰巧或特定的資料，有著「令人振奮的關係」。因此，一些結論看上去十分令人懷疑。儘管如此，一些探索性資料分析 還是需要應用統計分析尋找資料，所以好的統計方法和數據資料的界限並不是很清晰。\n更危險是出現根本不存在的關聯性。投資分析家似乎最容易犯這種錯誤。在一本叫做《顧客的遊艇在哪裡？》的書中寫道：「總是有相當數量的可憐人，忙於從上千次的賭輪盤的輪子上尋找可能的重複模式。十分不幸的是，他們通常會找到。」多數的資料探勘研究都關注於發現大量的資料集中，一個高度詳細的模式。在《大忙人的資料探勘》一書中， 西弗吉尼亞大學和不列顛哥倫比亞大學研究者討論了一個交替模式，用來發現一個資料集當中兩個元素的最小區別，它的目標是發現一個更簡單的模式來描述相關數據。\n\n\n== 参见 ==\n方法\n应用领域\n应用实例\n\n相关主题数据挖掘是关于分析数据的；有关从数据中提取信息的信息，参见：\n\n\n== 參考文獻 ==\n\n\n== 延伸阅读 ==\nCabena, Peter; Hadjnian, Pablo; Stadler, Rolf; Verhees, Jaap; Zanasi, Alessandro (1997); Discovering Data Mining: From Concept to Implementation, Prentice Hall, ISBN 0-13-743980-6\nM.S. Chen, J. Han, P.S. Yu (1996) \"Data mining: an overview from a database perspective （页面存档备份，存于互联网档案馆）\". Knowledge and data Engineering, IEEE Transactions on 8 (6), 866–883\nFeldman, Ronen; Sanger, James (2007); The Text Mining Handbook, Cambridge University Press, ISBN 978-0-521-83657-9\nGuo, Yike; and Grossman, Robert (editors) (1999); High Performance Data Mining: Scaling Algorithms, Applications and Systems, Kluwer Academic Publishers\nHan, Jiawei, Micheline Kamber, and Jian Pei. Data mining: concepts and techniques. Morgan kaufmann, 2006.\nHastie, Trevor, Tibshirani, Robert and Friedman, Jerome (2001); The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, ISBN 0-387-95284-5\nLiu, Bing (2007); Web Data Mining: Exploring Hyperlinks, Contents and Usage Data, Springer, ISBN 3-540-37881-2\nMurphy, Chris. Is Data Mining Free Speech?. InformationWeek (UMB). 16 May 2011: 12. \nNisbet, Robert; Elder, John; Miner, Gary (2009); Handbook of Statistical Analysis & Data Mining Applications, Academic Press/Elsevier, ISBN 978-0-12-374765-5\nPoncelet, Pascal; Masseglia, Florent; and Teisseire, Maguelonne (editors) (October 2007); \"Data Mining Patterns: New Methods and Applications\", Information Science Reference, ISBN 978-1-59904-162-9\nTan, Pang-Ning; Steinbach, Michael; and Kumar, Vipin (2005); Introduction to Data Mining, ISBN 0-321-32136-7\nTheodoridis, Sergios; and Koutroumbas, Konstantinos (2009); Pattern Recognition, 4th Edition, Academic Press, ISBN 978-1-59749-272-0\nWeiss, Sholom M.; and Indurkhya, Nitin (1998); Predictive Data Mining, Morgan Kaufmann\nWitten, Ian H.; Frank, Eibe; Hall, Mark A. Data Mining: Practical Machine Learning Tools and Techniques 3. Elsevier. 30 January 2011. ISBN 978-0-12-374856-0.  (See also Free Weka software)\nYe, Nong (2003); The Handbook of Data Mining, Mahwah, NJ: Lawrence Erlbaum\n\n\n== 外部連結 ==\n开放式目录计划中和知识发现软件相关的内容\n开放式目录计划中和数据挖掘工具供应商相关的内容", "靈敏度和特異度": "靈敏度和特異度（英語：Sensitivity and specificity），或稱敏感性和特異性，是統計學中用來表徵二項分類測試特徵的數據，在統計學中也被稱為統計分類，在醫學中廣為使用。\n\n靈敏度（Sensitivity，也稱為真陽性率、召回率（Recall） ）是指實際為陽性的樣本中，判斷為陽性的比例（例如真正有生病的人中，被判斷為有生病者的比例），計算方式是真陽性除以真陽性+假陰性（實際為陽性，但判斷為陰性）的比值。\n特異度（Specificity，也稱為真陰性率）是指實際為陰性的樣本中，判斷為陰性的比例（例如真正未生病的人中，被醫院判斷為未生病者的比例），計算方式是真陰性除以真陰性+假陽性（實際為陰性，但判斷為陽性）的比值。必須注意的是「陽性」與「陰性」這兩個詞並不指涉固定的值，僅是表示存在或不存在，當使用在不同主題時，其意義不會相同。例如應用在討論疾病時，「陽性」可以表示「染病」，「陰性」可以表示「健康」。\n包括醫學診斷檢驗的許多測試中，靈敏度是指真陽性沒有被忽視的程度（所以偽陰性很少），而特異度是真陰性確實鑑別的程度（所以偽陽性很少）。因此，一個高靈敏度的檢驗很少忽略真陽性（例如：即使有異常仍然檢驗為無異常）；而高特異度檢驗則很少將不是檢驗目標的其他東西鑑別為陽性（例如：檢驗出一種非常相似的細菌卻將其誤判為目標細菌）。一個高靈敏度且高特異度的檢驗表示其兩方面都做得好，所以這個檢驗「很少忽略它正在尋找的目標並且很少將其他東西誤判為目標。」\n靈敏度可以作為避免假陰性的量化指標，而特異度可以作為避免假陽性的量化指標。對於任何測試而言，都需要在靈敏度及特異度之間進行取捨。例如機場安檢中對於登機人員是否有攜帶危險物品的檢查，掃描器可能會在檢查到像皮帶頭或鎖匙等低危險物品時觸發（低特異度），但會減少實際攜帶了危險物品，但沒有檢查到的可能性（高靈敏度）。這個取捨可以用ROC曲线（接收者操作特徵曲線）來表示。完美的分類器可以達到100%的靈敏度（所有生病的人都會檢測為生病），及100%的特異度（沒有一個健康不生病的人會被檢測為生病）。但是理論上所有的分類器都會有最小的誤差範圍，稱為贝叶斯错误率。\n\n\n== 舉例 ==\n在個100人的樣本中，有10人事實上患有A病（陽性），經過檢測後，9人判定患有A病（真陽性），而1人判定並不患有A病（假陰性）；\n另外的90人實際上并不患有A病（陰性），然後經過檢測後，其中的5人被判定患有A病（假陽性），另外的85人判定不患有A病（真陰性）。\n靈敏度=真陽性/(真陽性+假陰性)=9/(9+1)=90%;\n特異度=真陰性/(真陰性+假陽性)=85/(85+5)=94.4%.\n此處，靈敏度即為在患病人群中，成功確證患病的概率；而特異度即為在不患病的人群中，成功排除患病的概率。\n\n\n== 參考資料 ==\n\n\n== 相關條目 ==", "反向传播算法": "反向传播（英語：Backpropagation，意為误差反向传播，缩写为BP）是對多層人工神经网络進行梯度下降的算法，也就是用链式法则以网络每层的权重為變數计算损失函数的梯度，以更新权重來最小化损失函数。\n\n\n== 动机 ==\n任何监督式学习算法的目标是找到一个能把一组输入最好地映射到其正确的输出的函数。例如一个简单的分类任务，其中输入是动物的图像，正确的输出将是动物的名称。一些输入和输出模式可以很容易地通过单层神经网络（如感知器）学习。但是这些单层的感知机只能学习一些比较简单的模式，例如那些非线性可分的模式。例如，人可以通过识别动物的图像的某些特征进行分类，例如肢的数目，皮肤的纹理（无论是毛皮，羽毛，鳞片等），该动物的体型，以及种种其他特征。但是，单层神经网络必须仅仅使用图像中的像素的强度来学习一个输出一个标签函数。因为它被限制为仅具有一个层，所以没有办法从输入中学习到任何抽象特征。多层的网络克服了这一限制，因为它可以创建内部表示，并在每一层学习不同的特征。 第一层可能负责从图像的单个像素的输入学习线条的走向。第二层可能就会结合第一层所学并学习识别简单形状（如圆形）。每升高一层就学习越来越多的抽象特征，如上文提到的用来图像分类。每一层都是从它下方的层中找到模式，就是这种能力创建了独立于为多层网络提供能量的外界输入的内部表达形式。\n反向传播算法的发展的目标和动机是找到一种训练的多层神经网络的方法，于是它可以学习合适的内部表达来让它学习任意的输入到输出的映射。\n\n\n== 概括 ==\n反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。\n\n\n=== 第1阶段：激励传播 ===\n每次迭代中的传播环节包含两步：\n\n（前向传播阶段）将训练输入送入网络以获得預測結果；\n（反向传播阶段）對預測結果同训练目标求差(损失函数)。\n\n\n=== 第2阶段：权重更新 ===\n对于每个突触上的权重，按照以下步骤进行更新：\n\n将输入激励和响应误差相乘，从而获得权重的梯度；\n将这个梯度乘上一个比例并取反后加到权重上。这个比例（百分比）将会影响到训练过程的速度和效果，因此成为「训练因子」。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。\n第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。\n\n\n== 算法 ==\n\n\n=== 數學推導 ===\n假設多層人工神经网络的第 \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   層是由线性算子 \n  \n    \n      \n        \n          W\n          \n            l\n          \n        \n        :\n        \n          \n            R\n          \n          \n            \n              n\n              \n                l\n                −\n                1\n              \n            \n          \n        \n        →\n        \n          \n            R\n          \n          \n            \n              n\n              \n                l\n              \n            \n          \n        \n      \n    \n    {\\displaystyle W^{l}:\\mathbb {R} ^{n_{l-1}}\\to \\mathbb {R} ^{n_{l}}}\n   和激活函數 \n  \n    \n      \n        \n          f\n          \n            l\n          \n        \n        :\n        \n          R\n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle f^{l}:\\mathbb {R} \\to \\mathbb {R} }\n   所構成，也就是說，第 \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   層的輸入是 \n  \n    \n      \n        \n          n\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle n_{l-1}}\n   維实数向量\n\n  \n    \n      \n        \n          y\n          \n            l\n            −\n            1\n          \n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n        \n          x\n          \n            2\n          \n        \n        ,\n        \n        ⋯\n        ,\n        \n        \n          x\n          \n            \n              n\n              \n                l\n                −\n                1\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle y^{l-1}=(x_{1},\\,x_{2},\\,\\cdots ,\\,x_{n_{l-1}})}\n  輸出則為\n  \n    \n      \n        \n          n\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle n_{l}}\n   維實向量\n\n  \n    \n      \n        \n          y\n          \n            l\n          \n        \n        =\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n        \n          y\n          \n            2\n          \n        \n        ,\n        \n        ⋯\n        ,\n        \n        \n          y\n          \n            \n              n\n              \n                l\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle y^{l}=(y_{1},\\,y_{2},\\,\\cdots ,\\,y_{n_{l}})}\n  換句話說，第 \n  \n    \n      \n        l\n        −\n        1\n      \n    \n    {\\displaystyle l-1}\n   層的輸出 \n  \n    \n      \n        \n          y\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle y^{l-1}}\n   就是第 \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   層的輸入。\n而 \n  \n    \n      \n        \n          y\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle y^{l}}\n   和 \n  \n    \n      \n        \n          y\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle y^{l-1}}\n   的具體(以第 \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   分量表示)遞迴關係為\n\n  \n    \n      \n        \n          \n            \n              y\n              \n                l\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          f\n          \n            l\n          \n        \n        {\n        \n        [\n        \n          W\n          \n            l\n          \n        \n        (\n        \n          y\n          \n            l\n            −\n            1\n          \n        \n        )\n        \n          ]\n          \n            i\n          \n        \n        \n        }\n        =\n        \n          f\n          \n            l\n          \n        \n        \n          [\n          \n            \n            \n              ∑\n              \n                j\n                =\n                1\n              \n              \n                \n                  n\n                  \n                    l\n                    −\n                    1\n                  \n                \n              \n            \n            \n              y\n              \n                j\n              \n              \n                l\n                −\n                1\n              \n            \n            \n              W\n              \n                j\n                i\n              \n              \n                l\n              \n            \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {y^{l}}_{i}=f^{l}\\{\\,[W^{l}(y^{l-1})]_{i}\\,\\}=f^{l}\\left[\\,\\sum _{j=1}^{n_{l-1}}y_{j}^{l-1}W_{ji}^{l}\\,\\right]}\n   （ \n  \n    \n      \n        1\n        ≤\n        i\n        ≤\n        \n          n\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle 1\\leq i\\leq n_{l}}\n   ）上式通常會簡寫為\n\n  \n    \n      \n        \n          y\n          \n            l\n          \n        \n        =\n        \n          f\n          \n            l\n          \n        \n        [\n        \n        \n          W\n          \n            l\n          \n        \n        (\n        \n          y\n          \n            l\n            −\n            1\n          \n        \n        )\n        \n        ]\n      \n    \n    {\\displaystyle y^{l}=f^{l}[\\,W^{l}(y^{l-1})\\,]}\n  若這個多層人工神經網路總共有 \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   層，也就是說，\n  \n    \n      \n        \n          y\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle y^{0}}\n   是最一開始的輸入，而 \n  \n    \n      \n        \n          y\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle y^{L}}\n   是最後一層的輸出，那跟损失函数 \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   是以最後一層輸出 \n  \n    \n      \n        \n          y\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle y^{L}}\n   的各分量 \n  \n    \n      \n        \n          \n            \n              y\n              \n                L\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {y^{L}}_{i}}\n   (與真實值)為變數。依據上面的遞迴關係，可以把 \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   進一步的轉成以第 \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   層的輸入 \n  \n    \n      \n        \n          y\n          \n            L\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle y^{L-1}}\n   與權重因子 \n  \n    \n      \n        \n          \n            \n              W\n              \n                m\n              \n            \n          \n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle {W^{m}}_{ij}}\n   為變數的函数 \n  \n    \n      \n        \n          g\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle g^{L}}\n  \n\n  \n    \n      \n        \n          g\n          \n            L\n          \n        \n        (\n        \n          W\n          \n            i\n            j\n          \n          \n            L\n          \n        \n        ,\n        \n        \n          \n            y\n            \n              k\n            \n            \n              L\n              −\n              1\n            \n          \n        \n        )\n        =\n        g\n        \n          [\n          \n            \n            \n              f\n              \n                L\n              \n            \n            \n              (\n              \n                \n                  ∑\n                  \n                    a\n                    =\n                    1\n                  \n                  \n                    \n                      n\n                      \n                        L\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  y\n                  \n                    a\n                  \n                  \n                    L\n                    −\n                    1\n                  \n                \n                \n                  W\n                  \n                    a\n                    b\n                  \n                  \n                    L\n                  \n                \n              \n              )\n            \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle g^{L}(W_{ij}^{L},\\,{y_{k}^{L-1}})=g\\left[\\,f^{L}\\left(\\sum _{a=1}^{n_{L-1}}y_{a}^{L-1}W_{ab}^{L}\\right)\\,\\right]}\n   （ \n  \n    \n      \n        1\n        ≤\n        k\n        ≤\n        \n          n\n          \n            L\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle 1\\leq k\\leq n_{L-1}}\n   ,  \n  \n    \n      \n        1\n        ≤\n        b\n        ≤\n        \n          n\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle 1\\leq b\\leq n_{L}}\n   ）由此可以歸納到 \n  \n    \n      \n        1\n        ≤\n        l\n        <\n        L\n      \n    \n    {\\displaystyle 1\\leq l<L}\n   的情況(注意到前幾層的權重因子不會消失在表達式中)\n\n  \n    \n      \n        \n          g\n          \n            l\n          \n        \n        (\n        \n          W\n          \n            i\n            j\n          \n          \n            l\n          \n        \n        ,\n        \n        ⋯\n        ,\n        \n        \n          W\n          \n            i\n            j\n          \n          \n            L\n          \n        \n        ,\n        \n        \n          \n            y\n            \n              k\n            \n            \n              l\n              −\n              1\n            \n          \n        \n        )\n        =\n        \n          g\n          \n            l\n            +\n            1\n          \n        \n        \n          [\n          \n            \n            \n              W\n              \n                i\n                j\n              \n              \n                l\n                +\n                1\n              \n            \n            ,\n            \n            ⋯\n            ,\n            \n            \n              W\n              \n                i\n                j\n              \n              \n                L\n              \n            \n            ,\n            \n            \n              f\n              \n                l\n              \n            \n            \n              (\n              \n                \n                  ∑\n                  \n                    a\n                    =\n                    1\n                  \n                  \n                    \n                      n\n                      \n                        l\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  y\n                  \n                    a\n                  \n                  \n                    l\n                    −\n                    1\n                  \n                \n                \n                  W\n                  \n                    a\n                    b\n                  \n                  \n                    l\n                  \n                \n              \n              )\n            \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle g^{l}(W_{ij}^{l},\\,\\cdots ,\\,W_{ij}^{L},\\,{y_{k}^{l-1}})=g^{l+1}\\left[\\,W_{ij}^{l+1},\\,\\cdots ,\\,W_{ij}^{L},\\,f^{l}\\left(\\sum _{a=1}^{n_{l-1}}y_{a}^{l-1}W_{ab}^{l}\\right)\\,\\right]}\n   （ \n  \n    \n      \n        1\n        ≤\n        k\n        ≤\n        \n          n\n          \n            l\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle 1\\leq k\\leq n_{l-1}}\n   ,  \n  \n    \n      \n        1\n        ≤\n        b\n        ≤\n        \n          n\n          \n            l\n          \n        \n      \n    \n    {\\displaystyle 1\\leq b\\leq n_{l}}\n   ）那這樣如果假設適當的可微分條件，由链式法则會有以下的遞迴關係 ( 若取 \n  \n    \n      \n        \n          g\n          \n            L\n            +\n            1\n          \n        \n        :=\n        g\n      \n    \n    {\\displaystyle g^{L+1}:=g}\n   和 \n  \n    \n      \n        1\n        ≤\n        l\n        ≤\n        L\n      \n    \n    {\\displaystyle 1\\leq l\\leq L}\n   )\n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                g\n                \n                  l\n                \n              \n            \n            \n              ∂\n              \n                W\n                \n                  c\n                  d\n                \n                \n                  l\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                g\n                \n                  l\n                  +\n                  1\n                \n              \n            \n            \n              ∂\n              \n                \n                  \n                    y\n                    \n                      l\n                    \n                  \n                \n                \n                  d\n                \n              \n            \n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            \n              \n                \n                  y\n                  \n                    l\n                  \n                \n              \n              \n                d\n              \n            \n            =\n            \n              f\n              \n                l\n              \n            \n            (\n            x\n            )\n          \n        \n        ×\n        \n          \n            \n              d\n              \n                f\n                \n                  l\n                \n              \n            \n            \n              d\n              x\n            \n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            x\n            =\n            ∑\n            \n              \n                \n                  y\n                  \n                    l\n                    −\n                    1\n                  \n                \n              \n              \n                a\n              \n            \n            \n              W\n              \n                a\n                d\n              \n              \n                l\n              \n            \n          \n        \n        ×\n        \n          \n            \n              y\n              \n                l\n                −\n                1\n              \n            \n          \n          \n            c\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial g^{l}}{\\partial W_{cd}^{l}}}={\\frac {\\partial g^{l+1}}{\\partial {y^{l}}_{d}}}{\\bigg |}_{{y^{l}}_{d}=f^{l}(x)}\\times {\\frac {df^{l}}{dx}}{\\bigg |}_{x=\\sum {y^{l-1}}_{a}W_{ad}^{l}}\\times {y^{l-1}}_{c}}\n  \n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                g\n                \n                  l\n                \n              \n            \n            \n              ∂\n              \n                \n                  \n                    y\n                    \n                      l\n                      −\n                      1\n                    \n                  \n                \n                \n                  c\n                \n              \n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            \n              n\n              \n                l\n              \n            \n          \n        \n        \n          [\n          \n            \n            \n              \n                \n                  ∂\n                  \n                    g\n                    \n                      l\n                      +\n                      1\n                    \n                  \n                \n                \n                  ∂\n                  \n                    \n                      \n                        y\n                        \n                          l\n                        \n                      \n                    \n                    \n                      i\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  |\n                \n              \n              \n                \n                  \n                    \n                      y\n                      \n                        l\n                      \n                    \n                  \n                  \n                    i\n                  \n                \n                =\n                \n                  f\n                  \n                    l\n                  \n                \n                (\n                x\n                )\n              \n            \n            ×\n            \n              \n                \n                  d\n                  \n                    f\n                    \n                      l\n                    \n                  \n                \n                \n                  d\n                  x\n                \n              \n            \n            \n              \n                \n                  |\n                \n              \n              \n                x\n                =\n                ∑\n                \n                  \n                    \n                      y\n                      \n                        l\n                        −\n                        1\n                      \n                    \n                  \n                  \n                    a\n                  \n                \n                \n                  W\n                  \n                    a\n                    i\n                  \n                  \n                    l\n                  \n                \n              \n            \n            ×\n            \n              W\n              \n                c\n                i\n              \n              \n                l\n              \n            \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\frac {\\partial g^{l}}{\\partial {y^{l-1}}_{c}}}=\\sum _{i=1}^{n_{l}}\\left[\\,{\\frac {\\partial g^{l+1}}{\\partial {y^{l}}_{i}}}{\\bigg |}_{{y^{l}}_{i}=f^{l}(x)}\\times {\\frac {df^{l}}{dx}}{\\bigg |}_{x=\\sum {y^{l-1}}_{a}W_{ai}^{l}}\\times W_{ci}^{l}\\,\\right]}\n  這樣就可以依據這個遞迴關係進行梯度下降，因為計算上是由 \n  \n    \n      \n        \n          \n            \n              y\n              \n                L\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {y^{L}}_{i}}\n   對 损失函数 \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   的偏微分出發，一層層向後遞推出前面各層的權重因子梯度，所以被稱為反向傳播。\n注意到可將輸入設為\n\n  \n    \n      \n        \n          y\n          \n            l\n            −\n            1\n          \n        \n        =\n        (\n        1\n        ,\n        \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n        \n          x\n          \n            2\n          \n        \n        ,\n        \n        ⋯\n        ,\n        \n        \n          x\n          \n            \n              n\n              \n                l\n                −\n                1\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle y^{l-1}=(1,\\,x_{1},\\,x_{2},\\,\\cdots ,\\,x_{n_{l-1}})}\n  並多加一行權重因子 \n  \n    \n      \n        \n          W\n          \n            i\n            0\n          \n          \n            l\n          \n        \n      \n    \n    {\\displaystyle W_{i0}^{l}}\n   為偏移，就可以把有偏移的多層網路納入剛剛討論的範圍內。\n\n\n=== 實際範例 ===\n三层网络算法（只有一个隐藏层）：\n\n  初始化网络权值（通常是小的随机值）\n  do\n     forEach 训练样本 ex\n        prediction = neural-net-output(network, ex)  // 正向传递\n        actual = teacher-output(ex)\n        计算输出单元的误差 (prediction - actual)\n        计算 \n  \n    \n      \n        Δ\n        \n          w\n          \n            h\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{h}}\n    对于所有隐藏层到输出层的权值                           // 反向传递\n        计算 \n  \n    \n      \n        Δ\n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Delta w_{i}}\n    对于所有输入层到隐藏层的权值                           // 继续反向传递\n        更新网络权值 // 输入层不会被误差估计改变\n  until 所有样本正确分类或满足其他停止标准\n  return 该网络\n\n这个算法的名称意味着误差会从输出结点反向传播到输入结点。严格地讲，反向传播算法对网络的可修改权值计算了网络误差的梯度。 这个梯度会在简单随机梯度下降法中经常用来求最小化误差的权重。通常“反向传播”这个词使用更一般的含义，用来指涵盖了计算梯度以及在随机梯度下降法中使用的整个过程。在适用反向传播算法的网络中，它通常可以快速收敛到令人满意的极小值。\n\n\n== 直观理解 ==\n\n\n=== 学习作为一个优化问题 ===\n在给出反向传播算法的数学推导之前，我们举一个例子来培养关于神经元的真实输出与正确输出间的直观感受。考虑一个有两个输入单元、一个输出单元、没有隐藏单元的简单神经网络。每个神经元都使用输入的加权作为线性输出。\n在训练之前，我们将随机分配权重\n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{1},w_{2}}\n  。之后神经元根据训练实例进行学习。在此例中，训练集为 (\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  , \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  , \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  )，其中 \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   与 \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n   是网络的输入，\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   为正确输出（在给定相同的输入时网络最终应当产生的输出）。网络在给定 \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   和 \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n   时，会计算一个输出 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ，很可能与 \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   不同（因为权重最初是随机的）。为了衡量期望输出 \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   与实际输出 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   之间的差异，一个常用的方法是采用平方误差测度：\n\n  \n    \n      \n        E\n        =\n        (\n        t\n        −\n        y\n        \n          )\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle E=(t-y)^{2}\\,}\n  ,其中 \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   为误差。\n\n举例来讲，考虑单一训练实例的网络：\n  \n    \n      \n        (\n        1\n        ,\n        1\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle (1,1,0)}\n  ，输入 \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n   与 \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n   均为1，正确输出 \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   为 0。现在若将实际输出 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   画在x轴，误差 \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   画在 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   轴，得出的是一条抛物线。抛物线的极小值对应输出 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ，最小化了误差 \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  。对于单一训练实例，极小值还会接触到 \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   轴，这意味着误差为零，网络可以产生与期望输出 \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   完全匹配的输出 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  。因此，把输入映射到输出的问题就化为了一个找到一个能产生最小误差的函数的最佳化問題。\n然而，一个神经元的输出取决于其所有输入的加权总和：\n\n  \n    \n      \n        y\n        =\n        \n          x\n          \n            1\n          \n        \n        \n          w\n          \n            1\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle y=x_{1}w_{1}+x_{2}w_{2}}\n  ,其中 \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle w_{1}}\n   和 \n  \n    \n      \n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle w_{2}}\n   是从输入单元到输出单元相连的权重。因此，误差取决于输入到该神经元的权重，也是网络要学习最终需要改变的。若每个权重都画在一个水平的轴上，而误差画在垂直轴上，得出的就是一个抛物面（若一个神经元有 \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   个权重，则误差曲面的維度就会是 \n  \n    \n      \n        k\n        +\n        1\n      \n    \n    {\\displaystyle k+1}\n  ，因而就是二维抛物线的 \n  \n    \n      \n        k\n        +\n        1\n      \n    \n    {\\displaystyle k+1}\n   维等价）。\n\n反向传播算法的目的是找到一组能最大限度地减小误差的权重。寻找抛物线或任意维度中的任何函数的极大值的方法有若干种。其中一种方法是通过求解方程组，但这依赖于网络是一个線性系統，而目标也需要可以训练多层非線性网络（因为多层线性网络与单层网络等价）。在反向传播中使用的方法是梯度下降法。\n\n\n=== 运用类比理解梯度下降法 ===\n\n梯度下降法背后的直观感受可以用假设情境进行说明。一个被卡在山上的人正在试图下山（即试图找到极小值）。大雾使得能见度非常低。因此，下山的道路是看不见的，所以他必须利用局部信息来找到极小值。他可以使用梯度下降法，该方法涉及到察看在他当前位置山的陡峭程度，然后沿着负陡度（即下坡）最大的方向前进。如果他要找到山顶（即极大值）的话，他需要沿着正陡度（即上坡）最大的方向前进。使用此方法，他会最终找到下山的路。不过，要假设山的陡度不能通过简单地观察得到，而需要复杂的工具测量，而这个工具此人恰好有。需要相当长的一段时间用仪器测量山的陡峭度，因此如果他想在日落之前下山，就需要最小化仪器的使用率。问题就在于怎样选取他测量山的陡峭度的频率才不致偏离路线。\n在这个类比中，此人代表反向传播算法，而下山路径表示能使误差最小化的权重集合。山的陡度表示误差曲面在该点的斜率。他要前行的方向对应于误差曲面在该点的梯度。用来测量陡峭度的工具是微分（误差曲面的斜率可以通过对平方误差函数在该点求导数计算出来）。他在两次测量之间前行的距离（与测量频率成正比）是算法的学习速率。参见限制一节中对此类型“爬山”算法的限制的讨论。\n\n\n== 限制 ==\n结果可能会收敛到极值。如果只有一个极小值，梯度下降的“爬山”策略一定可以起作用。然而，往往是误差曲面有许多局部最小值和最大值。如果梯度下降的起始点恰好介于局部最大值和局部最小值之间，则沿着梯度下降最大的方向会到达局部最小值。\n从反向传播学习获得的收敛很慢。\n在反向传播学习的收敛性不能保证。\n然而，收敛到全局最小值据说使用自适应终止条件得到保证。\n反向传播学习不需要输入向量的标准化（normalization）；然而，标准化可提高性能。\n\n\n== 历史 ==\n\n弗拉基米尔·瓦普尼克引用（Bryson, A.E.; W.F. Denham; S.E. Dreyfus. Optimal programming problems with inequality constraints. I: Necessary conditions for extremal solutions. AIAA J. 1, 11 (1963) 2544-2550）在他的书《支持向量机》中首次发表反向传播算法。在1969年Arthur E. Bryson和何毓琦将其描述为多级动态系统优化方法。  直到1974年以后在神经网络的背景下应用，并由Paul Werbos、David E. Rumelhart、杰弗里·辛顿和Ronald J. Williams的著作，它才获得认可，并引发了一场人工神经网络的研究领域的“文艺复兴”。在21世纪初人们对其失去兴趣，但在2010年后又拥有了兴趣，如今可以通过GPU等大型现代运算器件用于训练更大的网络。例如在2013年，顶级语音识别器现在使用反向传播算法训练神经网络。\n\n\n== 注释 ==\n\n\n== 参见 ==\n人工神经网络\n生物神经网络\n灾难性干扰\n表徵學習\nAdaBoost\n過適\n\n\n== 参考文献 ==\n\n\n== 外部連結 ==\nA Gentle Introduction to Backpropagation - An intuitive tutorial by Shashi Sathyanarayana The article contains pseudocode (\"Training Wheels for Training Neural Networks\") for implementing the algorithm.\nNeural Network Back-Propagation for Programmers (a tutorial)（页面存档备份，存于互联网档案馆）\nBackpropagation for mathematicians（页面存档备份，存于互联网档案馆）\nChapter 7 The backpropagation algorithm（页面存档备份，存于互联网档案馆） of Neural Networks - A Systematic Introduction（页面存档备份，存于互联网档案馆） by Raúl Rojas (ISBN 978-3540605058)\nImplementation of BackPropagation in C++（页面存档备份，存于互联网档案馆）\nImplementation of BackPropagation in C#（页面存档备份，存于互联网档案馆）\nImplementation of BackPropagation in Java（页面存档备份，存于互联网档案馆）\nAnother Implementation of BackPropagation in Java\nImplementation of BackPropagation in Ruby（页面存档备份，存于互联网档案馆）\nImplementation of BackPropagation in Python（页面存档备份，存于互联网档案馆）\nImplementation of BackPropagation in PHP（页面存档备份，存于互联网档案馆）\nQuick explanation of the backpropagation algorithm（页面存档备份，存于互联网档案馆）\nGraphical explanation of the backpropagation algorithm（页面存档备份，存于互联网档案馆）\nConcise explanation of the backpropagation algorithm using math notation（页面存档备份，存于互联网档案馆） by Anand Venkataraman\nBackpropagation neural network tutorial at the Wikiversity（页面存档备份，存于互联网档案馆）", "随机森林": "在機器學習中，隨機森林是一個包含多個決策樹的分類器，並且其輸出的類別是由個別樹輸出的類別的眾數而定。\n這個術語是1995年由貝爾實驗室的何天琴所提出的隨機決策森林（random decision forests）而來的。然后Leo Breiman和Adele Cutler發展出推論出隨機森林的演算法。而\"Random Forests\"是他們的商標。\n這個方法則是結合Breimans的\"Bootstrap aggregating\"想法和Ho的\"random subspace method\"以建造決策樹的集合。\n\n\n== 历史 ==\n随机森林的引入最初是由华裔美国人何天琴于1995年先提出的。然后随机森林由Leo Breiman于2001年在一篇论文中提出的。这篇文章描述了一种结合随机节点优化和bagging，利用类CART过程构建不相关树的森林的方法。此外，本文还结合了一些已知的、新颖的、构成了现代随机森林实践的基础成分，特别是\n\n使用out-of-bag误差来代替泛化误差\n通过排列度量变量的重要性\n\n\n== 算法 ==\n\n\n=== 预备：决策树学习 ===\n\n决策树是机器学習的常用方法。 Hastie等说：“树学习是如今最能满足于数据挖掘的方法，因为它在特征值的缩放和其他各种转换下保持不变，对无关特征是穩健的，而且能生成可被檢查的模型。然而，它通常並不準確。”特别的，生长很深的树容易学习到高度不规则的模式，即过学习，在训练集上具有低偏差和高變異數的特点。随机森林是平均多个深决策树以降低變異數的一种方法，其中，决策树是在一个数据集上的不同部分进行训练的。这是以偏差的小幅增加和一些可解释性的丧失为代價的，但是在最终的模型中通常会大大提高性能。\n\n\n=== Bagging ===\n\n随机森林训练算法把bagging的一般技术应用到树学习中。给定训练集X = x1, ..., xn和目标Y = y1, ..., yn，bagging方法重复（B次）从训练集中有放回地采样，然后在这些样本上训练树模型：\n\nFor b = 1, ..., B:\nSample, with replacement, n training examples from X, Y; call these Xb, Yb.\nTrain a classification or regression tree fb on Xb, Yb.在训练结束之后，对未知样本x的预测可以通过对x上所有单个回归树的预测求平均来实现：\n\n  \n    \n      \n        \n          \n            \n              f\n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            B\n          \n        \n        \n          ∑\n          \n            b\n            =\n            1\n          \n          \n            B\n          \n        \n        \n          f\n          \n            b\n          \n        \n        (\n        \n          x\n          ′\n        \n        )\n      \n    \n    {\\displaystyle {\\hat {f}}={\\frac {1}{B}}\\sum _{b=1}^{B}f_{b}(x')}\n  或者在分类任务中选择多数投票的类别。\n这种bagging方法在不增加偏置的情况下降低了方差，从而带来了更好的性能。这意味着，即使单个树模型的预测对训练集的噪声非常敏感，但对于多个树模型，只要这些树并不相关，这种情况就不会出现。简单地在同一个数据集上训练多个树模型会产生强相关的树模型（甚至是完全相同的树模型）。Bootstrap抽样是一种通过产生不同训练集从而降低树模型之间关联性的方法。\n此外，x'上所有单个回归树的预测的标准差可以作为预测的不确定性的估计：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                (\n                \n                  f\n                  \n                    b\n                  \n                \n                (\n                \n                  x\n                  ′\n                \n                )\n                −\n                \n                  \n                    \n                      f\n                      ^\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                B\n                −\n                1\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\frac {\\sum _{b=1}^{B}(f_{b}(x')-{\\hat {f}})^{2}}{B-1}}}.}\n  样本或者树的数量B是一个自由参数。通常使用几百到几千棵树，这取决于训练集的大小和性质。使用交叉验证，或者透過观察out-of-bag误差（那些不包含xᵢ的抽样集合在样本xᵢ的平均预测误差），可以找到最优的B值。当一些树训练到一定程度之后，训练集和测试集的误差开始趋于平稳。\n\n\n=== 从 bagging 到随机森林 ===\n上面的过程描述了树的原始的 bagging 算法。随机森林与这个通用的方案只有一点不同:它使用一种改进的学习算法，在学习过程中的每次候选分裂中选择特征的随机子集。这个过程有时又被称为“特征 bagging”。这样做的原因是 bootstrap 抽样导致的树的相关性：如果有一些特征预测目标值的能力很强，那么这些特征就会被许多树所选择，这样就会导致树的强相关性。何天琴分析了不同条件下 bagging 和随机子空间投影对精度提高的影响。典型地，对于一个包含 p 个特征的分类问题，可以在每次划分时使用 \n  \n    \n      \n        \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {p}}}\n   个特征。对于回归问题，作者推荐 p/3 但不少于 5 个特征。\n\n\n=== 极限树 ===\n再加上一个随机化步骤，就会得到极限随机树（extremely randomized trees），即极限树。与普通的随机森林相同，他们都是单个树的集成，但也有不同：首先，每棵树都使用整个学习样本进行了训练，其次，自上而下的划分是随机的。它并不计算每个特征的最优划分点（例如，基于信息熵或者基尼不纯度），而是随机选择划分点。该值是从特征经验范围内均匀随机选取的。在所有随机的划分点中，选择其中分数最高的作为结点的划分点。与普通的随机森林相似，可以指定每个节点要选择的特征的个数。该参数的默认值，对于分类问题，是\n  \n    \n      \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {n}}}\n  ，对于回归问题，是\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  ，其中 \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  是模型的特征个数。\n\n\n== 性质 ==\n\n\n=== 特征的重要性 ===\n随机森林天然可用来对回归或分类问题中变量的重要性进行排序。下面的技术来自Breiman的论文，R语言包randomForest包含它的实现。度量数据集 \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            n\n          \n        \n        =\n        {\n        (\n        \n          X\n          \n            i\n          \n        \n        ,\n        \n          Y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}_{n}=\\{(X_{i},Y_{i})\\}_{i=1}^{n}}\n  的特征重要性的第一步是，使用训练集训练一个随机森林模型。在训练过程中记录下每个数据点的out-of-bag误差，然后在整个森林上进行平均。\n为了度量第\n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  个特征的重要性，第\n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  个特征的值在训练数据中被打乱，并重新计算打乱后的数据的out-of-bag误差。则第\n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  个特征的重要性分数可以通过计算打乱前后的out-of-bag误差的差值的平均来得到，这个分数通过计算这些差值的标准差进行标准化。\n产生更大分数的特征比小分数的特征更重要。这种特征重要性的度量方法的统计定义由Zhu et al.给出。\n这种度量方法也有一些缺陷。对于包含不同取值个数的类别特征，随机森林更偏向于那些取值个数较多的特征，partial permutations、growing unbiased trees可以用来解决这个问题。如果数据包含一些相互关联的特征组，那么更小的组更容易被选择。\n\n\n=== 与最近邻算法的关系 ===\nLin和Jeon在2002年指出了随机森林算法和K-近邻算法(k-NN)的关系。 事实证明，这两种算法都可以被看作是所谓的“加权邻居的方案”。这些在数据集\n  \n    \n      \n        {\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{(x_{i},y_{i})\\}_{i=1}^{n}}\n  上训练的模型通过查看一个点的邻居来计算一个新点x'的预测值\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}}\n  ，并且使用权重函数W对这些邻居进行加权:\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          ′\n        \n        )\n        \n        \n          y\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {y}}=\\sum _{i=1}^{n}W(x_{i},x')\\,y_{i}.}\n  其中， \n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          ′\n        \n        )\n      \n    \n    {\\displaystyle W(x_{i},x')}\n  是第i个点在同一棵树中相对于新的数据点x'的非负权重。对于任一特定的点x'，\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  的权重的和必须为1。权重函数设定如下：\n\n对于k-NN算法，如果xi是距离x'最近的k个点之一，则\n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          ′\n        \n        )\n        =\n        \n          \n            1\n            k\n          \n        \n      \n    \n    {\\displaystyle W(x_{i},x')={\\frac {1}{k}}}\n  ，否则为0。\n对于树，如果xi与x'属于同一个包含k'个点的叶结点，则\n  \n    \n      \n        W\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          ′\n        \n        )\n        =\n        \n          \n            1\n            \n              k\n              ′\n            \n          \n        \n      \n    \n    {\\displaystyle W(x_{i},x')={\\frac {1}{k'}}}\n  ，否则为0。因为森林平均了m棵树的预测，且这些树具有独立的权重函数\n  \n    \n      \n        \n          W\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle W_{j}}\n  ，故森林的预测值是：\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          W\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          x\n          ′\n        \n        )\n        \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                1\n                m\n              \n            \n            \n              ∑\n              \n                j\n                =\n                1\n              \n              \n                m\n              \n            \n            \n              W\n              \n                j\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            ,\n            \n              x\n              ′\n            \n            )\n          \n          )\n        \n        \n        \n          y\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {y}}={\\frac {1}{m}}\\sum _{j=1}^{m}\\sum _{i=1}^{n}W_{j}(x_{i},x')\\,y_{i}=\\sum _{i=1}^{n}\\left({\\frac {1}{m}}\\sum _{j=1}^{m}W_{j}(x_{i},x')\\right)\\,y_{i}.}\n  上式表明了整个森林也采用了加权的邻居方案，其中的权重是各个树的平均。在这里，x'的邻居是那些在任一树中属于同一个叶节点的点\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  。只要\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  与x'在某棵树中属于同一个叶节点，\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  就是x'的邻居。\n\n\n== 基于随机森林的非监督学习 ==\n作为构建的一部分，随机森林预测器自然会导致观测值之间的不相似性度量。还可以定义未标记数据之间的随机森林差异度量：其思想是构造一个随机森林预测器，将“观测”数据与适当生成的合成数据区分开来。 观察到的数据是原始的未标记数据，合成数据是从参考分布中提取的。随机森林的不相似性度量之所以吸引人，是因为它能很好地处理混合变量类型，对输入变量的单调变换是不敏感的，而且在存在异常值的情况下度量结果依然可靠。由于其固有变量的选择，随机森林不相似性很容易处理大量的半连续变量。\n\n\n== 學習演算法 ==\n根據下列演算法而建造每棵樹：\n\n用N來表示訓練用例（样本）的個數，M表示特征数目。\n输入特征数目m，用于确定决策树上一个节点的决策结果；其中m應远小於M。\n從N個訓練用例（样本）中以有放回抽样的方式，取樣N次，形成一个训练集（即bootstrap取樣），並用未抽到的用例（样本）作預測，評估其誤差。\n對於每一個節點，隨機選擇m個特征，决策树上每个节点的决定都是基于这些特征确定的。根據這m個特征，計算其最佳的分裂方式。\n每棵樹都會完整成長而不會剪枝（Pruning，這有可能在建完一棵正常樹狀分類器後會被採用）。\n\n\n== 優點 ==\n隨機森林的優點有：\n\n對於很多種資料，它可以產生高準確度的分類器。\n它可以處理大量的輸入變數。\n它可以在決定類別時，評估變數的重要性。\n在建造森林時，它可以在內部對於一般化後的誤差產生不偏差的估計。\n它包含一個好方法可以估計遺失的資料，並且，如果有很大一部分的資料遺失，仍可以維持準確度。\n它提供一個實驗方法，可以去偵測variable interactions。\n對於不平衡的分類資料集來說，它可以平衡誤差。\n它計算各例中的親近度，對於数据挖掘、偵測離群點（outlier）和將資料視覺化非常有用。\n使用上述。它可被延伸應用在未標記的資料上，這類資料通常是使用非監督式聚類。也可偵測偏離者和觀看資料。\n學習過程是很快速的。\n\n\n== 开源实现 ==\nThe Original RF （页面存档备份，存于互联网档案馆） by Breiman and Cutler written in Fortran 77.\nALGLIB （页面存档备份，存于互联网档案馆） contains a modification of the random forest in C#, C++, Pascal, VBA.\nparty （页面存档备份，存于互联网档案馆） Implementation based on the conditional inference trees in R.\nrandomForest （页面存档备份，存于互联网档案馆） for classification and regression in R.\nPython implementation （页面存档备份，存于互联网档案馆） with examples in scikit-learn.\nOrange data mining suite includes random forest learner and can visualize the trained forest.\nMatlab （页面存档备份，存于互联网档案馆） implementation.\nSQP （页面存档备份，存于互联网档案馆） software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.\nWeka RandomForest （页面存档备份，存于互联网档案馆） in Java library and GUI.\nranger （页面存档备份，存于互联网档案馆） A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for R.\n\n\n== 参阅 ==\n提升方法 - 机器学习方法\n决策树学习  - 机器学习算法\n集成学习 - 统计与机器学习技术\n無母數統計\n随机化算法 - 将一定程度的随机性作为其逻辑或程序的一部分的算法\n\n\n== 参考文献 ==\n\n\n== 外部連結 ==\n（英文）Ho, Tin Kam (1995). \"Random Decision Forest\". Proc. of the 3rd Int'l Conf. on Document Analysis and Recognition,  Montreal, Canada, August 14-18, 1995, 278-282（Preceding Work）\n（英文）Ho, Tin Kam (1998). \"The Random Subspace Method for Constructing Decision Forests\".  IEEE Trans. on Pattern Analysis and Machine Intelligence 20 (8), 832-844（Preceding Work）\n（英文）Deng, H; Runger, G; Tuv, Eugene (2011). Bias of importance measures for multi-valued attributes and solutions, Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN2011)\n（英文）Amit, Yali and Geman, Donald (1997) \"Shape quantization and recognition with randomized trees\". Neural Computation 9, 1545-1588. （页面存档备份，存于互联网档案馆）（Preceding work）\n（英文）Breiman, Leo \"Looking Inside The Black Box\". Wald Lecture II（Lecture）\n（英文）Breiman, Leo (2001).  \"Random Forests\".  Machine Learning 45 (1), 5-32（Original Article）\n（英文）Random Forest classifier description（Site of Leo Breiman）\n（英文）Liaw, Andy & Wiener, Matthew \"Classification and Regression by randomForest\" R News (2002) Vol. 2/3 p. 18 （页面存档备份，存于互联网档案馆）（Discussion of the use of the random forest package for R）\n（英文）Ho, Tin Kam (2002). \"A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors\". Pattern Analysis and Applications 5, p. 102-112 （页面存档备份，存于互联网档案馆）（Comparison of bagging and random subspace method）", "K-平均算法": "k-均值算法（英文：k-means clustering）源于信号处理中的一种向量量化方法，现在则更多地作为一种聚类分析方法流行于数据挖掘领域。k-平均聚类的目的是：把\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  个点（可以是样本的一次观察或一个实例）划分到k个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。这个问题将归结为一个把数据空间划分为Voronoi cells的问题。\n这个问题在计算上是NP困难的，不过存在高效的启发式算法。一般情况下，都使用效率比较高的启发式算法，它们能够快速收敛于一个局部最优解。这些算法通常类似于通过迭代优化方法处理高斯混合分布的最大期望算法（EM算法）。而且，它们都使用聚类中心来为数据建模；然而k-平均聚类倾向于在可比较的空间范围内寻找聚类，期望-最大化技术却允许聚类有不同的形状。\nk-平均聚类与k-近邻之间没有任何关系（后者是另一流行的机器学习技术）。\n\n\n== 描述 ==\n已知观测集\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{1},x_{2},...,x_{n})}\n  ，其中每个观测都是一个\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  -维实向量，k-平均聚类要把这\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  个观测划分到k个集合中(k≤n),使得组内平方和（WCSS within-cluster sum of squares）最小。换句话说，它的目标是找到使得下式满足的聚类\n  \n    \n      \n        \n          S\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle S_{i}}\n  ，\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            \n              S\n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            k\n          \n        \n        \n          ∑\n          \n            \n              x\n            \n            ∈\n            \n              S\n              \n                i\n              \n            \n          \n        \n        \n          \n            ‖\n            \n              \n                x\n              \n              −\n              \n                \n                  μ\n                \n                \n                  i\n                \n              \n            \n            ‖\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}}\n  \n其中\n  \n    \n      \n        \n          μ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mu _{i}}\n  是\n  \n    \n      \n        \n          S\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle S_{i}}\n  中所有点的均值。\n\n\n== 历史源流 ==\n虽然其思想能够追溯到1957年的胡戈·施泰因豪斯\n \n，术语“k-均值”于1967年才被詹姆斯·麥昆（James MacQueen）\n \n首次使用。标准算法则是在1957年被史都華·勞埃德（Stuart Lloyd）作为一种脉冲码调制的技术所提出，但直到1982年才被贝尔实验室公开出版\n \n。在1965年，E·W·弗吉（E. W. Forgy）发表了本质上相同的方法，所以这一算法有时被称为勞埃德-弗吉方法。更高效的版本则被J·A·哈蒂根（J. A. Hartigan）和M·A·王（M. A. Wong）提出（1975/1979）\n。\n\n\n== 算法 ==\n\n\n=== 标准算法 ===\n最常用的算法使用了迭代优化的技术。它被称为k-平均算法而广为使用，有时也被称为Lloyd算法（尤其在计算机科学领域）。已知初始的k个均值点\n  \n    \n      \n        \n          m\n          \n            1\n          \n          \n            (\n            t\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          m\n          \n            k\n          \n          \n            (\n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle m_{1}^{(t)},...,m_{k}^{(t)}}\n  ,算法的按照下面两个步骤交替进行\n\n：\n\n分配(Assignment)：将每个观测分配到聚类中，使得组内平方和（WCSS）达到最小。因为这一平方和就是平方后的欧氏距离，所以很直观地把观测分配到离它最近的均值点即可\n \n。（数学上，这意味依照由这些均值点生成的Voronoi图来划分上述观测）。\n\n  \n    \n      \n        \n          S\n          \n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          {\n          \n            \n              x\n              \n                p\n              \n            \n            :\n            \n              \n                ‖\n                \n                  \n                    x\n                    \n                      p\n                    \n                  \n                  −\n                  \n                    m\n                    \n                      i\n                    \n                    \n                      (\n                      t\n                      )\n                    \n                  \n                \n                ‖\n              \n              \n                2\n              \n            \n            ≤\n            \n              \n                ‖\n                \n                  \n                    x\n                    \n                      p\n                    \n                  \n                  −\n                  \n                    m\n                    \n                      j\n                    \n                    \n                      (\n                      t\n                      )\n                    \n                  \n                \n                ‖\n              \n              \n                2\n              \n            \n            ∀\n            j\n            ,\n            1\n            ≤\n            j\n            ≤\n            k\n          \n          }\n        \n      \n    \n    {\\displaystyle S_{i}^{(t)}=\\left\\{x_{p}:\\left\\|x_{p}-m_{i}^{(t)}\\right\\|^{2}\\leq \\left\\|x_{p}-m_{j}^{(t)}\\right\\|^{2}\\forall j,1\\leq j\\leq k\\right\\}}\n  \n其中每个\n  \n    \n      \n        \n          x\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle x_{p}}\n  都只被分配到一个确定的聚类\n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S^{t}}\n  中，尽管在理论上它可能被分配到2个或者更多的聚类。\n\n更新(Update)：对于上一步得到的每一个聚类，以聚类中观测值的图心，作为新的均值点。\n  \n    \n      \n        \n          m\n          \n            i\n          \n          \n            (\n            t\n            +\n            1\n            )\n          \n        \n        =\n        \n          \n            1\n            \n              |\n              \n                S\n                \n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              |\n            \n          \n        \n        \n          ∑\n          \n            \n              x\n              \n                j\n              \n            \n            ∈\n            \n              S\n              \n                i\n              \n              \n                (\n                t\n                )\n              \n            \n          \n        \n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle m_{i}^{(t+1)}={\\frac {1}{\\left|S_{i}^{(t)}\\right|}}\\sum _{x_{j}\\in S_{i}^{(t)}}x_{j}}\n  \n因为算术平均是最小平方估计，所以这一步同样减小了目标函数组内平方和（WCSS）的值。\n这一算法将在对于观测的分配不再变化时收敛。由于交替进行的两个步骤都会减小目标函数WCSS的值，并且分配方案只有有限种，所以算法一定会收敛于某一（局部）最优解。注意：使用这一算法无法保证得到全局最优解。\n这一算法经常被描述为“把观测按照距离分配到最近的聚类”。标准算法的目标函数是组内平方和（WCSS），而且按照“最小平方和”来分配观测，确实是等价于按照最小欧氏距离来分配观测的。如果使用不同的距离函数来代替（平方）欧氏距离，可能使得算法无法收敛。然而，使用不同的距离函数，也能得到k-均值聚类的其他变体，如球体k-均值算法和k-中心点算法。\n\n\n=== 初始化方法 ===\n通常使用的初始化方法有Forgy和随机划分(Random Partition)方法\n\n。Forgy方法随机地从数据集中选择k个观测作为初始的均值点；而随机划分方法则随机地为每一观测指定聚类，然后运行“更新(Update)”步骤,即计算随机分配的各聚类的图心，作为初始的均值点。Forgy方法易于使得初始均值点散开，随机划分方法则把均值点都放到靠近数据集中心的地方。参考Hamerly et al的文章\n\n，可知随机划分方法一般更适用于k-调和均值和模糊k-均值算法。对于期望-最大化(EM)算法和标准k-均值算法，Forgy方法作为初始化方法的表现会更好一些。\n这是一个启发式算法，无法保证收敛到全局最优解，并且聚类的结果会依赖于初始的聚类。又因为算法的运行速度通常很快，所以一般都以不同的起始状态运行多次来得到更好的结果。不过，在最差的情况下，k-均值算法会收敛地特别慢：尤其是已经证明了存在这一的点集（甚至在2维空间中），使得k-均值算法收敛的时间达到指数级（\n  \n    \n      \n        \n          2\n          \n            Ω\n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle 2^{\\Omega (n)}}\n  ）\n\n。好在在现实中，这样的点集几乎不会出现：因为k-均值算法的平滑运行时间是多项式时间的\n\n。\n注：把“分配”步骤视为“期望”步骤，把“更新”步骤视为“最大化步骤”，可以看到，这一算法实际上是广义期望-最大化算法（GEM）的一个变体。\n\n\n== 复杂度 ==\n在\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  维空间中找到k-均值聚类问题的最优解的计算复杂度：\n\nNP-hard：一般欧式空间中，即使目标聚类数仅为2\nNP困难：平面中，不对聚类数目k作限制\n如果k和\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  都是固定的，时间复杂度为\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            d\n            k\n            +\n            1\n          \n        \n        l\n        o\n        g\n        n\n        )\n      \n    \n    {\\displaystyle O(n^{dk+1}logn)}\n  ,其中\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  为待聚类的观测点数目相比之下，Lloyds算法的运行时间通常为\n  \n    \n      \n        O\n        (\n        n\n        k\n        d\n        i\n        )\n      \n    \n    {\\displaystyle O(nkdi)}\n  ,k和\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  定义如上，\n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  为直到收敛时的迭代次数。如果数据本身就有一定的聚类结构，那么收敛所需的迭代数目通常是很少的，并且进行少数迭代之后，再进行迭代的话，对于结果的改善效果很小。鉴于上述原因，Lloyds算法在实践中通常被认为几乎是线性复杂度的。\n下面有几个关于这一算法复杂度的近期研究：\n\nLloyd's k-均值算法具有多项式平滑运行时间。对于落在空间\n  \n    \n      \n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle [0,1]^{d}}\n  任意的\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  点集合，如果每一个点都独立地受一个均值为\n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  ，标准差为\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  的正态分布所影响，那么k-均值算法的期望运行时间上界为\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            34\n          \n        \n        \n          k\n          \n            34\n          \n        \n        \n          d\n          \n            8\n          \n        \n        l\n        o\n        \n          g\n          \n            4\n          \n        \n        (\n        n\n        )\n        \n          /\n        \n        \n          σ\n          \n            6\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{34}k^{34}d^{8}log^{4}(n)/\\sigma ^{6})}\n  ，即对于\n  \n    \n      \n        n\n        ,\n        k\n        ,\n        i\n        ,\n        d\n      \n    \n    {\\displaystyle n,k,i,d}\n  和\n  \n    \n      \n        1\n        \n          /\n        \n        σ\n      \n    \n    {\\displaystyle 1/\\sigma }\n  都是多项式时间的。\n在更简单的情况下，有更好的上界。例如，在整数网格\n  \n    \n      \n        \n          \n            {\n            \n              1\n              ,\n              .\n              .\n              .\n              ,\n              M\n            \n            }\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\left\\{1,...,M\\right\\}^{d}}\n  中，k-均值算法运行时间的上界为\n  \n    \n      \n        O\n        (\n        d\n        \n          n\n          \n            4\n          \n        \n        \n          M\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(dn^{4}M^{2})}\n  。\n\n\n== 算法的变体 ==\n\n\n== 更多的讨论 ==\n使得k-均值算法效率很高的两个关键特征同时也被经常被视为它最大的缺陷：\n\n聚类数目k是一个输入参数。选择不恰当的k值可能会导致糟糕的聚类结果。这也是为什么要进行特征检查来决定数据集的聚类数目了。\n收敛到局部最优解，可能导致“反直观”的错误结果。k-均值算法的一个重要的局限性即在于它的聚类模型。这一模型的基本思想在于：得到相互分离的球状聚类，在这些聚类中，均值点趋向收敛于聚类中心。\n一般会希望得到的聚类大小大致相当，这样把每个观测都分配到离它最近的聚类中心（即均值点）就是比较正确的分配方案。\nk-均值聚类的结果也能理解为由均值点生成的Voronoi cells。\n\n\n== 相关应用 ==\nk-均值聚类（尤其是使用如Lloyd's算法的启发式方法的聚类）即使是在巨大的数据集上也非常容易部署实施。正因为如此，它在很多领域都得到成功的应用，如市场划分、机器视觉、 地质统计学、天文学和农业等。它经常作为其他算法的预处理步骤，比如要找到一个初始设置。\n\n\n=== 向量的量化 ===\n\nk-均值起源于信号处理领域，并且现在也能在这一领域找到应用。例如在计算机图形学中，色彩量化的任务，就是要把一张图像的色彩范围减少到一个固定的数目k上来。k-均值算法就能很容易地被用来处理这一任务，并得到不错的结果。其它得向量量化的例子有非随机抽样，在这里，为了进一步的分析，使用k-均值算法能很容易的从大规模数据集中选出k个合适的不同观测。\n\n\n=== 聚类分析 ===\n\n在聚类分析中，k-均值算法被用来将输入数据划分到k个部分(聚类)中。\n然而，纯粹的k-均值算法并不是非常灵活，同样地，在使用上有一定局限（不过上面说到的向量量化，确实是一个理想的应用场景）。特别是，当没有额外的限制条件时，参数k是很难选择的（正如上面讨论过的一样）。算法的另一个限制就是它不能和任意的距离函数一起使用、不能处理非数值数据。而正是为了满足这些使用条件，许多其他的算法才被发展起来。\n\n\n=== 特征学习 ===\n在（半）监督学习或无监督学习中，k-均值聚类被用来进行特征学习（或字典学习）步骤。基本方法是，首先使用输入数据训练出一个k-均值聚类表示，然后把任意的输入数据投射到这一新的特征空间。\nk-均值的这一应用能成功地与自然语言处理和计算机视觉中半监督学习的简单线性分类器结合起来。在对象识别任务中，它能展现出与其他复杂特征学习方法（如自动编码器、受限Boltzmann机等）相当的效果。然而，相比复杂方法，它需要更多的数据来达到相同的效果，因为每个数据点都只贡献了一个特征（而不是多重特征）。\n\n\n== 与其他统计机器学习方法的关系 ==\nk-均值聚类，以及它与EM算法的联系，是高斯混合模型的一个特例。很容易能把k-均值问题一般化为高斯混合模型。另一个k-均值算法的推广则是k-SVD算法，后者把数据点视为“编码本向量”的稀疏线性组合。而k-均值对应于使用单编码本向量的特殊情形（其权重为1）。\n\n\n=== Mean Shift 聚类 ===\n基本的Mean Shift聚类要维护一个与输入数据集规模大小相同的数据点集。初始时，这一集合就是输入集的副本。然后对于每一个点，用一定距离范围内的所有点的均值来迭代地替换它。与之对比，k-均值把这样的迭代更新限制在（通常比输入数据集小得多的）K个点上，而更新这些点时，则利用了输入集中与之相近的所有点的均值（亦即，在每个点的Voronoi划分内）。还有一种与k-均值类似的Mean shift算法，即 似然Mean shift，对于迭代变化的集合，用一定距离内在输入集中所有点的均值来更新集合里的点。Mean Shift聚类与k-均值聚类相比，有一个优点就是不用指定聚类数目，因为Mean shift倾向于找到尽可能少的聚类数目。然而，Mean shift会比k-均值慢得多，并且同样需要选择一个“宽度”参数。和k-均值一样，Mean shift算法有许多变体。\n\n\n=== 主成分分析（PCA） ===\n有一些研究表明，k-均值的放松形式解（由聚类指示向量表示），可由主成分分析中的主成分给出，并且主成分分析由主方向张成的子空间与聚类图心空间是等价的。不过，主成分分析是k-均值聚类的有效放松形式并不是一个新的结果(如，见)，并且还有的研究结果直接揭示了关于“聚类图心子空间是由主成分方向张成的”这一论述的反例\n。\n\n\n=== 独立成分分析(ICA) ===\n有研究表明，在稀疏假设以及输入数据经过白化的预处理后，k-均值得到的解就是独立成分分析的解。这一结果对于解释k-均值在特征学习方面的成功应用很有帮助。\n\n\n=== 双向过滤 ===\nk-均值算法隐含地假设输入数据的顺序不影响结果。双向过滤与k-均值算法和Mean shift算法类似之处在于它同样维护着一个迭代更新的数据集（亦是被均值更新）。然而，双向过滤限制了均值的计算只包含了在输入数据中顺序相近的点，这使得双向过滤能够被应用在图像去噪等数据点的空间安排是非常重要的问题中。\n\n\n== 相似问题 ==\n目标函数是使得聚类平方误差最小化的算法还有k-中心点算法，该方法保持聚类的中心在一个真实数据点上，亦即使用中心而非图心作为均值点。\n\n\n== 参考资料 ==\n\n\n== 外部链接 ==\nNumerical Example of k-means clustering （页面存档备份，存于互联网档案馆）\nApplication example which uses k-means clustering to reduce the number of colors in images （页面存档备份，存于互联网档案馆）\nInteractive demo of the k-means-algorithm(Applet) （页面存档备份，存于互联网档案馆）\nAn example of multithreaded application which uses k-means in Java\nk-means application in php \nk-means application in image retrieval \nAnother animation of the k-means-algorithm （页面存档备份，存于互联网档案馆）", "突變 (遺傳演算法)": "在遺傳演算法裡面，突變是用來維持演算法裡面，族群（population）裡面每一個世代的染色體到下一個世代時，還能夠維持遺傳多樣性的一個遺傳運算元（genetic operator）。這個運算元設計上雷同於生物學方面的突變(這也就是這個命名的由來)。\n有一種經典的突變運算元範例，會牽涉到將染色體裡面任意位元從原來的狀態改掉的機率。常用來實做這種突變運算元的方法是對染色體字串的每個位元分別產生一個隨機變數。這個隨機變數用來告訴我們這個特定的位元是否要被修改。這種突變的過程叫做單點突變（single point mutation），因為他的設計基於生物學上的點突變。其他的突變方式包含倒置（inversion）和浮點突變（floating point mutation）。 當基因的設計以排列問題的形式限制，那突變的方式就會變成交換，倒置或者打亂。\n遺傳演算法裡面突變的目的在於維持並且提昇多樣性。突變應該要能夠藉由避免讓族群裡的染色體過於相近，來防範演算法掉入區域極值，因此減慢或者停止進化過程。相同的理由也可以用來解釋為何大多數遺傳演算法的系統避免只使用最適合的染色體來產生下一代 ，而是在一些比較適合的裡面隨機（或者半隨機）的選擇出一些來產生下一代。\n\n\n== 参见 ==\n遺傳演算法\n\n\n== 參考資料 ==\n\nhttp://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php （页面存档备份，存于互联网档案馆）\nhttps://web.archive.org/web/20101110022208/http://www.slidefinder.net/g/genetic_algorithm_%E9%81%BA%E5%82%B3%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95/11760836", "梯度下降法": "梯度下降法（英語：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，但是不該與近似積分的最陡下降法（英語：Method of steepest descent）混淆。\n要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n\n\n== 描述 ==\n\n梯度下降方法基于以下的观察：如果实值函数\n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {x} )}\n  在点\n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n  处可微且有定义，那么函数\n  \n    \n      \n        F\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {x} )}\n  在\n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\mathbf {a} }\n  点沿着梯度相反的方向  \n  \n    \n      \n        −\n        ∇\n        F\n        (\n        \n          a\n        \n        )\n      \n    \n    {\\displaystyle -\\nabla F(\\mathbf {a} )}\n    下降最多。\n因而，如果\n\n  \n    \n      \n        \n          b\n        \n        =\n        \n          a\n        \n        −\n        γ\n        ∇\n        F\n        (\n        \n          a\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {b} =\\mathbf {a} -\\gamma \\nabla F(\\mathbf {a} )}\n  对于一個足够小数值\n  \n    \n      \n        γ\n        >\n        0\n      \n    \n    {\\displaystyle \\gamma >0}\n  時成立，那么\n  \n    \n      \n        F\n        (\n        \n          a\n        \n        )\n        ≥\n        F\n        (\n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle F(\\mathbf {a} )\\geq F(\\mathbf {b} )}\n  。\n考虑到这一点，我们可以从函数\n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  的局部极小值的初始估计\n  \n    \n      \n        \n          \n            x\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{0}}\n  出发，并考虑如下序列\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            0\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            2\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle \\mathbf {x} _{0},\\mathbf {x} _{1},\\mathbf {x} _{2},\\dots }\n  使得\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          \n            x\n          \n          \n            n\n          \n        \n        −\n        \n          γ\n          \n            n\n          \n        \n        ∇\n        F\n        (\n        \n          \n            x\n          \n          \n            n\n          \n        \n        )\n        ,\n         \n        n\n        ≥\n        0\n      \n    \n    {\\displaystyle \\mathbf {x} _{n+1}=\\mathbf {x} _{n}-\\gamma _{n}\\nabla F(\\mathbf {x} _{n}),\\ n\\geq 0}\n  。因此可得到\n\n  \n    \n      \n        F\n        (\n        \n          \n            x\n          \n          \n            0\n          \n        \n        )\n        ≥\n        F\n        (\n        \n          \n            x\n          \n          \n            1\n          \n        \n        )\n        ≥\n        F\n        (\n        \n          \n            x\n          \n          \n            2\n          \n        \n        )\n        ≥\n        ⋯\n        ,\n      \n    \n    {\\displaystyle F(\\mathbf {x} _{0})\\geq F(\\mathbf {x} _{1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots ,}\n  如果顺利的话序列\n  \n    \n      \n        (\n        \n          \n            x\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\mathbf {x} _{n})}\n  收敛到期望的局部极小值。注意每次迭代步长\n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  可以改变。\n右侧的图片示例了这一过程，这里假设\n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线（水平集），即函数\n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。（一点处的梯度方向与通过该点的等高线垂直）。沿着梯度下降方向，将最终到达碗底，即函数\n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  局部極小值的点。\n\n\n=== 例子 ===\n梯度下降法处理一些复杂的非线性函数会出现问题，例如Rosenbrock函數\n\n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        (\n        1\n        −\n        x\n        \n          )\n          \n            2\n          \n        \n        +\n        100\n        (\n        y\n        −\n        \n          x\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle f(x,y)=(1-x)^{2}+100(y-x^{2})^{2}.\\quad }\n  其最小值在\n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n        =\n        (\n        1\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (x,y)=(1,1)}\n  处，数值为\n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x,y)=0}\n  。但是此函数具有狭窄弯曲的山谷，最小值\n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n        =\n        (\n        1\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (x,y)=(1,1)}\n  就在这些山谷之中，并且谷底很平。优化过程是之字形的向极小值点靠近，速度非常缓慢。\n\n下面这个例子也鲜明的示例了\"之字\"的上升（非下降），这个例子用梯度上升（非梯度下降）法求\n  \n    \n      \n        F\n        (\n        x\n        ,\n        y\n        )\n        =\n        sin\n        ⁡\n        \n          (\n          \n            \n              \n                1\n                2\n              \n            \n            \n              x\n              \n                2\n              \n            \n            −\n            \n              \n                1\n                4\n              \n            \n            \n              y\n              \n                2\n              \n            \n            +\n            3\n          \n          )\n        \n        cos\n        ⁡\n        (\n        2\n        x\n        +\n        1\n        −\n        \n          e\n          \n            y\n          \n        \n        )\n      \n    \n    {\\displaystyle F(x,y)=\\sin \\left({\\frac {1}{2}}x^{2}-{\\frac {1}{4}}y^{2}+3\\right)\\cos(2x+1-e^{y})}\n  的局部极大值（非局部极小值）。\n\n\n=== 缺点 ===\n梯度下降法的缺點包括：\n靠近局部極小值时速度减慢。\n直線搜索可能會產生一些問題。\n可能會“之字型”地下降。上述例子也已体现出了这些缺点。\n\n\n== 参阅 ==\n\n\n== 参考文献 ==\n\nMordecai Avriel (2003). Nonlinear Programming: Analysis and Methods. Dover Publishing. ISBN 0-486-43227-0.\nJan A. Snyman (2005). Practical Mathematical Optimization: An Introduction to Basic Optimization Theory and Classical and  New Gradient-Based Algorithms. Springer Publishing. ISBN 0-387-24348-8\n\n\n== 外部链接 ==\n（英文）Interactive examples of gradient descent and some step size selection methods （页面存档备份，存于互联网档案馆）\n（英文）Using gradient descent in C++, Boost, Ublas for linear regression （页面存档备份，存于互联网档案馆）", "条件概率": "本文定义了表征两个或者多个随机变量概率分布特点的术语。\n条件概率（英語：conditional probability）就是事件A在事件B发生的条件下发生的概率。条件概率表示为P（A|B），读作“A在B发生的条件下发生的概率”。\n联合概率表示两个事件共同发生的概率。A与B的联合概率表示为\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  或者\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle P(A,B)}\n  或者\n  \n    \n      \n        P\n        (\n        A\n        B\n        )\n      \n    \n    {\\displaystyle P(AB)}\n  。\n边缘概率是某个事件发生的概率。边缘概率是這樣得到的：在聯合概率中，把最終結果中不需要的那些事件合并成其事件的全概率而消失（對离散隨机變量用求和得全概率，對連續隨机變量用積分得全概率）。這稱為邊緣化（marginalization）。A的边缘概率表示为P（A），B的边缘概率表示为P（B）。\n需要注意的是，在这些定义中A与B之间不一定有因果或者时间序列关系。A可能会先于B发生，也可能相反，也可能二者同时发生。A可能会导致B的发生，也可能相反，也可能二者之间根本就没有因果关系。\n例如考虑一些可能是新的信息的概率条件性可以通过贝叶斯定理实现。\n\n\n== 定义 ==\n设 A 与 B 为样本空间 Ω 中的两个事件，其中 P(B)>0。那么在事件 B 发生的条件下，事件 A 发生的条件概率为：\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∩\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(A\\cap B)}{P(B)}}}\n  条件概率有时候也称为：后验概率。\n\n\n== 统计独立性 ==\n当且仅当两个随机事件A与B满足\n\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n         \n        =\n         \n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)\\ =\\ P(A)P(B)}\n  的时候，它们才是统计独立的，这样联合概率可以表示为各自概率的简单乘积。\n同样，对于两个独立事件A与B有\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n         \n        =\n         \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A|B)\\ =\\ P(A)}\n  以及\n\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n         \n        =\n         \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B|A)\\ =\\ P(B)}\n  。换句话说，如果A与B是相互独立的，那么A在B这个前提下的条件概率就是A自身的概率；同样，B在A的前提下的条件概率就是B自身的概率。\n\n\n== 互斥性 ==\n当且仅当A与B满足\n\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(A\\cap B)=0}\n  且\n\n  \n    \n      \n        P\n        (\n        A\n        )\n        ≠\n        0\n      \n    \n    {\\displaystyle P(A)\\neq 0}\n  ，\n  \n    \n      \n        P\n        (\n        B\n        )\n        ≠\n        0\n      \n    \n    {\\displaystyle P(B)\\neq 0}\n  的时候，A与B是互斥的。\n因此，\n\n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(A\\mid B)=0}\n  \n\n  \n    \n      \n        P\n        (\n        B\n        ∣\n        A\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(B\\mid A)=0}\n  。换句话说，如果B已经发生，由于A不能和B在同一场合下发生，那么A发生的概率为零；同样，如果A已经发生，那么B发生的概率为零。\n\n\n== 其它 ==\n如果事件\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  的概率\n  \n    \n      \n        P\n        (\n        B\n        )\n        >\n        0\n      \n    \n    {\\displaystyle P(B)>0}\n  ，那么\n  \n    \n      \n        Q\n        (\n        A\n        )\n        =\n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n      \n    \n    {\\displaystyle Q(A)=P(A|B)}\n  在所有事件\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  上所定义的函数\n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  就是概率测度。\n如果\n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(B)=0}\n  ，\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n      \n    \n    {\\displaystyle P(A|B)}\n  没有定义。\n条件概率可以用决策树进行计算。\n\n\n== 形式定义 ==\n考虑概率空间Ω(S, σ(S))，其中σ(S)是集S上的σ代数，Ω上对应于随机变量X的概率测度（可以理解为概率分布）为PX；又A∈σ(S)，PX(A)≥0（这里可以理解为事件A，A不是零测集）。则∀E∈σ(S)，可以定义集函数PX|A如下：\nPX|A(E)=PX(A∩E)/PX(A)。\n易知PX|A也是Ω上的概率测度，此测度称为X在A下的条件测度（条件概率分布）。\n独立性：设A，B∈σ(S)，称A，B在概率测度P下为相互独立的，若P(A∩E)=P(A)P(E)。\n\n\n== 条件概率谬论 ==\n条件概率的谬论是假设P（A|B）大致等于P（B|A）。数学家John Allen Paulos在他的《数学盲》一书中指出医生、律师以及其他受过很好教育的非统计学家经常会犯这样的错误。这种错误可以通过用实数而不是概率来描述数据的方法来避免。\nP（A|B）與P（B|A）的關係如下所示：\n\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        =\n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        \n          \n            \n              P\n              (\n              B\n              )\n            \n            \n              P\n              (\n              A\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(B|A)=P(A|B){\\frac {P(B)}{P(A)}}.}\n  。下面是一個虛構但寫實的例子，P（A|B）與P（B|A）的差距可能令人驚訝，同時也相當明顯。\n若想分辨某些個體是否有重大疾病，以便早期治療，我們可能會對一大群人進行檢驗。雖然其益處明顯可見，但同時，檢驗行為有一個地方引起爭議，就是有檢出假陽性的結果的可能：若有個未得疾病的人，卻在初檢時被誤檢為得病，他可能會感到苦惱煩悶，一直持續到更詳細的檢測顯示他並未得病為止。而且就算在告知他其實是健康的人後，也可能因此對他的人生有負面影響。\n這個問題的重要性，最適合用條件機率的觀點來解釋。\n假設人群中有1%的人罹患此疾病，而其他人是健康的。我們隨機選出任一個體，並將患病以disease、健康以well表示：\n\n  \n    \n      \n        P\n        (\n        \n          disease\n        \n        )\n        =\n        1\n        %\n        =\n        0.01\n      \n    \n    {\\displaystyle P({\\text{disease}})=1\\%=0.01}\n  ，\n  \n    \n      \n        P\n        (\n        \n          well\n        \n        )\n        =\n        99\n        %\n        =\n        0.99\n      \n    \n    {\\displaystyle P({\\text{well}})=99\\%=0.99}\n  。假設檢驗動作實施在未患病的人身上時，有1%的機率其結果為假陽性（陽性以positive表示）。意即：\n\n  \n    \n      \n        P\n        (\n        \n          positive\n        \n        \n          |\n        \n        \n          well\n        \n        )\n        =\n        1\n        %\n      \n    \n    {\\displaystyle P({\\text{positive}}|{\\text{well}})=1\\%}\n  ，而且\n  \n    \n      \n        P\n        (\n        \n          negative\n        \n        \n          |\n        \n        \n          well\n        \n        )\n        =\n        99\n        %\n      \n    \n    {\\displaystyle P({\\text{negative}}|{\\text{well}})=99\\%}\n  。最後，假設檢驗動作實施在患病的人身上時，有1%的機率其結果為假陰性（陰性以negative表示）。意即：\n\n  \n    \n      \n        P\n        (\n        \n          negative\n        \n        \n          |\n        \n        \n          disease\n        \n        )\n        =\n        1\n        %\n      \n    \n    {\\displaystyle P({\\text{negative}}|{\\text{disease}})=1\\%}\n  且\n  \n    \n      \n        P\n        (\n        \n          positive\n        \n        \n          |\n        \n        \n          disease\n        \n        )\n        =\n        99\n        %\n      \n    \n    {\\displaystyle P({\\text{positive}}|{\\text{disease}})=99\\%}\n  。現在，由計算可知：\n\n  \n    \n      \n        P\n        (\n        \n          well\n        \n        ∩\n        \n          negative\n        \n        )\n        =\n        P\n        (\n        \n          well\n        \n        )\n        ×\n        P\n        (\n        \n          negative\n        \n        \n          |\n        \n        \n          well\n        \n        )\n        =\n        99\n        %\n        ×\n        99\n        %\n        =\n        98.01\n        %\n      \n    \n    {\\displaystyle P({\\text{well}}\\cap {\\text{negative}})=P({\\text{well}})\\times P({\\text{negative}}|{\\text{well}})=99\\%\\times 99\\%=98.01\\%}\n  是整群人中健康、且測定為陰性者的比率。\n\n  \n    \n      \n        P\n        (\n        \n          disease\n        \n        ∩\n        \n          positive\n        \n        )\n        =\n        P\n        (\n        \n          disease\n        \n        )\n        ×\n        P\n        (\n        \n          positive\n        \n        \n          |\n        \n        \n          disease\n        \n        )\n        =\n        1\n        %\n        ×\n        99\n        %\n        =\n        0.99\n        %\n      \n    \n    {\\displaystyle P({\\text{disease}}\\cap {\\text{positive}})=P({\\text{disease}})\\times P({\\text{positive}}|{\\text{disease}})=1\\%\\times 99\\%=0.99\\%}\n  是整群人中得病、且測定為陽性者的比率。\n\n  \n    \n      \n        P\n        (\n        \n          well\n        \n        ∩\n        \n          positive\n        \n        )\n        =\n        P\n        (\n        \n          well\n        \n        )\n        ×\n        P\n        (\n        \n          positive\n        \n        \n          |\n        \n        \n          well\n        \n        )\n        =\n        99\n        %\n        ×\n        1\n        %\n        =\n        0.99\n        %\n      \n    \n    {\\displaystyle P({\\text{well}}\\cap {\\text{positive}})=P({\\text{well}})\\times P({\\text{positive}}|{\\text{well}})=99\\%\\times 1\\%=0.99\\%}\n  是整群人中被測定為假陽性者的比率。\n\n  \n    \n      \n        P\n        (\n        \n          disease\n        \n        ∩\n        \n          negative\n        \n        )\n        =\n        P\n        (\n        \n          disease\n        \n        )\n        ×\n        P\n        (\n        \n          negative\n        \n        \n          |\n        \n        \n          disease\n        \n        )\n        =\n        1\n        %\n        ×\n        1\n        %\n        =\n        0.01\n        %\n      \n    \n    {\\displaystyle P({\\text{disease}}\\cap {\\text{negative}})=P({\\text{disease}})\\times P({\\text{negative}}|{\\text{disease}})=1\\%\\times 1\\%=0.01\\%}\n  是整群人中被測定為假陰性者的比率。\n進一步得出：\n\n  \n    \n      \n        P\n        (\n        \n          positive\n        \n        )\n        =\n        P\n        (\n        \n          well\n        \n        ∩\n        \n          positive\n        \n        )\n        +\n        P\n        (\n        \n          disease\n        \n        ∩\n        \n          positive\n        \n        )\n        =\n        0.99\n        %\n        +\n        0.99\n        %\n        =\n        1.98\n        %\n      \n    \n    {\\displaystyle P({\\text{positive}})=P({\\text{well}}\\cap {\\text{positive}})+P({\\text{disease}}\\cap {\\text{positive}})=0.99\\%+0.99\\%=1.98\\%}\n  是整群人中被測出為陽性者的比率。\n\n  \n    \n      \n        P\n        (\n        \n          disease\n        \n        \n          |\n        \n        \n          positive\n        \n        )\n        =\n        \n          \n            \n              P\n              (\n              \n                disease\n              \n              ∩\n              \n                positive\n              \n              )\n            \n            \n              P\n              (\n              \n                positive\n              \n              )\n            \n          \n        \n        =\n        \n          \n            \n              0.99\n              %\n            \n            \n              1.98\n              %\n            \n          \n        \n        =\n        50\n        %\n      \n    \n    {\\displaystyle P({\\text{disease}}|{\\text{positive}})={\\frac {P({\\text{disease}}\\cap {\\text{positive}})}{P({\\text{positive}})}}={\\frac {0.99\\%}{1.98\\%}}=50\\%}\n  是某人被測出為陽性時，實際上真的得了病的機率。\n這個例子裡面，我們很輕易可以看出P(positive|disease)=99%與P(disease|positive)=50%的差距：前者是你得了病，而被檢出為陽性的條件機率；後者是你被檢出為陽性，而你實際上真得了病的條件機率。由我們在本例中所選的數字，最終結果可能令人難以接受：被測定為陽性者，其中的半數實際上是假陽性。\n\n\n== 参见 ==\n贝叶斯定理\n最大似然估计\n先验概率\n概率论\n蒙提霍尔问题\n后验概率\n条件期望", "统计分类": "统计分类是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。分类是监督学习的一个实例，根据已知训练集提供的样本，通过计算选择特征参数，建立判别函数以对样本进行的分类。与之相对的是無監督學習，例如聚类分析。\n\n\n== 参考文献 ==", "AdaBoost": "AdaBoost為英文\"Adaptive Boosting\"（自适应增强）的缩写，是一种机器学习方法，由約阿夫·弗羅因德和羅伯特·沙皮爾提出。AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。AdaBoost方法对于噪声数据和异常数据很敏感。但在一些问题中，AdaBoost方法相对于大多数其它学习算法而言，不会很容易出现过拟合现象。AdaBoost方法中使用的分类器可能很弱（比如出现很大错误率），但只要它的分类效果比随机好一点（比如两类问题分类错误率略小于0.5），就能够改善最终得到的模型。而错误率高于随机分类器的弱分类器也是有用的，因为在最终得到的多个分类器的线性组合中，可以给它们赋予负系数，同样也能提升分类效果。\nAdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。在具体实现上，最初令每个样本的权重都相等，对于第k次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器Ck。然后就根据这个分类器，来提高被它分错的的样本的权重，并降低被正确分类的样本权重。然后，权重更新过的样本集被用于训练下一个分类器Ck。整个训练过程如此迭代地进行下去。\n\n\n== AdaBoost算法 ==\n用xi和yi表示原始样本集D的样本点和它们的类标。用Wk(i)表示第k次迭代时全体样本的权重分布。这样就有如下所示的AdaBoost算法：\n\n初始化：输入参数为训练集D={x1，y1，...，xn，yn}，最大循环次数kmax，采样权重Wk(i)=1/n，i=1，...，n；\n迭代计数器k赋值为0；\n计数器k自增1；\n使用Wk(i)采样权重对弱学习器Ck进行训练；\n对弱学习器Ck的训练结果进行评估并记录进误差矩阵Ek中；\n\n  \n    \n      \n        \n          α\n          \n            k\n          \n        \n        ←\n        \n          \n            \n              1\n              2\n            \n          \n        \n        ln\n        ⁡\n        \n          \n            \n              1\n              −\n              \n                E\n                \n                  k\n                \n              \n            \n            \n              E\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}\\gets {\\tfrac {1}{2}}\\ln {\\frac {1-E_{k}}{E_{k}}}}\n  \n\n  \n    \n      \n        \n          W\n          \n            k\n            +\n            1\n          \n        \n        (\n        i\n        )\n        ←\n        \n          \n            \n              \n                \n                  W\n                  \n                    k\n                  \n                \n                (\n                i\n                )\n              \n              \n                Z\n                \n                  k\n                \n              \n            \n          \n        \n        ×\n        \n          \n            {\n            \n              \n                \n                  \n                    e\n                    \n                      −\n                      \n                        α\n                        \n                          k\n                        \n                      \n                    \n                  \n                  ,\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  \n                    h\n                    \n                      k\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  =\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                \n                  \n                    e\n                    \n                      \n                        α\n                        \n                          k\n                        \n                      \n                    \n                  \n                  ,\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  \n                    h\n                    \n                      k\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  )\n                  ≠\n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle W_{k+1}(i)\\gets {\\dfrac {W_{k}(i)}{Z_{k}}}\\times {\\begin{cases}e^{-\\alpha _{k}},&{\\mbox{if  }}h_{k}(x^{i})=y_{i}\\\\e^{\\alpha _{k}},&{\\mbox{if  }}h_{k}(x^{i})\\neq y_{i}\\end{cases}}}\n  \n当k=kmax时停止训练\n返回结果 Ck和αk，k=1，...，kmax（带权值分类器的总体）\n结束注意第5行中，当前权重分布必须考虑到分类器Ck的误差率。在第7行中，Zk只是一个归一化系数，使得Wk(i)能够代表一个真正的分布，而hk(xi)是分量分类器Ck给出的对任一样本点xi的标记（+1或-1），hk(xi) = yi时，样本被正确分类。第8行中的迭代停止条件可以被换为判断当前误差率是否小于一个阈值。\n最后的总体分类的判决可以使用各个分量分类器加权平均来得到：\n\n  \n    \n      \n        g\n        (\n        x\n        )\n        =\n        [\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            \n              k\n              \n                m\n                a\n                x\n              \n            \n          \n        \n        \n          α\n          \n            k\n          \n        \n        \n          h\n          \n            k\n          \n        \n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle g(x)=[\\sum _{k=1}^{k_{max}}\\alpha _{k}h_{k}(x)]}\n  \n这样，最后对分类结果的判定规则是：\n\n  \n    \n      \n        H\n        (\n        x\n        )\n        =\n        \n          \n            sign\n          \n        \n        \n          (\n          \n            g\n            (\n            x\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle H(x)={\\textrm {sign}}\\left(g(x)\\right)}\n  \n\n\n== 软件实现 ==\nAdaBoost and the Super Bowl of Classifiers - A Tutorial on AdaBoost.（页面存档备份，存于互联网档案馆）\nAdaboost in C++（页面存档备份，存于互联网档案馆）, an implementation of Adaboost in C++ and boost by Antonio Gulli\nicsiboost（页面存档备份，存于互联网档案馆）, an open source implementation of Boostexter\nJBoost（页面存档备份，存于互联网档案馆）, a site offering a classification and visualization package, implementing AdaBoost among other boosting algorithms.\nMATLAB AdaBoost toolbox. Includes Real AdaBoost, Gentle AdaBoost and Modest AdaBoost implementations.\nA Matlab Implementation of AdaBoost（页面存档备份，存于互联网档案馆）\nMulti-threaded MATLAB-compatible implementation of Boosted Trees（页面存档备份，存于互联网档案馆）\nmilk（页面存档备份，存于互联网档案馆） for Python implements AdaBoost.\nMPBoost++（页面存档备份，存于互联网档案馆）, a C++ implementation of the original AdaBoost.MH algorithm and of an improved variant, the MPBoost algorithm.\nmultiboost, a fast C++ implementation of multi-class/multi-label/multi-task boosting algorithms. It is based on AdaBoost.MH but also implements popular cascade classifiers and FilterBoost along with a batch of common multi-class base learners（stumps, trees, products, Haar filters）。\nNPatternRecognizer （页面存档备份，存于互联网档案馆）, a fast machine learning algorithm library written in C#. It contains support vector machine, neural networks, bayes, boost, k-nearest neighbor, decision tree, ..., etc.\nOpenCV implementation of several boosting variants\nInto contains open source implementations of many AdaBoost and FloatBoost variants in C++.\nMallet（页面存档备份，存于互联网档案馆） Java implementation.\nadabag adabag: An R package for binary and multiclass Boosting and Bagging.\nScikit-learn Python implementation.\n\n\n== 参考书目 ==", "假說檢定": "假說檢定（英語：hypothesis testing）是推論統計中用于检验统计假设的一种方法。而“统计假设”是可通过观察一组随机变量的模型进行检验的科学假说。一旦能估計未知母數，就會希望根據結果對未知的真正參數值做出適當的推論。\n\n統計上對參數的假設，就是對一個或多個參數的論述。而其中欲檢驗其正確性的為虛無假說（Null hypothesis，記為\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ），虛無假設通常由研究者決定，反映研究者對未知參數的看法。相對於虛無假說的其他有關參數之論述是對立假說（Alternative hypothesis，記為\n  \n    \n      \n        \n          H\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle H_{a}}\n  或\n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  ），它通常反應了執行檢定的研究者對參數可能數值的另一種（對立的）看法（換句話說，對立假說通常才是研究者最想知道的）。\n假设检验的种类包括：t检验，Z检验，卡方检验，F检验等等。\n\n\n== 說明 ==\n假設檢定的過程，可以用法庭的審理來說明。先想像現在法庭上有一名被告，假設該被告是清白的，而檢察官必須要提出足夠的證據去證明被告的確有罪。\n在證明被告有罪前，被告是被假設為清白的。\n\n假設被告清白的假設，就相當於虛無假說。\n假設被告有罪的假設，則是對立假說。而檢察官提出的證據，是否足以確定該被告有罪，則要經過檢驗。這樣子的檢驗過程就相當於用T檢定或Z檢定去檢視研究者所搜集到的統計資料。\n\n\n== 檢定過程 ==\n在统计学的文献中，假设检验发挥了重要作用。假设检验大致有如下步骤：\n\n最初研究假设为真相不明。\n提出相关的虛無假說和對立假說。\n考虑检验中对样本做出的统计假设；例如，关于母體資料的分布形式或关于独立性的假设。无效的假设将意味此檢定的结果是无效的。\n选择一个顯著水準（α），若低于这个概率阈值，就拒绝零假设。最常用的是5%和1%。\n選擇適合的检验统计量（Test statistic）T。\n在設定虛無假說為真下推导检验统计量的分布。在标准情况下应该会得出一个熟知的结果。比如检验统计量可能会符合常態分布或司徒頓t分布。\n根據在零假设成立時的檢定統計量T分佈，找到機率為顯著水準 (α)的區域，此區域稱為「拒絕域」(記作RR或CR)，即在零假设成立的前提下，落在拒絕域的機率只有α。\n針對檢定統計量T，根據樣本計算其估計值tobs。\n若估計值tobs未落在拒絕域，則「不拒絕」虛無假說（do no reject \n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ）。若估計值tobs落在拒絕域，則拒絕零假设，接受對立假說。要注意的是一般不會將檢定結果稱作「接受」虛無假說，而是因沒有顯著證據證明虛無假說為非，所以「不拒絕」虛無假說。\n\n\n== 例子 ==\n女士品茶是一個有關假設檢定的著名例子。统计学家費雪的一個女同事，也是藻类学家的缪丽·布里斯托尔，她聲稱可以判斷在奶茶中是先加入茶還是先加入牛奶。費雪提議給她八杯奶茶。缪丽已知其中四杯先加茶，四杯先加牛奶，但隨機排列，而她要說出這八杯奶茶中，哪些先加牛奶，哪些先加茶，检验统计量是確認正確的次數。零假设是她無法判斷奶茶中的茶先加入還是牛奶先加入，對立假說為她有此能力。\n若單純以機率考慮（即缪丽沒有判斷的能力）下，八杯都正確的機率為1/70（这是个简单的组合问题），約1.43%，因此「拒絕域」為八杯的結果都正確。而測試結果為缪丽八杯的結果都正確，在統計上是相當顯著的的結果。也就是说，几乎可以排除她只是恰好猜对结果的可能。\n\n\n== 相關條目 ==\n型一錯誤與型二錯誤\n方差分析\n\n\n== 参考文献 ==", "算术平均数": "算术平均数（arithmetic mean）是表征数据集中趋势的一个统计指标。它是一组数据之和，除以这组数据个数/項数。\n算术平均数在统计学上的优点，就是它较中位数、众数更少受到随机因素影响，\n缺点是它更容易受到极端值影响。\n计算公式为：\n\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n            \n            n\n          \n        \n        =\n        \n          \n            \n              \n                x\n                \n                  1\n                \n              \n              +\n              \n                x\n                \n                  2\n                \n              \n              +\n              ⋯\n              +\n              \n                x\n                \n                  n\n                \n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}={\\frac {\\sum _{i=1}^{n}x_{i}}{n}}={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}}\n  在统计学中，对样本的平均值用 \n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {x}}}\n   表示，对母体数据的平均值用 \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n   表示。樣本平均數可作為母體平均數的一個不偏估計式。\n\n\n== 参考文献 ==\n\n\n== 参见 ==\n\n算术-几何平均数\n几何平均数\n调和平均数\n平方平均数\n平均数不等式", "混淆矩阵": "在機器學習領域和統計分類問題中，混淆矩阵（英語：confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。之所以如此命名，是因為通過這個矩陣可以方便地看出机器是否将两个不同的类混淆了（比如說把一個類錯當成了另一個）。\n混淆矩阵（也稱誤差矩陣）是一種特殊的, 具有兩個維度的（實際和預測）列联表（英語：contingency table），並且兩維度中都有著一樣的類別的集合。\n\n\n== 示例 ==\n如果已經訓練好了一個系統用來區分貓和狗，那混淆矩陣就可以概括算法的測試結果以便將來的檢查。假設一個13個動物的樣本，8隻貓和5隻狗，那混淆矩陣的結果可能如下表所示：\n\n在這個混淆矩陣中，系統預測了8只實際的貓，其中系統預測3只是狗，而5隻狗中，則預測有2只是貓。 所有正確的預測都位於表格的對角線上（以粗體突出顯示），因此很容易從視覺上檢查表格中的預測錯誤，因為它們將由對角線之外的值表示。\n\n\n== 混淆表 ==\n在預測分析中，混淆表（有時也稱為混淆矩陣）是具有兩行兩列的表，該表報告假陽性，假陰性，真陽性和真陰性的數量。這不僅可以進行正確分類（準確度）的分析，還可以進行更詳細的分析。對於分類器的真實性能，準確性不是可靠的指標，因為如果數據集不平衡（即，當不同類別中的觀察數發生很大變化時），它將產生誤導性結果。例如，如果數據中有95隻貓，只有5條狗，則特定的分類器可能會將所有觀察結果歸為貓。總體準確度為95％，但更詳細地，分類器對貓類別的識別率為100％（敏感性），對狗類別的識別率為0％。在這種情況下，F1得分（英語：F1 score）甚至更加不可靠，在這種情況下，F1得分將超過97.4％，而約登指數則消除了這種偏見，並且將0作為亂猜情況下能增加信息量的決定（英語：informed decision）的概率（這裡總是猜測貓）。約登指數為0的系統或測試不具有任何作用。\n\n  \n    \n      \n        J\n        =\n        \n          \n            95\n            \n              95\n              +\n              0\n            \n          \n        \n        +\n        \n          \n            0\n            \n              0\n              +\n              5\n            \n          \n        \n        −\n        1\n        =\n        0\n      \n    \n    {\\displaystyle J={\\frac {95}{95+0}}+{\\frac {0}{0+5}}-1=0}\n   （總是猜測貓的約登指數）\n\n\n== 参考文献 ==", "机器学习": "机器学习是人工智能的一个分支。人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径之一，即以机器学习为手段，解决人工智能中的部分问题。机器学习在近30多年已发展为一门多领域科际整合，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。\n机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法（要防止錯誤累積）。很多推论问题属于非程序化決策，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、游戏和机器人等领域。\n\n\n== 定义 ==\n机器学习有下面几种定义：\n\n机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。\n机器学习是对能通过经验自动改进的计算机算法的研究。\n机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。電腦科學家Tom M. Mitchell在其著作的Machine Learning一書中定义的機器學習為：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\n\n== 分类 ==\n机器学习可以分成下面几种类别：\n\n监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。监督学习和非监督学习的差别就是训练集目标是否有人为标注。他们都有训练集 且都有输入和输出\n\n无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成對抗網絡（GAN）、聚类。\n半监督学习介于监督学习与无监督学习之间。\n增强学习机器为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的。\n\n\n== 算法 ==\n具体的机器学习算法有：\n\n构造间隔理论分布：聚类分析和模式识别\n人工神经网络\n决策树\n感知器\n支援向量機\n集成学习AdaBoost\n降维与度量学习\n聚类\n贝叶斯分类器\n构造条件概率：回归分析和统计分类\n高斯过程回归\n线性判别分析\n最近邻居法\n径向基函数核\n通过再生模型构造概率密度函数：\n最大期望算法\n概率图模型：包括貝氏網路和Markov随机场\nGenerative Topographic Mapping\n近似推断技术：\n马尔可夫链\n蒙特卡罗方法\n变分法\n最优化：大多数以上方法，直接或者间接使用最优化算法。\n量子機器學習\n\n\n== 参考文献 ==\n\n\n=== 引用 ===\n\n\n=== 来源 ===\n\n\n== 外部链接 ==\n\n\n== 参见 ==", "混合模型": "在統計學中，混合模型（Mixture model）是用於表示母體中子母體的存在的機率模型，換句話說，混合模型表示了測量結果在母體中的機率分布，它是一個由數個子母體之機率分布組成的混合分布。混合模型不要求測量結果供關於各個子母體之機率分布的資訊即可計算測量結果在母體分布中的機率。\n\n\n== 高斯混合模型（Gaussian Mixture Model） ==\n對一維的隨機變數\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的高斯分佈存在以下機率密度函數：\n\n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n        =\n        \n          P\n          \n            X\n          \n        \n        (\n        X\n        ≤\n        x\n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                \n                  2\n                  π\n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          −\n          \n            \n              \n                (\n                x\n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                2\n                \n                  σ\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle F_{X}(x)=P_{X}(X\\leq x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}\\exp {(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}})}}\n  \n其中的\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  為\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的標準差，\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  為\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的期望值。\n而當將高斯分佈推廣到\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  維時，根據定義，若\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  維的隨機向量\n  \n    \n      \n        X\n        =\n        [\n        \n          X\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        \n          X\n          \n            k\n          \n        \n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X=[X_{1},...X_{k}]^{T}}\n  服從多變數的常態分佈，則存在一個對稱半正定的共變異數矩陣\n  \n    \n      \n        Σ\n      \n    \n    {\\displaystyle \\Sigma }\n  以及期望值向量\n  \n    \n      \n        μ\n        =\n        [\n        \n          μ\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          μ\n          \n            k\n          \n        \n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mu =[\\mu _{1},...,\\mu _{k}]^{T}}\n  滿足\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的特徵函數。若\n  \n    \n      \n        Σ\n      \n    \n    {\\displaystyle \\Sigma }\n  為非奇異的，則此分佈可以由以下的機率密度函數描述：\n\n  \n    \n      \n        \n          \n            \n              f\n              \n                \n                  x\n                \n              \n            \n            (\n            \n              x\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              x\n              \n                k\n              \n            \n            )\n            =\n            \n              \n                1\n                \n                  (\n                  2\n                  π\n                  \n                    )\n                    \n                      k\n                    \n                  \n                  \n                    |\n                  \n                  \n                    Σ\n                  \n                  \n                    |\n                  \n                \n              \n            \n            \n              \n                e\n              \n              \n                −\n                \n                  \n                    1\n                    2\n                  \n                \n                (\n                \n                  \n                    x\n                  \n                \n                −\n                \n                  μ\n                \n                \n                  )\n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    Σ\n                  \n                  \n                    −\n                    1\n                  \n                \n                (\n                \n                  \n                    x\n                  \n                \n                −\n                \n                  μ\n                \n                )\n              \n            \n            ,\n          \n        \n      \n    \n    {\\displaystyle {\\displaystyle f_{\\mathbf {x} }(x_{1},\\ldots ,x_{k})={\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\mathrm {e} ^{-{\\frac {1}{2}}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\mathrm {T} }{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})},}}\n  \n  \n    \n      \n        \n          |\n        \n        Σ\n        \n          |\n        \n      \n    \n    {\\displaystyle |\\Sigma |}\n  為共變異數矩陣的行列式。\n而高斯混合模型为单一高斯概率密度函数的延伸，用多个高斯概率密度函数（正态分布曲线）精确地量化变量分布，是将变量分布分解为若干基于高斯概率密度函数（正态分布曲线）分布的统计子模型，每個子模型可視為此混合模型的隱變量。\n舉一個不是那麼嚴謹的例子，若是我們手上有一個班級中所有學生某一次考試的各項科目分數分佈，並且每一科的分數都大致依照高斯分佈。則當我們要描述每個學生的總分分佈時，單高斯模型及多維的高斯模型不一定能很好的描述這個分佈，因為每一科的分布的情形都不盡相同，此時我們可以用高斯混合分佈更好的來描述這個問題。", "似然函数": "在数理统计学中，似然函数（英語：likelihood function）是一种关于统计模型中的参数的函数，表示模型参数中的似然性（英語：likelihood）。似然函数在統計推論中有重大作用，如在最大似然估计和费雪信息之中的应用等等。文字意義上，“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“概率”（或然性）有明确的区分：概率，用于在已知一些参数的情況下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是說已觀察到某事件後，對相關母數進行猜測。\n在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数B时，事件A会发生的概率写作：\n\n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A,B)}{P(B)}}\\!}\n  利用贝叶斯定理，\n\n  \n    \n      \n        P\n        (\n        B\n        ∣\n        A\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∣\n              B\n              )\n              \n              P\n              (\n              B\n              )\n            \n            \n              P\n              (\n              A\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(B\\mid A)={\\frac {P(A\\mid B)\\;P(B)}{P(A)}}\\!}\n  因此，我们可以反过来构造表示似然性的方法：已知有事件A发生，运用似然函数\n  \n    \n      \n        \n          L\n        \n        (\n        B\n        ∣\n        A\n        )\n      \n    \n    {\\displaystyle \\mathbb {L} (B\\mid A)}\n  ，我们估计或猜測参数B的不同值的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：\n\n  \n    \n      \n        b\n        ↦\n        P\n        (\n        A\n        ∣\n        B\n        =\n        b\n        )\n        \n      \n    \n    {\\displaystyle b\\mapsto P(A\\mid B=b)\\!}\n  注意到这里并不要求似然函数满足归一性：\n  \n    \n      \n        \n          ∑\n          \n            b\n            ∈\n            \n              \n                B\n              \n            \n          \n        \n        P\n        (\n        A\n        ∣\n        B\n        =\n        b\n        )\n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{b\\in {\\mathcal {B}}}P(A\\mid B=b)=1}\n  。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有\n  \n    \n      \n        α\n        >\n        0\n      \n    \n    {\\displaystyle \\alpha >0}\n  ，都可以有似然函数：\n\n  \n    \n      \n        L\n        (\n        b\n        ∣\n        A\n        )\n        =\n        α\n        \n        P\n        (\n        A\n        ∣\n        B\n        =\n        b\n        )\n        \n      \n    \n    {\\displaystyle L(b\\mid A)=\\alpha \\;P(A\\mid B=b)\\!}\n  \n\n\n== 例子 ==\n考虑投掷硬币的实验。通常来说，已知掷出一枚“公平的硬币”（即正面朝上和反面朝上的機率相同）時，正面（Head）朝上的概率为\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  ，我們可以此推論得知投掷若干次后出现各种结果的可能性。比如说，連續投两次都是正面朝上的概率是\n  \n    \n      \n        0.25\n      \n    \n    {\\displaystyle 0.25}\n  。用条件概率表示，就是：\n\n  \n    \n      \n        P\n        (\n        \n          \n            HH\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        )\n        =\n        \n          0.5\n          \n            2\n          \n        \n        =\n        0.25\n      \n    \n    {\\displaystyle P({\\mbox{HH}}\\mid p_{H}=0.5)=0.5^{2}=0.25}\n  其中\n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{H}}}\n  表示正面朝上。\n在统计学中，我们更关心的是在已知一系列投掷的结果时，关于單獨投擲一次硬币时正面朝上的機率（即\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  ）爲何。我們實際上是無法從一系列投擲的結果來逆推真實的\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  ，但是我們可以推估\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  是某個值的可能性爲何。舉例來說，假設因爲這可能不是一枚真正“公平的硬幣”，所以我們不知道\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  是多少，也無法計算投擲三次硬幣其中兩次是正面的機率是多少。現在如果我們真的實際去擲了三次硬幣，結果其中兩次爲正面，那我們是否能夠依此次實驗逆推出\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  的資訊？如果無法逆推出真實的\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  ，那我們有沒有辦法知道，譬如說\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  的可能性爲何？\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle p_{H}=0.6}\n  的可能性又爲何？或甚至再更退一步，至少我們能不能知道\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  跟\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle p_{H}=0.6}\n  哪一個比較有可能？\n投擲一次硬幣，正面朝上的機率用\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  來代表，它就是我們這個例子的母數，而我們用事件\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{A}}}\n  來代表投擲三次硬幣其中兩次是正面這個事實。使用聯合機率（英語：joint probability）計算可知\n\n  \n    \n      \n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        )\n        =\n        3\n        ×\n        \n          p\n          \n            H\n          \n          \n            2\n          \n        \n        ×\n        (\n        1\n        −\n        \n          p\n          \n            H\n          \n        \n        )\n      \n    \n    {\\displaystyle P({\\mbox{A}}\\mid p_{H})=3\\times p_{H}^{2}\\times (1-p_{H})}\n  我們首先假設\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  ，則看到三次投擲中兩次是正面的機率爲\n  \n    \n      \n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        )\n        =\n        0.375\n      \n    \n    {\\displaystyle P({\\mbox{A}}\\mid p_{H}=0.5)=0.375}\n  。再來如果假設\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle p_{H}=0.6}\n  ，則看到三次投擲中兩次是正面的機率爲\n  \n    \n      \n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n        )\n        =\n        0.432\n      \n    \n    {\\displaystyle P({\\mbox{A}}\\mid p_{H}=0.6)=0.432}\n  。顯然地，如果\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle p_{H}=0.6}\n  的話，我們看到兩個正面的機會比較高。所以當我們投擲了三次硬幣並且看到了兩次正面，即使我們無法知道實際\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  到底是多少，我們至少知道\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  是\n  \n    \n      \n        0.6\n      \n    \n    {\\displaystyle 0.6}\n  的可能性比是\n  \n    \n      \n        0.5\n      \n    \n    {\\displaystyle 0.5}\n  的可能性還要高。我們可以合理猜測，\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  比較可能是\n  \n    \n      \n        0.6\n      \n    \n    {\\displaystyle 0.6}\n  而非\n  \n    \n      \n        0.5\n      \n    \n    {\\displaystyle 0.5}\n  。\n這裏我們就引進了概似性的概念：概似性代表某個母數爲特定值的可能性。從上面例子得知在已觀察到事件\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{A}}}\n  的情況下，关于事件A的似然估计为\n\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        )\n      \n    \n    {\\displaystyle L(p_{H}\\mid {\\mbox{A}})=P({\\mbox{A}}\\mid p_{H})}\n  其中\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  为我们所要确定的参数。所以當我們投擲硬幣三次，其中兩次是正面，則\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  的概似性是\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        )\n        =\n        0.375\n      \n    \n    {\\displaystyle L(p_{H}=0.5\\mid {\\mbox{A}})=P({\\mbox{A}}\\mid p_{H}=0.5)=0.375}\n  ，而\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle p_{H}=0.6}\n  的概似性是\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        P\n        (\n        \n          \n            A\n          \n        \n        ∣\n        \n          p\n          \n            H\n          \n        \n        =\n        0.6\n        )\n        =\n        0.432\n      \n    \n    {\\displaystyle L(p_{H}=0.6\\mid {\\mbox{A}})=P({\\mbox{A}}\\mid p_{H}=0.6)=0.432}\n  。注意，\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        0.375\n      \n    \n    {\\displaystyle L(p_{H}=0.5\\mid {\\mbox{A}})=0.375}\n  並不是說當已知\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{A}}}\n  發生了，則\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  爲\n  \n    \n      \n        0.5\n      \n    \n    {\\displaystyle 0.5}\n  的機率是\n  \n    \n      \n        0.375\n      \n    \n    {\\displaystyle 0.375}\n  。概似性跟機率具有不同的意義。\n若單獨看\n  \n    \n      \n        0.375\n      \n    \n    {\\displaystyle 0.375}\n  這個數字或\n  \n    \n      \n        0.432\n      \n    \n    {\\displaystyle 0.432}\n  這個數字是沒有意義的，因爲概似性並不是機率，並不是一定介於\n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  到\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  之間，而所有可能的\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  的概似性加起來也不是\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  ，所以單獨得知\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        0.375\n      \n    \n    {\\displaystyle L(p_{H}=0.5\\mid {\\mbox{A}})=0.375}\n  是沒有意義的。概似性是用在把各種可能的\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  值放在一起比較，來得知哪個\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  值的可能性比較高。而概似函數（在這個例子中，即\n  \n    \n      \n        L\n        (\n        \n          p\n          \n            H\n          \n        \n        ∣\n        \n          \n            A\n          \n        \n        )\n        =\n        3\n        ×\n        \n          p\n          \n            H\n          \n          \n            2\n          \n        \n        ×\n        (\n        1\n        −\n        \n          p\n          \n            H\n          \n        \n        )\n      \n    \n    {\\displaystyle L(p_{H}\\mid {\\mbox{A}})=3\\times p_{H}^{2}\\times (1-p_{H})}\n  ），除了用來計算概似性外，則是用來瞭解當母數\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  改變時，概似性怎麼變化，用來尋找最大可能性的\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  值會是多少。\n\n圖1所示爲連續擲兩次硬幣都爲正面的情況下（即此節開頭的事件\n  \n    \n      \n        \n          \n            HH\n          \n        \n      \n    \n    {\\displaystyle {\\mbox{HH}}}\n  ），\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  從\n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  到\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  的概似性。我們可以看出最大概似性發生在\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle p_{H}=1}\n  ，所以當我們投擲硬幣兩次，兩次都正面時，我們可以猜說\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  最有可能是\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  （即使實際上\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  也許是\n  \n    \n      \n        0.5\n      \n    \n    {\\displaystyle 0.5}\n  ，但我們無法知道這件事)。圖2則爲投擲硬幣三次，其中兩次爲正面、一次爲反面的情況下，\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle p_{H}}\n  從\n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  到\n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  的概似性。最大概似性發生在\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        \n          \n            2\n            3\n          \n        \n      \n    \n    {\\displaystyle p_{H}={\\frac {2}{3}}}\n  。所以當我們擲了三次硬幣得到兩次正面，最合理的猜測應該是\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        \n          \n            2\n            3\n          \n        \n      \n    \n    {\\displaystyle p_{H}={\\frac {2}{3}}}\n  （同理，也許實際上\n  \n    \n      \n        \n          p\n          \n            H\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle p_{H}=0.5}\n  ，但我們無從得知，所以只能做“最合理”猜測）。我們可以得到一個結論：\n\n      对同一个似然函数，其所代表的模型中，某项参数值具有多种可能，但如果存在一个参数值，使得概似函数值达到最大的话，那么这个值就是该项参数最为“合理”的参数值。\n\n\n== 应用 ==\n\n\n=== 最大似然估计 ===\n\n最大似然估计是似然函数最初也是最自然的应用。上文已经提到，似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是概率密度函数或概率质量函数），整理之后求最大值点。实际应用中一般会取似然函数的对数作为求最大值的函数，这样求出的最大值点和直接求最大值点得到的结果是相同的。似然函数的最大值点不一定唯一，也不一定存在。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量较大。\n\n\n=== 似然比检验 ===\n\n似然比检验是利用似然函数来检测某个假设（或限制）是否有效的一种检验。一般情况下，要检测某个附加的参数限制是否是正确的，可以将加入附加限制条件的较复杂模型的似然函数最大值与之前的较简单模型的似然函数最大值进行比较。如果参数限制是正确的，那么加入这样一个参数应当不会造成似然函数最大值的大幅变动。一般使用两者的比例来进行比较，這個比值是卡方分配。\n尼曼-皮尔森引理说明，似然比检验是所有具有同等显著性差异的检验中最有统计效力的检验。\n\n\n== 参考来源 ==\nStephen Stigler. Francis Ysidro Edgeworth, Statistician. Journal of the Royal Statistical Society. Series A (General). 1978, 141 (3): 287–322. JSTOR 2344804. （页面存档备份，存于互联网档案馆）\nStephen Stigler. The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press. ISBN 0-674-40340-1. \nStephen Stigler. Statistics on the Table: The History of Statistical Concepts and Methods. Harvard University Press. 1999. ISBN 0-674-83601-4. \nAnders Hald. On the History of Maximum Likelihood in Relation to Inverse Probability and Least Squares. Statistical Science. 1999-05, 14 (2): 214–222  [2023-01-18]. JSTOR 2676741. （原始内容存档于2016-03-05）. \nHald, A. A History of Mathematical Statistics from 1750 to 1930. New York: Wiley. 1998.", "决策树": "决策论中 （如风险管理），决策树（Decision tree）由一个决策图和可能的结果（包括资源成本和风险）组成， 用来创建到达目标的规划。决策树建立并用来辅助决策，是一种特殊的树结构。决策树是一个利用像树一样的图形或决策模型的决策支持工具，包括随机事件结果，资源代价和实用性。它是一个算法显示的方法。决策树经常在运筹学中使用，特别是在决策分析中，它帮助确定一个能最可能达到目标的策略。如果在实际中，决策不得不在没有完备知识的情况下被在线采用，一个决策树应该平行概率模型作为最佳的选择模型或在线选择模型算法。决策树的另一个使用是作为计算条件概率的描述性手段。\n\n\n== 简介 ==\n机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关係。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有複数输出，可以建立独立的决策树以处理不同输出。\n数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。\n从数据产生决策树的机器学习技术叫做决策树学习,通俗说就是决策树。\n一个决策树包含三种类型的节点：\n\n决策节点：通常用矩形框来表示\n机会节点：通常用圆圈来表示\n终结点：通常用三角形来表示\n决策树学习也是数据挖掘中一个普通的方法。在这裡，每个决策树都表述了一种树型结构，它由它的分支来对该类型的对象依靠属性进行分类。每个决策树可以依靠对源数据库的分割进行数据测试。这个过程可以递归式的对树进行修剪。  当不能再进行分割或一个单独的类可以被应用于某一分支时，递归过程就完成了。另外，随机森林分类器将许多决策树结合起来以提升分类的正确率。\n决策树同时也可以依靠计算条件概率来构造。\n决策树如果依靠数学的计算方法可以取得更加理想的效果。\n数据库已如下所示：\n(x, y) = (x1, x2, x3…, xk, y)\n相关的变量Y表示我们尝试去理解，分类或者更一般化的结果。\n其他的变量x1, x2, x3等则是帮助我们达到目的的变量。\n\n\n== 类型 ==\n决策树有幾種產生方法：\n\n分类树分析是当预计结果可能为離散类型（例如三個種類的花，输赢等）使用的概念。\n回归树分析是当局域结果可能为实数（例如房价，患者住院时间等）使用的概念。\nCART分析是结合了上述二者的一个概念。CART是Classification And Regression Trees的缩写.\nCHAID（Chi-Square Automatic Interaction Detector）\n\n\n== 建立方法 ==\n以資料母群體為根節點。\n作單因子變異數分析等，找出變異量最大的變項作為分割準則。（決策樹每個葉節點即為一連串法則的分類結果。）\n若判斷結果的正確率或涵蓋率未滿足條件，則再依最大變異量條件長出分岔。\n\n\n== 在教学中的使用 ==\n决策树，影响性图表，应用函数以及其他的决策分析工具和方法主要的授课对象是学校里商业、健康经济学和公共卫生专业的本科生，属于运筹学和管理科学的范畴。\n\n\n== 举例阐述 ==\n下面我们用例子来说明：\n小王是一家著名高尔夫俱乐部的经理。但是他被雇员数量问题搞得心情十分不好。某些天好像所有人都來玩高尔夫，以至于所有员工都忙的团团转还是应付不过来，而有些天不知道什么原因却一个人也不来，俱乐部为雇员数量浪费了不少资金。\n小王的目的是通过下周天气预报寻找什么时候人们会打高尔夫，以适时调整雇员数量。因此首先他必须了解人们决定是否打球的原因。\n在2周时间内我们得到以下记录：\n天气状况有晴，云和雨；气温用华氏温度表示；相对湿度用百分比；还有有无风。当然还有顾客是不是在这些日子光顾俱乐部。最终他得到了14行5列的数据表格。\n\n决策树模型就被建起来用于解决问题。\n\n决策树是一个有向无环图。根结点代表所有数据。分类树算法可以通过变量outlook，找出最好地解释非独立变量play（打高尔夫的人）的方法。变量outlook的范畴被划分为以下三个组：\n晴天，多云天和雨天。\n我们得出第一个结论：如果天气是多云，人们总是选择玩高尔夫，而只有少数很着迷的甚至在雨天也会玩。\n接下来我们把晴天组的分为两部分，我们发现顾客不喜欢湿度高于70%的天气。最终我们还发现，如果雨天还有风的话，就不会有人打了。\n这就通过分类树给出了一个解决方案。小王（老板）在晴天，潮湿的天气或者刮风的雨天解雇了大部分员工，因为这种天气不会有人打高尔夫。而其他的天气会有很多人打高尔夫，因此可以雇用一些临时员工来工作。\n\n\n== 公式 ==\nC4.5和C5.0生成树算法使用熵。这一度量是基于資訊科學理论中熵的概念。\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        i\n        )\n        =\n        −\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            m\n          \n        \n        f\n        (\n        i\n        ,\n        j\n        )\n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        f\n        (\n        i\n        ,\n        j\n        )\n      \n    \n    {\\displaystyle I_{E}(i)=-\\sum _{j=1}^{m}f(i,j)\\log _{2}^{}f(i,j)}\n  \n\n\n== 决策树的优点 ==\n相对于其他数据挖掘算法，决策树在以下几个方面拥有优势：\n\n决策树易于理解和实现.人们在通过解释后都有能力去理解决策树所表达的意义。\n对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。\n能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。\n是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。\n易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。\n在相对短的时间内能够对大型数据源做出可行且效果良好的结果。\n\n\n== 决策树的缺点 ==\n决策树：\n\n对于那些各类别样本数量不一致的数据，在决策树当中信息增益的结果偏向于那些具有更多数值的特征。\n\n\n== 决策树的剪枝 ==\n\n剪枝是决策树停止分支的方法之一，剪枝有分预先剪枝和后剪枝两种。预先剪枝是在树的生长过程中设定一个指标，当达到该指标时就停止生长，这样做容易产生“视界局限”，就是一旦停止分支，使得节点N成为叶节点，就断绝了其后继节点进行“好”的分支操作的任何可能性。不严格的说这会已停止的分支会误导学习算法，导致产生的树不纯度降差最大的地方过分靠近根节点。后剪枝中树首先要充分生长，直到叶节点都有最小的不纯度值为止，因而可以克服“视界局限”。然后对所有相邻的成对叶节点考虑是否消去它们，如果消去能引起令人满意的不纯度增长，那么执行消去，并令它们的公共父节点成为新的叶节点。这种“合并”叶节点的做法和节点分支的过程恰好相反，经过剪枝后叶节点常常会分布在很宽的层次上，树也变得非平衡。后剪枝技术的优点是克服了“视界局限”效应，而且无需保留部分样本用于交叉验证，所以可以充分利用全部训练集的信息。但后剪枝的计算量代价比预剪枝方法大得多，特别是在大样本集中，不过对于小样本的情况，后剪枝方法还是优于预剪枝方法的。\n\n\n== 由决策树扩展为決策圖 ==\n在决策树中所有从根到葉節點的路径都是透過“與”（AND）运算连接。在决策图中可以使用“或”来连接多于一个的路徑。\n\n\n== 决策树的实现 ==\n\n\n=== Bash ===\n决策树的代码实现可参考：决策树算法实现——Bash（页面存档备份，存于互联网档案馆）\n\n\n== 相关条目 ==\n贝叶斯定理\n贝叶斯概率\n專家系統\n真值表\n运筹学\n形态学分析\n决策表\n马尔科夫链\n\n\n== 参考文献 ==\n[1] T. Menzies, Y. Hu, Data Mining For Very Busy People.  IEEE Computer, October 2003, pgs. 18-25.\n[2]决策树分析（页面存档备份，存于互联网档案馆）mindtools.com\n[3]J.W. Comley and D.L. Dowe（页面存档备份，存于互联网档案馆）, \"Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages\", 第十一章 (pp265-294) in P. Grunwald, M.A. Pitt and I.J. Myung (eds)., Advances in Minimum Description Length: Theory and Applications, M.I.T. Press, April 2005, ISBN 0262072629.  （本论文把决策树作为贝叶斯网络使用Minimum Message Length（页面存档备份，存于互联网档案馆） (MML的内部结点).早期版本：Comley and Dowe (2003)（页面存档备份，存于互联网档案馆）, .pdf（页面存档备份，存于互联网档案馆）.）\n[4]P.J. Tan and D.L. Dowe（页面存档备份，存于互联网档案馆） (2004), MML Inference of Oblique Decision Trees（页面存档备份，存于互联网档案馆）,人工智能讲义3339, Springer-Verlag, pp1082-1088（页面存档备份，存于互联网档案馆）.（此论文使用Minimum Message Length.）\n[5] Eruditionhome（页面存档备份，存于互联网档案馆）数据挖掘资源大词典\n[6]决策树基础 （页面存档备份，存于互联网档案馆）vanguardsw.com\n[7]General Morphological Analysis: A General Method for Non-Quantified Modelling （页面存档备份，存于互联网档案馆） From the Swedish Morphological Society（页面存档备份，存于互联网档案馆）\n[8]decisiontrees.net Interactive Tutorial\n[9][Morgan.Kaufmann.Data.Mining.Practical.Machine.Learning.Tools.and.Techniques.Second.Edition]Jun.2005,非常好的一本介绍决策树的书，是一本可以从基础学起的好书，另外介绍的weka决策树软件也不错。是JAVA编的。", "迴歸分析": "迴歸分析（英語：Regression Analysis）是一種統計學上分析數據的方法，目的在於了解兩個或多個變數間是否相關、相關方向與強度，並建立數學模型以便觀察特定變數來預測研究者感興趣的變數。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。\n迴歸分析是建立被解釋變數\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  （或稱應變數、依變數、反應變數）與解釋變數\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  （或稱自變數、獨立變數）之間關係的模型。簡單線性回歸使用一個自變量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ，複迴歸使用超過一個自變量（\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        .\n        .\n        .\n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2}...X_{i}}\n  ）。\n\n\n== 起源 ==\n回归的最早形式是最小二乘法，由1805年的勒让德（Legendre），和1809年的高斯（Gauss）出版。勒让德和高斯都将该方法应用于从天文观测中确定关于太阳的物体的轨道（主要是彗星，但后来是新发现的小行星）的问题。 高斯在1821年发表了最小二乘理论的进一步发展，包括高斯－马尔可夫定理的一个版本。\n「迴歸」一詞最早由法蘭西斯·高爾頓（Francis Galton）所使用。他曾對親子間的身高做研究，發現父母的身高雖然會遺傳給子女，但子女的身高卻有逐漸「迴歸到中等（即人的平均值）」的現象。不過當時的迴歸和現在的迴歸在意義上已不盡相同。\n在1950年代和60年代，经济学家使用机械电子桌面计算器来计算回归。在1970年之前，这种计算方法有时需要长达24小时才能得出结果。\n\n\n== 迴歸分析原理 ==\n目的在於找出一條最能夠代表所有觀測資料的函數曲线（迴歸估計式）。\n用此函數代表因變數和自變數之間的關係。\n\n\n=== 母數估計 ===\n動差估計（Method of Moment、MOM）\n最小二乘法（Ordinary Least Square Estimation, OLSE）\n最大似然估计（Maximum Likelihood Estimation, MLE）\n\n\n== 回归模型 ==\n回归模型主要包括以下变量：\n\n未知参数，记为\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  ，可以代表一个标量或一个向量。\n自变量，\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  。\n因变量，\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  。回归模型将\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  和一个关于\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  和\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  的函数关联起来。\n在不同的应用领域有各自不同的术语代替这里的“自变量”和“因变量”。\n\n  \n    \n      \n        Y\n        ≈\n        f\n        (\n        \n          X\n        \n        ,\n        \n          β\n        \n        )\n      \n    \n    {\\displaystyle Y\\approx f(\\mathbf {X} ,{\\boldsymbol {\\beta }})}\n  这个估计值通常写作:\n  \n    \n      \n        E\n        (\n        X\n        \n          |\n        \n        Y\n        )\n        =\n        f\n        (\n        \n          X\n        \n        ,\n        \n          β\n        \n        )\n      \n    \n    {\\displaystyle E(X|Y)=f(\\mathbf {X} ,{\\boldsymbol {\\beta }})}\n  。\n在进行回归分析时，函数\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  的形式必须预先指定。有时函数\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  的形式是在对\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  和\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  关系的已有知识上建立的，而不是在数据的基础之上。如果没有这种已有知识，那么就要选择一个灵活和便于回归的\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  的形式。\n假设现在未知向量\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  的维数为k。为了进行回归分析，必须要先有关于\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的信息：\n\n如果以\n  \n    \n      \n        (\n        Y\n        ,\n        \n          X\n        \n        )\n      \n    \n    {\\displaystyle (Y,\\mathbf {X} )}\n  的形式给出了\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  个数据点，当\n  \n    \n      \n        N\n        <\n        k\n      \n    \n    {\\displaystyle N<k}\n  时，大多数传统的回归分析方法都不能进行，因为数据量不够导致回归模型的系统方程不能完全确定\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  。\n如果恰好有\n  \n    \n      \n        N\n        =\n        k\n      \n    \n    {\\displaystyle N=k}\n  个数据点，并且函数\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  的形式是线性的，那么方程\n  \n    \n      \n        Y\n        =\n        f\n        (\n        \n          X\n        \n        ,\n        \n          β\n        \n        )\n      \n    \n    {\\displaystyle Y=f(\\mathbf {X} ,{\\boldsymbol {\\beta }})}\n  能精确求解。这相当于解一个有\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  个未知量和\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  个方程的方程组。在\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  线性无关的情况下，这个方程组有唯一解。但如果\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  是非线性形式的，解可能有多个或不存在。\n实际中\n  \n    \n      \n        N\n        >\n        k\n      \n    \n    {\\displaystyle N>k}\n  的情况占大多数。这种情况下，有足够的信息用于估计一个与数据最接近的\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  值，这时当回归分析应用于这些数据时，可以看作是解一个关于\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  的超定方程。在最后一种情况下，回归分析提供了一种完成以下任务的工具：\n⒈找出一个未知量\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  的解使因变量\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的预测值和实际值差别最小(又称最小二乘法)。\n⒉在特定统计假设下，回归分析使用数据中的多余信息给出关于因变量\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  和未知量\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  之间的关系。\n\n\n== 迴歸分析的種類 ==\n\n\n=== 簡單線性回歸 ===\n簡單線性迴歸（英語：simple linear regression）\n\n應用時機以單一變數預測\n判斷兩變數之間相關的方向和程度\n\n\n=== 複迴歸（或多變量迴歸） ===\n複回歸分析（英語：multiple regression analysis）是簡單線性迴歸的一種延伸應用，用以瞭解一個依變項與兩組以上自變項的函數關係。\n\n\n=== 對數線性迴歸 ===\n對數線性迴歸（英語：Log-linear model），是將解釋變項（實驗設計中的自變項）和反應變項（實驗設計中的依變項）都取對數值之後再進行線性迴歸，所以依據解釋變項的數量，可能是對數簡單線性迴歸，也可能是對數複迴歸。\n\n\n=== 非線性迴歸 ===\n\n\n=== 对数几率回归 ===\n\n对数几率回归（英語：Logistic Regression）\n\n\n=== 偏迴歸 ===\n偏迴歸（英語：Partial Regression）\n\n\n=== 自迴歸 ===\n\n\n==== 自迴歸滑動平均模型 ====\n\n\n==== 差分自迴歸滑動平均模型 ====\n\n\n==== 向量自迴歸模型 ====\n\n\n== 參閱 ==\n\n\n== 参考资料 ==\n\n\n== 外部連結 ==\n解讀迴歸分析的原理及結構", "条件概率分布": "条件概率分布（Conditional Probability Distribution，或者 条件分布，Conditional Distribution ）是现代概率论中的概念。已知两个相关的随机变量X 和Y，随机变量Y 在条件{X =x}下的条件概率分布是指当已知X 的取值为某个特定值x之时，Y 的概率分布。 如果Y 在条件{X =x}下的条件概率分布是连续分布，那么其密度函数称作Y 在条件{X =x}下的条件概率密度函数（条件分布密度、条件密度函数）。与条件分布有关的概念，常常以“条件”作为前缀，如条件期望、条件方差等等。\n\n\n== 例子 ==\n\n假设在桌子上抛掷一枚普通的骰子，则其点数结果的概率分布是集合\n  \n    \n      \n        {\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n        ,\n        5\n        ,\n        6\n        }\n      \n    \n    {\\displaystyle \\{1,2,3,4,5,6\\}}\n  的均匀分布：每个点数出现的概率都是均等的六分之一。然而，如果据某个坐在桌边的人观察，向着他的侧面是6点，那么，在此条件下，向上的一面不可能是6点，也不可能是6点对面的1点。因此，在此条件下，抛骰子的点数结果是集合\n  \n    \n      \n        {\n        2\n        ,\n        3\n        ,\n        4\n        ,\n        5\n        }\n      \n    \n    {\\displaystyle \\{2,3,4,5\\}}\n  的均匀分布：有四分之一的可能性出现\n  \n    \n      \n        2\n        ,\n        3\n        ,\n        4\n        ,\n        5\n      \n    \n    {\\displaystyle 2,3,4,5}\n  四种点数中的一种。可以看出，增加的条件或信息量（某个侧面是6点）导致了点数结果的概率分布的变化。这个新的概率分布就是条件概率分布。\n\n\n== 数学定义 ==\n更为严格清晰的定义需要用到数学语言。当随机变量是离散或连续时，条件概率分布有不同的表达方法。\n\n\n=== 离散条件分布 ===\n对于离散型的随机变量X 和Y（取值范围分别是\n  \n    \n      \n        \n          \n            I\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}}\n  和\n  \n    \n      \n        \n          \n            J\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {J}}}\n  ），随机变量Y 在条件{X =x}下的条件概率分布是：\n\n  \n    \n      \n        ∀\n        j\n        ∈\n        \n          \n            J\n          \n        \n        ,\n        \n        \n          p\n          \n            Y\n            ∣\n            X\n          \n        \n        (\n        j\n        )\n        =\n        \n          p\n          \n            Y\n          \n        \n        (\n        j\n        ∣\n        X\n        =\n        i\n        )\n        =\n        P\n        (\n        Y\n        =\n        j\n        ∣\n        X\n        =\n        i\n        )\n        =\n        \n          \n            \n              P\n              (\n              X\n              =\n              i\n              ,\n              Y\n              =\n              j\n              )\n            \n            \n              P\n              (\n              X\n              =\n              i\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\forall j\\in {\\mathcal {J}},\\quad p_{Y\\mid X}(j)=p_{Y}(j\\mid X=i)=P(Y=j\\mid X=i)={\\frac {P(X=i,Y=j)}{P(X=i)}}.}\n   （\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        )\n        >\n        0\n      \n    \n    {\\displaystyle P(X=i)>0}\n  ）同样的，X 在条件{Y=y}下的条件概率分布是：\n\n  \n    \n      \n        ∀\n        i\n        ∈\n        \n          \n            I\n          \n        \n        ,\n        \n        \n          p\n          \n            X\n            ∣\n            Y\n          \n        \n        (\n        i\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        i\n        ∣\n        Y\n        =\n        j\n        )\n        =\n        P\n        (\n        X\n        =\n        i\n        ∣\n        Y\n        =\n        j\n        )\n        =\n        \n          \n            \n              P\n              (\n              X\n              =\n              i\n              ,\n              Y\n              =\n              j\n              )\n            \n            \n              P\n              (\n              Y\n              =\n              j\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\forall i\\in {\\mathcal {I}},\\quad p_{X\\mid Y}(i)=p_{X}(i\\mid Y=j)=P(X=i\\mid Y=j)={\\frac {P(X=i,Y=j)}{P(Y=j)}}.}\n   （\n  \n    \n      \n        P\n        (\n        Y\n        =\n        j\n        )\n        >\n        0\n      \n    \n    {\\displaystyle P(Y=j)>0}\n  ）其中，\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        ,\n        Y\n        =\n        j\n        )\n      \n    \n    {\\displaystyle P(X=i,Y=j)}\n  是X 和Y 联合分布概率，即“\n  \n    \n      \n        X\n        =\n        i\n      \n    \n    {\\displaystyle X=i}\n  ，并且\n  \n    \n      \n        Y\n        =\n        j\n      \n    \n    {\\displaystyle Y=j}\n  发生的概率”。如果用\n  \n    \n      \n        \n          p\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle p_{ij}}\n  表示\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        ,\n        Y\n        =\n        j\n        )\n      \n    \n    {\\displaystyle P(X=i,Y=j)}\n  的值：\n\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        ,\n        Y\n        =\n        j\n        )\n        =\n        \n          p\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle P(X=i,Y=j)=p_{ij}}\n  \n那么随机变量X 和Y 的边际分布就是：\n\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        )\n        =\n        \n          p\n          \n            i\n            .\n          \n        \n        =\n        \n          ∑\n          \n            j\n            ∈\n            \n              \n                J\n              \n            \n          \n        \n        \n          p\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle P(X=i)=p_{i.}=\\sum _{j\\in {\\mathcal {J}}}p_{ij}}\n  \n\n  \n    \n      \n        P\n        (\n        Y\n        =\n        j\n        )\n        =\n        \n          p\n          \n            .\n            j\n          \n        \n        =\n        \n          ∑\n          \n            i\n            ∈\n            \n              \n                I\n              \n            \n          \n        \n        \n          p\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle P(Y=j)=p_{.j}=\\sum _{i\\in {\\mathcal {I}}}p_{ij}}\n  因此， 随机变量Y 在条件{X =x}下的条件概率分布也可以表达为：\n\n  \n    \n      \n        \n          p\n          \n            Y\n            ∣\n            X\n          \n        \n        (\n        j\n        )\n        =\n        P\n        (\n        Y\n        =\n        j\n        ∣\n        X\n        =\n        i\n        )\n        =\n        \n          \n            \n              p\n              \n                i\n                j\n              \n            \n            \n              p\n              \n                i\n                .\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle p_{Y\\mid X}(j)=P(Y=j\\mid X=i)={\\frac {p_{ij}}{p_{i.}}}.}\n  （\n  \n    \n      \n        \n          p\n          \n            i\n            .\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle p_{i.}>0}\n  ）同样的，X 在条件{Y=y}下的条件概率分布也可以表达为：\n\n  \n    \n      \n        \n          p\n          \n            X\n            ∣\n            Y\n          \n        \n        (\n        i\n        )\n        =\n        \n          \n            \n              p\n              \n                i\n                j\n              \n            \n            \n              p\n              \n                .\n                j\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle p_{X\\mid Y}(i)={\\frac {p_{ij}}{p_{.j}}}.}\n  （\n  \n    \n      \n        \n          p\n          \n            .\n            j\n          \n        \n        >\n        0\n      \n    \n    {\\displaystyle p_{.j}>0}\n  ）\n\n\n=== 连续条件分布 ===\n对于连续型的随机变量X 和Y，\n  \n    \n      \n        P\n        (\n        X\n        =\n        i\n        )\n        =\n        P\n        (\n        Y\n        =\n        j\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(X=i)=P(Y=j)=0}\n  ，因此对离散型随机变量的条件分布定义不适用。假设其联合密度函数为\n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f(x,y)}\n  ，X 和Y 的边际密度函数分别是\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  和\n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{Y}(y)}\n  ，那么Y 在条件{X =x}下的条件概率密度函数是：\n\n  \n    \n      \n        \n          f\n          \n            Y\n            \n              |\n            \n            X\n          \n        \n        (\n        y\n        \n          |\n        \n        x\n        )\n        =\n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        ∣\n        X\n        =\n        x\n        )\n        =\n        \n          \n            \n              f\n              (\n              x\n              ,\n              y\n              )\n            \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle f_{Y|X}(y|x)=f_{Y}(y\\mid X=x)={\\frac {f(x,y)}{f_{X}(x)}}.}\n  同样的，X 在条件{Y=y}下的条件概率密度函数是：\n\n  \n    \n      \n        \n          f\n          \n            X\n            \n              |\n            \n            Y\n          \n        \n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        ∣\n        Y\n        =\n        y\n        )\n        =\n        \n          \n            \n              f\n              (\n              x\n              ,\n              y\n              )\n            \n            \n              \n                f\n                \n                  Y\n                \n              \n              (\n              y\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle f_{X|Y}(x|y)=f_{X}(x\\mid Y=y)={\\frac {f(x,y)}{f_{Y}(y)}}.}\n  \n\n\n== 条件分布和独立分布 ==\n在一定意义上，条件分布和独立分布是相对的。如果两个随机变量X 和Y 是独立分布的，那么不论是否已知某个关于X 的条件，都不会影响Y 的概率分布。用数学语言来说，就是：\n\n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        ∣\n        X\n        =\n        x\n        )\n        =\n        P\n        (\n        Y\n        =\n        y\n        )\n        =\n        \n          p\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle P(Y=y\\mid X=x)=P(Y=y)=p_{Y}(y)}\n  这与独立分布的定义是相合的，事实上，随机变量X 和Y 相互独立分布，则：\n\n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        ,\n        X\n        =\n        x\n        )\n        =\n        P\n        (\n        Y\n        =\n        y\n        )\n        ⋅\n        P\n        (\n        X\n        =\n        x\n        )\n        .\n      \n    \n    {\\displaystyle P(Y=y,X=x)=P(Y=y)\\cdot P(X=x).}\n  因此\n\n  \n    \n      \n        P\n        (\n        Y\n        =\n        y\n        )\n        =\n        \n          \n            \n              P\n              (\n              Y\n              =\n              y\n              ,\n              X\n              =\n              x\n              )\n            \n            \n              P\n              (\n              X\n              =\n              x\n              )\n            \n          \n        \n        =\n        P\n        (\n        Y\n        =\n        y\n        ∣\n        X\n        =\n        x\n        )\n        .\n      \n    \n    {\\displaystyle P(Y=y)={\\frac {P(Y=y,X=x)}{P(X=x)}}=P(Y=y\\mid X=x).}\n  \n\n\n== 参见 ==\n条件概率\n正则条件概率\n\n\n== 参考资料 ==\n赵衡秀. 《概率论与数理统计》. 清华大学出版社. 2005.", "异常值": "在统计学中，异常值（又稱離群值）是指与其他观测值有显著差异的数据点。异常值可能是由实验误差造成；后者有时会从数据集中排除。异常值可能会导致统计分析中出现严重问题。\n能妥善處理異常值的估计量，稱為「穩健」。例如，中位數是集中趋势的穩健統計量，但平均數則不然。\n\n\n== 参考文献 ==", "决策树学习": "决策树学习是统计学、数据挖掘和机器学习中使用的一种预测建模方法。它使用决策树作为预测模型，从样本的观测数据（对应决策树的分支）推断出该样本的预测结果（对应决策树的叶节点）。\n按预测结果的差异，决策树学习可细分两类。（1）分类树，其预测结果仅限于一组离散数值。树的每个分支对应一组由逻辑与连接的分类特征，而该分支上的叶节点对应由上述特征可以预测出的分类标签。（2）回归树，其预测结果为连续值（例如实数）。\n在决策分析中，一棵可视的决策树可以向使用者形象地展示决策的结果和过程。在数据挖掘和机器学习中，一棵决策树主要用于描述数据（此后亦可基于习得的预测模型去支持决策)。本页侧重描述数据挖掘中的决策树。\n\n\n== 推广 ==\n\n在数据挖掘中决策树训练是一个常用的方法。目标是创建一个模型来预测样本的目标值。例如右图。每个 内部节点 对应于一个输入属性，子节点代表父节点的属性的可能取值。每个叶子节点代表输入属性得到的可能输出值。 \n一棵树的训练过程为：根据一个指标，分裂训练集为几个子集。这个过程不断的在产生的子集里重复递归进行，即递归分割。当一个训练子集的类标都相同时 递归停止。这种决策树的自顶向下归纳 (TDITD)  是 贪心算法的一种, 也是目前为止最为常用的一种训练方法，但不是唯一的方法。\n\n数据以如下方式表示:    \n\n  \n    \n      \n        (\n        \n          \n            x\n          \n        \n        ,\n        Y\n        )\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            k\n          \n        \n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle ({\\textbf {x}},Y)=(x_{1},x_{2},x_{3},...,x_{k},Y)}\n  其中Y是目标值，向量x由这些属性构成, x1, x2, x3 等等，用来得到目标值。\n\n\n== 决策树的类型 ==\n在数据挖掘中，决策树主要有两种类型:\n\n分类树 的输出是样本的类标（例如花的分類，股票漲跌等）。\n回归树 的输出是一个实数 (例如房子的价格，病人待在医院的时间等)。术语分类和回归树 (CART) 包含了上述两种决策树, 最先由Breiman 等提出. 分类树和回归树有些共同点和不同点—例如处理在何处分裂的问题。有些集成的方法产生多棵树：\n\n装袋算法（Bagging）, 是一个早期的集成方法，用有放回抽样法来训练多棵决策树，最终结果用投票法产生。\n随机森林（Random Forest） 使用多棵决策树来改进分类性能。\n提升树（Boosting Tree） 可以用来做回归分析和分类决策\n旋转森林（Rotation forest） – 每棵树的训练首先使用主元分析法 (PCA)。还有其他很多决策树算法，常见的有:\n\nID3算法\nC4.5算法\nCHi-squared Automatic Interaction Detector (CHAID). 在生成树的过程中用多层分裂.\nMARS:可以更好的处理数值型数据。\n\n\n== 模型表达式 ==\n构建决策树时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂。  \"最好\" 的定义是使得子节点中的训练集尽量的纯，表示所分裂出的子节点中的集合越相近。不同的算法使用不同的指标来定义\"最好\"。本部分介绍一些最常见的指标。\n\n\n=== 基尼不纯度指标 ===\n \n在CART算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。                    \n假设y的可能取值为\n  \n    \n      \n        J\n      \n    \n    {\\displaystyle J}\n  个类别，令\n  \n    \n      \n        i\n        ∈\n        {\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        J\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,2,...,J\\}}\n  ，\n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  表示被标定为第\n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  类的概率，则基尼不纯度的计算为：              \n\n  \n    \n      \n        \n          I\n          \n            G\n          \n        \n        ⁡\n        (\n        p\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          ∑\n          \n            k\n            ≠\n            i\n          \n        \n        \n          p\n          \n            k\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        (\n        1\n        −\n        \n          p\n          \n            i\n          \n        \n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        (\n        \n          p\n          \n            i\n          \n        \n        −\n        \n          \n            \n              p\n              \n                i\n              \n            \n          \n          \n            2\n          \n        \n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        −\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          \n            \n              p\n              \n                i\n              \n            \n          \n          \n            2\n          \n        \n        =\n        1\n        −\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          \n            \n              p\n              \n                i\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {I} _{G}(p)=\\sum _{i=1}^{J}p_{i}\\sum _{k\\neq i}p_{k}=\\sum _{i=1}^{J}p_{i}(1-p_{i})=\\sum _{i=1}^{J}(p_{i}-{p_{i}}^{2})=\\sum _{i=1}^{J}p_{i}-\\sum _{i=1}^{J}{p_{i}}^{2}=1-\\sum _{i=1}^{J}{p_{i}}^{2}}\n  \n\n\n=== 信息增益 ===\nID3, C4.5 和 C5.0 决策树的生成使用信息增益。信息增益 是基于信息论中信息熵与自信息理论.\n信息熵定义为：\n\n  \n    \n      \n        \n          H\n        \n        (\n        T\n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        ⁡\n        \n          (\n          \n            \n              p\n              \n                1\n              \n            \n            ,\n            \n              p\n              \n                2\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              p\n              \n                J\n              \n            \n          \n          )\n        \n        =\n        −\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          \n            p\n            \n              i\n            \n          \n          \n            log\n            \n              2\n            \n          \n          ⁡\n          \n            p\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {H} (T)=\\operatorname {I} _{E}\\left(p_{1},p_{2},...,p_{J}\\right)=-\\sum _{i=1}^{J}{p_{i}\\log _{2}p_{i}}}\n  其中\n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n      \n    \n    {\\displaystyle p_{1},p_{2},...}\n  加和为1，表示当前节点中各个类别的百分比。\n\n  \n    \n      \n        =\n        −\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            J\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            p\n            \n              i\n            \n          \n        \n        −\n        \n          ∑\n          \n            a\n          \n        \n        \n          p\n          (\n          a\n          )\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              J\n            \n          \n          −\n          Pr\n          (\n          i\n          \n            |\n          \n          a\n          )\n          \n            log\n            \n              2\n            \n          \n          ⁡\n          \n            Pr\n            (\n            i\n            \n              |\n            \n            a\n            )\n          \n        \n      \n    \n    {\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}}\n  例如，数据集有4个属性：outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), 目标值play（yes, no）, 总共14个数据点。为建造决策树，需要比较4棵决策树的信息增益，每棵决策树用一种属性做划分。信息增益最高的划分作为第一次划分，并在每个子节点继续此过程，直至其信息增益为0。\n使用属性windy做划分时，产生2个子节点：windy值为真与为假。当前数据集，6个数据点的windy值为真，其中3个点的play值为真，3个点的play值为假；其余8个数据点的windy为假，其中6个点的play值为真，2个点的play值为假。 windy=true的子节点的信息熵计算为：\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        )\n        =\n        −\n        \n          \n            3\n            6\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            3\n            6\n          \n        \n        −\n        \n          \n            3\n            6\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            3\n            6\n          \n        \n        =\n        −\n        \n          \n            1\n            2\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            1\n            2\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            1\n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle I_{E}([3,3])=-{\\frac {3}{6}}\\log _{2}^{}{\\frac {3}{6}}-{\\frac {3}{6}}\\log _{2}^{}{\\frac {3}{6}}=-{\\frac {1}{2}}\\log _{2}^{}{\\frac {1}{2}}-{\\frac {1}{2}}\\log _{2}^{}{\\frac {1}{2}}=1}\n  windy=false的子节点的信息熵计算为：\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        −\n        \n          \n            6\n            8\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            6\n            8\n          \n        \n        −\n        \n          \n            2\n            8\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            2\n            8\n          \n        \n        =\n        −\n        \n          \n            3\n            4\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            3\n            4\n          \n        \n        −\n        \n          \n            1\n            4\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            1\n            4\n          \n        \n        =\n        0.8112781\n      \n    \n    {\\displaystyle I_{E}([6,2])=-{\\frac {6}{8}}\\log _{2}^{}{\\frac {6}{8}}-{\\frac {2}{8}}\\log _{2}^{}{\\frac {2}{8}}=-{\\frac {3}{4}}\\log _{2}^{}{\\frac {3}{4}}-{\\frac {1}{4}}\\log _{2}^{}{\\frac {1}{4}}=0.8112781}\n  这个划分（使用属性windy）的信息熵是两个子节点信息熵的加权和：\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        ,\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        (\n        \n          windy or not\n        \n        )\n        =\n        \n          \n            6\n            14\n          \n        \n        ⋅\n        1\n        +\n        \n          \n            8\n            14\n          \n        \n        ⋅\n        0.8112781\n        =\n        0.8921589\n      \n    \n    {\\displaystyle I_{E}([3,3],[6,2])=I_{E}({\\text{windy or not}})={\\frac {6}{14}}\\cdot 1+{\\frac {8}{14}}\\cdot 0.8112781=0.8921589}\n  为计算使用属性windy的信息增益，必须先计算出最初（未划分）的数据集的信息熵，数据集的play有9个yes与5个no：\n\n  \n    \n      \n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        9\n        ,\n        5\n        ]\n        )\n        =\n        −\n        \n          \n            9\n            14\n          \n        \n        \n          log\n          \n            2\n          \n          \n\n          \n        \n        ⁡\n        \n          \n            9\n            14\n          \n        \n        −\n        \n          \n            5\n            14\n          \n        \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        \n          \n            5\n            14\n          \n        \n        =\n        0.940286\n      \n    \n    {\\displaystyle I_{E}([9,5])=-{\\frac {9}{14}}\\log _{2}^{}{\\frac {9}{14}}-{\\frac {5}{14}}\\log _{2}{\\frac {5}{14}}=0.940286}\n  使用属性windy的信息增益是：\n\n  \n    \n      \n        I\n        G\n        (\n        \n          windy\n        \n        )\n        =\n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        9\n        ,\n        5\n        ]\n        )\n        −\n        \n          I\n          \n            E\n          \n        \n        (\n        [\n        3\n        ,\n        3\n        ]\n        ,\n        [\n        6\n        ,\n        2\n        ]\n        )\n        =\n        0.940286\n        −\n        0.8921589\n        =\n        0.0481271\n      \n    \n    {\\displaystyle IG({\\text{windy}})=I_{E}([9,5])-I_{E}([3,3],[6,2])=0.940286-0.8921589=0.0481271}\n  \n\n\n== 决策树的优点 ==\n与其他的数据挖掘算法相比，决策树有许多优点:\n\n易于理解和解释 人们很容易理解决策树的意义。\n只需很少的数据准备 其他技术往往需要数据归一化。\n即可以处理数值型数据也可以处理类别型 数据。其他技术往往只能处理一种数据类型。例如关联规则只能处理类别型的而神经网络只能处理数值型的数据。\n使用白箱 模型. 输出结果容易通过模型的结构来解释。而神经网络是黑箱模型，很难解释输出的结果。\n可以通过测试集来验证模型的性能 。可以考虑模型的稳定性。\n強健控制. 对噪声处理有好的強健性。\n可以很好的处理大规模数据 。\n\n\n== 缺点 ==\n训练一棵最优的决策树是一个完全NP问题。 因此, 实际应用时决策树的训练采用启发式搜索算法例如 贪心算法 来达到局部最优。这样的算法没办法得到最优的决策树。\n决策树创建的过度复杂会导致无法很好的预测训练集之外的数据。这称作过拟合.  剪枝机制可以避免这种问题。\n有些问题决策树没办法很好的解决,例如 异或问题。解决这种问题的时候，决策树会变得过大。  要解决这种问题，只能改变问题的领域 或者使用其他更为耗时的学习算法 (例如统计关系学习 或者 归纳逻辑编程).对那些有类别型属性的数据, 信息增益 会有一定的偏置。\n\n\n== 延伸 ==\n\n\n=== 决策图 ===\n在决策树中, 从根节点到叶节点的路径采用汇合。\n而在决策图中, 可以采用 最小消息长度 (MML)来汇合两条或多条路径。\n\n\n=== 用演化算法来搜索 ===\n演化算法可以用来避免局部最优的问题\n\n\n== 参见 ==\n决策树剪枝\n二元决策图\nCART\nID3算法\nC4.5算法\n\n\n== 参考资料 ==\n\n\n== 外部链接 ==\nBuilding Decision Trees in Python From O'Reilly.\nAn Addendum to \"Building Decision Trees in Python\"（页面存档备份，存于互联网档案馆） From O'Reilly.\nDecision Trees page at aaai.org, a page with commented links.\nDecision tree implementation in Ruby (AI4R)（页面存档备份，存于互联网档案馆）\nBuilding Decision Tree In Bash（页面存档备份，存于互联网档案馆）", "联合分布": "在概率论中, 对定义在相同样本空间的两个随机变量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  ，其联合分布是同时对于\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的概率分布。\n\n\n== 离散随机变量的联合分布 ==\n对离散随机变量而言，联合分布概率质量函数为\n  \n    \n      \n        P\n        r\n        (\n        X\n        =\n        x\n        \n        &\n        \n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle Pr(X=x\\,\\&\\,Y=y)}\n  ，即\n\n  \n    \n      \n        P\n        (\n        X\n        =\n        x\n        \n        \n          a\n          n\n          d\n        \n        \n        Y\n        =\n        y\n        )\n        \n        =\n        \n        P\n        (\n        Y\n        =\n        y\n        \n          |\n        \n        X\n        =\n        x\n        )\n        P\n        (\n        X\n        =\n        x\n        )\n        =\n        P\n        (\n        X\n        =\n        x\n        \n          |\n        \n        Y\n        =\n        y\n        )\n        P\n        (\n        Y\n        =\n        y\n        )\n        .\n        \n      \n    \n    {\\displaystyle P(X=x\\;\\mathrm {and} \\;Y=y)\\;=\\;P(Y=y|X=x)P(X=x)=P(X=x|Y=y)P(Y=y).\\;}\n  因为是概率分布函数，所以必须有\n\n  \n    \n      \n        \n          ∑\n          \n            x\n          \n        \n        \n          ∑\n          \n            y\n          \n        \n        P\n        (\n        X\n        =\n        x\n         \n        \n          a\n          n\n          d\n        \n         \n        Y\n        =\n        y\n        )\n        =\n        1.\n        \n      \n    \n    {\\displaystyle \\sum _{x}\\sum _{y}P(X=x\\ \\mathrm {and} \\ Y=y)=1.\\;}\n  \n\n\n== 连续随机变量的联合分布 ==\n类似地，对连续随机变量而言，联合分布概率密度函数为\n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle f_{X,Y}(x,y)}\n  ，其中\n  \n    \n      \n        \n          f\n          \n            Y\n            \n              |\n            \n            X\n          \n        \n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle f_{Y|X}(y|x)}\n  和\n  \n    \n      \n        \n          f\n          \n            X\n            \n              |\n            \n            Y\n          \n        \n        (\n        x\n        \n          |\n        \n        y\n        )\n      \n    \n    {\\displaystyle f_{X|Y}(x|y)}\n  分别代表\n  \n    \n      \n        X\n        =\n        x\n      \n    \n    {\\displaystyle X=x}\n  时\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的条件分布以及\n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n  时\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的条件分布；\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  和\n  \n    \n      \n        \n          f\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle f_{Y}(y)}\n  分别代表\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的边缘分布。\n同样地，因为是概率分布函数，所以必须有\n\n\n== 独立变量的联合分布 ==\n對於兩相互獨立的事件\n  \n    \n      \n        P\n        (\n        X\n        )\n      \n    \n    {\\displaystyle P(X)}\n  及\n  \n    \n      \n        P\n        (\n        Y\n        )\n      \n    \n    {\\displaystyle P(Y)}\n  ，任意x和y而言有离散随机变量\n  \n    \n      \n         \n        P\n        (\n        X\n        =\n        x\n         \n        \n          a\n          n\n          d\n        \n         \n        Y\n        =\n        y\n        )\n        =\n        P\n        (\n        X\n        =\n        x\n        )\n        ⋅\n        P\n        (\n        Y\n        =\n        y\n        )\n      \n    \n    {\\displaystyle \\ P(X=x\\ \\mathrm {and} \\ Y=y)=P(X=x)\\cdot P(Y=y)}\n  ，或者有连续随机变量\n  \n    \n      \n         \n        \n          p\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        ⋅\n        \n          p\n          \n            Y\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle \\ p_{X,Y}(x,y)=p_{X}(x)\\cdot p_{Y}(y)}\n  。\n\n\n== 多元联合分布 ==\n2元联合分布可以推广到任意多元的情况\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\ldots ,X_{n}}\n  \n\n  \n    \n      \n        \n          f\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n              \n            \n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        =\n        \n          f\n          \n            \n              X\n              \n                n\n              \n            \n            \n              |\n            \n            \n              X\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n                −\n                1\n              \n            \n          \n        \n        (\n        \n          x\n          \n            n\n          \n        \n        \n          |\n        \n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n            −\n            1\n          \n        \n        )\n        \n          f\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                n\n                −\n                1\n              \n            \n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n            −\n            1\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle f_{X_{1},\\ldots ,X_{n}}(x_{1},\\ldots ,x_{n})=f_{X_{n}|X_{1},\\ldots ,X_{n-1}}(x_{n}|x_{1},\\ldots ,x_{n-1})f_{X_{1},\\ldots ,X_{n-1}}(x_{1},\\ldots ,x_{n-1}).}\n  \n\n\n== 相关条目 ==\n耦合 (概率)\n\n\n== 外部链接 ==\nJoint continuous density function. PlanetMath.", "機率密度函數": "在数学中，连续型随机变量的概率密度函數（Probability density function，簡寫作PDF ），在不致於混淆时可简称为密度函数，是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。圖中，橫軸為隨機變量的取值，縱軸為概率密度函數的值，而随机变量的取值落在某个区域内的概率為概率密度函数在这个区域上的积分。当概率密度函数存在的时候，累積分佈函數是概率密度函数的积分。\n概率密度函数有时也被称为概率分布函数，但这种称法可能会和累积分布函数(CDF)或概率质量函数(PMF)混淆。\n\n\n== 常见定义 ==\n对于一维实随机变量X，设它的累积分布函数是\n  \n    \n      \n        \n          F\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle F_{X}(x)}\n  。如果存在可测函数 \n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  ，满足：\n\n  \n    \n      \n        ∀\n        −\n        ∞\n        <\n        a\n        <\n        ∞\n        ,\n        \n        \n          F\n          \n            X\n          \n        \n        (\n        a\n        )\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            a\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\forall -\\infty <a<\\infty ,\\quad F_{X}(a)=\\int _{-\\infty }^{a}f_{X}(x)\\,dx}\n  那么X 是一个连续型随机变量，并且\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  是它的概率密度函数。\n\n\n=== 性质 ===\n连续型随机变量的概率密度函数有如下性质：\n\n  \n    \n      \n        ∀\n        −\n        ∞\n        <\n        x\n        <\n        ∞\n        ,\n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        ≥\n        0\n      \n    \n    {\\displaystyle \\forall -\\infty <x<\\infty ,\\quad f_{X}(x)\\geq 0}\n  \n\n  \n    \n      \n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n        =\n        1\n      \n    \n    {\\displaystyle \\int _{-\\infty }^{\\infty }f_{X}(x)\\,dx=1}\n  \n\n  \n    \n      \n        ∀\n        −\n        ∞\n        <\n        a\n        <\n        b\n        <\n        ∞\n        ,\n        \n        \n          P\n        \n        \n          [\n          \n            a\n            <\n            X\n            ≤\n            b\n          \n          ]\n        \n        =\n        \n          F\n          \n            X\n          \n        \n        (\n        b\n        )\n        −\n        \n          F\n          \n            X\n          \n        \n        (\n        a\n        )\n        =\n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\forall -\\infty <a<b<\\infty ,\\quad \\mathbb {P} \\left[a<X\\leq b\\right]=F_{X}(b)-F_{X}(a)=\\int _{a}^{b}f_{X}(x)\\,dx}\n  如果概率密度函数\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  在一点\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   上连续，那么累积分布函数可导，并且它的导数：\n  \n    \n      \n        \n          F\n          \n            X\n          \n          \n            ′\n          \n        \n        (\n        x\n        )\n        =\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle F_{X}^{\\prime }(x)=f_{X}(x)}\n  \n由于随机变量X的取值\n  \n    \n      \n        \n          P\n        \n        \n          [\n          \n            a\n            <\n            X\n            ≤\n            b\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\mathbb {P} \\left[a<X\\leq b\\right]}\n   只取决于概率密度函数的积分，所以概率密度函数在个别点上的取值并不会影响随机变量的表现。更准确来说，如果一个函数和X的概率密度函数取值不同的点只有有限个、可数无限个或者相对于整个实数轴来说测度为0（是一个零测集），那么这个函数也可以是X的概率密度函数。\n连续型的随机变量取值在任意一点的概率都是0。作为推论，连续型随机变量在区间上取值的概率与这个区间是开区间还是闭区间无关。要注意的是，概率\n\n  \n    \n      \n        \n          P\n        \n        \n          [\n          \n            X\n            =\n            a\n          \n          ]\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\mathbb {P} \\left[X=a\\right]=0}\n  ，但\n  \n    \n      \n        {\n        X\n        =\n        a\n        }\n      \n    \n    {\\displaystyle \\{X=a\\}}\n  并不是不可能事件。\n\n\n== 例子 ==\n\n最简单的概率密度函数是均匀分布的密度函数。对于一个取值在区间\n  \n    \n      \n        [\n        a\n        ,\n        b\n        ]\n      \n    \n    {\\displaystyle [a,b]}\n  上的均匀分布函数\n  \n    \n      \n        \n          \n            I\n          \n          \n            [\n            a\n            ,\n            b\n            ]\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {I} _{[a,b]}}\n  ，它的概率密度函数：\n\n  \n    \n      \n        \n          f\n          \n            \n              \n                I\n              \n              \n                [\n                a\n                ,\n                b\n                ]\n              \n            \n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              b\n              −\n              a\n            \n          \n        \n        \n          \n            I\n          \n          \n            [\n            a\n            ,\n            b\n            ]\n          \n        \n      \n    \n    {\\displaystyle f_{\\mathbf {I} _{[a,b]}}(x)={\\frac {1}{b-a}}\\mathbf {I} _{[a,b]}}\n  也就是说，当x 不在区间\n  \n    \n      \n        [\n        a\n        ,\n        b\n        ]\n      \n    \n    {\\displaystyle [a,b]}\n  上的时候，函数值等于0，而在区间\n  \n    \n      \n        [\n        a\n        ,\n        b\n        ]\n      \n    \n    {\\displaystyle [a,b]}\n  上的时候，函数值等于\n  \n    \n      \n        \n          \n            \n              1\n              \n                b\n                −\n                a\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {\\frac {1}{b-a}}}\n   。这个函数并不是完全的连续函数，但是是可积函数。\n\n正态分布是重要的概率分布。它的概率密度函数是：\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                \n                  2\n                  π\n                \n              \n            \n          \n        \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  (\n                  x\n                  −\n                  μ\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={1 \\over \\sigma {\\sqrt {2\\pi }}}\\,e^{-{(x-\\mu )^{2} \\over 2\\sigma ^{2}}}}\n  随着参数\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  和\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  变化，概率分布也产生变化。\n\n\n== 应用 ==\n随机变量X的n阶矩是X的n次方的期望值，即\n\n  \n    \n      \n        \n          E\n        \n        [\n        \n          X\n          \n            n\n          \n        \n        ]\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          x\n          \n            n\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\mathbb {E} [X^{n}]=\\int _{-\\infty }^{\\infty }x^{n}f_{X}(x)\\,dx}\n  X的方差为\n\n  \n    \n      \n        \n          σ\n          \n            X\n          \n          \n            2\n          \n        \n        =\n        \n          E\n        \n        \n          [\n          \n            \n              (\n              \n                X\n                −\n                \n                  E\n                \n                [\n                X\n                ]\n              \n              )\n            \n            \n              2\n            \n          \n          ]\n        \n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        (\n        x\n        −\n        E\n        [\n        X\n        ]\n        \n          )\n          \n            2\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\sigma _{X}^{2}=\\mathbb {E} \\left[\\left(X-\\mathbb {E} [X]\\right)^{2}\\right]=\\int _{-\\infty }^{\\infty }(x-E[X])^{2}f_{X}(x)\\,dx}\n  更广泛的说，设\n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   为一个有界连续函数，那么随机变量\n  \n    \n      \n        g\n        (\n        X\n        )\n      \n    \n    {\\displaystyle g(X)}\n  的数学期望\n\n  \n    \n      \n        \n          E\n        \n        [\n        g\n        (\n        X\n        )\n        ]\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        g\n        (\n        x\n        )\n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\mathbb {E} [g(X)]=\\int _{-\\infty }^{\\infty }g(x)f_{X}(x)\\,dx}\n  \n\n\n== 特征函数 ==\n對機率密度函數作类似傅立葉變換可得特徵函數。\n\n  \n    \n      \n        \n          Φ\n          \n            X\n          \n        \n        (\n        j\n        ω\n        )\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        f\n        (\n        x\n        )\n        \n          e\n          \n            j\n            ω\n            x\n          \n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle \\Phi _{X}(j\\omega )=\\int _{-\\infty }^{\\infty }f(x)e^{j\\omega x}\\,dx}\n  特徵函數與機率密度函數有一對一的關係。因此，知道一個分佈的特徵函數就等同於知道一個分佈的機率密度函數。\n\n\n== 參見 ==\n概率分布\n概率质量函数\n累积分布函数\n条件概率密度函数\n核密度估计\n似然函数\n\n\n== 参考文献 ==\n\n\n=== 引用 ===\n\n\n=== 书籍 ===\n钟开莱. 《概率论教程》. 上海科学技术出版社. 1989. ISBN 7-5323-0648-8.", "主成分分析": "在多元统计分析中，主成分分析（英語：Principal components analysis，PCA）是一種统计分析、簡化數據集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。具体地，主成分可以看做一个线性方程，其包含一系列线性系数来指示投影方向。PCA对原始数据的正则化或预处理敏感（相对缩放）。\n基本思想：\n\n将坐标轴中心移到数据的中心，然后旋转坐标轴，使得数据在C1轴上的方差最大，即全部n个数据个体在该方向上的投影最为分散。意味着更多的信息被保留下来。C1成为第一主成分。\nC2第二主成分：找一个C2，使得C2与C1的协方差（相关系数）为0，以免与C1信息重叠，并且使数据在该方向的方差尽量最大。\n以此类推，找到第三主成分，第四主成分……第p个主成分。p个随机变量可以有p个主成分。主成分分析经常用于减少数据集的维数，同时保留数据集當中对方差贡献最大的特征。这是通过保留低維主成分，忽略高維主成分做到的。这样低維成分往往能够保留住数据的最重要部分。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。\n主成分分析由卡尔·皮尔逊於1901年發明，用於分析數據及建立數理模型，在原理上与主轴定理相似。之后在1930年左右由哈罗德·霍特林独立发展并命名。依据应用领域的不同，在信号处理中它也叫做离散K-L 转换（discrete Karhunen–Loève transform (KLT)）。其方法主要是通過對共變異數矩陣進行特征分解，以得出數據的主成分（即特征向量）與它們的權值（即特征值）。PCA是最簡單的以特征量分析多元統計分布的方法。其結果可以理解為對原數據中的方差做出解釋：哪一個方向上的數據值對方差的影響最大？換而言之，PCA提供了一種降低數據維度的有效辦法；如果分析者在原數據中除掉最小的特征值所對應的成分，那麼所得的低維度數據必定是最優化的（也即，這樣降低維度必定是失去訊息最少的方法）。主成分分析在分析複雜數據時尤為有用，比如人臉識別。\nPCA是最简单的以特征量分析多元统计分布的方法。通常，这种运算可以被看作是揭露数据的内部结构，從而更好地展現数据的變異度。如果一个多元数据集是用高维数据空间之坐标系來表示的，那么PCA能提供一幅较低维度的图像，相當於数据集在讯息量最多之角度上的一個投影。这样就可以利用少量的主成分讓数据的维度降低了。\nPCA 跟因子分析密切相关。因子分析通常包含更多特定領域底層結構的假設，並且求解稍微不同矩陣的特徵向量。\nPCA 也跟典型相關分析（CCA）有關。CCA定義的坐標系可以最佳地描述兩個數據集之間的互協方差，而PCA定義了新的正交坐標系，能最佳地描述單個數據集當中的變異數。\n\n\n== 数学定义 ==\nPCA的数学定义是：一个正交化线性变换，把数据变换到一个新的坐标系统中，使得这一数据的任何投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推。\n定义一个\n  \n    \n      \n        n\n        ×\n        m\n      \n    \n    {\\displaystyle n\\times m}\n  的矩阵, \n  \n    \n      \n        \n          X\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X^{T}}\n  为去平均值（以平均值为中心移动至原点）的数据，其行为数据样本，列为数据类别（注意，这里定义的是\n  \n    \n      \n        \n          X\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X^{T}}\n   而不是\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ）。则\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的奇异值分解为\n  \n    \n      \n        X\n        =\n        W\n        Σ\n        \n          V\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X=W\\Sigma V^{T}}\n  ，其中\n  \n    \n      \n        W\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            m\n          \n        \n      \n    \n    {\\displaystyle W\\in \\mathbf {R} ^{m\\times m}}\n  是\n  \n    \n      \n        X\n        \n          X\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle XX^{T}}\n  的特征向量矩阵， \n  \n    \n      \n        Σ\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle \\Sigma \\in \\mathbf {R} ^{m\\times n}}\n  是奇异值矩阵，\n  \n    \n      \n        V\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle V\\in \\mathbf {R} ^{n\\times n}}\n  是\n  \n    \n      \n        \n          X\n          \n            T\n          \n        \n        X\n      \n    \n    {\\displaystyle X^{T}X}\n  的特征向量矩阵。据此，\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    Y\n                  \n                  \n                    ⊤\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    X\n                  \n                  \n                    ⊤\n                  \n                \n                \n                  W\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  V\n                \n                \n                  \n                    Σ\n                  \n                  \n                    ⊤\n                  \n                \n                \n                  \n                    W\n                  \n                  \n                    ⊤\n                  \n                \n                \n                  W\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  V\n                \n                \n                  \n                    Σ\n                  \n                  \n                    ⊤\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\boldsymbol {Y}}^{\\top }&={\\boldsymbol {X}}^{\\top }{\\boldsymbol {W}}\\\\&={\\boldsymbol {V}}{\\boldsymbol {\\Sigma }}^{\\top }{\\boldsymbol {W}}^{\\top }{\\boldsymbol {W}}\\\\&={\\boldsymbol {V}}{\\boldsymbol {\\Sigma }}^{\\top }\\end{aligned}}}\n  当 m < n − 1时，V 在通常情况下不是唯一定义的，而Y 则是唯一定义的。W 是一个正交矩阵，YTWT=XT，且YT的第一列由第一主成分组成，第二列由第二主成分组成，依此类推。\n为了得到一种降低数据维度的有效办法，我们可以利用WL把 X 映射到一个只应用前面L个向量的低维空间中去：\n\n  \n    \n      \n        \n          Y\n        \n        =\n        \n          \n            \n              W\n              \n                L\n              \n            \n          \n          \n            ⊤\n          \n        \n        \n          X\n        \n        =\n        \n          \n            Σ\n            \n              L\n            \n          \n        \n        \n          \n            V\n          \n          \n            ⊤\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Y} =\\mathbf {W_{L}} ^{\\top }\\mathbf {X} =\\mathbf {\\Sigma _{L}} \\mathbf {V} ^{\\top }}\n  其中\n  \n    \n      \n        \n          \n            Σ\n            \n              L\n            \n          \n        \n        =\n        \n          \n            I\n          \n          \n            L\n            ×\n            m\n          \n        \n        \n          Σ\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma _{L}} =\\mathbf {I} _{L\\times m}\\mathbf {\\Sigma } }\n  ，且\n  \n    \n      \n        \n          \n            I\n          \n          \n            L\n            ×\n            m\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {I} _{L\\times m}}\n  為\n  \n    \n      \n        L\n        ×\n        m\n      \n    \n    {\\displaystyle L\\times m}\n  的單位矩陣。\nX 的单向量矩阵W相当于协方差矩阵的特征向量 C = X XT,\n\n  \n    \n      \n        \n          X\n        \n        \n          \n            X\n          \n          \n            ⊤\n          \n        \n        =\n        \n          W\n        \n        \n          Σ\n        \n        \n          \n            Σ\n          \n          \n            ⊤\n          \n        \n        \n          \n            W\n          \n          \n            ⊤\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} \\mathbf {X} ^{\\top }=\\mathbf {W} \\mathbf {\\Sigma } \\mathbf {\\Sigma } ^{\\top }\\mathbf {W} ^{\\top }}\n  在欧几里得空间给定一组点数，第一主成分对应于通过多维空间平均点的一条线，同时保证各个点到这条直线距离的平方和最小。去除掉第一主成分后，用同样的方法得到第二主成分。依此类推。在Σ中的奇异值均为矩阵 XXT的特征值的平方根。每一个特征值都与跟它们相关的方差是成正比的，而且所有特征值的总和等于所有点到它们的多维空间平均点距离的平方和。PCA提供了一种降低维度的有效办法，本质上，它利用正交变换将围绕平均点的点集中尽可能多的变量投影到第一维中去，因此，降低维度必定是失去讯息最少的方法。PCA具有保持子空间拥有最大方差的最优正交变换的特性。然而，当与离散余弦变换相比时，它需要更大的计算需求代价。非线性降维技术相对于PCA来说则需要更高的计算要求。\nPCA对变量的缩放很敏感。如果我们只有两个变量，而且它们具有相同的样本方差，并且成正相关，那么PCA将涉及两个变量的主成分的旋转。但是，如果把第一个变量的所有值都乘以100，那么第一主成分就几乎和这个变量一样，另一个变量只提供了很小的贡献，第二主成分也将和第二个原始变量几乎一致。这就意味着当不同的变量代表不同的单位（如温度和质量）时，PCA是一种比较武断的分析方法。但是在Pearson的题为\n\"On Lines and Planes of Closest Fit to Systems of Points in Space\"的原始文件里，是假设在欧几里得空间里不考虑这些。一种使PCA不那么武断的方法是使用变量缩放以得到单位方差。\n\n\n== 讨论 ==\n通常，为了确保第一主成分描述的是最大方差的方向，我们会使用平均减法进行主成分分析。如果不执行平均减法，第一主成分有可能或多或少的对应于数据的平均值。另外，为了找到近似数据的最小均方误差，我们必须选取一个零均值。\n假设零经验均值，数据集 X 的主成分w1可以被定义为：\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            1\n          \n        \n        =\n        \n          \n            \n              arg\n              \n              m\n              a\n              x\n            \n            \n              ‖\n              \n                w\n              \n              ‖\n              =\n              1\n            \n          \n        \n        \n        Var\n        ⁡\n        {\n        \n          \n            w\n          \n          \n            ⊤\n          \n        \n        \n          X\n        \n        }\n        =\n        \n          \n            \n              arg\n              \n              m\n              a\n              x\n            \n            \n              ‖\n              \n                w\n              \n              ‖\n              =\n              1\n            \n          \n        \n        \n        E\n        \n          {\n          \n            \n              (\n              \n                \n                  \n                    w\n                  \n                  \n                    ⊤\n                  \n                \n                \n                  X\n                \n              \n              )\n            \n            \n              2\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{1}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\operatorname {Var} \\{\\mathbf {w} ^{\\top }\\mathbf {X} \\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,E\\left\\{\\left(\\mathbf {w} ^{\\top }\\mathbf {X} \\right)^{2}\\right\\}}\n  为了得到第 k个主成分，必须先从X中减去前面的 \n  \n    \n      \n        k\n        −\n        1\n      \n    \n    {\\displaystyle k-1}\n   个主成分：\n\n  \n    \n      \n        \n          \n            \n              \n                X\n                ^\n              \n            \n          \n          \n            k\n            −\n            1\n          \n        \n        =\n        \n          X\n        \n        −\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            k\n            −\n            1\n          \n        \n        \n          \n            w\n          \n          \n            i\n          \n        \n        \n          \n            w\n          \n          \n            i\n          \n          \n            ⊤\n          \n        \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {X}} _{k-1}=\\mathbf {X} -\\sum _{i=1}^{k-1}\\mathbf {w} _{i}\\mathbf {w} _{i}^{\\top }\\mathbf {X} }\n  然后把求得的第k个主成分带入数据集，得到新的数据集，继续寻找主成分。\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            k\n          \n        \n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            \n              ‖\n              \n                w\n              \n              ‖\n              =\n              1\n            \n          \n        \n        \n        E\n        \n          {\n          \n            \n              (\n              \n                \n                  \n                    w\n                  \n                  \n                    ⊤\n                  \n                \n                \n                  \n                    \n                      \n                        X\n                        ^\n                      \n                    \n                  \n                  \n                    k\n                    −\n                    1\n                  \n                \n              \n              )\n            \n            \n              2\n            \n          \n          }\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {w} _{k}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {arg\\,max} }}\\,E\\left\\{\\left(\\mathbf {w} ^{\\top }\\mathbf {\\hat {X}} _{k-1}\\right)^{2}\\right\\}.}\n  \nPCA相当于在气象学中使用的经验正交函数（EOF）,同时也类似于一个线性隐层神经网络。 隐含层 K 个神经元的权重向量收敛后，将形成一个由前 K 个主成分跨越空间的基础。但是与PCA不同的是，这种技术并不一定会产生正交向量。\nPCA是一种很流行且主要的的模式识别技术。然而，它并不能最优化类别可分离性 。另一种不考虑这一点的方法是线性判别分析。\n\n\n== 符号和缩写表 ==\n\n\n== 主成分分析的属性和限制 ==\n如上所述，主成分分析的结果依赖于变量的缩放。\n主成分分析的适用性受到由它的派生物产生的某些假设 的限制。\n\n\n== 主成分分析和信息理论 ==\n通过使用降维来保存大部分数据信息的主成分分析的观点是不正确的。确实如此，当没有任何假设信息的信号模型时，主成分分析在降维的同时并不能保证信息的不丢失，其中信息是由香农熵来衡量的。\n基于假设得\n\n  \n    \n      \n        \n          x\n        \n        =\n        \n          s\n        \n        +\n        \n          n\n        \n      \n    \n    {\\displaystyle \\mathbf {x} =\\mathbf {s} +\\mathbf {n} }\n  \n也就是说，向量 x 是含有信息的目标信号 s 和噪声信号 n 之和，从信息论角度考虑主成分分析在降维上是最优的。\n特别地，Linsker证明了如果 s 是高斯分布，且 n 是 与密度矩阵相应的协方差矩阵的高斯噪声，\n\n\n== 使用统计方法计算PCA ==\n以下是使用统计方法计算PCA的详细说明。但是请注意，如果利用奇异值分解（使用标准的软件）效果会更好。\n我们的目标是把一个给定的具有 M 维的数据集X 变换成具有较小维度 L的数据集Y。现在要求的就是矩阵Y，Y是矩阵X  Karhunen–Loève变换。:\n  \n    \n      \n        \n          Y\n        \n        =\n        \n          K\n          L\n          T\n        \n        {\n        \n          X\n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {Y} =\\mathbb {KLT} \\{\\mathbf {X} \\}}\n  \n\n\n== 组织数据集 ==\n假设有一组 M 个变量的观察数据，我们的目的是减少数据，使得能够用L 个向量来描述每个观察值，L < M。进一步假设，该数据被整理成一组具有N个向量的数据集，其中每个向量都代表M 个变量的单一观察数据。\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        …\n        \n          \n            x\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{1}\\ldots \\mathbf {x} _{N}}\n  为列向量，其中每个列向量有M 行。将列向量放入M × N的单矩阵X 裡。\n\n\n== 计算经验均值 ==\n对每一维m = 1, ..., M计算经验均值将计算得到的均值放入一个 M × 1维的经验均值向量u中\n  \n    \n      \n        u\n        [\n        m\n        ]\n        =\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            N\n          \n        \n        X\n        [\n        m\n        ,\n        n\n        ]\n      \n    \n    {\\displaystyle u[m]={1 \\over N}\\sum _{n=1}^{N}X[m,n]}\n  \n\n\n== 计算平均偏差 ==\n对于在最大限度地减少近似数据的均方误差的基础上找到一个主成分来说，均值减去法是该解决方案的不可或缺的组成部分 。因此，我们继续如下步骤：\n\n从数据矩阵X的每一列中减去经验均值向量 u将平均减去过的数据存储在M × N矩阵B中\n  \n    \n      \n        \n          B\n        \n        =\n        \n          X\n        \n        −\n        \n          u\n        \n        \n          h\n        \n      \n    \n    {\\displaystyle \\mathbf {B} =\\mathbf {X} -\\mathbf {u} \\mathbf {h} }\n  \n其中h是一个长度为N的全为1的行向量：\n  \n    \n      \n        h\n        [\n        n\n        ]\n        =\n        1\n        \n        \n        \n        \n          for \n        \n        n\n        =\n        1\n        ,\n        …\n        ,\n        N\n      \n    \n    {\\displaystyle h[n]=1\\,\\qquad \\qquad {\\text{for }}n=1,\\ldots ,N}\n  \n\n\n== 求协方差矩阵 ==\n从矩阵B 中找到M × M 的经验协方差矩阵C\n  \n    \n      \n        \n          C\n        \n        =\n        \n          E\n        \n        \n          [\n          \n            \n              B\n            \n            ⊗\n            \n              B\n            \n          \n          ]\n        \n        =\n        \n          E\n        \n        \n          [\n          \n            \n              B\n            \n            ⋅\n            \n              \n                B\n              \n              \n                ∗\n              \n            \n          \n          ]\n        \n        =\n        \n          \n            1\n            \n              N\n              −\n              1\n            \n          \n        \n        \n          ∑\n          \n\n          \n        \n        \n          B\n        \n        ⋅\n        \n          \n            B\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {C} =\\mathbb {E} \\left[\\mathbf {B} \\otimes \\mathbf {B} \\right]=\\mathbb {E} \\left[\\mathbf {B} \\cdot \\mathbf {B} ^{*}\\right]={1 \\over N-1}\\sum _{}\\mathbf {B} \\cdot \\mathbf {B} ^{*}}\n  其中  \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbb {E} }\n  为期望值\n\n  \n    \n      \n        ⊗\n      \n    \n    {\\displaystyle \\otimes }\n  是最外层运算符\n\n  \n    \n      \n        ∗\n         \n      \n    \n    {\\displaystyle *\\ }\n  是共轭转置运算符。\n请注意，如果B完全由实数组成，那么共轭转置与正常的转置一样。\n\n为什么是N-1,而不是N，Bessel's correction给出了解释\n\n\n=== 查找协方差矩阵的特征值和特征向量 ===\n计算矩阵C 的特征向量\n  \n    \n      \n        \n          \n            V\n          \n          \n            −\n            1\n          \n        \n        \n          C\n        \n        \n          V\n        \n        =\n        \n          D\n        \n      \n    \n    {\\displaystyle \\mathbf {V} ^{-1}\\mathbf {C} \\mathbf {V} =\\mathbf {D} }\n  其中，D 是C的特征值对角矩阵，这一步通常会涉及到使用基于计算机的计算特征值和特征向量的算法。在很多矩阵代数系统中这些算法都是现成可用的，如R语言，MATLAB, Mathematica, SciPy, IDL(交互式数据语言), 或者GNU Octave以及OpenCV。矩阵D为M × M的对角矩阵各个特征值和特征向量都是配对的，m个特征值对应m个特征向量。\n\n\n== 参见 ==\n\n圖模式\n马尔可夫链\n马尔可夫逻辑网络\n\n\n== 注释 ==\n\n\n== 参考 ==\nJolliffe, I. T. Principal Component Analysis. Springer-Verlag. 1986: 487  [2012-01-24]. ISBN 978-0-387-95442-4. doi:10.1007/b98835. （原始内容存档于2019-10-16）.", "显著性差异": "統計學的假說檢定中，顯著性差異（或统计学意义，英語：statistical significance）是對數據差異性的評價，當某次實驗的结果在虛無假說下不大可能发生时，就認為該結果具有顯著性差異。更準確而言，譬如某項研究設定了一個數值α（顯著水準），表示虛無假說本來正確但卻被拒絕的出錯概率，然後用p值表示虛無假說為真時得到某結果或比這個結果更極端的情況的概率。當p ⩽ α時，就可以認為結果具有統計學意義，或數據之間具有了顯著性差異。顯著水準應當在開始數據收集前就設定，通常習慣設定為5%或更低，因研究的具體學科領域而異。在任何涉及到从总体中抽取样本的实验或观察性研究中，观察到的结果都有可能只不过是由抽样误差产生的。但是，如果一个观察结果的p值小于（或等于）显著性水平α，研究者就可以得出“该结果能反映总体的特征”的结论，并拒绝零假设。\n顯著性差異的原因可能是：\n\n參與比對的數據是來自不同實驗對象，如比－西一般能力測驗中，大學學歷被試組的成績與小學學歷被試組之間，會存在顯著性差異；\n也可能是因為實驗處理對實驗對象造成了改變，因而前測、後測的數據會有顯著性差異。例如，記憶術研究發現，被試者學習某記憶法前的成績，和學習記憶法後的記憶成績會有顯著性差異，則這一差異很可能來自於這種記憶法對被試記憶能力的改變。\n\n\n== 歷史 ==\n顯著性差異的提出可追溯到18世纪，约翰·阿巴思诺特和皮埃尔-西蒙·拉普拉斯作出了男女出生概率均等的零假设，然后计算了人类出生时性别比的p值。1925年，羅納德·費雪在《研究工作者的统计方法》一书中提出了统计假设检验的思想，称之为“显著性检验”（tests of significance）。費雪建議将1/20（=0.05）的概率作为拒绝虛無假說的一个截断值。在1933年的一篇论文中，耶日·内曼和埃贡·皮尔逊把这个截断值称为“显著性水平”，並賦予它符號α。他们建议，α值應當在收集任何数据收集之前提前设定。費雪最初將显著性水平定為0.05，但他并不打算将这一截断值定死。在他1956年出版的《统计方法与科学推断》一书中，他建议根据具体情况确定显著性水平。\n\n\n=== 相關概念 ===\n显著性水平α是p值的阈值，當p ⩽ α時就拒絕零假设（即使零假设仍有可能是正确的）。这意味着α也是在零假设正确的情况下错误地将其否定的概率，称为伪阳性或型一錯誤、棄真錯誤、α錯誤。\n而有些研究者偏好使用置信水平γ = 1 − α。它是零假设成立时不拒绝零假设的概率。置信水平和置信区间是Neyman于1937年提出的。\n\n\n== 顯著水準 ==\n\n顯著水準（significance level，符號：α）常用于假设检验中检验假设和实验结果是否一致，它代表在虛無假說（記作\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ）為真時，錯誤地拒絕\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  的機率，即發生型一錯誤（棄真錯誤、α錯誤）的機率。\n比如，我們從兩個母體中分別抽取了兩組樣本數據A和B，這兩組數據在顯著水準α = 0.05下具備顯著性差異。這是說，兩組數據所代表的母體具備顯著性差異的可能性為95%；但它們代表的母體仍有5%的可能性是沒有顯著性差異的，這5%是由於抽样误差造成的。也可表述为：\n\n如果拒绝“两组数据一致（二者不具备显著性差异）”的零假设（接受“两组数据不一致”的备择假设），此时有5%的可能性犯第一类错误；\n如果A=两组数据不具备显著差异；B=实际数据具有显著差异，則P(A|B) = 0.05，即統計100次，預期是B情況，但可能出現5次的A情況。當假說檢定所測得之數據之間具有顯著性差異，實驗的虛無假說就可被推翻，也就是拒絕\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ，接受對立假說（alternative hypothesis，記作\n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  或\n  \n    \n      \n        \n          H\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle H_{a}}\n  ）；反之，若數據之間不具備顯著性差異，則拒絕對立假說，不拒絕虛無假說。通常情況下，實驗結果需要證明達到顯著水準α = 0.05或0.01，才可以說數據之間具備了顯著性差異，否則就如上所述，容易作出錯誤的推論。在作結論時，應確實描述方向性（例如顯著大於或顯著小於）。\n数学表述为：引入p值作为检验样本（test statistic）观察值的最低顯著水準。在α = 0.01或α = 0.05的条件下，若零假设成立的概率（p）小于α，则表示零假设成立的情况下得到这种观测结果的概率，比1%或5%還低，在该显著性水平下，我们可拒绝该零假设。\n\nP(X=x)<α=0.05为“显著（significant)”，统计分析软件SPSS中以*标记；\nP(X=x)<α=0.01为“极显著（extremely significant）”，通常以**标记。\n\n\n== 局限性 ==\n研究人员常常只关注他们的结果是否具有统计学意义，但其报告的结果可能并没有实质性，或者研究结果无法重现。统计学意义与实际意义之间也不能等同，有统计学意义的研究未必就有实际意义。\n\n\n=== 效应值 ===\n\n效应值是衡量一项研究的实际意义。统计上显著的结果可能效应量很低。为了衡量结果的研究意义，研究人员最好同时给出效应值和p值。效应量量化了效应的强度，例如以标准差为单位的两个平均值之间的距离（Cohen's d）、两个变量之间的相关系数或其平方，以及其他度量。\n\n\n=== 再现性 ===\n\n统计上显著的结果未必能够轻易再现。特别是一些有显著性差异的结果实际上是假阳性。重现结果每失败一次，都意味着研究结果实际上为假阳性的可能性增加。\n\n\n== 参见 ==\n假說檢定\nA/B測試\n查看别处效应\n多重比較謬誤\n样本量确定\n德州神槍手謬誤\n\n\n== 参考文献 ==", "数据分析": "数据分析是一種统计学常用方法，其主要特点是多维性和描述性。有些几何方法有助于揭示不同的数据之间存在的关系，并绘制出统计信息图，以更简洁的解释这些数据中包含的主要信息。其他一些用于收集数据，以便弄清哪些是同质的，从而更好地了解数据。\n資料分析可以处理大量数据，并确定这些数据最有用的部分。本学科近年来的成功，很大程度上是因为制图技术的提高。这些图可以通过直接分析数据，来突出难以捕捉的关系；更重要的是，这些表达方法与基于现象分布的“先验”观念无关，与经典统计方法正相反。\n資料分析的数学基础在20世纪早期就已确立，但直到计算机的出现才使得实际操作成为可能，并使得資料分析得以推广，而資料分析是数学与计算机科学相结合的产物，，且相關的應用還能在未來起到預測輿情、風險控管的效果。若是以固定时间为資料分析的颗粒单位，则称为时间序列分析，是主要作为销售数据商业分析的方法之一。", "关联规则学习": "关联规则学习（英語：Association rule learning）是一种在大型数据库中发现变量之间的有趣性关系的方法。它的目的是利用一些有趣性的量度来识别数据库中发现的强规则。  基于强规则的概念，Rakesh Agrawal等人引入了关联规则以发现由超市的POS系统记录的大批交易数据中产品之间的规律性。例如，从销售数据中发现的规则 {洋葱, 土豆}→{汉堡} 会表明如果顾客一起买洋葱和土豆，他们也有可能买汉堡的肉。此类信息可以作为做出促销定价或产品植入等营销活动决定的根据。除了上面购物篮分析中的例子以外， 关联规则如今还被用在许多应用领域中，包括网络用法挖掘、入侵检测、连续生产及生物信息学中。与序列挖掘相比，关联规则学习通常不考虑在事务中、或事务间的项目的顺序。\n\n\n== 基本概念 ==\n根据韩家炜等，关联规则定义为：\n假设\n  \n    \n      \n        I\n        =\n        {\n        \n          I\n          \n            1\n          \n        \n        ,\n        \n          I\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          I\n          \n            m\n          \n        \n        }\n      \n    \n    {\\displaystyle I=\\{I_{1},I_{2},\\ldots ,I_{m}\\}}\n  是项目的集合（項集）。给定一个交易数据库\n  \n    \n      \n        D\n        =\n        {\n        \n          t\n          \n            1\n          \n        \n        ,\n        \n          t\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          t\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle D=\\{t_{1},t_{2},\\ldots ,t_{n}\\}}\n  ，其中每个交易（Transaction）t是I的非空子集，即\n  \n    \n      \n        t\n        ⊆\n        I\n      \n    \n    {\\displaystyle t\\subseteq I}\n  ，每一个交易都与一个唯一的标识符TID（Transaction ID）对应。关联规则是形如\n  \n    \n      \n        X\n        ⇒\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n  的蕴涵式，其中\n  \n    \n      \n        X\n        ,\n        Y\n        ⊆\n        I\n      \n    \n    {\\displaystyle X,Y\\subseteq I}\n  且\n  \n    \n      \n        X\n        ∩\n        Y\n        =\n        ∅\n      \n    \n    {\\displaystyle X\\cap Y=\\emptyset }\n  ， \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  分别称为关联规则的先导（antecedent 或 left-hand-side, LHS）和后继（consequent 或 right-hand-side, RHS） 。关联规则\n  \n    \n      \n        X\n        ⇒\n        Y\n      \n    \n    {\\displaystyle X\\Rightarrow Y}\n  在D中的支持度（support）是D中交易包含\n  \n    \n      \n        X\n        ∪\n        Y\n      \n    \n    {\\displaystyle X\\cup Y}\n  的百分比，即概率\n  \n    \n      \n        P\n        (\n        X\n        ∪\n        Y\n        \n          |\n        \n        D\n        )\n      \n    \n    {\\displaystyle P(X\\cup Y|D)}\n  ；置信度（confidence）是包含X的交易中同时包含Y的百分比，即条件概率\n  \n    \n      \n        P\n        \n          (\n          \n            Y\n            \n              |\n            \n            X\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(Y|X\\right)}\n  。如果同时满足最小支持度阈值和最小置信度阈值，则认为关联规则是有利或有用的。这些阈值由用户或者专家设定。\n用一个简单的例子说明。表1是顾客购买记录的数据库D，包含6个交易。项集I={网球拍,网球,运动鞋,羽毛球}。考虑关联规则：网球拍\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  网球，交易1,2,3,4,6包含网球拍，交易1,2,6同时包含网球拍和网球，支持度\n  \n    \n      \n        s\n        u\n        p\n        p\n        o\n        r\n        t\n        =\n        \n          \n            3\n            6\n          \n        \n        =\n        0.5\n      \n    \n    {\\displaystyle support={\\frac {3}{6}}=0.5}\n  ，置信度\n  \n    \n      \n        c\n        o\n        n\n        f\n        i\n        d\n        e\n        n\n        t\n        =\n        \n          \n            3\n            5\n          \n        \n        =\n        0.6\n      \n    \n    {\\displaystyle confident={\\frac {3}{5}}=0.6}\n  。若给定最小支持度\n  \n    \n      \n        α\n        =\n        0.5\n      \n    \n    {\\displaystyle \\alpha =0.5}\n  ，最小置信度\n  \n    \n      \n        β\n        =\n        0.6\n      \n    \n    {\\displaystyle \\beta =0.6}\n  ，关联规则网球拍\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  网球是有趣的，认为购买网球拍和购买网球之间存在强关联。\n\n\n== 分类 ==\n关联规则有以下常见分类：\n根据关联规则所处理的值的类型\n\n如果考虑关联规则中的数据项是否出现，则这种关联规则是布尔关联规则（Boolean association rules）。例如上面的例子。\n如果关联规则中的数据项是数量型的，这种关联规则是数量关联规则（quantitative association rules）。例如年龄(\"20-25\")\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  购买(\"网球拍\")，年龄是一个数量型的数据项。在这种关联规则中，一般将数量离散化（discretize）为区间。根据关联规则所涉及的数据维数\n\n如果关联规则各项只涉及一个维，则它是单维关联规则（single-dimensional association rules），例如购买(\"网球拍\")\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  购买(\"网球\")只涉及“购买”一个维度。\n如果关联规则涉及两个或两个以上维度，则它是多维关联规则（multi-dimensional association rules），例如年龄(\"20-25\")\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  购买(\"网球拍\")涉及“年龄”和“购买”两个维度。根据关联规则所涉及的抽象层次\n\n如果不涉及不同层次的数据项，得到的是单层关联规则（single-level association rules）。\n在不同抽象层次中挖掘出的关联规则称为广义关联规则（generalized association rules）。例如年龄(\"20-25\")\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  购买(\"HEAD网球拍\")和年龄(\"20-25\")\n  \n    \n      \n        ⇒\n      \n    \n    {\\displaystyle \\Rightarrow }\n  购买(\"网球拍\")是广义关联规则，因为\"HEAD网球拍\"和\"网球拍\"属于不同的抽象层次。\n\n\n== 算法 ==\n\n\n=== Apriori 演算法 ===\n\nApriori演算法所使用的前置统计量包括：\n\n最大规则物件数：规则中物件组所包含的最大物件数量；\n最小支援：规则中物件或是物件组必须符合的最低案例数；\n最小信心水准：计算规则所必须符合的最低信心水准门槛。\n\n\n=== F-P算法 ===\n\n\n== 参考文献 ==", "司徒頓t檢定": "司徒頓t 檢定（英語：Student's t-test）是指虛無假說成立時的任一檢定統計有司徒頓t分布的統計假說檢定，屬於參數統計。學生t檢驗常作為檢驗一群來自常態分配母體的獨立樣本之期望值是否為某一實數，或是二（两）群來自常態分配母體的獨立樣本之期望值的差是否為某一實數。舉個簡單的例子，在某個學校中我們可以從某個年級中隨機抽樣一群男生，以檢驗該年級男生與全校男生之身高差異程度是否如我們所假設的某個值。\n\n\n== 由來 ==\n司徒頓t檢定是威廉·戈塞為了觀測釀酒品質於1908年所提出的，「司徒頓 (student)」則是他的筆名。\n基於克勞德·健力士（Claude Guinness）聘用從牛津大學和劍橋大學出來的最好的畢業生，以將生物化學及統計學應用到健力士工業流程的創新政策，戈塞受雇於都柏林的健力士釀酒廠擔任統計學家。戈塞提出了t检验以降低啤酒重量监控的成本。戈塞於1908年在《Biometrika》期刊上公布t檢驗，但因其老闆認為其為商業機密而被迫使用筆名，統計學論文內容也跟釀酒無關。實際上，其他统计学家是知道戈塞真實身份的。\n\n\n== 應用 ==\n常見的應用有：\n\n单样本检验：检验一个正态分布的总体的均值是否在满足零假设的值之内，例如檢驗一群軍校男生的身高的平均是否符合全國標準的170公分界線。\n獨立樣本t檢定（双样本）：其零假设为两个正态分布的总体的均值之差為某實數，例如檢定二群人之平均身高是否相等。若两母體的變異數是相等的情况下（同質變異數），自由度為兩樣本數相加再減二；若為異質變異數（母體變異數不相等），自由度則為Welch自由度，此情況下有时被称为Welch检验。\n配对樣本t檢定（成對樣本t檢定）：檢定自同一母體抽出的成對樣本間差异是否为零。例如，檢测一位病人接受治疗前和治疗后的肿瘤尺寸大小。若治疗是有效的，我们可以推定多数病人接受治疗后，肿瘤尺寸將縮小。\n检验一迴歸模型的偏迴歸係數是否显著不为零，即檢定解釋變數X是否存在對被解釋變數Y的解釋能力，其檢定統計量稱之為t-比例（t-ratio）。\n\n\n== 前提假設 ==\n大多數的t檢定之統計量具有t = Z/s的形式，其中Z與s是已知資料的函數。Z通常被設計成對於對立假說有關的形式，而s是一個比例母數使t服從於t分佈。以單樣本t檢驗為例，\n  \n    \n      \n        Z\n        =\n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        \n          /\n        \n        (\n        σ\n        \n          /\n        \n        \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle Z={\\bar {X}}/(\\sigma /{\\sqrt {n}})}\n  ，其中\n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n  為樣本平均數，\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  為樣本數，\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  為总体標準差。至於s在單樣本t檢驗中為\n  \n    \n      \n        \n          \n            \n              σ\n              ^\n            \n          \n        \n        \n          /\n        \n        σ\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}/\\sigma }\n  ，其中\n  \n    \n      \n        \n          \n            \n              σ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}}\n  為樣本的標準差。在符合零假說的條件下，t檢定有以下前提：\n\nZ 服從標準常態分佈\n(n - 1)s2 服從自由度(n - 1)的卡方分佈\nZ與s互相獨立\n\n\n== 計算 ==\n\n\n=== 單樣本t檢驗 ===\n檢驗虛無假說為一群來自常態分配獨立樣本xi之母體期望值μ為μ0可利用以下統計量\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  x\n                  ¯\n                \n              \n              −\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              s\n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\overline {x}}-\\mu _{0}}{s/{\\sqrt {n}}}}}\n  其中\n  \n    \n      \n        i\n        =\n        1\n        …\n        n\n      \n    \n    {\\displaystyle i=1\\ldots n}\n  ，\n  \n    \n      \n        \n          \n            x\n            ¯\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}={\\frac {\\sum _{i=1}^{n}x_{i}}{n}}}\n  為樣本平均數，\n  \n    \n      \n        s\n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                  x\n                  \n                    i\n                  \n                \n                −\n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                n\n                −\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s={\\sqrt {\\frac {\\sum _{i=1}^{n}(x_{i}-{\\overline {x}})^{2}}{n-1}}}}\n  為樣本標準差，n為樣本數。該統計量t在虛無假說：μ = μ0為真的條件下服從自由度為n − 1的t分佈。\n\n\n=== 配對樣本t檢驗 ===\n配對樣本t檢驗可視為單樣本t檢驗的擴展，不過檢驗的對象由一群來自常態分配獨立樣本更改為兩配對樣本之觀測值之差。\n若兩配對樣本x1i與x2i之差為di = x1i − x2i獨立且來自常態分配，則di之母體期望值μ是否為μ0可利用以下統計量\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  d\n                  ¯\n                \n              \n              −\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              \n                s\n                \n                  d\n                \n              \n              \n                /\n              \n              \n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\overline {d}}-\\mu _{0}}{s_{d}/{\\sqrt {n}}}}}\n  其中\n  \n    \n      \n        i\n        =\n        1\n        …\n        n\n      \n    \n    {\\displaystyle i=1\\ldots n}\n  ，\n  \n    \n      \n        \n          \n            d\n            ¯\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                d\n                \n                  i\n                \n              \n            \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\overline {d}}={\\frac {\\sum _{i=1}^{n}d_{i}}{n}}}\n  為配對樣本差值之平均數，\n  \n    \n      \n        \n          s\n          \n            d\n          \n        \n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                (\n                \n                  d\n                  \n                    i\n                  \n                \n                −\n                \n                  \n                    d\n                    ¯\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                n\n                −\n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s_{d}={\\sqrt {\\frac {\\sum _{i=1}^{n}(d_{i}-{\\overline {d}})^{2}}{n-1}}}}\n  為配對樣本差值之標準差，n為配對樣本數。該統計量t在虛無假說：μ = μ0為真的條件下服從自由度為n − 1的t分布。\n\n\n=== 獨立雙樣本t檢驗 ===\n\n\n==== 同質變異數假設 (Homoscedasticity)、樣本數相等 ====\n若兩獨立樣本x1i與x2i具有相同之樣本數n，且來自兩個母體變異數相同（同質變異數假設）的常態分配，則兩母體之期望值差μ1 - μ2是否為μ0可利用以下統計量\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  1\n                \n              \n              −\n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  2\n                \n              \n              −\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              2\n              \n                s\n                \n                  p\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              n\n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\overline {x}}_{1}-{\\overline {x}}_{2}-\\mu _{0}}{\\sqrt {2s_{p}^{2}/n}}}}\n  其中\n  \n    \n      \n        i\n        =\n        1\n        …\n        n\n      \n    \n    {\\displaystyle i=1\\ldots n}\n  ，\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            1\n            i\n          \n        \n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle {\\overline {x}}_{1}=(\\sum _{i=1}^{n}x_{1i})/n}\n  及\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            2\n            i\n          \n        \n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle {\\overline {x}}_{2}=(\\sum _{i=1}^{n}x_{2i})/n}\n  為兩樣本各自的平均數，\n  \n    \n      \n        \n          s\n          \n            p\n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            1\n            i\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            2\n            i\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        \n          /\n        \n        (\n        2\n        n\n        −\n        2\n        )\n      \n    \n    {\\displaystyle s_{p}^{2}=(\\sum _{i=1}^{n}(x_{1i}-{\\overline {x}}_{1})^{2}+\\sum _{i=1}^{n}(x_{2i}-{\\overline {x}}_{2})^{2})/(2n-2)}\n  為樣本之共同方差。該統計量t在虛無假說：μ1 - μ2 = μ0為真的條件下服從自由度為2n − 2的t分佈。\n\n\n==== 同質變異數假設 (Homoscedasticity)、樣本數不相等 ====\n若兩獨立樣本x1i與x2j具有不相同之樣本數n1與n2，且來自兩個母體變異數相同（同質變異數假設）的常態分配，則兩母體之期望值之差μ1 - μ2是否為μ0可利用以下統計量\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  1\n                \n              \n              −\n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  2\n                \n              \n              −\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              \n                s\n                \n                  p\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  1\n                \n              \n              +\n              \n                s\n                \n                  p\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\overline {x}}_{1}-{\\overline {x}}_{2}-\\mu _{0}}{\\sqrt {s_{p}^{2}/n_{1}+s_{p}^{2}/n_{2}}}}}\n  其中\n  \n    \n      \n        i\n        =\n        1\n        …\n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle i=1\\ldots n_{1}}\n  ，其中\n  \n    \n      \n        j\n        =\n        1\n        …\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j=1\\ldots n_{2}}\n  ，\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            1\n            i\n          \n        \n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle {\\overline {x}}_{1}=(\\sum _{i=1}^{n}x_{1i})/n}\n  及\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          x\n          \n            2\n            i\n          \n        \n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle {\\overline {x}}_{2}=(\\sum _{i=1}^{n}x_{2i})/n}\n  為兩樣本各自的平均數，\n  \n    \n      \n        \n          s\n          \n            p\n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            1\n            i\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            2\n            j\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        \n          /\n        \n        (\n        \n          n\n          \n            1\n          \n        \n        +\n        \n          n\n          \n            2\n          \n        \n        −\n        2\n        )\n      \n    \n    {\\displaystyle s_{p}^{2}=(\\sum _{i=1}^{n}(x_{1i}-{\\overline {x}}_{1})^{2}+\\sum _{j=1}^{n}(x_{2j}-{\\overline {x}}_{2})^{2})/(n_{1}+n_{2}-2)}\n  為兩樣本共同之方差。該統計量t在虛無假說：μ1 - μ2 = μ0為真的條件下服從自由度為n1 + n2 − 2的t分佈。\n\n\n==== 異質變異數假設 (Heteroscedasticity) ====\n若兩獨立樣本x1i與x2j具有相同或不相同之樣本數n1與n2，且兩者母體變異數不相等（異質變異數假設）的常態分配，則兩母體之期望值之差μ1 - μ2是否為μ0可利用以下統計量\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  1\n                \n              \n              −\n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n                \n                  2\n                \n              \n              −\n              \n                μ\n                \n                  0\n                \n              \n            \n            \n              \n                s\n                \n                  1\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  1\n                \n              \n              +\n              \n                s\n                \n                  2\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\overline {x}}_{1}-{\\overline {x}}_{2}-\\mu _{0}}{\\sqrt {s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}}}\n  其中\n  \n    \n      \n        i\n        =\n        1\n        …\n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle i=1\\ldots n_{1}}\n  ，其中\n  \n    \n      \n        j\n        =\n        1\n        …\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j=1\\ldots n_{2}}\n  ，\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            \n              n\n              \n                1\n              \n            \n          \n        \n        \n          x\n          \n            1\n            i\n          \n        \n        )\n        \n          /\n        \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}_{1}=(\\sum _{i=1}^{n_{1}}x_{1i})/n_{1}}\n  及\n  \n    \n      \n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            \n              n\n              \n                2\n              \n            \n          \n        \n        \n          x\n          \n            2\n            j\n          \n        \n        )\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle {\\overline {x}}_{2}=(\\sum _{j=1}^{n_{2}}x_{2j})/n}\n  為兩樣本各自的平均數，\n  \n    \n      \n        \n          s\n          \n            1\n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            1\n            i\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            1\n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        \n          /\n        \n        (\n        \n          n\n          \n            1\n          \n        \n        −\n        1\n        )\n      \n    \n    {\\displaystyle s_{1}^{2}=(\\sum _{i=1}^{n}(x_{1i}-{\\overline {x}}_{1})^{2})/(n_{1}-1)}\n  及\n  \n    \n      \n        \n          s\n          \n            2\n          \n          \n            2\n          \n        \n        =\n        (\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          x\n          \n            2\n            j\n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        \n          /\n        \n        (\n        \n          n\n          \n            2\n          \n        \n        −\n        1\n        )\n      \n    \n    {\\displaystyle s_{2}^{2}=(\\sum _{j=1}^{n}(x_{2j}-{\\overline {x}}_{2})^{2})/(n_{2}-1)}\n  分別為兩樣本之方差。該統計量t在虛無假說：μ1 - μ2 = μ0為真的條件下服從自由度為\n\n  \n    \n      \n        d\n        f\n        =\n        \n          \n            \n              (\n              \n                s\n                \n                  1\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  1\n                \n              \n              +\n              \n                s\n                \n                  2\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  2\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n            \n              (\n              \n                s\n                \n                  1\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  1\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              \n                /\n              \n              (\n              \n                n\n                \n                  1\n                \n              \n              −\n              1\n              )\n              +\n              (\n              \n                s\n                \n                  2\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                n\n                \n                  2\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              \n                /\n              \n              (\n              \n                n\n                \n                  2\n                \n              \n              −\n              1\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle df={\\frac {(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^{2}}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}}}\n  之t分布。這種方法又常稱為Welch檢驗。\n\n\n=== 其它相關檢驗 ===\n\n\n==== 偏迴歸係數是否為零之檢定 ====\n\n\n===== 以簡單線性迴歸為例 =====\n\n模型假設：\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        α\n        +\n        β\n        \n          x\n          \n            i\n          \n        \n        +\n        \n          ε\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle y_{i}=\\alpha +\\beta x_{i}+\\varepsilon _{i},}\n  其中xi，i = 1, ..., n為已知，α與β為未知係數，εi為殘差獨立且服從期望值0且方差σ2未知的常態分佈，yi，i = 1, ..., n為觀測值。我們可以檢驗迴歸係數β是否相等於特定的β0，通常使β0 = 0以檢定xi對yi是否存在解釋能力，在此例（簡單線性迴歸模型）即為檢定迴歸式之斜率是否為零。\n令\n  \n    \n      \n        \n          \n            \n              α\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\alpha }}}\n  與\n  \n    \n      \n        \n          \n            \n              β\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\beta }}}\n  為最小平方法之估計值，\n  \n    \n      \n        S\n        \n          E\n          \n            \n              \n                α\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle SE_{\\widehat {\\alpha }}}\n  與\n  \n    \n      \n        S\n        \n          E\n          \n            \n              \n                β\n                ^\n              \n            \n          \n        \n      \n    \n    {\\displaystyle SE_{\\widehat {\\beta }}}\n  為最小平方法估計值之標準誤差，則\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              \n                \n                  \n                    β\n                    ^\n                  \n                \n              \n              −\n              \n                β\n                \n                  0\n                \n              \n            \n            \n              S\n              \n                E\n                \n                  \n                    \n                      β\n                      ^\n                    \n                  \n                \n              \n            \n          \n        \n        ∼\n        \n          \n            \n              T\n            \n          \n          \n            n\n            −\n            2\n          \n        \n      \n    \n    {\\displaystyle t={\\frac {{\\widehat {\\beta }}-\\beta _{0}}{SE_{\\widehat {\\beta }}}}\\sim {\\mathcal {T}}_{n-2}}\n  在虛無假說為β = β0的情況下服從自由度為n − 2之t分布，此檢定統計量被稱作「t比率 (t-ratio)」，其中\n\n  \n    \n      \n        S\n        \n          E\n          \n            \n              \n                β\n                ^\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  1\n                  \n                    n\n                    −\n                    2\n                  \n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    \n                      y\n                      ^\n                    \n                  \n                \n                \n                  i\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              −\n              \n                \n                  x\n                  ¯\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle SE_{\\widehat {\\beta }}={\\frac {\\sqrt {{\\frac {1}{n-2}}\\sum _{i=1}^{n}(y_{i}-{\\widehat {y}}_{i})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\overline {x}})^{2}}}}}\n  由於\n\n  \n    \n      \n        \n          \n            \n              \n                ε\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        −\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        −\n        (\n        \n          \n            \n              α\n              ^\n            \n          \n        \n        +\n        \n          \n            \n              β\n              ^\n            \n          \n        \n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\widehat {\\varepsilon }}_{i}=y_{i}-{\\widehat {y}}_{i}=y_{i}-({\\widehat {\\alpha }}+{\\widehat {\\beta }}x_{i})}\n  為殘差（即估計誤差），而\n\n  \n    \n      \n        \n          SSR\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            \n              \n                ε\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\text{SSR}}=\\sum _{i=1}^{n}{\\widehat {\\varepsilon }}_{i}^{\\;2}}\n  \n為殘差之離均平方和，我們可改寫t為\n\n  \n    \n      \n        t\n        =\n        \n          \n            \n              (\n              \n                \n                  \n                    β\n                    ^\n                  \n                \n              \n              −\n              \n                β\n                \n                  0\n                \n              \n              )\n              \n                \n                  n\n                  −\n                  2\n                \n              \n            \n            \n              \n                SSR\n              \n              \n                /\n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                \n                  (\n                  \n                    \n                      x\n                      \n                        i\n                      \n                    \n                    −\n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle t={\\frac {({\\widehat {\\beta }}-\\beta _{0}){\\sqrt {n-2}}}{\\sqrt {{\\text{SSR}}/\\sum _{i=1}^{n}\\left(x_{i}-{\\overline {x}}\\right)^{2}}}}}\n  另请参阅：F检验\n\n\n== 電腦軟體 ==\n大多數的試算表軟體及統計軟體，諸如QtiPlot、OpenOffice.org Calc、LibreOffice Calc、Microsoft Excel、SAS、SPSS、Stata、DAP、gretl、R、Python ([1]（页面存档备份，存于互联网档案馆）)、PSPP、Minitab等，都可以進行t檢驗運算。\n\n\n== 參見 ==\n司徒頓t分布\nF檢定\n機率分布\n\n\n== 參考文獻 ==", "信噪比": "信噪比（英語：Signal-to-noise ratio，缩写为SNR或S/N），又稱訊噪比，是科学和工程中所用的一种度量，用於比較所需訊號的强度與背景雜訊的强度。其定義為訊號功率与雜訊功率的比率，以分貝（dB）为单位表示。大於比率1:1（高於0分貝）表示訊號多於雜訊。信噪比通常用於描述電子訊號，也可以應用在各種形式的訊號，比如冰芯內的同位素量，或細胞間的生物化學信號。\n\n\n== 定义 ==\n所指為有用信号功率（Power of Signal）与雜訊功率（Power of Noise）的比。因此為振幅（Amplitude）平方的比：\n\n  \n    \n      \n        \n          S\n          N\n          R\n        \n        =\n        \n          \n            \n              P\n              \n                \n                  s\n                  i\n                  g\n                  n\n                  a\n                  l\n                \n              \n            \n            \n              P\n              \n                \n                  n\n                  o\n                  i\n                  s\n                  e\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              A\n              \n                \n                  s\n                  i\n                  g\n                  n\n                  a\n                  l\n                \n              \n              \n                2\n              \n            \n            \n              A\n              \n                \n                  n\n                  o\n                  i\n                  s\n                  e\n                \n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {SNR} ={P_{\\mathrm {signal} } \\over P_{\\mathrm {noise} }}={A_{\\mathrm {signal} }^{2} \\over A_{\\mathrm {noise} }^{2}}}\n  它的单位一般使用分贝，其值为十倍对数信号与噪声功率比：\n\n  \n    \n      \n        \n          S\n          N\n          R\n          (\n          d\n          B\n          )\n        \n        =\n        10\n        \n          log\n          \n            10\n          \n        \n        ⁡\n        \n          (\n          \n            \n              \n                P\n                \n                  \n                    s\n                    i\n                    g\n                    n\n                    a\n                    l\n                  \n                \n              \n              \n                P\n                \n                  \n                    n\n                    o\n                    i\n                    s\n                    e\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        20\n        \n          log\n          \n            10\n          \n        \n        ⁡\n        \n          (\n          \n            \n              \n                A\n                \n                  \n                    s\n                    i\n                    g\n                    n\n                    a\n                    l\n                  \n                \n              \n              \n                A\n                \n                  \n                    n\n                    o\n                    i\n                    s\n                    e\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathrm {SNR(dB)} =10\\log _{10}\\left({P_{\\mathrm {signal} } \\over P_{\\mathrm {noise} }}\\right)=20\\log _{10}\\left({A_{\\mathrm {signal} } \\over A_{\\mathrm {noise} }}\\right)}\n  其中\n\n  \n    \n      \n        \n          P\n          \n            \n              s\n              i\n              g\n              n\n              a\n              l\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P_{\\mathrm {signal} }\\,}\n  为信号功率（Power of Signal）。\n\n  \n    \n      \n        \n          P\n          \n            \n              n\n              o\n              i\n              s\n              e\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P_{\\mathrm {noise} }\\,}\n  为噪声功率（Power of Noise）。\n\n  \n    \n      \n        \n          A\n          \n            \n              s\n              i\n              g\n              n\n              a\n              l\n            \n          \n        \n        \n      \n    \n    {\\displaystyle A_{\\mathrm {signal} }\\,}\n  为信号振幅（Amplitude of Signal）。\n\n  \n    \n      \n        \n          A\n          \n            \n              n\n              o\n              i\n              s\n              e\n            \n          \n        \n        \n      \n    \n    {\\displaystyle A_{\\mathrm {noise} }\\,}\n  为噪声振幅（Amplitude of Noise）。\n\n\n== 各种调制系统的信噪比 ==\n\n\n=== 调幅 ===\n信道信噪比为\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              C\n              ,\n              A\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  C\n                \n                \n                  2\n                \n              \n              (\n              1\n              +\n              \n                k\n                \n                  a\n                \n                \n                  2\n                \n              \n              P\n              )\n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{C,AM}} ={\\frac {A_{C}^{2}(1+k_{a}^{2}P)}{2WN_{0}}}}\n  其中 W 是带宽，\n  \n    \n      \n        \n          k\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle k_{a}}\n   是调制系数\n（AM接收机的）输出信噪比为\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              O\n              ,\n              A\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  c\n                \n                \n                  2\n                \n              \n              \n                k\n                \n                  a\n                \n                \n                  2\n                \n              \n              P\n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{O,AM}} ={\\frac {A_{c}^{2}k_{a}^{2}P}{2WN_{0}}}}\n  \n\n\n=== 调频 ===\n信道信噪比为\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              C\n              ,\n              F\n              M\n            \n          \n        \n        =\n        \n          \n            \n              A\n              \n                c\n              \n              \n                2\n              \n            \n            \n              2\n              W\n              \n                N\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{C,FM}} ={\\frac {A_{c}^{2}}{2WN_{0}}}}\n  输出信噪比为\n\n  \n    \n      \n        \n          (\n          S\n          N\n          R\n          \n            )\n            \n              O\n              ,\n              F\n              M\n            \n          \n        \n        =\n        \n          \n            \n              \n                A\n                \n                  c\n                \n                \n                  2\n                \n              \n              \n                k\n                \n                  f\n                \n                \n                  2\n                \n              \n              P\n            \n            \n              2\n              \n                N\n                \n                  0\n                \n              \n              \n                W\n                \n                  3\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {(SNR)_{O,FM}} ={\\frac {A_{c}^{2}k_{f}^{2}P}{2N_{0}W^{3}}}}", "抽樣": "在统计学中，抽样（Sampling）是一种推論統計方法，它是指从目标总体（Population，或称为母体）中抽取一部分个体作为样本（Sample），通过观察样本的某一或某些属性，依据所获得的数据对总体的数量特征得出具有一定可靠性的估计判断，从而达到对总体的认识。\n\n\n== 基本过程 ==\n抽样过程主要包括以下几个阶段：\n\n定义总体（母体）\n确定抽样框\n确定抽样方法\n决定样本量\n实施抽样计划\n抽样与数据收集\n回顾抽样过程\n\n\n== 总体 ==\n目标是所要研究的对象的全体。例如，制造商检查某个批次的产品质量是否合格，目标总体就是这一批次的产品。抽样总体是用于从中抽取样本的总体。按理，抽样总体应该与目标总体一致，但实践中时常发生不一致的情况。例如，科学家通过小白鼠试验来检测药物用于人类总体的效果。\n\n\n== 抽样框 ==\n在抽样之前，总体应划分成抽样单位，抽样单位互不重叠且能合成总体，总体中的每个个体只属于一个单位。抽样框是一份包含所有抽样单元的名单。\n\n\n== 抽样方法 ==\n\n\n=== 简单随机抽样 ===\n\n简单随机抽样（simple random sampling），也叫纯随机抽样。从总体N个单位中随机地抽取n个单位作为样本，使得每一个容量为样本都有相同的概率被抽中。特点是：每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。简单随机抽样是其它各种抽样形式的基础。通常只是在总体单位之间差异程度较小和数目较少时，才采用这种方法。\n\n\n=== 系统抽样 ===\n\n系统抽样（systematic sampling），也称等距抽样。将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。先从数字1到k之间随机抽取一个数字r作为初始单位，以后依次取r+k、r+2k……等单位。这种方法操作简便，可提高估计的精度。\n\n\n=== 分层抽样 ===\n\n分层抽样（stratified sampling）。将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。从而保证样本的结构与总体的结构比较相近，从而提高估计的精度。\n\n\n=== 整群抽样 ===\n整群抽样（cluster sampling）。又稱群集抽樣，将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。抽样时只需群的抽样框，可简化工作量，缺点是估计的精度较差。\n\n\n== 抽樣標準 ==\n\n\n=== 國際標準化組織 (ISO) ===\nISO 2859 series\nISO 3951 series\n\n\n=== 美國材料和試驗協會(ASTM) ===\nASTM E105 Standard Practice for Probability Sampling Of Materials\nASTM E122 Standard Practice for Calculating Sample Size to Estimate, With a Specified Tolerable Error, the Average for Characteristic of a Lot or Process\nASTM E141 Standard Practice for Acceptance of Evidence Based on the Results of Probability Sampling\nASTM E1402 Standard Terminology Relating to Sampling\nASTM E1994 Standard Practice for Use of Process Oriented AOQL and LTPD Sampling Plans\nASTM E2234 Standard Practice for Sampling a Stream of Product by Attributes Indexed by AQL\n\n\n=== 美國國家標準協會 (ANSI) ===\nANSI/ASQ Z1.4\n\n\n=== 美國軍用標準 ===\nMIL-STD-105\nMIL-STD-1916\n\n\n== 相关书籍 ==\n胡健颖，孙山泽. 抽样调查的理论、方法和应用. 北京大学出版社:北京, 2000.6. ISBN 7-301-04547-6.\n金勇进，蒋妍，李序颖. 抽样技术. 中国人民大学出版社:北京, 2002.6. ISBN 7-300-04079-9\n\n\n== 參考文獻 ==", "集成学习": "在统计学和机器学习中，集成学习（英語：Ensemble learning）方法使用多种学习算法来获得比单独使用任何单独的学习算法更好的预测性能。不像统计力学中的系综通常是无限的，机器学习集合仅由一组具体的有限的可替代模型组成，但通常允许在这些可替代方案中存在更灵活的结构。\n\n\n== 概述 ==\n监督学习算法通常被描述为执行搜索假设空间的任务以找到合适的假设，该假设将对特定问题做出良好预测。即使假设空间包含非常适合特定问题的假设，也可能很难找到一个很好的假设。集成学习结合多个假设，形成一个（希望）更好的假设。术语集成通常保留用于使用相同基础学习器生成多个假设的方法。多分类器系统的更广泛术语还包括由非相同基础学习器得到的假设的结合。这种方法和现象也被另一个术语“群智”所描述，该术语来自多个DREAM生物医学数据科学挑战。\n评估集成学习的预测通常需要比评估单个模型的预测更多的计算，因此集成可以被认为是通过执行大量额外计算来补偿差的学习算法的方式。诸如决策树之类的快速算法通常用于集合方法（如随机森林），尽管较慢的算法也可以从集成方法中受益。\n通过类比，集成技术也已用于无监督学习场景中，如共识聚类或异常检测。\n\n\n== 集成理论 ==\n集成学习本身是一种监督学习算法，因为它可以被训练然后用于进行预测。因此，训练后的集成模型代表了一个假设，但这个假设不一定被包含在构建它的模型的假设空间内。因此，可以证明集成学习在它们可以表示的功能方面具有更大的灵活性。理论上，这种灵活性使他们能够比单一模型更多地过拟合训练数据，但在实践中，一些集成算法（如Bagging算法）倾向于减少对训练数据过拟合相关的问题。\n根据经验，当模型之间存在显著差异时，集成往往会产生更好的结果。因此，许多集成方法试图促进它们组合的模型之间的多样性。尽管可能不是直观的，更随机的算法（如随机决策树）可用于产生比非常有意识的算法（如熵减少决策树）更强大的集成模型。然而，使用各种强大的学习算法已被证明是比使用试图愚弄模型以促进多样性的技术更有效。\n\n\n== 集成模型大小 ==\n虽然集成中的组成分类器的数量对预测的准确性具有很大影响，但是解决该问题的研究数量有限。先验地确定集成模型的大小以及大数据流的体积和速度使得这对于在线集成分类器来说更加重要，其中大多数统计测试被用于确定适当数量的组件。最近，理论框架表明对于集成模型存在理想数量的分类器，具有多于或少于该数量的分类器将使精度变差，这被称为“集成构建效果递减规律”。理论框架表明，使用与类标签数相同的独立分类器可以达到最高的准确度。 \n\n\n== 常见的集成类型 ==\n\n\n=== 贝叶斯最优分类器 ===\n贝叶斯最优分类器是一种分类技术，它是假设空间中所有假设的集合。平均而言，没有其他集成模型可以超越它。朴素贝叶斯最优分类器假定数据在类上有条件地独立并使计算更可行，如果该假设为真，则对每个假设进行投票，该投票与从系统采样训练数据集的可能性成比例。为了促进有限大小的训练数据，每个假设的投票也乘以该假设的先验概率。贝叶斯最优分类器可以用以下等式表示：\n\n  \n    \n      \n        y\n        =\n        \n          \n            \n              a\n              r\n              g\n              m\n              a\n              x\n            \n            \n              \n                c\n                \n                  j\n                \n              \n              ∈\n              C\n            \n          \n        \n        \n          ∑\n          \n            \n              h\n              \n                i\n              \n            \n            ∈\n            H\n          \n        \n        \n          P\n          (\n          \n            c\n            \n              j\n            \n          \n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          T\n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          \n            h\n            \n              i\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle y={\\underset {c_{j}\\in C}{\\mathrm {argmax} }}\\sum _{h_{i}\\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}\n  其中 \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  是预测标签， \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  是所有可能类的集合， \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  是假设空间， \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  是概率， \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  是训练数据。 作为集成，贝叶斯最优分类器表示不一定在\n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  中的假设。然而，由贝叶斯最优分类器表示的假设是集合空间中的最优假设（所有可能的集合的空间仅由\n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  中的假设组成）。\n这个公式可以用贝叶斯定理重新表述，贝叶斯定理表明后验与先验的可能性成正比：\n\n  \n    \n      \n        P\n        (\n        \n          h\n          \n            i\n          \n        \n        \n          |\n        \n        T\n        )\n        ∝\n        P\n        (\n        T\n        \n          |\n        \n        \n          h\n          \n            i\n          \n        \n        )\n        P\n        (\n        \n          h\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle P(h_{i}|T)\\propto P(T|h_{i})P(h_{i})}\n  因此， \n\n  \n    \n      \n        y\n        =\n        \n          \n            \n              a\n              r\n              g\n              m\n              a\n              x\n            \n            \n              \n                c\n                \n                  j\n                \n              \n              ∈\n              C\n            \n          \n        \n        \n          ∑\n          \n            \n              h\n              \n                i\n              \n            \n            ∈\n            H\n          \n        \n        \n          P\n          (\n          \n            c\n            \n              j\n            \n          \n          \n            |\n          \n          \n            h\n            \n              i\n            \n          \n          )\n          P\n          (\n          \n            h\n            \n              i\n            \n          \n          \n            |\n          \n          T\n          )\n        \n      \n    \n    {\\displaystyle y={\\underset {c_{j}\\in C}{\\mathrm {argmax} }}\\sum _{h_{i}\\in H}{P(c_{j}|h_{i})P(h_{i}|T)}}\n  \n\n\n=== Bootstrap聚合（Bagging） ===\nBootstrap聚合（Bootstrap Aggregating，Bagging）使集成模型中的每个模型在投票时具有相同的权重。為了降低不穩定過程如樹的方差，Bagging對B個(比如說，B使用與類標籤數相同的數量)bootstrap datasets上的模型求平均，從而降低其方差並導致預測性能的改善。例如，随机森林算法将随机决策树与Bagging相结合，以实现更高的分类准确度。\n\n\n=== Boosting ===\nBoosting通过在训练新模型实例时更注重先前模型错误分类的实例来增量构建集成模型。在某些情况下，Boosting已被证明比Bagging可以得到更好的准确率，不过它也更倾向于对训练数据过拟合。目前比较常见的增强实现有AdaBoost等算法。\n\n\n=== 贝叶斯参数平均 ===\n贝叶斯参数平均（Bayesian Parameter Averaging，BPA）是一种集成方法，它试图通过对假设空间中的假设进行抽样来近似贝叶斯最优分类器，并使用贝叶斯定律将它们组合起来。与贝叶斯最优分类器不同，贝叶斯模型平均（Bayesian Model Averaging，BMA）可以实际实现。通常使用诸如MCMC的蒙特卡罗方法对假设进行采样。例如，可以使用吉布斯采样来绘制代表分布\n  \n    \n      \n        P\n        (\n        T\n        \n          |\n        \n        H\n        )\n      \n    \n    {\\displaystyle P(T|H)}\n  的假设。已经证明，在某些情况下，当以这种方式绘制假设并根据贝叶斯定律求平均时，该算法具有预期误差，该误差被限制为贝叶斯最优分类器的预期误差的两倍。尽管这种技术理论正确，但早期工作中的实验结果表明，与简单的集成方法如Bagging相比，该方法促进了过拟合并且表现更差； however, these conclusions appear to be based on a misunderstanding of the purpose of Bayesian model averaging vs. model combination.然而，这些结论似乎是基于对目的的误解贝叶斯模型平均与模型组合。此外，BMA的理论和实践取得了相当大的进展，最近的严格证明证明了BMA在高维设置中变量选择和估计的准确性，并提供了实验证据，强调了BMA中的稀疏执行先验在缓解过拟合方面的作用。\n\n\n=== 贝叶斯模型组合 ===\n贝叶斯模型组合（BMC）是对贝叶斯模型平均（BMA）的算法校正。 它不是单独对整体中的每个模型进行采样，而是从可能的集合空间中进行采样（模型权重从具有均匀参数的Dirichlet分布中随机抽取） 这种修改克服了BMA趋向于将所有权重赋予单个模型的趋势 尽管BMC在计算上比BMA更昂贵，但它往往会产生显着更好的结果 BMC的结果显示平均值优于（具有统计显着性）BMA和Bagging。使用贝叶斯定律来计算模型权重需要计算给定每个模型的数据的概率，通常集成中的模型都不是生成训练数据的分布，因此对于该项，它们都正确地接收到接近于零的值。如果集成足够大以对整个模型空间进行采样，这将很有效，但这种情况很少发生。因此，训练数据中的每个模式将使集成权重朝向最接近训练数据分布的集合中的模型移动，这实质上减少了用于进行模型选择的不必要的复杂方法。\n集成的可能权重可以看作是躺在单面上，在单形的每个顶点处，所有权重都被赋予集成中的单个模型。BMA会聚到最接近训练数据分布的顶点。相比之下，BMC汇聚到这种分布投射到单纯形态的点上。换句话说，它不是选择最接近生成分布的一个模型，而是寻找最接近生成分布的模型的组合。\nBMA的结果通常可以通过使用交叉验证从一系列模型中选择最佳模型来近似。同样地，可以通过使用交叉验证来近似来自BMC的结果，以从可能的权重的随机采样中选择最佳的集成组合。\n\n\n=== 桶模型 ===\n“桶模型”（英语：bucket of models）是一种使用模型选择算法为每个问题选择最佳模型的集成方法。当仅使用一个问题进行测试时，一组模型不会产生比集成中的最佳模型更好的结果，但是当针对许多问题进行评估时，它通常会产生比集成中的任何模型更好的结果。\n最常见的方法用于模型的选择是交叉验证。它用以下伪代码描述：\n\nFor each model m in the bucket:\n  Do c times: (where 'c' is some constant)\n    Randomly divide the training dataset into two datasets: A, and B.\n    Train m with A\n    Test m with B\nSelect the model that obtains the highest average score\n\n交叉验证选择可以概括为：“使用训练集尝试所有选择，并选择最有效的方法”。门控是交叉验证选择的一般化。它涉及训练另一种学习模型，以确定桶中哪些模型最适合解决问题。通常，感知器被应用于门控模型。它可用于选择“最佳”模型，或者可用于为桶中每个模型的预测提供线性权重。\n当使用具有大量问题的桶模型时，可能希望避免需要花费很长时间训练的一些模型。地标学习是一种寻求解决这一问题的元学习方法，它涉及仅训练桶中的快速（但不精确）算法，然后使用这些算法的性能来帮助确定哪种慢（但准确）算法最有可能做得最好。\n\n\n=== Stacking ===\n堆叠（英语：Stacking）（有时称为堆叠泛化）涉及训练学习算法以组合其他几种学习算法的预测。首先，使用可用数据训练所有其他算法，然后训练组合器算法以使用其他算法的所有预测作为附加输入进行最终预测。如果使用任意组合器算法，那么堆叠理论上可以表示本文中描述的任何集合技术，但实际上，通常用逻辑回归模型作为组合器。\nStacking通常比任何一个经过训练的模型都能产生更好的性能，它已成功用于监督学习任务（如回归、 分类和距离学习 ）和无监督学习（如密度估计）。 Stacking也被用于评估Bagging的错误率。 据报道，它的表现超过了贝叶斯模型的平均值。在Netflix竞赛中两个表现最好的人使用混合方法（英语：Blending），这可以被认为是一种Stacking形式。\n\n\n== 实现库 ==\nR：至少有三个软件包提供贝叶斯模型平均工具，包括BMS（贝叶斯模型选择）包、BAS（贝叶斯自适应采样的首字母缩写）包、和BMA包。H2O包提供了许多机器学习模型，包括一个集成模型，也可以使用Spark进行训练。\nPython：Scikit-learn，一个用于Python机器学习的软件包，提供用于集成学习的软件包，包括用于Bagging和平均方法的软件包。\nMATLAB：分类集成在统计和机器学习工具箱中实现。\n\n\n== 集成学习应用 ==\n近年来，由于计算能力不断提高，允许在合理的时间范围内训练大型集成模型，其应用数量也越来越多。集成分类器的一些应用包括：\n\n\n=== 遥感 ===\n\n\n==== 土地覆盖测绘 ====\n土地覆盖测绘是地球观测卫星传感器的主要应用之一，利用遥感和地理空间数据识别位于目标区域表面的材料和物体。一般来说，目标材料的类别包括道路、建筑物、河流、湖泊和植被。基于人工神经网络、核主成分分析（KPCA）、Boosting决策树、随机森林和自动设计多分类器系统等不同的集成学习方法可以有效识别土地覆盖物。\n\n\n==== 变化的检测 ====\n变化检测是一种图像分析问题，识别土地覆盖随时间变化的地方。变化检测广泛应用于城市发展、森林和植被动态、土地利用和灾害监测等领域。集成分类器在变化检测中的最早应用是通过多数投票、贝叶斯平均和最大后验概率设计的。\n\n\n=== 计算机安全 ===\n\n\n==== 分布式拒绝服务 ====\n分布式拒绝服务攻击是互联网服务提供商可能遭受的最具威胁性的网络攻击之一。通过组合单个分类器的输出，集成分类器减少了检测和区分此类攻击与Slashdot效应的总误差。\n\n\n==== 恶意软件检测 ====\n使用机器学习技术对计算机病毒、计算机蠕虫、特洛伊木马、勒索软件和间谍软件等恶意软件代码进行分类，其灵感来自文本分类问题。 集成学习系统在这方面已经显示出适当的功效。\n\n\n==== 入侵检测 ====\n入侵检测系统监控计算机网络或计算机系统，以识别入侵者代码，如异常检测过程。集成学习成功地帮助这种监控系统减少了它们的总误差。\n\n\n=== 人脸识别 ===\n人脸识别最近已经成为最受欢迎的模式识别研究领域之一，它通过他/她的数字图像来处理人的识别或验证。基于Gabor Fisher分类器和独立分量分析预处理技术的分层集成是该领域中最早使用的一些集成方法。\n\n\n=== 情感识别 ===\n语音识别主要基于深度学习，因为谷歌、微软和IBM这一领域的大多数业内人士都表示，他们的语音识别的核心技术是基于这种方法。基于语音与集成学习的情感识别也可以有令人满意的表现。它也被成功用于面部情绪识别。\n\n\n=== 欺诈检测 ===\n欺诈检测涉及银行欺诈的识别，例如洗钱、信用卡欺诈和电信欺诈，它们具有广泛的机器学习研究和应用领域。由于集成学习提高了正常行为建模的稳健性，因此有人提出将其作为检测银行和信用卡系统中此类欺诈案件和活动的有效技术。\n\n\n=== 金融决策 ===\n预测业务失败的准确性是财务决策中非常关键的问题。因此，不同的集成分类器被提出用于预测金融危机和财务困境。此外，在基于交易的操纵问题中，交易者试图通过买卖活动来操纵股票价格，集成分类器需要分析股票市场数据的变化并检测股票价格操纵的可疑症状。=== 医学生\n集成分类器已成功应用于脑-机接口、蛋白质组学和医学诊断，例如基于MRI数据集的神经认知障碍（即阿尔茨海默氏症或肌强直性营养不良）检测。\n\n\n== 参考文献 ==", "偽陽性和偽陰性": "偽陽性和偽陰性（英語：False positives and false negatives）是指進行實用測試之後，測試結果有機會不呈現真正的狀況。\n偽陽性、假陽性（英語：false positive）是指測試結果呈陽性的反應，但事實上卻是沒有；相反，偽陰性、假陰性（英語：false negative）是指測試結果呈陰性的反應，但事實上卻是有。\n\n\n== 例子 ==\n妊娠試驗可能呈陽性，但事實上沒有懷孕；也可能呈陰性，但事實上卻有懷孕。\n香港政府，以及某些記者將「初步篩檢結果為陽性」簡稱為「初陽」。\n\n\n== 參見 ==\n靈敏度和特異度\n型一錯誤與型二錯誤\n快速診斷測試\n狼來了\n貝氏定理關於檢測的範例\n\n\n== 注釋 ==", "异常检测": "在数据挖掘中，异常检测（英語：anomaly detection）对不符合预期模式或数据集中其他项目的项目、事件或观测值的识别。通常异常项目会转变成银行欺诈、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。特别是在检测滥用与网络入侵时，有趣性对象往往不是罕见对象，但却是超出预料的突发活动。这种模式不遵循通常统计定义中把异常点看作是罕见对象，于是许多异常检测方法（特别是无监督的方法）将对此类数据失效，除非进行了合适的聚集。相反，聚类分析算法可能可以检测出这些模式形成的微聚类。有三大类异常检测方法。 在假设数据集中大多数实例都是正常的前提下，无监督异常检测方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。监督式异常检测方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的统计分类问题的关键区别是异常检测的内在不均衡性）。半监督式异常检测方法根据一个给定的正常训练数据集建立一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。\n\n\n== 应用 ==\n异常检测技术用于各种领域，如入侵检测、欺诈检测、故障检测、系统健康监测、感測器网络事件检测和生态系统干扰检测等。它通常用于在预处理中删除从数据集的异常数据。在監督式學習中，去除异常数据的数据集往往会在统计上显著提升准确性。\n\n\n== 热门方法 ==\n文献中提出了几种异常检测方法。一些热门方法有：\n\n基于密度的方法（最近鄰居法、局部异常因子及此概念的更多变化)。\n基于子空间与相关性的高维数据的孤立点检测。\n一类支持向量机。\n复制神经网络。\n基于聚类分析的孤立点检测。\n与关联规则和频繁项集的偏差。\n基于模糊逻辑的孤立点检测。\n运用特征袋、分数归一化与不同多样性来源的集成方法。不同方法的性能在很大程度上取决于数据集和参数，比较许多数据集和参数时，各种方法与其他方法相比的系统优势不大。\n\n\n== 数据安全方面的应用 ==\n多萝西·丹宁教授在1986年提出了入侵检测系统（IDS）的异常检测方法。入侵检测系统的异常检测通常是通过阈值和统计完成的，但也可以用软计算和归纳学习。 在1999年提出的统计类型包括检测用户、工作站、网络、远程主机与用户组的配置文件，以及基于频率、均值、方差、协方差和标准差的程序。 在入侵检测系统中，与异常检测模式相对应的还有误用检测模式。\n\n\n== 软件 ==\nELKI是一个包含若干异常检测算法及其索引加速的开源Java数据挖掘工具箱。\n\n\n== 参见 ==\n统计学中的离群值\n变化检测\n新奇检测\n分级暂存记忆\n\n\n== 参考文献 ==", "型一錯誤與型二錯誤": "型一錯誤與型二錯誤（英語：Type I error & Type II error）為统计学中推論統計學統計術語，表示統計學假說檢定中的两种錯誤。\n\n\n== 定義 ==\n\n\n=== 統計背景 ===\n在統計檢驗理論中，統計误差的概念是假說檢定(hypothesis testing)的一個組成部分。 該測試涉及選擇兩個相互競爭的命題，稱為零假設(Null hypothesis)，用H0表示，另一種對立假說(Alternative hypothesis)，用H1表示。 這在概念上類似於法庭審判中的判決。零假設對應於被告的立場：正如他在被證明有罪之前被假定為無罪一樣，在數據提供反對它的令人信服的證據之前，零假設也被假定為真。 對立假說對應於反對被告的立場。 具體來說，零假設還涉及不存在差異或不存在關聯。 因此，零假設永遠不可能是存在差異或關聯。\n如果測試結果與現實相符，則做出了正確的決定。 但是，如果測試結果與實際不符，則發生錯誤。決定錯誤的情況有兩種。 零假設可能為真，而我們拒絕H0。 另一方面，對立假說H1可能為真，而我們不拒絕H0。 兩種錯誤被區分：型一錯誤，與型二錯誤。\n\n\n=== 型一錯誤 ===\n型一錯誤是錯誤地拒絕原假設作為檢驗程序的結果。又稱偽陽性（false positive），有時也稱為第一類錯誤。 就法庭示例而言，型一錯誤對應於對無辜被告定罪。\n\n\n=== 型二錯誤 ===\n型二錯誤是錯誤地未能拒絕原假設作為測試程序的結果。又稱偽陰性（false negative），也稱為第二類錯誤。 就法庭示例而言，型二錯誤對應於無罪釋放罪犯。\n\n\n=== 交叉錯誤率 ===\n交叉錯誤率 (CER) 是型一錯誤和型二錯誤相等的點，代表了衡量生物識別有效性的最佳方法。 具有較低CER值的系統比具有較高CER值的系統提供更高的準確度。\n\n\n=== 偽陽性和偽陰性 ===\n\n在偽陽性和偽陰性方面，陽性結果對應於拒絕零假设，而陰性結果對應於未能拒絕零假设； “偽”表示得出的結論不正確。 因此，型一錯誤相當於偽陽性，型二錯誤相當於偽陰性。\n\n\n== 簡介 ==\n在假說檢定中，有一種假說稱為“零假设”，記為\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ，假說检验的目的是利用統計的方式，推翻虛無假說的成立，也就是對立假說（Alternative hypothesis，記為\n  \n    \n      \n        \n          H\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle H_{a}}\n  或\n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  ）成立。\n\n若零假设事實上成立，但統計檢驗的結果拒絕零假设（接受對立假說），這種錯誤稱為型一錯誤（弃真错误、α錯誤、偽陽性）；\n若零假设事實上不成立，但統計檢驗的結果不拒絕零假设，這種錯誤稱為型二錯誤（存偽錯誤、β錯誤、偽陰性）。以利用驗孕棒驗孕為例，此時沒有懷孕為零假设。若用驗孕棒替一位未懷孕者驗孕，結果呈已懷孕，此即型一錯誤。若用驗孕棒替一位已懷孕者驗孕，結果呈未懷孕，此即型二錯誤。\n\n\n== 參考 ==\n\n\n== 相關條目 ==\n對立假說\n信息檢索\n零假设\nROC曲线\n靈敏度和特異度\n偽陽性和偽陰性\n\n\n== 外部链接 ==\n直观展示第一型及第二型錯誤的趣图 （页面存档备份，存于互联网档案馆）", "自然选择": "自然选择（英語：natural selection，傳統上也譯為天擇）指生物的遺傳特徵在生存競爭中，由於具有某種優勢或某種劣勢，因而在生存能力上產生差異，並進而導致繁殖能力的差異，使得這些特徵被保存或是淘汰。自然選擇則是演化的主要機制，經過自然選擇而能夠成功生存，稱為「適應」。自然選擇是唯一可以解釋生物適應環境的機制。\n1858年，英國生物學家查爾斯·達爾文和阿爾弗雷德·羅素·華萊士發表了《On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection》，首次提出天擇的概念。\n查爾斯·達爾文在1859年出版的《物種起源》中提出，其於早年在加拉巴哥群島觀察了數種動物後發現，島上很少有與鄰近大陸相似的物種，並且還演化出許多獨有物種，如巨型的加拉巴哥象龜，達爾文在一開始以為，島上的鷽鳥應與南美洲發現的為同種，經研究，十三種燕雀中只有一種是與其大陸近親類似的，其餘皆或多或少發生了演化現象，他們爲了適應島上的生存環境，改變了鳥喙的大小。\n\n\n== 自然選擇的因素 ==\n因為生物族群呈指數成長，但資源有限，因此不是每個個體都能存活。影響存活率的因素包括無機環境，如溫度、溼度、日照、空氣；以及生物因子，如掠食者、共生、種內競爭、食物、疾病等等。個體之間存在的個體差異（變異）使每個個體的存活率不同，如果個體可以耐乾旱、在空中飛、在海裡呼吸、用爪子掠食、抵抗病原、製造工具、或是合作獵食等等，就會有不同的適應力，因此個體間的存活率不同。這些個體差異如果可以遺傳，則會造成演化。\n由于进化主要关注的是有机体是否能存活到繁衍下一代的年龄，在有机体繁衍年龄之后才会影響存活率的因素几乎会被自然選擇机制忽略。例如遗传性疾病亨廷顿舞蹈症通常在40岁左右发病，这个年龄超过了大多数人生育孩子的年龄，因而几乎不会由于进化机制而被自然淘汰。当中性或有害的基因靠近基因组中一个对人高度有益的基因旁边，中性或有害的基因会同这个有益基因一起遗传给后代，从而达到比其原有更高的遗传概率。\n\n\n== 類別 ==\n\n定向選擇（Directional selection）：往某一方向偏離平均的個體存活率最高，例如較白的北極熊毛。\n穩定化選擇（stabilizing selection）：性狀位於中間的個體存活率最高，兩側的個體低。\n破壞選擇（Disruptive selection）：性狀位在中間的個體存活率最低，例如環境中有黑色和白色石頭時，灰色的蛾無保護色。可能導致種化。性擇和生殖選擇和人擇有時被視為廣義的自然選擇的次分類。狹義的自然選擇（又稱生態選擇）指個體在自然環境中存活率的差異，性擇是取得交配對象的能力差異，而生殖選擇則是交配後生育後代的數目差異。人工選擇則將自然選擇概念應用在受人類圈養的生物上，例如家畜、寵物與農作物的育種。\n\n\n== 單位 ==\n最容易觀察到的自然選擇是作用在個體上，依據個體之間性狀的不同。\n因為是基因帶有個體的大多數資訊但不能自我複製，George C. Williams和理察·道金斯主张以基因為單位的演化觀點。個體只是基因為了自我複製所製造的「機器」。這種觀點在解釋轉座子和親緣選擇上較為方便。\n在癌症的例子中，基因突變造成細胞的性狀差異，引發在細胞層次的自然選擇。\n自然選擇作用在族群上時，稱為族群選擇（group selection）。族群選擇在自然界中的普遍程度仍有爭議。\n自然選擇也可能運作在物種的層次。當一個物種中的不同族群因為自然選擇而產生生物分類學上的差異時，則稱為「物種形成」；若是族群因為不受自然選擇青睞而導致族群規模縮小進而消失，則稱為「滅絕」，物種形成和滅絕的速率決定一個支系的適應力。\n\n\n== 参考文献 ==\n\n\n== 延伸閱讀 ==\n\n\n== 外部連結 ==", "先验算法": "在计算机科学以及数据挖掘领域中， 先验算法（Apriori Algorithm）是关联规则学习的经典算法之一。先验算法的设计目的是为了处理包含交易信息内容的数据库（例如,顾客购买的商品清单，或者网页常访清单。）而其他的算法则是设计用来寻找无交易信息（如Winepi算法和Minepi算法）或无时间标记（如DNA测序）的数据之间的联系规则。\n在关联式规则中,一般对于给定的项目集合（例如，零售交易集合，每个集合都列出的单个商品的购买信息），算法通常尝试在项目集合中找出至少有C个相同的子集。先验算法采用自底向上的处理方法，即频繁子集每次只扩展一个对象（该步骤被称为候选集产生），并且候选集由数据进行检验。当不再产生符合条件的扩展对象时，算法终止。\n先验算法采用广度优先搜索算法进行搜索并采用树结构来对候选项目集进行高效计数。它通过长度为\n  \n    \n      \n        k\n        −\n        1\n      \n    \n    {\\displaystyle k-1}\n  的候选项目集来产生长度为\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  的候选项目集，然后从中删除包含不常见子模式的候选项。根据向下封闭性引理,该候选项目集包含所有长度为\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  的频繁项目集。之后，就可以通过扫描交易数据库来决定候选项目集中的频繁项目集。\n虽然先验算法具有显著的历史地位，但是其中的一些低效与权衡弊端也进而引致了许多其他的算法的产生。候选集产生过程生成了大量的子集（先验算法在每次对数据库进行扫描之前总是尝试加载尽可能多的候选集）。并且自底而上的子集浏览过程（本质上为宽度优先的子集格遍历）也直到遍历完所有 \n  \n    \n      \n        \n          2\n          \n            \n              |\n            \n            S\n            \n              |\n            \n          \n        \n        −\n        1\n      \n    \n    {\\displaystyle 2^{|S|}-1}\n   个可能的子集之后才寻找任意最大子集S。\n\n\n== 例子 ==\n一个大型超级市场根据最小存货单位（SKU）来追踪每件物品的销售数据。从而也可以得知哪些物品通常被同时购买。通过采用先验算法来从这些销售数据中建立频繁购买商品组合的清单是一个效率适中的方法。假设交易数据库包含以下子集{1,2,3,4}，{1,2}，{2,3,4}，{2,3}，{1,2,4}，{3,4}，{2,4}。每个标号表示一种商品，如“黄油”或“面包”。先验算法首先要分别计算单个商品的购买频率。下表解释了先验算法得出的单个商品购买频率。\n\n然后我们可以定义一个最少购买次数来定义所谓的“频繁”。在这个例子中，我们定义最少的购买次数为3。因此，所有的购买都为频繁购买。接下来，就要生成频繁购买商品的组合及购买频率。先验算法通过修改树结构中的所有可能子集来进行这一步骤。然后我们仅重新选择频繁购买的商品组合：\n\n并且生成一个包含3件商品的频繁组合列表（通过将频繁购买商品组合与频繁购买的单件商品联系起来得出）。在上述例子中，不存在包含3件商品组合的频繁组合。最常见的3件商品组合为{1,2,4}和{2,3,4}，但是他们的购买次数为2，低于我们设定的最低购买次数。\n\n\n== 算法的局限 ==\n因此Apriori算法中的一些低效与权衡弊端也进而引致了许多其他的算法的产生，例如FP-growth算法。候选集产生过程生成了大量的子集（先验算法在每次对数据库进行扫描之前总是尝试加载尽可能多的候选集）。并且自底而上的子集浏览过程（本质上为宽度优先的子集格遍历）也直到遍历完所有 \n  \n    \n      \n        \n          2\n          \n            \n              |\n            \n            S\n            \n              |\n            \n          \n        \n        −\n        1\n      \n    \n    {\\displaystyle 2^{|S|}-1}\n   个可能的子集之后才寻找任意最大子集S。\n\n\n== 参考资料 ==", "人工神经网络": "人工神经网络（英語：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或類神經網絡，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中樞神經系統，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗地讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。\n和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。\n\n\n== 背景 ==\n对人类中枢神经系统的观察启发了人工神经网络这个概念。在人工神经网络中，简单的人工节点，称作神經元（neurons），连接在一起形成一个类似生物神经网络的网状结构。\n人工神经网络目前没有一个统一的正式定义。不过，具有下列特点的统计模型可以被称作是“神经化”的：\n\n具有一组可以被调节的权重（被学习算法调节的数值参数）\n可以估计输入数据的非线性函数关系这些可调节的权重可以被看做神经元之间的连接强度。\n人工神经网络与生物神经网络的相似之处在于，它可以集体地、并行地计算函数的各个部分，而不需要描述每一个单元的特定任务。神经网络这个词一般指统计学、认知心理学和人工智能领域使用的模型，而控制中央神经系统的神经网络属于理论神经科学和计算神经科学。在神经网络的现代软件实现中，由生物学启发的方法已经有了很重大的延伸，現在主流的是基于统计学和信号处理的更加实用的方法。在一些软件系统中，神经网络或者神经网络的一部分（例如人工神经元）是大型系统中的一个部分。这些系统结合了适应性的和非适应性的元素。虽然这种系统使用的这种更加普遍的方法更适宜解决现实中的问题，但是这和传统的连接主义人工智能已经没有什么关联了。不过它们还有一些共同点：非线性、分布式、并行化，局部性计算以及适应性。从历史的角度讲，神经网络模型的应用标志着二十世纪八十年代后期从高度符号化的人工智能（以用条件规则表达知识的专家系统为代表）向低符号化的机器学习（以用动力系统的参数表达知识为代表）的转变。\n\n\n== 历史 ==\n沃伦·麦卡洛克和沃尔特·皮茨（1943）基于数学和一种称为阈值逻辑的算法创造了一种神经网络的计算模型。这种模型使得神经网络的研究分裂为两种不同研究思路。一种主要关注大脑中的生物学过程，另一种主要关注神经网络在人工智能裡的应用。\n\n\n=== 赫布型学习 ===\n二十世纪40年代后期，心理学家唐纳德·赫布根据神经可塑性的机制创造了一种对学习的假说，现在称作赫布型学习。赫布型学习被认为是一种典型的非监督式学习规则，它后来的变种是长期增强作用的早期模型。从1948年开始，研究人员将这种计算模型的思想应用到B型图灵机上。\n法利和韦斯利·A·克拉克（1954）首次使用计算机，当时称作计算器，在MIT模拟了一个赫布网络。纳撒尼尔·罗切斯特（1956）等人模拟了一台 IBM 704计算机上的抽象神经网络的行为。 \n弗兰克·罗森布拉特创造了感知机。这是一种模式识别算法，用简单的加减法实现了两层的计算机学习网络。罗森布拉特也用数学符号描述了基本感知机里没有的回路，例如异或回路。这种回路一直无法被神经网络处理，直到保罗·韦伯斯(1975)创造了反向传播算法。\n在马文·明斯基和西摩爾·派普特（1969）发表了一项关于机器学习的研究以后，神经网络的研究停滞不前。他们发现了神经网络的两个关键问题。第一是基本感知机无法处理异或回路。第二个重要的问题是电脑没有足够的能力来处理大型神经网络所需要的很长的计算时间。直到计算机具有更强的计算能力之前，神经网络的研究进展缓慢。\n\n\n=== 反向传播算法与复兴 ===\n后来出现的一个关键的进展是保罗·韦伯斯发明的反向传播算法（Werbos 1975）。这个算法有效地解决了异或的问题，还有更普遍的训练多层神经网络的问题。\n在二十世纪80年代中期，分布式并行处理（当时称作联结主义）流行起来。戴维·鲁姆哈特和詹姆斯·麦克里兰德的教材对于联结主义在计算机模拟神经活动中的应用提供了全面的论述。\n神经网络传统上被认为是大脑中的神经活动的简化模型，虽然这个模型和大脑的生理结构之间的关联存在争议。人们不清楚人工神经网络能多大程度地反映大脑的功能。\n支持向量机和其他更简单的方法（例如线性分类器）在机器学习领域的流行度逐渐超过了神经网络，但是在2000年代后期出现的深度学习重新激发了人们对神经网络的兴趣。\n\n\n=== 2006年之后的进展 ===\n人们用CMOS创造了用于生物物理模拟和神经形态计算的计算装置。最新的研究显示了用于大型主成分分析和卷积神经网络的纳米装置具有良好的前景。如果成功的话，这会创造出一种新的神经计算装置，因为它依赖于学习而不是编程，并且它从根本上就是模拟的而不是数字化的，虽然它的第一个实例可能是数字化的CMOS装置。\n在2009到2012年之间，Jürgen Schmidhuber在Swiss AI Lab IDSIA的研究小组研发的循环神经网络和深前馈神经网络赢得了8项关于模式识别和机器学习的国际比赛。例如，艾力克斯·格雷夫斯的双向、多维的LSTM赢得了2009年ICDAR的3项关于连笔字识别的比赛，而且之前并不知道关于将要学习的3种语言的信息。IDSIA的Dan Ciresan和同事根据这个方法编写的基于GPU的实现赢得了多项模式识别的比赛，包括IJCNN 2011交通标志识别比赛等等。他们的神经网络也是第一个在重要的基准测试中（例如IJCNN 2012交通标志识别和NYU的楊立昆的MNIST手写数字问题）能达到或超过人类水平的人工模式识别器。\n类似1980年福島邦彥发明的neocognitron和视觉标准结构（由David H. Hubel和Torsten Wiesel在初级视皮层中发现的那些简单而又复杂的细胞启发）那样有深度的、高度非线性的神经结构可以被多伦多大学杰弗里·辛顿实验室的非监督式学习方法所训练。\n2012年，神經網路出現了快速的發展，主要原因在於計算技術的提高，使得很多複雜的運算變得成本低廉。以AlexNet為標誌，大量的深度網路開始出現。\n2014年出现了残差神经网络，该网络极大解放了神经网络的深度限制，出现了深度学习的概念。\n\n\n== 构成 ==\n典型的人工神经网络具有以下三个部分：\n\n结构（Architecture）结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。\n激励函数（Activation Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。\n学习规则（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激發。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激發。最后，输出神经元的激励值决定了识别出来的是哪个字母。\n\n\n== 神经元 ==\n神经元示意图：\n\na1~an为输入向量的各个分量\nw1~wn为神经元各个突触的权重值(weight)\nb为偏置(bias)\nf为传递函数，通常为非线性函数。一般有traingd(),tansig(),hardlim()。以下默认为hardlim()\nt为神经元输出数学表示\n\n  \n    \n      \n        t\n        =\n        f\n        (\n        \n          \n            \n              \n                W\n                ′\n              \n              →\n            \n          \n        \n        \n          \n            \n              A\n              →\n            \n          \n        \n        +\n        b\n        )\n      \n    \n    {\\displaystyle t=f({\\vec {W'}}{\\vec {A}}+b)}\n  \n\n  \n    \n      \n        \n          \n            \n              W\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {W}}}\n  为权向量，\n  \n    \n      \n        \n          \n            \n              \n                W\n                ′\n              \n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {W'}}}\n  为\n  \n    \n      \n        \n          \n            \n              W\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {W}}}\n  的转置\n\n  \n    \n      \n        \n          \n            \n              A\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {A}}}\n  为输入向量\n\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  为偏置\n\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  为传递函数可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。\n单个神经元的作用：把一个n维向量空间用一个超平面分割成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。\n该超平面的方程：\n  \n    \n      \n        \n          \n            \n              \n                W\n                ′\n              \n              →\n            \n          \n        \n        \n          \n            \n              p\n              →\n            \n          \n        \n        +\n        b\n        =\n        0\n      \n    \n    {\\displaystyle {\\vec {W'}}{\\vec {p}}+b=0}\n   \n\n  \n    \n      \n        \n          \n            \n              W\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {W}}}\n  权向量\n\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  偏置\n\n  \n    \n      \n        \n          \n            \n              p\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {p}}}\n  超平面上的向量\n\n\n== 神经元网络 ==\n\n\n=== 单层神经元网络 ===\n是最基本的神经元网络形式，由有限个神经元构成，所有神经元的输入向量都是同一个向量。由于每一个神经元都会产生一个标量结果，所以单层神经元的输出是一个向量，向量的维数等于神经元的数目。\n示意图：\n\n\n=== 多层神经元网络 ===\n\n\n== 人工神经网络的实用性 ==\n人工神经网络是一个能够学习，能够总结归纳的系统，也就是说它能够通过已知数据的实验运用来学习和归纳总结。人工神经网络通过对局部情况的对照比较（而这些比较是基于不同情况下的自动学习和要实际解决问题的复杂性所决定的），它能够推理产生一个可以自动识别的系统。与之不同的基于符号系统下的学习方法，它们也具有推理功能，只是它们是建立在逻辑演算法的基础上，也就是说它们之所以能够推理，基础是需要有一个推理演算法则的集合。\n\n\n== 人工神经元网络模型 ==\n通常来说，一个人工神经元网络是由一个多层神经元结构组成，每一层神经元拥有输入（它的输入是前一层神经元的输出）和输出，每一层（我们用符号记做）Layer(i)是由Ni(Ni代表在第i层上的N)个网络神经元组成，每个Ni上的网络神经元把对应在Ni-1上的神经元输出做为它的输入，我们把神经元和与之对应的神经元之间的连线用生物学的名称，叫做突触（英語：Synapse），在数学模型中每个突触有一个加权数值，我们称做权重，那么要计算第i层上的某个神经元所得到的势能等于每一个权重乘以第i-1层上对应的神经元的输出，然后全体求和得到了第i层上的某个神经元所得到的势能，然后势能数值通过该神经元上的激活函数（activation function，常是∑函数（英語：Sigmoid function）以控制輸出大小，因為其可微分且連續，方便差量规则（英語：Delta rule）處理。），求出该神经元的输出，注意的是该输出是一个非线性的数值，也就是说通过激励函数求的数值根据极限值来判断是否要激活该神经元，换句话说我们对一个神经元网络的输出是否线性不感兴趣。\n\n\n== 基本結構 ==\n一种常见的多层结构的前馈网络（Multilayer Feedforward Network）由三部分組成，\n\n輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入訊息。輸入的訊息稱為輸入向量。\n輸出層（Output layer），訊息在神經元鏈接中傳輸、分析、權衡，形成輸出結果。輸出的訊息稱為輸出向量。\n隱藏層（Hidden layer），簡稱「隱層」，是輸入層和輸出層之間眾多神經元和鏈接組成的各個層面。隱層可以有一层或多層。隱層的節點（神經元）數目不定，但數目越多神經網絡的非線性越顯著，從而神經網絡的強健性（控制系統在一定結構、大小等的參數攝動下，維持某些性能的特性）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。这种网络一般称为感知器（对单隐藏层）或多层感知器（对多隐藏层），神经网络的类型已经演变出很多种，这种分层的结构也并不是对所有的神经网络都适用。\n\n\n== 學習過程 ==\n通過訓練樣本的校正，對各個層的權重進行校正（learning）而建立模型的過程，稱為自動學習過程（training algorithm）。具体的学习方法则因网络结构和模型不同而不同，常用反向传播算法（Backpropagation/倒傳遞/逆傳播，以output利用一次微分Delta rule來修正weight）來驗證。\n\n\n== 種類 ==\n人工神經網络分類為以下兩種：\n1.依學習策略（Algorithm）分類主要有：\n\n监督式学习网络（Supervised Learning Network）為主\n无监督式学习网络（Unsupervised Learning Network）\n混合式学习网络（Hybrid Learning Network）\n联想式学习网络（Associate Learning Network）\n最适化学习网络（Optimization Application Network）2.依網络架構（Connectionism）分類主要有：\n\n前馈神经网络（Feed Forward Network）\n循环神经网络（Recurrent Network）\n强化式架構（Reinforcement Network）\n\n\n== 理论性质 ==\n\n\n=== 计算能力 ===\n多层感知器（Multilayer Perceptron，縮寫MLP）是一个通用的函数逼近器，由Cybenko定理证明。然而，证明不依赖特定的神经元数量或权重。Hava Siegelmann和Eduardo D. Sontag的工作证明了，一个具有有理数权重值的特定递归结构（与全精度实数权重值相对应）由有限个神经元和标准的线性关系构成的神经网络相当于一个通用图灵机。他们进一步表明，使用无理数权重值会产生一个超图灵机。\n\n\n=== 容量 ===\n人工神经网络模型有一个属性，称为“容量”，这大致相当于他们记住（而非正确分类）输入数据的能力。它与网络的参数、和结构有关。谷歌在研究中使用打乱标签的方法，来测试模型是否能记住所有的输出。虽然很明显，这样模型在测试集上的表现几乎是随机猜测，但是模型能够记住所有训练集的输入数据，即记住他们被打乱后的标签。而记住有限的样本的信息（Expressivity），需要的模型的参数（权重）数量存在下限。\n\n\n=== 收敛性 ===\n模型并不总是收敛到唯一解，因为它取决于一些因素。首先，函数可能存在许多局部极小值，这取决于成本函数和模型。其次，在远离局部最小值时，优化方法可能无法保证收敛。第三，对大量的数据或参数，一些方法变得不切实际。在一般情况下，我们发现，理论保证的收敛不能成为实际应用的一个可靠的指南。\n\n\n=== 综合统计 ===\n在目标是创建一个普遍系统的应用程序中，过度训练的问题出现了。这出现在回旋或过度具体的系统中当网络的容量大大超过所需的自由参数。为了避免这个问题，有两个方向：第一个是使用交叉验证和类似的技术来检查过度训练的存在和选择最佳参数如最小化泛化误差。二是使用某种形式的正规化。这是一个在概率化（贝叶斯）框架里出现的概念，其中的正则化可以通过为简单模型选择一个较大的先验概率模型进行；而且在统计学习理论中，其目的是最大限度地减少了两个数量：“风险”和“结构风险”，相当于误差在训练集和由于过度拟合造成的预测误差。\n\n\n== 相关 ==\n機器學習\n深度學習\n邏輯迴歸\n線性迴歸\n\n\n== 參看 ==\n感知机\n多层感知器\nER随机图\n\n\n== 参见 ==\n人工智能\n\n\n== 参考文献 ==\n\n\n== 外部連結 ==\n神經網絡介紹 （页面存档备份，存于互联网档案馆）\nPerformance comparison of neural network algorithms tested on UCI data sets （页面存档备份，存于互联网档案馆）\nA close view to Artificial Neural Networks Algorithms （页面存档备份，存于互联网档案馆）\n开放式目录计划中和Neural Networks相关的内容\nA Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\nNeural Networks in Materials Science （页面存档备份，存于互联网档案馆）\nA practical tutorial on Neural Networks （页面存档备份，存于互联网档案馆）\nApplications of neural networks\nXOR實例 （页面存档备份，存于互联网档案馆）", "標準差": "標準差，又稱標準偏差、均方差 （英語：standard deviation，縮寫SD，符號σ），在概率統計中最常使用作為測量一組數值的離散程度之用。標準差定義：為方差開算术平方根，反映组内個體間的離散程度；標準差與期望值之比為標準離差率。測量到分佈程度的結果，原則上具有兩種性質：\n\n為非負數值（因為平方後再做平方根）；\n與測量資料具有相同單位（這樣才能比對）。一個總量的標準差或一個隨機變量的標準差，及一個子集合樣品數的標準差之間，有所差別。其公式如下所列。\n標準差的概念由卡爾·皮爾森引入到統計中。\n\n\n== 闡述及應用 ==\n簡單來說，標準差是一組數值自平均值分散開來的程度的一種測量觀念。一個較大的標準差，代表大部分的數值和其平均值之間差異較大；一個較小的標準差，代表這些數值較接近平均值。\n例如，兩組數的集合{0, 5, 9, 14}和{5, 6, 8, 9}其平均值都是7，但第二個集合具有較小的標準差。\n表述“相差\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  个标准差”，即在 \n  \n    \n      \n        \n          \n            X\n            ¯\n          \n        \n        ±\n        k\n        S\n      \n    \n    {\\displaystyle {\\overline {X}}\\pm kS}\n   的样本（sample）范围内考量。\n標準差可以當作不確定性的一種測量。例如在物理科學中，做重複性測量時，測量數值集合的標準差代表這些測量的精確度。當要決定測量值是否符合預測值，測量值的標準差佔有決定性重要角色：如果測量平均值與預測值相差太遠（同時與標準差數值做比較），則認為測量值與預測值互相矛盾。這很容易理解，因為如果測量值都落在一定數值範圍之外，可以合理推論預測值是否正確。\n標準差應用於投資上，可作為量度回報穩定性的指標。標準差數值越大，代表回報遠離過去平均數值，回報較不穩定故風險越高。相反，標準差數值越小，代表回報較為穩定，風險亦較小。\n\n\n== 母體的標準差 ==\n\n\n=== 基本定義 ===\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            \n              \n                x\n                ¯\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-{\\overline {x}})^{2}}}}\n  \n  \n    \n      \n        \n          \n            x\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}}\n  为平均值。\n\n\n=== 简化计算公式 ===\n上述公式可以如下代換而簡化：\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                (\n                \n                  X\n                  \n                    i\n                  \n                \n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n\n                \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                (\n                \n                  X\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n                −\n                2\n                \n                  X\n                  \n                    i\n                  \n                \n                μ\n                +\n                \n                  μ\n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                −\n                \n                  (\n                  \n                    2\n                    μ\n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                +\n                N\n                \n                  μ\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                −\n                2\n                μ\n                (\n                N\n                μ\n                )\n                +\n                N\n                \n                  μ\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                −\n                2\n                N\n                \n                  μ\n                  \n                    2\n                  \n                \n                +\n                N\n                \n                  μ\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    \n                      X\n                      \n                        i\n                      \n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                −\n                N\n                \n                  μ\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sum _{i=1}^{N}(X_{i}-\\mu )^{2}&={}\\sum _{i=1}^{N}(X_{i}^{2}-2X_{i}\\mu +\\mu ^{2})\\\\&{}=\\left(\\sum _{i=1}^{N}X_{i}^{2}\\right)-\\left(2\\mu \\sum _{i=1}^{N}X_{i}\\right)+N\\mu ^{2}\\\\&{}=\\left(\\sum _{i=1}^{N}X_{i}^{2}\\right)-2\\mu (N\\mu )+N\\mu ^{2}\\\\&{}=\\left(\\sum _{i=1}^{N}X_{i}^{2}\\right)-2N\\mu ^{2}+N\\mu ^{2}\\\\&{}=\\left(\\sum _{i=1}^{N}X_{i}^{2}\\right)-N\\mu ^{2}\\end{aligned}}}\n  所以：\n\n  \n    \n      \n        \n          \n            \n              \n                σ\n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        N\n                      \n                    \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    (\n                    \n                      X\n                      \n                        i\n                      \n                    \n                    −\n                    μ\n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        N\n                      \n                    \n                    \n                      (\n                      \n                        \n                          ∑\n                          \n                            i\n                            =\n                            1\n                          \n                          \n                            N\n                          \n                        \n                        \n                          X\n                          \n                            i\n                          \n                          \n                            2\n                          \n                        \n                      \n                      )\n                    \n                    −\n                    \n                      \n                        1\n                        N\n                      \n                    \n                    N\n                    \n                      μ\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \n                          \n                            ∑\n                            \n                              i\n                              =\n                              1\n                            \n                            \n                              N\n                            \n                          \n                          \n                            X\n                            \n                              i\n                            \n                            \n                              2\n                            \n                          \n                        \n                        N\n                      \n                    \n                    −\n                    \n                      μ\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sigma &={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(X_{i}-\\mu )^{2}}}\\\\&={\\sqrt {{\\frac {1}{N}}\\left(\\sum _{i=1}^{N}X_{i}^{2}\\right)-{\\frac {1}{N}}N\\mu ^{2}}}\\\\&={\\sqrt {{\\frac {\\sum _{i=1}^{N}X_{i}^{2}}{N}}-\\mu ^{2}}}\\end{aligned}}}\n  根號裡面，亦即變異數（\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  ）的簡易口訣為：「平方和的平均」減去「平均的平方」。\n\n\n=== 母體為随机变量 ===\n一隨機變量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的標準差定義為：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            E\n            ⁡\n            (\n            (\n            X\n            −\n            E\n            ⁡\n            (\n            X\n            )\n            \n              )\n              \n                2\n              \n            \n            )\n          \n        \n        =\n        \n          \n            E\n            ⁡\n            (\n            \n              X\n              \n                2\n              \n            \n            )\n            −\n            (\n            E\n            ⁡\n            (\n            X\n            )\n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\operatorname {E} ((X-\\operatorname {E} (X))^{2})}}={\\sqrt {\\operatorname {E} (X^{2})-(\\operatorname {E} (X))^{2}}}}\n  須注意並非所有隨機變量都具有標準差，因為有些隨機變量不存在期望值。\n如果隨機變量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  為\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\cdots ,x_{n}}\n  具有相同機率，則可用上述公式計算標準差。\n\n\n==== 離散随机变量的标准差 ====\n若\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  是由實數\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{n}}\n  構成的離散隨機變數（英語：discrete random variable），且每個值的機率相等，則\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的標準差定義為：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              [\n              \n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                (\n                \n                  x\n                  \n                    2\n                  \n                \n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                ⋯\n                +\n                (\n                \n                  x\n                  \n                    N\n                  \n                \n                −\n                μ\n                \n                  )\n                  \n                    2\n                  \n                \n              \n              ]\n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {{\\frac {1}{N}}\\left[(x_{1}-\\mu )^{2}+(x_{2}-\\mu )^{2}+\\cdots +(x_{N}-\\mu )^{2}\\right]}}}\n  　，其中　\n  \n    \n      \n        μ\n        =\n        \n          \n            1\n            N\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          x\n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu ={\\frac {1}{N}}(x_{1}+\\cdots +x_{N})}\n  換成用\n  \n    \n      \n        ∑\n      \n    \n    {\\displaystyle \\sum }\n  來寫，就成為：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-\\mu )^{2}}}}\n  　，其中　\n  \n    \n      \n        μ\n        =\n        \n          \n            1\n            N\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          x\n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mu ={\\frac {1}{N}}(x_{1}+\\cdots +x_{N})}\n  目前為止，與母體標準差的基本公式一致。\n然而若每個\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  可以有不同機率\n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  ，則\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的标准差定義為：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            \n              p\n              \n                i\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\sum _{i=1}^{N}p_{i}(x_{i}-\\mu )^{2}}}}\n  　，其中　\n  \n    \n      \n        μ\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          p\n          \n            i\n          \n        \n        \n          x\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mu =\\sum _{i=1}^{N}p_{i}x_{i}.}\n  这里，\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  为\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的数学期望。\n\n\n==== 连续随机变量的标准差 ====\n若\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  為概率密度\n  \n    \n      \n        p\n        (\n        X\n        )\n      \n    \n    {\\displaystyle p(X)}\n  的连续随机变量（英語：continuous random variable），則\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的标准差定義為：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            ∫\n            (\n            x\n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n            \n            f\n            (\n            x\n            )\n            \n            d\n            x\n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\int (x-\\mu )^{2}\\,f(x)\\,dx}}}\n  其中\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  为\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的数学期望：\n\n  \n    \n      \n        μ\n        =\n        ∫\n        x\n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\mu =\\int x\\,f(x)\\,dx}\n  \n\n\n=== 标准差的特殊性质 ===\n对于常数\n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  和随机变量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  ：\n\n  \n    \n      \n        σ\n        (\n        X\n        +\n        c\n        )\n        =\n        σ\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\sigma (X+c)=\\sigma (X)}\n  \n\n  \n    \n      \n        σ\n        (\n        c\n        X\n        )\n        =\n        c\n        ⋅\n        σ\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\sigma (cX)=c\\cdot \\sigma (X)}\n  \n\n  \n    \n      \n        σ\n        (\n        X\n        +\n        Y\n        )\n        =\n        \n          \n            \n              σ\n              \n                2\n              \n            \n            (\n            X\n            )\n            +\n            \n              σ\n              \n                2\n              \n            \n            (\n            Y\n            )\n            +\n            2\n            ⋅\n            \n              \n                cov\n              \n            \n            (\n            X\n            ,\n            Y\n            )\n          \n        \n      \n    \n    {\\displaystyle \\sigma (X+Y)={\\sqrt {\\sigma ^{2}(X)+\\sigma ^{2}(Y)+2\\cdot {\\mbox{cov}}(X,Y)}}}\n  \n其中：\n\n  \n    \n      \n        \n          \n            cov\n          \n        \n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle {\\mbox{cov}}(X,Y)}\n  表示随机变量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  和\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  的协方差。\n\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\sigma ^{2}(X)}\n  表示\n  \n    \n      \n        [\n        σ\n        (\n        X\n        )\n        \n          ]\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle [\\sigma (X)]^{2}}\n  ，即\n  \n    \n      \n        V\n        a\n        r\n        (\n        X\n        )\n      \n    \n    {\\displaystyle Var(X)}\n  （\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的變異數），對\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  亦同。\n\n\n== 样本的标准差 ==\n在真实世界中，找到一个总体的真实的标准差並不實際。大多数情况下，总体标准差是通过随机抽取一定量的样本并计算样本标准差估计的。\n從一大組數值\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          X\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\cdots ,X_{N}}\n  當中取出一樣本數值組合\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          x\n          \n            n\n          \n        \n        :\n        n\n        <\n        N\n      \n    \n    {\\displaystyle x_{1},\\cdots ,x_{n}:n<N}\n  ，常定義其樣本標準差：\n\n  \n    \n      \n        s\n        =\n        \n          \n            \n              \n                1\n                \n                  n\n                  −\n                  1\n                \n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s={\\sqrt {{\\frac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}}\n  样本方差\n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s^{2}}\n  是对总体方差\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  的无偏估计。之所以\n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  中的分母要用\n  \n    \n      \n        n\n        −\n        1\n      \n    \n    {\\displaystyle n-1}\n  而不是像总体样本差那样用\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  ，是因为\n  \n    \n      \n        \n          (\n          \n            \n              x\n              \n                i\n              \n            \n            −\n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(x_{i}-{\\bar {x}}\\right)}\n  的自由度为\n  \n    \n      \n        n\n        −\n        1\n      \n    \n    {\\displaystyle n-1}\n  ，这是由于存在约束条件\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          (\n          \n            \n              x\n              \n                i\n              \n            \n            −\n            \n              \n                \n                  x\n                  ¯\n                \n              \n            \n          \n          )\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}\\left(x_{i}-{\\bar {x}}\\right)=0}\n  。\n\n\n== 範例 ==\n這裡示範如何計算一組數的標準差。例如一群孩童年齡的數值為{ 5, 6, 8, 9 }：\n\n第一步，計算平均值\n  \n    \n      \n        \n          \n            x\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}}\n  ︰\n  \n    \n      \n        \n          \n            x\n            ¯\n          \n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}={\\frac {1}{N}}\\sum _{i=1}^{N}x_{i}}\n  當\n  \n    \n      \n        \n          \n            \n              \n                \n                  N\n                  =\n                  4\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{smallmatrix}N=4\\end{smallmatrix}}}\n  （因為集合裏有4個數），分別設為：\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  \n                    1\n                  \n                \n              \n              \n                \n                =\n                5\n                ,\n              \n            \n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                6\n                ,\n              \n            \n            \n              \n                \n                  x\n                  \n                    3\n                  \n                \n              \n              \n                \n                =\n                8\n                ,\n              \n            \n            \n              \n                \n                  x\n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                9\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x_{1}&=5,\\\\x_{2}&=6,\\\\x_{3}&=8,\\\\x_{4}&=9,\\end{aligned}}}\n  則平均值為\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    4\n                  \n                \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    4\n                  \n                \n                \n                  x\n                  \n                    i\n                  \n                \n              \n              \n                (\n                N\n                =\n                4\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    4\n                  \n                \n                \n                  (\n                  \n                    \n                      x\n                      \n                        1\n                      \n                    \n                    +\n                    \n                      x\n                      \n                        2\n                      \n                    \n                    +\n                    \n                      x\n                      \n                        3\n                      \n                    \n                    +\n                    \n                      x\n                      \n                        4\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    4\n                  \n                \n                \n                  (\n                  \n                    5\n                    +\n                    6\n                    +\n                    8\n                    +\n                    9\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                7.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\overline {x}}&={\\frac {1}{4}}\\sum _{i=1}^{4}x_{i}&(N=4)\\\\&={\\frac {1}{4}}\\left(x_{1}+x_{2}+x_{3}+x_{4}\\right)\\\\&={\\frac {1}{4}}\\left(5+6+8+9\\right)\\\\&=7.\\end{aligned}}}\n  第二步，計算標準差\n  \n    \n      \n        σ\n        \n      \n    \n    {\\displaystyle \\sigma \\,}\n  ︰\n  \n    \n      \n        \n          \n            \n              \n                σ\n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        N\n                      \n                    \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        N\n                      \n                    \n                    (\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    −\n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        4\n                      \n                    \n                    (\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    −\n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                (\n                N\n                =\n                4\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        4\n                      \n                    \n                    (\n                    \n                      x\n                      \n                        i\n                      \n                    \n                    −\n                    7\n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              \n                (\n                \n                  \n                    x\n                    ¯\n                  \n                \n                =\n                7\n                )\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      [\n                      \n                        (\n                        \n                          x\n                          \n                            1\n                          \n                        \n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        \n                          x\n                          \n                            2\n                          \n                        \n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        \n                          x\n                          \n                            3\n                          \n                        \n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        \n                          x\n                          \n                            4\n                          \n                        \n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                      \n                      ]\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      [\n                      \n                        (\n                        5\n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        6\n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        8\n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        9\n                        −\n                        7\n                        \n                          )\n                          \n                            2\n                          \n                        \n                      \n                      ]\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      (\n                      \n                        (\n                        −\n                        2\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        (\n                        −\n                        1\n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        \n                          1\n                          \n                            2\n                          \n                        \n                        +\n                        \n                          2\n                          \n                            2\n                          \n                        \n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        1\n                        4\n                      \n                    \n                    \n                      (\n                      \n                        4\n                        +\n                        1\n                        +\n                        1\n                        +\n                        4\n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      10\n                      4\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                ≈\n                1.58114\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sigma &={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-{\\overline {x}})^{2}}}\\\\&={\\sqrt {{\\frac {1}{4}}\\sum _{i=1}^{4}(x_{i}-{\\overline {x}})^{2}}}&(N=4)\\\\&={\\sqrt {{\\frac {1}{4}}\\sum _{i=1}^{4}(x_{i}-7)^{2}}}&({\\overline {x}}=7)\\\\&={\\sqrt {{\\frac {1}{4}}\\left[(x_{1}-7)^{2}+(x_{2}-7)^{2}+(x_{3}-7)^{2}+(x_{4}-7)^{2}\\right]}}\\\\&={\\sqrt {{\\frac {1}{4}}\\left[(5-7)^{2}+(6-7)^{2}+(8-7)^{2}+(9-7)^{2}\\right]}}\\\\&={\\sqrt {{\\frac {1}{4}}\\left((-2)^{2}+(-1)^{2}+1^{2}+2^{2}\\right)}}\\\\&={\\sqrt {{\\frac {1}{4}}\\left(4+1+1+4\\right)}}\\\\&={\\sqrt {\\frac {10}{4}}}\\\\&\\approx 1.58114\\,.\\end{aligned}}}\n  \n\n\n== 常態分佈的規則 ==\n\n在實際應用上，常考慮一組數據具有近似於常態分佈的機率分佈。若其假設正確，則約68%數值分佈在距離平均值有1個標準差之內的範圍，約95%數值分佈在距離平均值有2個標準差之內的範圍，以及約99.7%數值分佈在距離平均值有3個標準差之內的範圍。稱為「68-95-99.7法則」。\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        \n          σ\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                \n                  2\n                  π\n                \n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      −\n                      μ\n                    \n                    σ\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x;\\mu ,\\sigma ^{2})={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  \n  \n    \n      \n        \n          Proportion\n        \n        =\n        erf\n        ⁡\n        \n          (\n          \n            \n              z\n              \n                2\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\text{Proportion}}=\\operatorname {erf} \\left({\\frac {z}{\\sqrt {2}}}\\right)}\n  \n  \n    \n      \n        \n          Proportion\n        \n        ≤\n        x\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          [\n          \n            1\n            +\n            erf\n            ⁡\n            \n              (\n              \n                \n                  \n                    x\n                    −\n                    μ\n                  \n                  \n                    σ\n                    \n                      \n                        2\n                      \n                    \n                  \n                \n              \n              )\n            \n          \n          ]\n        \n        =\n        \n          \n            1\n            2\n          \n        \n        \n          [\n          \n            1\n            +\n            erf\n            ⁡\n            \n              (\n              \n                \n                  z\n                  \n                    2\n                  \n                \n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle {\\text{Proportion}}\\leq x={\\frac {1}{2}}\\left[1+\\operatorname {erf} \\left({\\frac {x-\\mu }{\\sigma {\\sqrt {2}}}}\\right)\\right]={\\frac {1}{2}}\\left[1+\\operatorname {erf} \\left({\\frac {z}{\\sqrt {2}}}\\right)\\right]}\n  .\n\n\n== 標準差與平均值之間的關係 ==\n一組數據的平均值及標準差常常同時作為參考的依據。从某种意义上说，如果用平均值來考量數值的中心的话，則標準差也就是对统计的分散度的一个“自然”的测度。因为由平均值所得的标准差要小于到其他任何一个点的标准差。較確切的敘述為：設\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          X\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle X_{1},\\cdots ,X_{N}}\n  為實數，定義函数：\n\n  \n    \n      \n        σ\n        (\n        μ\n        )\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma (\\mu )={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-\\mu )^{2}}}}\n  使用微積分或者通过配方法，不難算出\n  \n    \n      \n        σ\n        (\n        μ\n        )\n      \n    \n    {\\displaystyle \\sigma (\\mu )}\n  在下面情況下具有唯一最小值：\n\n  \n    \n      \n        μ\n        =\n        \n          \n            x\n            ¯\n          \n        \n      \n    \n    {\\displaystyle \\mu ={\\overline {x}}}\n  \n\n\n== 几何学解释 ==\n从几何学的角度出发，标准差可以理解为一个从\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  维空间的一个点到一条直线的距离的函数。举一个简单的例子，一组数据中有3个值，\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \n          X\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2},X_{3}}\n  。它们可以在3维空间中确定一个点\n  \n    \n      \n        P\n        =\n        (\n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \n          X\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle P=(X_{1},X_{2},X_{3})}\n  。想像一条通过原点的直线\n  \n    \n      \n        L\n        =\n        \n          (\n          r\n          ,\n          r\n          ,\n          r\n          )\n          :\n          r\n          ∈\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle L={(r,r,r):r\\in \\mathbb {R} }}\n  。如果这组数据中的3个值都相等，则点\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  就是直线\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  上的一个点，\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  到\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  的距离为0，所以标准差也为0。若这3个值不都相等，过点\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  作垂线\n  \n    \n      \n        P\n        R\n      \n    \n    {\\displaystyle PR}\n  垂直于\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  ，\n  \n    \n      \n        P\n        R\n      \n    \n    {\\displaystyle PR}\n  交\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  于点\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  ，则\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  的坐标为这3个值的平均数：\n\n  \n    \n      \n        R\n        =\n        (\n        \n          \n            x\n            ¯\n          \n        \n        ,\n        \n          \n            x\n            ¯\n          \n        \n        ,\n        \n          \n            x\n            ¯\n          \n        \n        )\n      \n    \n    {\\displaystyle R=({\\overline {x}},{\\overline {x}},{\\overline {x}})}\n  运用一些代数知识，不难发现点\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  与点\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  之间的距离（也就是点\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  到直线\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  的距离）是\n  \n    \n      \n        σ\n        \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\sigma {\\sqrt {3}}}\n  。在\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  维空间中，这个规律同样适用，把\n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  换成\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  就可以了。\n\n\n== 参考文献 ==\n\n\n== 外部链接 ==\nStandard Deviation Calculator，标准差计算器 （英文）", "贝叶斯定理": "貝葉斯定理（英語：Bayes' theorem）是概率論中的一個定理，描述在已知一些条件下，某事件的发生機率。比如，如果已知某種健康問題与寿命有关，使用贝叶斯定理则可以通过得知某人年龄，来更加准确地计算出某人有某種健康問題的機率。\n通常，事件A在事件B已發生的條件下发生的機率，與事件B在事件A已發生的條件下发生的機率是不一樣的。然而，這兩者是有確定的關係的，貝葉斯定理就是這種關係的陳述。貝葉斯公式的一個用途，即透過已知的三個機率而推出第四個機率。贝叶斯定理跟隨機變量的條件機率以及邊際機率分布有關。\n作為一個普遍的原理，貝葉斯定理對於所有機率的解釋是有效的。这一定理的主要应用为贝叶斯推断，是推论统计学中的一种推断法。这一定理名稱來自於托马斯·贝叶斯。\n\n\n== 陈述 ==\n\n贝叶斯定理是关于随机事件A和B的条件概率的一則定理。\n\n其中\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  以及\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  為隨機事件，且\n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n  不為零。\n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  是指在事件\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  发生的情况下事件\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  发生的概率。\n在贝叶斯定理中，每个名词都有约定俗成的名称：\n\n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  是已知\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  發生后，\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  的條件概率。也稱作\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  的事後概率。\n\n  \n    \n      \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A)}\n  是\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  的先驗概率（或边缘概率）。其不考慮任何\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  方面的因素。\n\n  \n    \n      \n        P\n        (\n        B\n        ∣\n        A\n        )\n      \n    \n    {\\displaystyle P(B\\mid A)}\n  是已知\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  發生后，\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  的條件概率。也可稱爲\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  的事後機率。某些文獻又称其为在特定\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  時，\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  的似然性，因爲\n  \n    \n      \n        P\n        (\n        B\n        ∣\n        A\n        )\n        =\n        L\n        (\n        A\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle P(B\\mid A)=L(A\\mid B)}\n  。\n\n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n  是\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  的先驗概率。按這些術語，贝叶斯定理可表述為：\n\n后验概率 = (似然性*先驗概率)/標準化常量也就是說，后验概率与先驗概率和相似度的乘積成正比。\n另外，比例\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        \n          /\n        \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B|A)/P(B)}\n  也有時被稱作標准似然度（standardised likelihood），贝叶斯定理可表述為：\n\n后验概率 = 標準似然度*先驗概率\n由贝叶斯公式\n  \n    \n      \n        P\n        (\n        θ\n        \n          |\n        \n        X\n        )\n        =\n        \n          \n            \n              P\n              (\n              θ\n              )\n              P\n              (\n              X\n              \n                |\n              \n              θ\n              )\n            \n            \n              P\n              (\n              X\n              )\n            \n          \n        \n        ∝\n        P\n        (\n        θ\n        )\n        P\n        (\n        X\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle P(\\theta |X)={\\frac {P(\\theta )P(X|\\theta )}{P(X)}}\\propto P(\\theta )P(X|\\theta )}\n  可以看出，这里面的 \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   是一个随机变量（因为 \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   有概率 \n  \n    \n      \n        P\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle P(\\theta )}\n   ）。因为\n  \n    \n      \n        P\n        (\n        θ\n        \n          |\n        \n        X\n        )\n        ∝\n        P\n        (\n        θ\n        )\n        P\n        (\n        X\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle P(\\theta |X)\\propto P(\\theta )P(X|\\theta )}\n   ，所以这也是贝叶斯估计和极大似然估计的区别所在，极大似然估计中要估计的参数是个一般变量，而贝叶斯估计中要估计的参数是个随机变量。\n\n\n== 從條件概率推導貝氏定理 ==\n根據條件概率的定義。在事件\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  发生的条件下事件\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  发生的概率是：\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∩\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(A\\cap B)}{P(B)}}}\n  其中 \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  与\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  的联合概率表示为\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  或者\n  \n    \n      \n        P\n        (\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle P(A,B)}\n  或者\n  \n    \n      \n        P\n        (\n        A\n        B\n        )\n      \n    \n    {\\displaystyle P(AB)}\n  。\n同樣地，在事件\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  发生的条件下事件\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  发生的概率\n\n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∩\n              B\n              )\n            \n            \n              P\n              (\n              A\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(B|A)={\\frac {P(A\\cap B)}{P(A)}}\\!}\n  整理与合并這兩個方程式，我們可以得到\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        \n        P\n        (\n        B\n        )\n        =\n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        \n        P\n        (\n        A\n        )\n        \n      \n    \n    {\\displaystyle P(A|B)\\,P(B)=P(A\\cap B)=P(B|A)\\,P(A)\\!}\n  这个引理有时称作概率乘法规则。上式兩邊同除以\n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n  ，若\n  \n    \n      \n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(B)}\n  是非零的，我們可以得到贝叶斯定理:\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              \n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)\\,P(A)}{P(B)}}\\!}\n  \n\n\n== 二中擇一的形式 ==\n貝氏定理通常可以再寫成下面的形式：\n\n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        P\n        (\n        A\n        ∩\n        B\n        )\n        +\n        P\n        (\n        \n          A\n          \n            C\n          \n        \n        ∩\n        B\n        )\n        =\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        \n          |\n        \n        \n          A\n          \n            C\n          \n        \n        )\n        P\n        (\n        \n          A\n          \n            C\n          \n        \n        )\n      \n    \n    {\\displaystyle P(B)=P(A\\cap B)+P(A^{C}\\cap B)=P(B|A)P(A)+P(B|A^{C})P(A^{C})}\n  ，其中AC是A的補集（即非A）。故上式亦可寫成：\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              \n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n              +\n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  C\n                \n              \n              )\n              P\n              (\n              \n                A\n                \n                  C\n                \n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)\\,P(A)}{P(B|A)P(A)+P(B|A^{C})P(A^{C})}}\\!}\n  在更一般化的情況，假設{Ai}是事件集合裡的部份集合，對於任意的Ai，貝氏定理可用下式表示：\n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            i\n          \n        \n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  i\n                \n              \n              )\n              \n              P\n              (\n              \n                A\n                \n                  i\n                \n              \n              )\n            \n            \n              \n                ∑\n                \n                  j\n                \n              \n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  j\n                \n              \n              )\n              \n              P\n              (\n              \n                A\n                \n                  j\n                \n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A_{i}|B)={\\frac {P(B|A_{i})\\,P(A_{i})}{\\sum _{j}P(B|A_{j})\\,P(A_{j})}}\\!}\n  \n\n\n=== 以可能性與相似率表示貝氏定理 ===\n\n貝氏定理亦可由相似率Λ和可能性O表示：\n\n  \n    \n      \n        O\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        O\n        (\n        A\n        )\n        ⋅\n        Λ\n        (\n        A\n        \n          |\n        \n        B\n        )\n      \n    \n    {\\displaystyle O(A|B)=O(A)\\cdot \\Lambda (A|B)}\n  其中\n\n  \n    \n      \n        O\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \n                |\n              \n              B\n              )\n            \n            \n              P\n              (\n              \n                A\n                \n                  C\n                \n              \n              \n                |\n              \n              B\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle O(A|B)={\\frac {P(A|B)}{P(A^{C}|B)}}\\!}\n  定義為B發生時，A發生的可能性（odds）；\n\n  \n    \n      \n        O\n        (\n        A\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              \n                A\n                \n                  C\n                \n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle O(A)={\\frac {P(A)}{P(A^{C})}}\\!}\n  則是A發生的可能性。相似率（Likelihood ratio）則定義為：\n\n  \n    \n      \n        Λ\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              L\n              (\n              A\n              \n                |\n              \n              B\n              )\n            \n            \n              L\n              (\n              \n                A\n                \n                  C\n                \n              \n              \n                |\n              \n              B\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n            \n            \n              P\n              (\n              B\n              \n                |\n              \n              \n                A\n                \n                  C\n                \n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\Lambda (A|B)={\\frac {L(A|B)}{L(A^{C}|B)}}={\\frac {P(B|A)}{P(B|A^{C})}}\\!}\n  \n\n\n=== 貝氏定理與概率密度 ===\n貝氏定理亦可用於連續機率分佈。由於概率密度函数嚴格上並非機率，由機率密度函數導出貝氏定理觀念上較為困難（詳細推導參閱）。貝氏定理與機率密度的關係是由求極限的方式建立：\n\n  \n    \n      \n        f\n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        \n          \n            \n              f\n              (\n              x\n              ,\n              y\n              )\n            \n            \n              f\n              (\n              y\n              )\n            \n          \n        \n        =\n        \n          \n            \n              f\n              (\n              y\n              \n                |\n              \n              x\n              )\n              \n              f\n              (\n              x\n              )\n            \n            \n              f\n              (\n              y\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle f(x|y)={\\frac {f(x,y)}{f(y)}}={\\frac {f(y|x)\\,f(x)}{f(y)}}\\!}\n  全機率定理則有類似的論述：\n\n  \n    \n      \n        f\n        (\n        x\n        \n          |\n        \n        y\n        )\n        =\n        \n          \n            \n              f\n              (\n              y\n              \n                |\n              \n              x\n              )\n              \n              f\n              (\n              x\n              )\n            \n            \n              \n                ∫\n                \n                  −\n                  ∞\n                \n                \n                  ∞\n                \n              \n              f\n              (\n              y\n              \n                |\n              \n              x\n              )\n              \n              f\n              (\n              x\n              )\n              \n              d\n              x\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle f(x|y)={\\frac {f(y|x)\\,f(x)}{\\int _{-\\infty }^{\\infty }f(y|x)\\,f(x)\\,dx}}.\\!}\n  如同離散的情況，公式中的每項均有名稱。\nf(x, y)是X和Y的聯合分佈；\nf（x|y）是給定Y=y後，X的事後分佈；\nf（y|x）= L（x|y）是Y=y後，X的相似度函數（為x的函數)；\nf（x）和f（y）則是X和Y的邊際分佈；\nf（x）則是X的事前分佈。\n為了方便起見，這裡的f在這些專有名詞中代表不同的函數（可以由引數的不同判斷之）。\n\n\n=== 貝氏定理的推廣 ===\n對於變數有二個以上的情況，貝氏定理亦成立。例如：\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        ,\n        C\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              )\n              \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              A\n              ,\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              B\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A|B,C)={\\frac {P(A)\\,P(B|A)\\,P(C|A,B)}{P(B)\\,P(C|B)}}\\!}\n  這個式子可以由套用多次二個變數的貝氏定理及條件機率的定義導出：\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        ,\n        C\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              B\n              ,\n              C\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              A\n              ,\n              B\n              ,\n              C\n              )\n            \n            \n              P\n              (\n              B\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A|B,C)={\\frac {P(A,B,C)}{P(B,C)}}={\\frac {P(A,B,C)}{P(B)\\,P(C|B)}}}\n  \n\n  \n    \n      \n        =\n        \n          \n            \n              P\n              (\n              C\n              \n                |\n              \n              A\n              ,\n              B\n              )\n              \n              P\n              (\n              A\n              ,\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              B\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              A\n              )\n              \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              A\n              ,\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n              \n              P\n              (\n              C\n              \n                |\n              \n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle ={\\frac {P(C|A,B)\\,P(A,B)}{P(B)\\,P(C|B)}}={\\frac {P(A)\\,P(B|A)\\,P(C|A,B)}{P(B)\\,P(C|B)}}}\n  一般化的方法則是利用聯合機率去分解待求的條件機率，並對不加以探討的變數積分（意即對欲探討的變數計算邊緣機率）。取決於不同的分解形式，可以證明某些積分必為1，因此分解形式可被簡化。利用這個性質，貝氏定理的計算量可能可以大幅下降。貝氏網路為此方法的一個例子，貝氏網路指定數個變數的聯合機率分佈的分解型式，該機率分佈滿足下述條件：當其他變數的條件機率給定時，該變數的條件機率為一簡單型式。\n\n\n== 範例 ==\n\n\n=== 吸毒者检测 ===\n下面展示贝叶斯定理在检测吸毒者时的应用。假设一个常规的检测结果的靈敏度和特異度均为99%，即吸毒者每次检测呈阳性（+）的概率为99%。而不吸毒者每次检测呈阴性（-）的概率为99%。从检测结果的概率来看，检测结果是比较准确的，但是贝叶斯定理卻可以揭示一个潜在的问题。假设某公司对全体雇员进行吸毒检测，已知0.5%的雇员吸毒。请问每位检测结果呈阳性的雇员吸毒的概率有多高？\n令“D”为雇员吸毒事件，“N”为雇员不吸毒事件，“+”为检测呈阳性事件。可得\n\nP(D)代表雇员吸毒的概率，不考虑其他情况，该值为0.005。因为公司的预先统计表明该公司的雇员中有0.5%的人吸食毒品，所以这个值就是D的先验概率。\nP(N)代表雇员不吸毒的概率，显然，该值为0.995，也就是1-P(D)。\nP(+|D)代表吸毒者被驗出為阳性的概率，这是一个条件概率，由于阳性检测准确性是99%，因此该值为0.99。\nP(+|N)代表不吸毒者被驗出為阳性的概率，也就是出错检测的概率，该值为0.01。因为对于不吸毒者，其检测为阴性的概率为99%，因此，其被误检测成阳性的概率为1 - 0.99 = 0.01。\nP(+)代表不考虑其他因素的影响的阳性检出率，白話來說，即該公司有多少比例的檢測結果為陽性。该值为0.0149或者1.49%。我们可以通过全概率公式计算得到：此概率 = 身爲吸毒者的概率 x 吸毒被驗出陽性的概率（0.5% x 99% = 0.495%) + 身爲不吸毒者的概率 x 不吸毒卻被驗出陽性的概率（99.5% x 1% = 0.995%)。P(+)=0.0149是检测呈阳性的先验概率。用数学公式描述为：\n  \n    \n      \n        P\n        (\n        +\n        )\n        =\n        P\n        (\n        +\n        ∩\n        D\n        )\n        +\n        P\n        (\n        +\n        ∩\n        N\n        )\n        =\n        P\n        (\n        +\n        \n          |\n        \n        D\n        )\n        P\n        (\n        D\n        )\n        +\n        P\n        (\n        +\n        \n          |\n        \n        N\n        )\n        P\n        (\n        N\n        )\n      \n    \n    {\\displaystyle P(+)=P(+\\cap D)+P(+\\cap N)=P(+|D)P(D)+P(+|N)P(N)}\n  根据上述描述，我们可以计算某人检测呈阳性时确实吸毒的条件概率P(D|+)：\n\n  \n    \n      \n        \n          \n            \n              \n                P\n                (\n                D\n                \n                  |\n                \n                +\n                )\n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      +\n                      \n                        |\n                      \n                      D\n                      )\n                      P\n                      (\n                      D\n                      )\n                    \n                    \n                      P\n                      (\n                      +\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      P\n                      (\n                      +\n                      \n                        |\n                      \n                      D\n                      )\n                      P\n                      (\n                      D\n                      )\n                    \n                    \n                      P\n                      (\n                      +\n                      \n                        |\n                      \n                      D\n                      )\n                      P\n                      (\n                      D\n                      )\n                      +\n                      P\n                      (\n                      +\n                      \n                        |\n                      \n                      N\n                      )\n                      P\n                      (\n                      N\n                      )\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    \n                      0.99\n                      ×\n                      0.005\n                    \n                    \n                      0.99\n                      ×\n                      0.005\n                      +\n                      0.01\n                      ×\n                      0.995\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                0.3322.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P(D|+)&={\\frac {P(+|D)P(D)}{P(+)}}\\\\&={\\frac {P(+|D)P(D)}{P(+|D)P(D)+P(+|N)P(N)}}\\\\&={\\frac {0.99\\times 0.005}{0.99\\times 0.005+0.01\\times 0.995}}\\\\&=0.3322.\\end{aligned}}}\n  尽管吸毒检测的准确率高达99%，但贝叶斯定理告诉我们：如果某人检测呈阳性，其吸毒的概率只有大约33%，不吸毒的可能性比较大。假阳性高，则检测的结果不可靠。這是因爲該公司不吸毒的人數遠遠大於吸毒人數，所以即使不吸毒者被誤檢為陽性的概率僅爲1%，其實際被誤檢人數還是很龐大。舉例來說，若該公司總共有1000人（其中5人吸毒，995人不吸），不吸毒的人被檢測出陽性的人數有大約10人（1% x 995），而吸毒被驗出陽性的人數有5人（99% x 5），總共15人被驗出陽性（10 + 5）。在這15人裏面，只有約33%的人是真正有吸毒。所以貝氏定理可以揭露出此檢測在這個案例中的不可靠。\n同時，也因爲不可靠的主因是不吸毒卻被誤檢陽性的人數遠多於吸毒被檢測出來的人數（上述例子中10人 >  5 人），所以即使陽性檢測靈敏度能到100%（即只要吸毒一定驗出陽性），檢測結果陽性的員工，真正吸毒的概率\n  \n    \n      \n        P\n        (\n        D\n        \n          |\n        \n        +\n        )\n      \n    \n    {\\displaystyle P(D|+)}\n  也只會提高到約33.4%。但如果靈敏度仍然是99%，而特異度卻提高到99.5%（即不吸毒的人中，約0.5%會被誤檢為陽性），則檢測結果陽性的員工，真正吸毒的概率可以提高到49.9%。\n\n\n=== 胰腺癌检测 ===\n基于贝叶斯定理：即使100%的胰腺癌症患者都有某症状，而某人有同样的症状，绝对不代表该人有100%的概率得胰腺癌，还需要考虑先验概率，假设胰腺癌的发病率是十万分之一，而全球有同样症状的人有万分之一，则此人得胰腺癌的概率只有十分之一，90%的可能是是假阳性。\n\n\n=== 不良种子检测 ===\n基于贝叶斯定理：假设100%的不良种子都表现A性状，而种子表现A性状，并不代表此种子100%是不良种子，还需要考虑先验概率，假设一共有6万颗不良种子，在种子中的比例是十万分之一（假設总共有60亿颗种子），假设所有种子中有1/3表现A性状（即20亿颗种子表现A性状），则此种子为不良种子的概率只有十万分之三。\n\n\n== 参见 ==\n概率论\n贝叶斯概率\n贝叶斯推理\n\n\n== 參考文獻 ==\n\n\n== 外部連結 ==\n数学之美番外篇：平凡而又神奇的贝叶斯方法（页面存档备份，存于互联网档案馆）", "总体": "总体（英語：statistical population，香港作總體，台湾作母體，又稱為母集團或整體），是指統計學中是指由許多有某種共同性質的事物組成的集合，會在此集合中選出样本進行統計推斷，選取樣本的方式可能會用亂數或是其他抽樣方式。\n例如要針對所有烏鴉的共有特性進行研究，總體是目前存在、以前曾經存在或是未來可能存在的所有烏鴉，此情形下，因為時間的限制、地域可取得性的限制、以及研究者的有限資源等，不可能觀測總體中的每一個，因此研究者會從總體中產生样本，再由樣本的特性去了解總體的特性。\n產生樣本的目的之一就是為了要知道總體的特性，包括\n\n總體均值，用\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  表示，若總體的數量是有限的，總體均值等於所有數值的算術平均數。\n總體標準差，用\n表示，基本定義如下\n  \n    \n      \n        σ\n        =\n        \n          \n            \n              \n                1\n                N\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                N\n              \n            \n            (\n            \n              x\n              \n                i\n              \n            \n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {{\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-\\mu )^{2}}}}\n  。\n\n\n== 子總體 ==\n總體的子集稱為子總體，若不同的子總體有不同的性質，則整個總體具有異質性，若將總體區分為不同的子總體，可以對整個總體的特質有較多的了解。例如某特定藥物可能對不同的的子總體有不同的影響，若在取樣時沒有取到該子總體，可能就忽略了這樣的影響。\n區分子總體也有助於更精確的估計參數，例如考慮男性和女性是不同的子總體，可以針對人類身高的分佈有更好的建模。\n包括子總體的總體可以用混合模式建模，將各子總體的分佈結合成整個總體的分佈，不過即使子總體都可以用簡單的模型來表示，總體仍可能在用簡單模型來拟合時有很差的效果。例如有二個都是常態分佈的子總體，兩者的標準差相同，但平均值不同，所得的總體分布會是峰度較低的常態分佈，若兩者平均值的差距過大，甚至還會變成双峰分布，而其標準差也可能會比原來子總體的要大。例如有二個都是常態分佈的子總體，兩者的平均值相同，但標準差不同，會有峰度較高的常態分佈。\n\n\n== 相關條目 ==\n抽樣\n\n\n== 參考資料 ==", "集中趋势": "在統計學中，集中趨勢（central tendency）或中央趨勢，在口語上也經常被稱為平均，表示一個機率分佈的中間值。最常見的幾種集中趨勢包括算數平均數、中位數及眾數。集中趨勢可以由有限的數組（如一群樣本）中或理論上的機率分配（如常態分佈）中求得。有些人使用集中趨勢（或集中性）這個詞以表示「數量化的資料之中央值的趨勢」。在這種意義下，我們可以利用資数据的離散程度（例如標準偏差或四分差等相似的統計量）判別其集中趨勢的程度。\n集中趨勢（central tendency）一詞於1920年代後期出現。\n\n\n== 集中趨勢的統計量 ==\n一維資料的集中趨勢可能有以下數種統計方法。在某些情況下，經轉型（data transformation）後的資料才採用以下的方法。\n\n算术平均数\n觀測值的總和除以觀測值的個數，即\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                \n                  x\n                  \n                    2\n                  \n                \n                +\n                \n                  x\n                  \n                    3\n                  \n                \n                …\n                +\n                \n                  x\n                  \n                    n\n                  \n                \n              \n              n\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {x_{1}+x_{2}+x_{3}\\ldots +x_{n}}{n}}}\n  。常簡稱為平均數，也往往是背後機率分佈的期望值之不偏估計。\n中位數\n將所有觀測值按大小排序後在順序上居中的數值。\n眾數\n出現最多次的觀測值。\n幾何平均數\n觀測值的乘積之觀測值個數方根，即\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ×\n        \n          x\n          \n            2\n          \n        \n        ×\n        \n          x\n          \n            3\n          \n        \n        …\n        ×\n        \n          x\n          \n            n\n          \n        \n        \n          )\n          \n            \n              1\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle (x_{1}\\times x_{2}\\times x_{3}\\ldots \\times x_{n})^{\\frac {1}{n}}}\n  \n調和平均數\n觀測值個數除以觀測值倒數的總和，即\n  \n    \n      \n        \n          \n            n\n            \n              \n                \n                  1\n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              +\n              \n                \n                  1\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              +\n              .\n              .\n              .\n              +\n              \n                \n                  1\n                  \n                    x\n                    \n                      n\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {n}{{\\frac {1}{x_{1}}}+{\\frac {1}{x_{2}}}+...+{\\frac {1}{x_{n}}}}}}\n  \n加權平均數\n考慮不同群資料貢獻程度不同時的算數平均數\n截尾平均數（truncated mean）\n忽略特定比例或特定數值之外的極端值後所得的平均數。例如，四分平均數（interquartile mean）正是忽略25%前及75%後的資料後所得的算數平均數。\n中程數（midrange）又稱全距中值\n最大值與最小值的算數平均數，即\n  \n    \n      \n        \n          \n            \n              min\n              (\n              x\n              )\n              +\n              max\n              (\n              x\n              )\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\min(x)+\\max(x)}{2}}}\n  。\n中樞紐（midhinge）\n第一四分位數與第三四分位數的算數平均數，即\n  \n    \n      \n        \n          \n            \n              \n                Q\n                \n                  1\n                \n              \n              +\n              \n                Q\n                \n                  3\n                \n              \n            \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {Q_{1}+Q_{3}}{2}}}\n  。\n三均值（trimean）\n考慮三個四分位數的加權平均數，即\n  \n    \n      \n        \n          \n            \n              \n                Q\n                \n                  1\n                \n              \n              +\n              2\n              \n                Q\n                \n                  2\n                \n              \n              +\n              \n                Q\n                \n                  3\n                \n              \n            \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\frac {Q_{1}+2Q_{2}+Q_{3}}{4}}}\n  。\n極端值調整平均數（winsorized mean）\n以最接近的觀測值取代特定比例的極端值後取得的算數平均數。舉例來說，考慮10個觀測值（由小到大排列為\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  至\n  \n    \n      \n        \n          x\n          \n            10\n          \n        \n      \n    \n    {\\displaystyle x_{10}}\n  ）的情況下，10%的極端值調整平均數為\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      x\n                      \n                        2\n                      \n                    \n                    +\n                    \n                      x\n                      \n                        2\n                      \n                    \n                  \n                  ⏞\n                \n              \n              +\n              \n                x\n                \n                  3\n                \n              \n              +\n              \n                x\n                \n                  4\n                \n              \n              +\n              \n                x\n                \n                  5\n                \n              \n              +\n              \n                x\n                \n                  6\n                \n              \n              +\n              \n                x\n                \n                  7\n                \n              \n              +\n              \n                x\n                \n                  8\n                \n              \n              +\n              \n                \n                  \n                    \n                      x\n                      \n                        9\n                      \n                    \n                    +\n                    \n                      x\n                      \n                        9\n                      \n                    \n                  \n                  ⏞\n                \n              \n            \n            10\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\overbrace {x_{2}+x_{2}} +x_{3}+x_{4}+x_{5}+x_{6}+x_{7}+x_{8}+\\overbrace {x_{9}+x_{9}} }{10}}}\n  ，\n其中分別以\n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  和\n  \n    \n      \n        \n          x\n          \n            9\n          \n        \n      \n    \n    {\\displaystyle x_{9}}\n  取代了\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  和\n  \n    \n      \n        \n          x\n          \n            10\n          \n        \n      \n    \n    {\\displaystyle x_{10}}\n  。以上的統計量在多維變數中仍可單獨地被套用在各個維度上進行，但並不能保證在轉軸後仍維持一致的結果。\n\n\n== 平均數、中位數與眾數的關係 ==\n\n在左右對稱的機率分佈中，不同的集中趨勢統計量有相同結果，但在偏度遠離0時則可能不一致。在單峰型的機率分佈（unimodal probability distribution）中，平均數（μ）、中位數（ν）與眾數（θ）的關係如下：\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              θ\n              −\n              μ\n              \n                |\n              \n            \n            σ\n          \n        \n        ≤\n        \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\frac {|\\theta -\\mu |}{\\sigma }}\\leq {\\sqrt {3}}}\n  ，\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              ν\n              −\n              μ\n              \n                |\n              \n            \n            σ\n          \n        \n        ≤\n        \n          \n            0.6\n          \n        \n      \n    \n    {\\displaystyle {\\frac {|\\nu -\\mu |}{\\sigma }}\\leq {\\sqrt {0.6}}}\n  ，\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              θ\n              −\n              ν\n              \n                |\n              \n            \n            σ\n          \n        \n        ≤\n        \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\frac {|\\theta -\\nu |}{\\sigma }}\\leq {\\sqrt {3}}}\n  ，其中σ為標準偏差。至於任一機率分佈，\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              ν\n              −\n              μ\n              \n                |\n              \n            \n            σ\n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle {\\frac {|\\nu -\\mu |}{\\sigma }}\\leq 1}\n  。\n\n\n== 參考文獻 ==", "中位數": "統計學上，中位數（英語：Median），又稱中央值、中值，是一個樣本、種群或概率分佈中之一個數值，其可將數值集合劃分爲数量相等的上下兩部分。對於有限的數集，可以通過把所有觀察值高低排序後找出正中間的一個作爲中位數。如果觀察值有偶數個，則中位數不唯一，通常取最中間的兩個數值的平均數作爲中位數。\n一個數集中最多有一半的數值小於中位數，也最多有一半的數值大於中位數。如果大於和小於中位數的數值個數均少於一半，那麽數集中必有若干值等同於中位數。\n设连续随机变量X的分布函数为F(X)，那么满足条件P(X≤m)=F(m)=1/2的数称为X或分布F的中位数。\n对于一组有限个数的数据来说，其中位数是这样的一种数：这群数据的一半的数据比它大，而另外一半数据比它小。\n计算有限个数的数据的中位数的方法是：把所有的同类数据按照大小的顺序排列。如果数据的个数是奇数，则中间那个数据就是这群数据的中位数；如果数据的个数是偶数，则中间那2个数据的算术平均值就是这群数据的中位数。\n\n\n== 公式 ==\n實數\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},\\dots ,x_{n}}\n  按大小順序（順序，降序皆可）排列為\n  \n    \n      \n        \n          x\n          \n            1\n          \n          ′\n        \n        ,\n        \n          x\n          \n            2\n          \n          ′\n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n          ′\n        \n      \n    \n    {\\displaystyle x'_{1},x'_{2},\\dots ,x'_{n}}\n  、\n實數數列\n  \n    \n      \n        x\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle x=(x_{1},x_{2},\\dots ,x_{n})}\n  的中位數 \n  \n    \n      \n        \n          \n            Q\n          \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Q} _{\\frac {1}{2}}(x)}\n   為\n\n  \n    \n      \n        \n          \n            Q\n          \n          \n            \n              1\n              2\n            \n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    x\n                    \n                      \n                        \n                          n\n                          +\n                          1\n                        \n                        2\n                      \n                    \n                    ′\n                  \n                  ,\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  n\n                  \n                    \n                       is odd number.\n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      1\n                      2\n                    \n                  \n                  (\n                  \n                    x\n                    \n                      \n                        n\n                        2\n                      \n                    \n                    ′\n                  \n                  +\n                  \n                    x\n                    \n                      \n                        \n                          n\n                          2\n                        \n                      \n                      +\n                      1\n                    \n                    ′\n                  \n                  )\n                  ,\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  n\n                  \n                    \n                       is even number.\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {Q} _{\\frac {1}{2}}(x)={\\begin{cases}x'_{\\frac {n+1}{2}},&{\\mbox{if }}n{\\mbox{ is odd number.}}\\\\{\\frac {1}{2}}(x'_{\\frac {n}{2}}+x'_{{\\frac {n}{2}}+1}),&{\\mbox{if }}n{\\mbox{ is even number.}}\\end{cases}}}\n  其中 odd number 表示奇數，even number 表示偶數。\n\n\n== 中位數特性 ==\n中位數在敘述統計學上和平均数、众数並列為數據的集中趨勢。三者的位置排序亦對應著偏度的正負偏態意義。一般而言，平均數是最常被使用做為數據的集中趨勢，但如果有極端值存在，平均數的代表性降低，則中位數就是最佳的集中趨勢代表。因此，在各國的每人所得分布上，通常以中位數代表集中趨勢，而非平均數。\n中位數通常出現在描述统计学和無母數統計，有母數的統計分析很少提及。中位數為集中趨勢時，對應的離散趨勢係數為平均絕對離差（Mean absolute deviation, MAD）或是四位位距(Q3 - Q1)。不過如果論及母體中位數的統計量時，仍需根據統計分析對抽樣分配的要求，尋找母體中位數統計量的期望值與變異數，再依照點估計的充分、不偏、效率、一致性進行討論。而母體中位數的統計量通常是樣本中位數。因此，樣本中位數的期望值與變異數就值得被討論，進行基礎研究。\n\n\n=== 常態分配下的中位數 ===\n常態分配下的平均數、中位數、眾數都是同一個位置。目前最為世人熟知的是平均數的抽樣分配會是常態分配，期望值為母體平均數\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  且變異數為母體變異數(\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{2}}\n  )。統計學對常態分配的母體平均數統計量說明甚多，並發展完善。那麼中位數可基於機率分配模擬器和數值分析發展，在n個獨立隨機變數來自常態分配可生成n個隨機樣本，則E(樣本中位數)=\n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  且Var(樣本中位數)=\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n        k\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\sigma _{2}k(n)}\n  ，其中，k(n)受到樣本個數(n)影響。當樣本個數介於2至200時，兩者的關係不明顯，但可計算出樣本個數和k(n)的關聯表。\n\n如果樣本個數超過200，但不超過1000時，兩者有明顯的關係，並且受到樣本個數是否為奇數或偶數影響。此時可使用迴歸分析尋找兩者的關係。\n1. 樣本個數為偶數，迴歸式為k(n) = 0.0000148965 + 1.5599936862 / n。\n2. 樣本個數為奇數，迴歸式為k(n) = 0.0000084608 + 1.5674001064 / n。\n由此可得到樣本中位數的變異數和母體常態分配的變異數形成穩定的對應關係。\n\n\n== 參考交獻 ==\n\n\n== 外部链接 ==\nCalculating the median\nA problem involving the mean, the median, and the mode.（页面存档备份，存于互联网档案馆）\nmathworld: Statistical Median（页面存档备份，存于互联网档案馆）本條目含有来自PlanetMath《Median of a distribution》的內容，版权遵守知识共享协议：署名-相同方式共享协议。", "后验概率": "在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率（Posterior probability）是在考虑和给出相关证据或数据后所得到的条件概率。同样，后验概率分布是一个未知量（视为随机变量）基于试验和调查后得到的概率分布。“后验”在本文中代表考虑了被测试事件的相关证据。\n\n\n== 定义 ==\n后验概率是在给定证据\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  后，参数\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  的概率：\n  \n    \n      \n        p\n        (\n        θ\n        \n          |\n        \n        X\n        )\n      \n    \n    {\\displaystyle p(\\theta |X)}\n  。\n与似然函数相对，其为在给定了参数\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  后，证据\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的概率：\n  \n    \n      \n        p\n        (\n        X\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle p(X|\\theta )}\n  。\n两者有以下联系：\n首先定义先验概率服从以下概率分布函数，\n  \n    \n      \n        p\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle p(\\theta )}\n  ，则样本\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  的似然性为\n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle p(x|\\theta )}\n  ，那么后验概率可以定义为\n\n  \n    \n      \n        p\n        (\n        θ\n        \n          |\n        \n        x\n        )\n        =\n        \n          \n            \n              p\n              (\n              x\n              \n                |\n              \n              θ\n              )\n              p\n              (\n              θ\n              )\n            \n            \n              p\n              (\n              x\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(\\theta |x)={\\frac {p(x|\\theta )p(\\theta )}{p(x)}}}\n  此处\n  \n    \n      \n        p\n        (\n        x\n        )\n      \n    \n    {\\displaystyle p(x)}\n  为标准化常量，对于连续的\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  ，按如下方法计算\n\n  \n    \n      \n        p\n        (\n        x\n        )\n        =\n        ∫\n        p\n        (\n        x\n        \n          |\n        \n        θ\n        )\n        p\n        (\n        θ\n        )\n        d\n        θ\n      \n    \n    {\\displaystyle p(x)=\\int p(x|\\theta )p(\\theta )d\\theta }\n  \n对于离散的\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  ，应对所有可能的\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  取值求和\n  \n    \n      \n        p\n        (\n        x\n        \n          |\n        \n        θ\n        )\n        p\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle p(x|\\theta )p(\\theta )}\n   。\n因此，后验概率与似然性和先验概率的乘积是成比例的。\n\n\n== 实例 ==\n假设一个学校裡有60％男生和40%女生。女生穿裤子的人数和穿裙子的人数相等，所有男生穿裤子。一个人在远处随机看到了一个穿裤子的学生。那么这个学生是女生的概率是多少？\n使用贝叶斯定理，事件A是看到女生，事件B是看到一个穿裤子的学生。我们所要计算的是P(A|B)。\nP(A)是忽略其它因素，看到女生的概率，在这里是40%\nP(A')是忽略其它因素，看到不是女生（即看到男生）的概率，在这里是60%\nP(B|A)是女生穿裤子的概率，在这里是50%\nP(B|A')是男生穿裤子的概率，在这里是100%\nP(B)是忽略其它因素，学生穿裤子的概率，P(B) = P(B|A)P(A) + P(B|A')P(A')，在这里是0.5×0.4 + 1×0.6 = 0.8.\n根据贝叶斯定理，我们计算出后验概率P(A|B)\n\n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              \n                |\n              \n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        =\n        \n          \n            \n              0.5\n              ×\n              0.4\n            \n            0.8\n          \n        \n        =\n        0.25\n      \n    \n    {\\displaystyle P(A|B)={\\frac {P(B|A)P(A)}{P(B)}}={\\frac {0.5\\times 0.4}{0.8}}=0.25}\n  。可见，后验概率实际上就是条件概率。\n\n\n== 计算 ==\n根据贝叶斯定理，一个随机变量在给定另一随机变量值之后的后验概率分布可以通过先验概率分布与似然函数相乘并除以归一化常数求得\n\n  \n    \n      \n        \n          f\n          \n            X\n            ∣\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              \n                f\n                \n                  X\n                \n              \n              (\n              x\n              )\n              \n                L\n                \n                  X\n                  ∣\n                  Y\n                  =\n                  y\n                \n              \n              (\n              x\n              )\n            \n            \n              \n                ∫\n                \n                  −\n                  ∞\n                \n                \n                  ∞\n                \n              \n              \n                f\n                \n                  X\n                \n              \n              (\n              u\n              )\n              \n                L\n                \n                  X\n                  ∣\n                  Y\n                  =\n                  y\n                \n              \n              (\n              u\n              )\n              \n              d\n              u\n            \n          \n        \n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)={f_{X}(x)L_{X\\mid Y=y}(x) \\over {\\int _{-\\infty }^{\\infty }f_{X}(u)L_{X\\mid Y=y}(u)\\,du}}}\n  上式为给出了随机变量\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  在给定数据\n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n  后的后验概率分布函数，式中\n\n  \n    \n      \n        \n          f\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X}(x)}\n  为\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的先验密度函数，\n\n  \n    \n      \n        \n          L\n          \n            X\n            ∣\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n        =\n        \n          f\n          \n            Y\n            ∣\n            X\n            =\n            x\n          \n        \n        (\n        y\n        )\n      \n    \n    {\\displaystyle L_{X\\mid Y=y}(x)=f_{Y\\mid X=x}(y)}\n  为\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  的似然函数，\n\n  \n    \n      \n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          f\n          \n            X\n          \n        \n        (\n        u\n        )\n        \n          L\n          \n            X\n            ∣\n            Y\n            =\n            y\n          \n        \n        (\n        u\n        )\n        \n        d\n        u\n      \n    \n    {\\displaystyle \\int _{-\\infty }^{\\infty }f_{X}(u)L_{X\\mid Y=y}(u)\\,du}\n  为归一化常数，\n\n  \n    \n      \n        \n          f\n          \n            X\n            ∣\n            Y\n            =\n            y\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{X\\mid Y=y}(x)}\n  为考虑了数据\n  \n    \n      \n        Y\n        =\n        y\n      \n    \n    {\\displaystyle Y=y}\n  后\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  的后验密度函数。\n\n\n== 置信区间 ==\n后验概率是考虑了一系列随机观测数据的条件概率。对于一个随机变量来说，量化其不确定性非常重要。其中一个实现方法便是提供其后验概率的置信区间。\n\n\n== 参见 ==\n经验贝叶斯方法\n边缘分布\nLindley's 悖论\n\n\n== 引用 ==", "ROC曲线": "在信号检测理论中，接收者操作特征曲線，或者叫ROC曲线（英語：Receiver operating characteristic curve），是一种坐標圖式的分析工具，用於选择最佳的信號偵測模型、捨棄次佳的模型或者在同一模型中設定最佳閾值。\n在做決策時，ROC分析能不受成本／效益的影響，給出客觀中立的建議。\nROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来偵测战场上的敌军載具（飛機、船艦），也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。數十年來，ROC分析被用於医学、无线电、生物學、犯罪心理學领域中，而且最近在机器学习（machine learning）和数据挖掘（data mining）领域也得到了很好的发展。\n\n\n== 基本概念 ==\n\n分类模型（又稱分类器，或診斷）是将一个实例映射到一个特定类的过程。ROC分析的是二元分類模型，也就是輸出結果只有兩種類別的模型，例如：（陽性／陰性）（有病／沒病）（垃圾郵件／非垃圾郵件）（敵軍／非敵軍）。\n當訊號偵測（或變數測量）的结果是一個連續值時，類與類的邊界必须用一个阈值（英語：threshold）來界定。举例来说，用血压值来检测一个人是否有高血压，測出的血壓值是連續的實數（從0~200都有可能），以收縮壓140／舒張壓90為閾值，閾值以上便診斷為有高血壓，閾值未滿者診斷為無高血壓。二元分類模型的個案預測有四種結局：\n\n真陽性（TP）：診斷為有，實際上也有高血壓。\n偽阳性（FP）：診斷為有，实际卻没有高血壓。\n真陰性（TN）：診斷為沒有，實際上也沒有高血壓。\n偽阴性（FN）：診斷為沒有，实际却有高血壓。這四種結局可以畫成2 × 2的混淆矩阵：\n\n\n== ROC空間 ==\nROC空间将偽陽性率（FPR）定義為 X 軸，真陽性率（TPR）定义为 Y 轴。\n\nTPR：在所有實際為陽性的樣本中，被正確地判斷為陽性之比率。\n  \n    \n      \n        T\n        P\n        R\n        =\n        T\n        P\n        \n          /\n        \n        (\n        T\n        P\n        +\n        F\n        N\n        )\n      \n    \n    {\\displaystyle TPR=TP/(TP+FN)}\n  FPR：在所有實際為阴性的样本中，被錯誤地判斷為陽性之比率。\n  \n    \n      \n        F\n        P\n        R\n        =\n        F\n        P\n        \n          /\n        \n        (\n        F\n        P\n        +\n        T\n        N\n        )\n      \n    \n    {\\displaystyle FPR=FP/(FP+TN)}\n  給定一個二元分類模型和它的閾值，就能從所有樣本的（陽性／陰性）真實值和預測值計算出一個 (X=FPR, Y=TPR) 座標點。\n在这条线的以上的点代表了一个好的分类结果（勝過隨機分類），而在这条线以下的点代表了差的分类结果（劣於隨機分類）。\n完美的預測是一个在左上角的点，在ROC空间座标 (0,1)点，X=0 代表着没有偽阳性，Y=1 代表著沒有偽阴性（所有的陽性都是真陽性）；也就是說，不管分類器輸出結果是陽性或陰性，都是100%正確。一个随机的预测会得到位於从 (0, 0) 到 (1, 1) 对角线（也叫无识别率线）上的一个点；最直观的随机预测的例子就是抛硬币。\n让我们来看在實際有100个阳性和100个阴性的案例時，四種預測方法（可能是四種分類器，或是同一分類器的四種閾值設定）的結果差異：\n\n將這4種结果畫在ROC空间裡：\n\n點與随机猜测线的距離，是預測力的指標：离左上角越近的點預測（診斷）準確率越高。離右下角越近的點，预测越不準。\n在A、B、C三者當中，最好的結果是A方法。\nB方法的结果位於随机猜测线（對角線）上，在例子中我们可以看到B的準確度（ACC，定義見前面表格）是50%。\nC雖然預測準確度最差，甚至劣於隨機分類，也就是低於0.5（低於對角線）。然而，当将C以 (0.5, 0.5) 為中點作一个镜像后，C'的结果甚至要比A还要好。这个作镜像的方法，简单說，不管C（或任何ROC點低於對角線的情況）预测了什么，就做相反的結論。\n\n\n== ROC曲線 ==\n\n上述ROC空間裡的單點，是給定分類模型且給定閾值後得出的。但同一個二元分類模型的閾值可能設定為高或低，每種閾值的設定會得出不同的FPR和TPR。\n\n將同一模型每個閾值 的 (FPR, TPR) 座標都畫在ROC空間裡，就成為特定模型的ROC曲線。例如右圖，人體的血液蛋白濃度是呈正态分布的連續變數，病人的分布是紅色，平均值為A g/dL，健康人的分布是藍色，平均值是C g/dL。健康檢查會測量血液樣本中的某種蛋白質濃度，達到某個值（閾值，threshold）以上診斷為有疾病徵兆。研究者可以調整閾值的高低（將左上圖的B垂直線往左或右移動），便會得出不同的偽陽性率與真陽性率，總之即得出不同的預測準確率。\n1. 由於每個不同的分類器（診斷工具、偵測工具）有各自的測量標準和測量值的單位（標示為：「健康人－病人分佈圖」的橫軸），所以不同分類器的「健康人－病人分佈圖」都長得不一樣。\n2. 比較不同分類器時，ROC曲線的實際形狀，便視兩個實際分佈的重疊範圍而定，沒有規律可循。\n3. 但在同一個分類器之內，閾值的不同設定對ROC曲線的影響，仍有一些規律可循：\n\n當閾值設定為最高時，亦即所有樣本都被預測為陰性，沒有樣本被預測為陽性，此時在偽陽性率 FPR = FP / ( FP + TN ) 算式中的 FP = 0，所以 FPR = 0%。同時在真陽性率（TPR）算式中， TPR = TP / ( TP  + FN ) 算式中的 TP = 0，所以 TPR = 0%→  當閾值設定為最高時，必得出ROC座標系左下角的點 (0, 0)。當閾值設定為最低時，亦即所有樣本都被預測為陽性，沒有樣本被預測為陰性，此時在偽陽性率FPR = FP / ( FP + TN ) 算式中的 TN = 0，所以 FPR = 100%。同時在真陽性率 TPR = TP / ( TP  + FN ) 算式中的 FN = 0，所以 TPR=100%→  當閾值設定為最低時，必得出ROC座標系右上角的點 (1, 1)。因為TP、FP、TN、FN都是累積次數，TN和FN隨著閾值調低而減少（或持平），TP和FP隨著閾值調低而增加（或持平），所以FPR和TPR皆必隨著閾值調低而增加（或持平）。→ 隨著閾值調低，ROC點 往右上（或右／或上）移動，或不動；但絕不會往左下(或左／或下)移動。\n\n\n== 曲線下面積（AUC） ==\n\n在比較不同的分類模型時，可以將每個模型的ROC曲線都畫出來，比較曲線下面積做為模型優劣的指標。\n\n\n=== 意義 ===\nROC曲線下方的面積（英語：Area under the Curve of ROC (AUC ROC)），其意義是：\n\n因為是在1x1的方格裡求面積，AUC必在0~1之間。\n假設閾值以上是陽性，以下是陰性；\n若隨機抽取一個陽性樣本和一個陰性樣本，分類器正確判斷陽性樣本的值高於陰性樣本之機率 \n  \n    \n      \n        =\n        A\n        U\n        C\n      \n    \n    {\\displaystyle =AUC}\n  。\n簡單說：AUC值越大的分類器，正確率越高。從AUC判斷分類器（預測模型）優劣的標準：\n\nAUC = 1，是完美分類器，採用這個預測模型時，存在至少一個閾值能得出完美預測。絕大多數預測的場合，不存在完美分類器。\n0.5 < AUC < 1，優於隨機猜測。這個分類器（模型）妥善設定閾值的話，能有預測價值。\nAUC = 0.5，跟隨機猜測一樣（例：丟銅板），模型沒有預測價值。\nAUC < 0.5，比隨機猜測還差；但只要總是反預測而行，就優於隨機猜測。\n\n\n=== 計算 ===\nAUC的計算有兩種方式，都是以逼近法求近似值。\n\n\n==== 梯形法 ====\n梯形法（英語：trapezoid method）：簡單地將每個相鄰的點以直線連接，計算連線下方的總面積。因為每一線段下方都是一個梯形，所以叫梯形法。\n\n優點：簡單，所以常用。\n缺點：傾向於低估AUC。\n\n\n==== ROC AUCH法 ====\n\n\n=== 潛在問題 ===\nAUC of ROC是機器學習的社群最常使用來比較不同模型優劣的方法 。然而近來這個做法開始受到質疑，因為有些機器學習的研究指出，AUC的杂訊太多，並且很常求不出可信又有效的AUC值（此時便不能保證AUC傳達本節開頭所述之意義），使得AUC在模型比較時產生的問題比解釋的問題更多 。\n\n\n== 分析軟體 ==\n所有常用於統計分析的軟體（例：SPSS、SAS、SYSTAT、S-Plus、ROCKIT、RscorePlus）都有依據不同閾值自動計算真陽性和偽陽性比率、並依此繪製ROC曲線的功能。\n离散分类器（英語：discrete，或稱「間斷分類器」），如决策树，产生的是离散的数值或者一个二元标签。应用到实例中，这样的分类器最后只会在ROC空间产生单一的点。而一些其他的分类器，如朴素贝叶斯分类器，逻辑回归或者人工神经网络，产生的是实例属于某一类的可能性，对于这些方法，一个阈值就决定了ROC空间中点的位置。举例来说，如果可能值低于或者等于0.8这个阈值就将其认为是阳性的类，而其他的值被认为是阴性类。这样就可以通过画每一个阈值的ROC点来生成一个生成一条曲线。MedCalc是较好的ROC曲线分析软件。\n\n\n== 参考文献 ==\n\n\n=== 引用 ===\n\n\n=== 来源 ===\n\n\n== 外部链接 ==\nAn introduction to ROC analysis\nA more thorough treatment of ROC curves and signal detection theory\nTom Fawcett's ROC Convex Hull: tutorial, program and papers（页面存档备份，存于互联网档案馆）\nPeter Flach's tutorial on ROC analysis in machine learning\nThe magnificent ROC（页面存档备份，存于互联网档案馆） — An explanation and interactive demonstration of the connection of ROCs to archetypal bi-normal test result plots", "多元分类": "在机器学习中，多元分类是将实例分配到多个（多于两个）类别中的其中一个（将实例分配到两个类别中的其中一个被称为二分类）。\n显然，分类算法可以分为二分类和多分类两种，而多分类算法可以通过将其转化为多个二分类来实现。\n需要注意的是，多分类不应和多标签分类相混淆：多标签分类可以为每个实例预测多个标签，即同一个实例可以同时被分配到多个类别。 \n\n\n== 一般策略 ==\n这部分讨论将多分类问题转化为多个二分类问题的策略。\n\n\n=== One-vs.-rest ===\none-vs.-rest （或one-vs.-all，OvA或OvR）策略需要为每一个类别分别建立一个唯一的二分类基分类器，属于此类的所有样本均为正例，其余的全部为负例。这一策略需要基分类器去产生一个实值置信度以供决策，而不仅仅是预测出一个类标签：只是预测出类标签可能会导致归类的不明确（可能有多个基分类器都预测为正例），以致于一个样本会被预测属于多个类别。通过OvR方法使用二分类算法L建立多分类学习器，其伪代码表示如下：\n\n输入：\n二分类训练算法L\n样本集合X\n标签集合y 使yi ∈ {1, … K} 是样本Xi的类标签\n输出：\n一个二分类分类器序列fk，k ∈ {1, …, K}\n执行过程：\n对于{1, …, K}中的每个元素k：\n构建一个新标签向量z，其中yi = k时zi = 1，否则 zi = 0（或-1）\n将L 应用于X、z 以获得fk当进行多分类时，需要将所有的二分类分类器应用于一个未知样本x，x的最终分类类别即为产生最大置信度的分类器所对应的标签k：\n\n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        arg\n        ⁡\n        \n          max\n          \n            k\n            ∈\n            1\n            …\n            K\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\hat {y}}=\\arg \\max _{k\\in 1\\ldots K}f_{k}(x)}\n  尽管这一策略很流行，但它是一个受到些许问题困扰的启发式算法。首先，不同分类器之间置信度分布可能不同，这些分类器各自输出的置信度之间不一定具有可比性。其次，即使一个多分类训练集的类别是均衡分布的，其所对应的二分类所看到的类别分布也是不均衡的，因为它们所看到的负例个数通常远多于正例个数（即类别不平衡问题）。\n\n\n=== One-vs.-one ===\n在one-vs.-one （OvO） 的转化中，对于一个K类多分类问题，训练 K (K − 1) / 2 个二分类分类器；每一个二分类分类器从初始多分类训练集中收集其中两个类别的所有样本，并学习去区分这两个类别。在预测时，会有一个投票：所有 K (K − 1) / 2 个二分类分类器被应用于一个未知样本，并且那个得到最多“+1”预测的类别会成为最终的多分类预测结果。像OvR一样, OvO也受些许问题困扰：在它输入空间的一些区域会收到相同数目的投票。\n\n\n== 另见 ==\n二分类\n一分类\n多标签分类\n\n\n== 注释 ==\n\n\n== 参考资料 ==", "模糊聚类": "硬聚类（hard clustering）是指把数据点划分到确切的某一聚类中，如K-均值聚类。而模糊聚类（Fuzzy clustering，亦称软聚类，Soft clustering）中，数据点则可能归属于不止一个聚类中。这些聚类与数据点通过一个成员水平（实际上类似于模糊集合中隶属度的概念）联系起来。成员水平显示了数据点与某一聚类之间的联系有多强。模糊聚类就是计算这些成员水平，按照成员水平来决定数据点属于哪一个或哪些聚类的过程。\n模糊C-均值算法（FCM）是应用最为广泛的模糊聚类算法之一。详见模糊C-均值算法。\n\n\n== 与硬聚类的对比 ==\n非模糊聚类（硬聚类）会将数据分到不同类别中，即每个数据仅属于一个确定的类别。模糊聚类会将数据点分到多个可能的类别中。例如，一个苹果可以是红的或绿的（硬聚类）；一个苹果可以是红的和绿的（模糊聚类）。这个苹果可能是某种程度的红同时另一种程度的绿。与苹果是绿的而非红的（green=1，red=0）相比，苹果可以既绿又红（green=0.5，red=0.5）。这些值被归一化到0-1之间，但它们并非概率，因此并不需要相加为1。", "专家系统": "专家系统是早期人工智能的一个重要分支，它可以看作是一类具有专门知识和经验的计算机智能程序系统，一般采用人工智能中的知识表示和知识推理技术来模拟通常由领域专家才能解决的复杂问题。\n一般来说，专家系统=知识库+推理机，因此专家系统也被称为基于知识的系统。一个专家系统必须具备三要素：\n\n领域专家级知识\n模拟专家思维\n达到专家级的水準\n\n\n== 概述 ==\n专家系统适合于完成那些没有公认的理论和方法、数据不精确或訊息不完整、人类专家短缺或专门知识十分昂贵的诊断、解释、监控、预测、规划和设计等任务。一般专家系统执行的求解任务是知识密集型的。\n专家系统能为它的用户带来明显的经济效益。用比较经济的方法执行任务而不需要有经验的专家，可以极大地减少劳务开支和培养费用。由于软件易于复制，所以专家系统能够广泛传播专家知识和经验，推广应用数量有限的和昂贵的专业人员及其知识。\n专家系统在给它的用户带来经济利益的同时，也造成失业。\n专家系统的应用技术不仅代替了人的一些体力劳动，也代替了人的某些脑力劳动，有时甚至行使着本应由人担任的职能，免不了引起法律纠纷。比如医疗诊断专家系统万一出现失误，导致医疗事故，怎么样来处理，开发专家系统者是否要负责任，使用专家系统者应负什么责任，等等。\n\n\n== 有效性 ==\n專家系統的有效性包括和一個理想系統或專家的性能進行比較，根據結果對知識庫和推理過程進行改進。\n\n性能評價的定性方法\n圖靈測試法：測試一台機器是否具有智慧的方法\n敏感度分析法：對係數的變化做出反應\n性能評價的定量方法\nPaired T測試\nT2測試\n具有多個專家的性能評價的定量方法：討論多個專家的一致性問題，一般用關聯係數來評測專家系統能為它的用戶帶來明顯的經濟效益。用比較經濟的方法執行任務而不需要有經驗的專家，可以極大地減少勞務開支和培養費用。由於軟體易於複製，所以專家系統能夠廣泛傳播專家知識和經驗，推廣應用數量有限的和昂貴的專業人員及其知識。\n\n\n== 开发工具 ==\nCLIPS\nProlog\nJess\nMQL 4\n\n\n== 著名的专家系统 ==\nExSys：第一個商用專家系統。\nMycin：一个诊断系统，其表现出人意料的好，误诊率低達专家级水準，超過一些诊所的医生。\nSiri：一個透過辨識語音作業的專家系統，由蘋果公司收購並且推廣到自家產品內作為一個人秘書功能。", "四分位数": "四分位数（英語：Quartile）是统计学中分位数的一种，即把所有数值由小到大排列并分成四等份，处于三个分割点位置的數值就是四分位数。\n\n\n== 概念 ==\n第一四分位数（\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle Q_{1}}\n  ），又称较小四分位数，等于该样本中所有数值由小到大排列后第25%的数字。\n第二四分位数（\n  \n    \n      \n        \n          Q\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle Q_{2}}\n  ），又称中位数，等于该样本中所有数值由小到大排列后第50%的数字。\n第三四分位数（\n  \n    \n      \n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{3}}\n  ），又称较大四分位数，等于该样本中所有数值由小到大排列后第75%的数字。第三四分位数与第一四分位数的差距又称四分位距（InterQuartile Range, IQR）。\n\n\n== 运算过程 ==\n关于四分位数值的选择尚存争议。\n主要选择四分位的百分比值\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  ，及样本总量\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  有以下数学公式可以表示：\n\n  \n    \n      \n        \n          L\n          \n            p\n          \n        \n        =\n        n\n        ⋅\n        \n          \n            p\n            100\n          \n        \n      \n    \n    {\\displaystyle L_{p}=n\\cdot {\\frac {p}{100}}}\n  情况1：如果\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  是一个整数，则取第\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  和第\n  \n    \n      \n        L\n        +\n        1\n      \n    \n    {\\displaystyle L+1}\n  的平均值\n情况2：如果\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  不是一个整数，则取下一个最近的整数。（比如\n  \n    \n      \n        L\n        =\n        1.2\n      \n    \n    {\\displaystyle L=1.2}\n  ， 则取\n  \n    \n      \n        2\n      \n    \n    {\\displaystyle 2}\n  ）\n\n\n== 舉例 ==\n\n一个算法如下（可以兼用TI-83计算器）：\n\n利用中位数使数据分成两列（不要把中位数放入已分好的数列）。\n第一四分位数为第一组数列的中位数；第三四分位数为第二组数列的中位数。以下例子可以用来参考。\n\n例1数据总量：\n  \n    \n      \n        6\n        ,\n        47\n        ,\n        49\n        ,\n        15\n        ,\n        42\n        ,\n        41\n        ,\n        7\n        ,\n        39\n        ,\n        43\n        ,\n        40\n        ,\n        36\n      \n    \n    {\\displaystyle 6,47,49,15,42,41,7,39,43,40,36}\n  \n由小到大排列的结果：\n  \n    \n      \n        6\n        ,\n        7\n        ,\n        15\n        ,\n        36\n        ,\n        39\n        ,\n        40\n        ,\n        41\n        ,\n        42\n        ,\n        43\n        ,\n        47\n        ,\n        49\n      \n    \n    {\\displaystyle 6,7,15,36,39,40,41,42,43,47,49}\n  \n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    Q\n                    \n                      1\n                    \n                  \n                  =\n                  15\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      2\n                    \n                  \n                  =\n                  40\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      3\n                    \n                  \n                  =\n                  43\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}Q_{1}=15\\\\Q_{2}=40\\\\Q_{3}=43\\end{cases}}}\n  例2数据总量：\n  \n    \n      \n        7\n        ,\n        15\n        ,\n        36\n        ,\n        39\n        ,\n        40\n        ,\n        41\n      \n    \n    {\\displaystyle 7,15,36,39,40,41}\n  \n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    Q\n                    \n                      1\n                    \n                  \n                  =\n                  15\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      2\n                    \n                  \n                  =\n                  37.5\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      3\n                    \n                  \n                  =\n                  40\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}Q_{1}=15\\\\Q_{2}=37.5\\\\Q_{3}=40\\end{cases}}}\n  例3数据总量：\n  \n    \n      \n        1\n        ,\n        2\n        ,\n        3\n        ,\n        4\n      \n    \n    {\\displaystyle 1,2,3,4}\n  \n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    Q\n                    \n                      1\n                    \n                  \n                  =\n                  1.5\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      2\n                    \n                  \n                  =\n                  2.5\n                \n              \n              \n                \n                  \n                    Q\n                    \n                      3\n                    \n                  \n                  =\n                  3.5\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}Q_{1}=1.5\\\\Q_{2}=2.5\\\\Q_{3}=3.5\\end{cases}}}\n  \n\n\n== 應用 ==\n不論\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        ,\n        \n          Q\n          \n            2\n          \n        \n        ,\n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{1},Q_{2},Q_{3}}\n   的變異量數數值為何，均視為一個分界點，以此將總數分成四個相等部份，可以通过比较\n  \n    \n      \n        \n          Q\n          \n            1\n          \n        \n        ,\n        \n          Q\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle Q_{1},Q_{3}}\n  ，分析其数据变量的趋势。\n\n\n== 參考文獻 ==", "查詢語言": "查询语言或数据查询语言（Data Query Language, DQL）是用于从数据库或信息系统中查询数据的计算机语言。例如SQL语言是查询语言里比较知名的一种。\n\n\n== 查詢語言範例 ==\n\n\n== 数据查询语言 ==\n數據查詢語言泛指向資料庫或信息系统查詢的各種编程语言。数据查询语言必须要能表达所有关系代数所能表达的查询，这样才被称为关系完整的（英語：Relational complete）。DQL的主要功能是查询数据，本身核心指令为SELECT，为了进行精细的查询，加入了各类辅助指令。SELECT是查询的指令，例如：\n\n\n== 引用 ==\n\n\n== 參考資料 ==\n完整的SQL中文參考網站\nMySQL SQL Syntax （页面存档备份，存于互联网档案馆）\nOracle® Database SQL Language Reference\nTransact-SQL Reference （页面存档备份，存于互联网档案馆）\nPostgreSQL SQL Commands （页面存档备份，存于互联网档案馆）\n一种新的查询语言：OttoQL Reference", "极值": "在数学中，极值（extremum）是极大值（maximum）与极小值（minimum）的统称，意指在一个域上函数取得最大值或最小值的点的函数值。而使函数取得极值的点（的横坐标）被称作极值点。这个域既可以是一个邻域，又可以是整个函数域（这时极值称为最值、全局极值、绝对极值）。\n\n\n== 定义 ==\n局部（相对）最大值：如果存在一个ε > 0，使得所有满足|x-x*| < ε的x都有f(x*)≥ f(x)，我们就把点x*对应的函数值f(x*)称为一个函数f的局部最大值。从函数图像上看，局部最大值就像是山顶。\n局部（相对）最小值：如果存在一个ε > 0，使得所有满足|x-x*| < ε的x都有f(x*)≤ f(x)，我们就把点x*对应的函数值f(x*)称为一个函数f的局部最小值。从函数图像上看，局部最小值就像是山谷的底部。\n全局（绝对）最大值：如果点x*对于任何x都满足f(x*)≥ f(x)，则点f(x*)称为全局最大值。\n全局（绝对）最小值：如果点x*对于任何x都满足f(x*)≤ f(x)，则点f(x*)称为全局最小值。极值的概念不仅仅限于定义在实数域上的函数。定义在任何集合上的实数值函数都可以讨论其最大最小值。为了定义局部极值，函数值必须为实数，同时此函数的定义域上必须能够定义邻域。邻域的概念使得在x的定义域上可以有|x - x*| < ε。\n局部最大值（最小值）也被称为极值（或局部最优值），全局最大值（最小值）也被称为最值（或全局最优值）。\n\n\n== 求极值的方法 ==\n求全局极值是最优化方法的目的。对于一元二阶可导函数，求极值的一种方法是求驻点（亦称为静止点，停留点，英語：stationary point），也就是求一阶导数为零的点。如果在驻点的二阶导数为正，那么这个点就是局部最小值；如果二阶导数为负，则是局部最大值；如果为零，则还需要进一步的研究。\n一般地，如果在驻点处的一阶、二阶、三阶……直到N阶导数都是零，而N+1阶导数不为零，则当N奇数且N+1阶导数为正时，该点为极小值；当N是奇数且N+1阶导数为负时，该点为极大值；如果N是偶数，则该点不是极值。\n如果这个函数定义在一个有界区域内，则还要检查局域的边界点。如果函数在定义域内存在不可导点，则这些不可导点也可能是极值点。\n\n\n== 例子 ==\n函数\n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x^{2}}\n  有惟一最小值，在x = 0　处取得。\n函数\n  \n    \n      \n        \n          x\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle x^{3}}\n  没有最值，也没有极值，尽管其一阶导数\n  \n    \n      \n        3\n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 3x^{2}}\n  在x = 0处也为0。因为其二阶导数(6x)在该点也是0，但三阶导数不是零。\n函数cos(x)有无穷多个最大值，在x =0, ±2π, ±4π, ...，与无穷多个最小值　在x =±π, ±3π  ... .求函数的极值时还应当考虑其不可导点，即导数不存在的点。如函数y=|x|中0处的导数不存在，事实上从图像上也能看出这一点来。而且0就是该函数的一个极小值。\n\n\n== 多变量函数 ==\n对于多变量函数（多元函数），同样存在在极值点的概念。其定义为：\n\n设\n  \n    \n      \n        f\n        (\n        P\n        )\n      \n    \n    {\\displaystyle f(P)}\n  在点\n  \n    \n      \n        \n          P\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle P_{0}}\n  某邻域\n  \n    \n      \n        U\n        (\n        \n          P\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle U(P_{0})}\n  内有定义，若对于所有\n  \n    \n      \n        \n          P\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle P_{0}}\n  的去心邻域的点\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  ，都有\n  \n    \n      \n        f\n        (\n        P\n        )\n        <\n        f\n        (\n        \n          P\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f(P)<f(P_{0})}\n  ，则称\n  \n    \n      \n        \n          P\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle P_{0}}\n  是\n  \n    \n      \n        f\n        (\n        P\n        )\n      \n    \n    {\\displaystyle f(P)}\n  的极大值；反之，则为极小值。此外，也有鞍点的概念。\n\n\n== 参见 ==\n机械平衡\n极值定理\n\n\n== 注脚 ==", "谱聚类": "在多元变量统计中，谱聚类（英語：spectral clustering）技术利用数据相似矩阵的谱（特征值），在对数据进行降维后，以较少的维度进行聚类。相似矩阵作为输入，提供了对数据集中每一对点相对相似性的定量评估。\n在图像分割中，谱聚类被称为基于分割的物体分类。\n\n\n== 算法 ==\n基本算法计算拉普拉斯矩阵 \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   （或归一化的拉普拉斯矩阵）\n计算前 \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   个特征向量（这些特征向量对应 \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   的 \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   个最小的特征值）\n考虑由这 \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   个特征向量组成的矩阵，矩阵的第 \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   行定义了图节点 \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   的特征\n根据这些特征对图节点进行聚类（例如使用k-均值聚类）大型图的（归一化）拉普拉斯矩阵通常是病态的（ill-conditioned，即高条件数），这会减缓迭代求解的收敛速度。预处理（Preconditioning）可以加速收敛。通过首先确定结构，然后对群落进行聚类，谱聚类可以成功应用于大型图。谱聚类与非线性降维密切相关，局部线性嵌入（Locally-linear embedding）等降维技术可用于减少噪声或异常值的误差。\n\n\n== 软件 ==\n有不少大型开源项目实现谱聚类，包括Scikit-learn（使用带有多网格预处理（multigrid method）或ARPACK的LOBPCG算法），MLlib（使用幂迭代法，Power iteration method）进行伪特征向量聚类，以及R。\n\n\n== 和其它聚类算法的关系 ==\n谱聚类算法它可以在核聚类方法的背景下进行描述，这顯示了它与其他方法的相似之处。\n\n\n=== 和k-means聚类算法的关系 ===\n加权核K-means问题与谱聚类问题的目标函数相同，可以通过多级方法直接优化。\n\n\n== 参考资料 ==", "虛無假說": "在推論統計學中，零假设（英語：Null hypothesis，又译虚无假设、原假设，符号：\n  \n    \n      \n        \n          H\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle H_{0}}\n  ）是做统计检验时的一类假說。\n零假设的内容一般是希望能证明为错误的假设，与零假设相对的是對立假說，即希望证明是正确的另一种可能。从数学上来看，零假设和备择假设的地位是相等的，但是在统计学的实际运用中，常常需要强调一类假设为应当或期望实现的假设，例如在相关性检验中，一般会取“两者之间无关联”作为零假设，而在独立性检验中，一般会取“两者之间非獨立”作为零假设。\n如果一个统计检验的结果拒绝(reject) 零假设（结论不支持零假设），而实际上真实的情况属于零假设，那么称这个检验犯了型一錯誤。反之，如果检验结果支持零假设，而实际上真实的情况属于备择假设，那么称这个检验犯了型二錯誤。通常的做法是，在保持第一类错误出现的机会在某个特定水平上的时候（即显著性差异值或α值），尽量减少第二类错误出现的概率。\n\n\n== 相关条目 ==\n假說檢定\n對立假說\n檢定力\n型一錯誤與型二錯誤\n顯著性差異\n似然比检验\n司徒頓t檢定\n\n\n== 参考来源 ==\n统计学原理：假设检验\n\n\n== 延伸閱讀 ==\n\n\n== 外部連結 ==\nHyperStat Online: Null hypothesis （页面存档备份，存于互联网档案馆）", "降维": "在机器学习和统计学领域，降维是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程。 降维可进一步细分为变量选择和特征提取两大方法。\n\n\n== 变量选择 ==\n\n变量选择假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中找出主要变量。现代统计学中对变量选择的研究文献，大多集中于高维回归分析，其中最具代表性的方法包括：\n\nLasso算法 (Robert Tibshirani提出)\nElastic net regularization (邹晖和Trevor Hastie提出)\nSCAD (范剑青和李润泽提出)\nSURE screening (范剑青和吕金翅提出)\nPLUS (张存惠提出)\n\n\n== 特征提取 ==\n\n特徵提取可以看作变量选择方法的一般化：变量选择假设在原始数据中，变量数目浩繁，但只有少数几个真正起作用；而特征提取则认为在所有变量可能的函数(比如这些变量各种可能的线性组合)中，只有少数几个真正起作用。有代表性的方法包括：\n\n主成分分析(PCA)\n因子分析\n核方法(教科书中称为“Kernel method”或“Kernel trick”，常与其他方法如PCA组合使用)\n基于距离的方法，例如：\n多维尺度分析\n非负矩阵分解\n随机投影法(理论依据是约翰逊-林登斯特劳斯定理)\n\n\n== 参见 ==\n变量选择\n特徵提取\n约翰逊-林登斯特劳斯定理\n\n\n== 参考文献 ==", "測量尺度": "測量尺度（scale of measure）或稱度量水平（level of measurement）、测量标尺，若為定性度量，可稱度量類別，是統計學和定量研究中，對不同種類的數據，依據其尺度水平所劃分的類別；這些尺度水平分別為：名目（nominal）、次序（ordinal）、等距（interval）、等比（ratio）。\n名目尺度和次序尺度是定性的，而等距尺度和等比尺度是定量的。定量數據，又根據數據是否可數，分為离散的和连续的。\n\n\n== 綜覽 ==\n\n\n== 名目尺度 ==\n例如，对一个气球的颜色进行测量，其可能的结果为红，黄，绿等不同的颜色类。同理，一个人的性别也是一个名目尺度，因为该变量只能在「男」或者「女」中选值。\n名目尺度只能用来比较相等或者不相等，而不能比较大小，更不能用来进行四则算术运算。以性别为例，两个人的性别只能用相同与否来区分，而讨论「谁的性别大」，或者「两个人性别的和是多少」等问题是没有意义的。\n在统计学中，一个名目尺度的分布情况可以用众数和離散程度来描述。\n\n\n== 次序尺度 ==\n次序尺度也用来描述一个对象的类别，但与名目尺度不同的是，次序尺度的类别有一定的顺序或大小。次序尺度的變量之间除比较是否相等外，还可以比较大小。但是，加减乘除的运算仍然不能用在次序尺度中。例如，一场比赛中选手的名次（第一，第二，第三等等）就是一个次序變量。我们可以比较两个选手的名次谁較前面，但我们不能比较第一名和第二名的差距比第二名和第三名的差距哪个更大。\n次序尺度的分布可以用众数和中位数来描述。\n\n\n== 等距尺度 ==\n等距尺度具有次序尺度所有的特性。除了能比较大小外，等距尺度测量值之间的差别也可以比较大小。等距尺度测量值可以相加和相减，其结果仍然有意义。另一方面，由于等距尺度的原點是任意选取的，所以乘法和除法运算的结果不唯一，因而是没有意义的。年份、摄氏温度、华氏温度就是等距尺度。\n等距尺度可以用众数，中位数或者算术平均值来描述。\n\n\n== 等比尺度 ==\n也稱比率尺度。等比變量具有等距變量的所有特点，同时它也允许乘除运算。大多数物理量，如质量，长度、绝对温度或者能量等等都是等比尺度。等比尺度可以用众数，中位数，算术平均数和几何平均数来描述。\n只有等距尺度和等比尺度有计量单位（units of measurement）。\n\n\n== 参考文献 ==\n\n\n== 参见 ==\n多维标度", "遗传算法": "遗传算法（英語：Genetic Algorithm，GA）是计算数学中用于解决最佳化的搜索算法，是进化算法的一种。进化算法最初是借鉴了进化生物学中的一些现象而发展起来的，这些现象包括遗传、突变、自然选择以及杂交等等。\n遗传算法通常实现方式为一种计算机模拟。对于一个最优化问题，一定数量的候选解（称为个体）可抽象表示为染色體，使种群向更好的解进化。传统上，解用二进制表示（即0和1的串），但也可以用其他表示方法。进化从完全随机个体的种群开始，之后一代一代发生。在每一代中评价整个种群的适应度，从当前种群中随机地选择多个个体（基于它们的适应度），通过自然选择和突变产生新的生命种群，该种群在算法的下一次迭代中成为当前种群。\n\n\n== 遗传算法的机理 ==\n在遗传算法裡，优化问题的解被称为个体，它表示为一个变量序列，叫做染色体或者基因串。染色体一般被表达为简单的字符串或数字串，不过也有其他的依赖于特殊问题的表示方法适用，这一过程称为编码。首先，算法随机生成一定数量的个体，有时候操作者也可以干预这个随机产生过程，以提高初始种群的质量。在每一代中，都會评价每一个体，并通过计算适应度函数得到适应度数值。按照适应度排序种群个体，适应度高的在前面。这里的“高”是相对于初始的种群的低适应度而言。\n下一步是产生下一代个体并组成种群。这个过程是通过选择和繁殖完成，其中繁殖包括交配（crossover，在算法研究领域中我们称之为交叉操作）和突变（mutation）。选择则是根据新个体的适应度进行，但同时不意味着完全以适应度高低为导向，因为单纯选择适应度高的个体将可能导致算法快速收敛到局部最优解而非全局最优解，我们称之为早熟。作为折中，遗传算法依据原则：适应度越高，被选择的机会越高，而适应度低的，被选择的机会就低。初始的数据可以通过这样的选择过程组成一个相对优化的群体。之后，被选择的个体进入交配过程。一般的遗传算法都有一个交配概率（又称为交叉概率），范围一般是0.6~1，这个交配概率反映两个被选中的个体进行交配的概率。例如，交配概率为0.8，则80%的“夫妻”会生育后代。每两个个体通过交配产生两个新个体，代替原来的“老”个体，而不交配的个体则保持不变。交配父母的染色体相互交換，从而产生两个新的染色体，第一个个体前半段是父亲的染色体，后半段是母亲的，第二个个体则正好相反。不过这里的半段並不是真正的一半，这个位置叫做交配点，也是随机产生的，可以是染色体的任意位置。再下一步是突變，通过突變产生新的“子”个体。一般遗传算法都有一个固定的突变常数（又称为变异概率），通常是0.1或者更小，这代表变异发生的概率。根据这个概率，新个体的染色体随机的突變，通常就是改变染色体的一个字节（0变到1，或者1变到0）。\n经过这一系列的过程（选择、交配和突变），产生的新一代个体不同于初始的一代，并一代一代向增加整体适应度的方向发展，因为总是更常选择最好的个体产生下一代，而适应度低的个体逐渐被淘汰掉。这样的过程不断的重复：评价每个个体，计算适应度，两兩交配，然后突變，产生第三代。周而复始，直到终止条件满足为止。一般终止条件有以下几种：\n\n进化次数限制；\n计算耗费的资源限制（例如计算时间、计算占用的内存等）；\n一个个体已经满足最优值的条件，即最优值已经找到；\n适应度已经达到饱和，继续进化不会產生适应度更好的个体；\n人为干预；\n以及以上两种或更多种的组合。\n\n\n=== 算法 ===\n选择初始生命种群\n循环\n评价种群中的个体适应度\n以比例原則（分數高的挑中機率也較高）选择产生下一个种群（輪盤法（roulette wheel selection）、競爭法（tournament selection）及等級輪盤法（Rank Based Wheel Selection））。不僅僅挑分數最高的的原因是這麼做可能收斂到局部的最佳點，而非整體的。\n改变该种群（交叉和变异）\n直到停止循环的条件满足.\n\n\n=== GA参数 ===\n种群规模（P,population size）：即种群中染色体个体的数目。\n字串长度（l, string length）：个体中染色体的长度。\n交配概率（pc, probability of performing crossover）：控制着交配算子的使用频率。交配操作可以加快收敛，使解达到最有希望的最佳解区域，因此一般取较大的交配概率，但交配概率太高也可能导致过早收敛，則稱為早熟。\n突變概率（pm, probability of mutation）：控制着突變算子的使用频率。\n中止条件（termination criteria）\n\n\n=== 特点 ===\n遗传算法在解决优化问题过程中有如下特点：\n\n遗传算法在适应度函数选择不当的情况下有可能收敛于局部最优，而不能达到全局最优。\n初始种群的数量很重要，如果初始种群数量过多，算法会占用大量系统资源；如果初始种群数量过少，算法很可能忽略掉最优解。\n对于每个解，一般根据实际情况进行编码，这样有利于编写变异函数和适应度函数(Fitness Function)。\n在编码过的遗传算法中，每次变异的编码长度也影响到遗传算法的效率。如果变异代码长度过短，变异的多样性会受到限制；如果变异代码过长，变异的效率会非常低下，选择适当的变异长度是提高效率的关键。\n变异率也是一个重要的参数。\n对于动态数据，用遗传算法求最优解比较困难，因为染色体种群很可能过早地收敛，而对以后变化了的数据不再产生变化。对于这个问题，研究者提出了一些方法增加基因的多样性，从而防止过早的收敛。其中一种是所谓触發式超级变异，就是当染色体群体的质量下降（彼此的区别减少）时增加变异概率；另一种叫随机外来染色体，是偶尔加入一些全新的随机生成的染色体个体，从而增加染色体多样性。\n选择过程很重要，但交叉和变异的重要性存在争议。一种观点认为交叉比变异更重要，因为变异仅仅是保证不丢失某些可能的解；而另一种观点则认为交叉过程的作用只不过是在种群中推广变异过程所造成的更新，对于初期的种群来说，交叉几乎等效于一个非常大的变异率，而这么大的变异很可能影响进化过程。\n遗传算法很快就能找到良好的解，即使是在很复杂的解空间中。\n遗传算法并不一定总是最好的优化策略，优化问题要具体情况具体分析。所以在使用遗传算法的同时，也可以尝试其他算法，互相补充，甚至根本不用遗传算法。\n遗传算法不能解决那些“大海捞针”的问题，所谓“大海捞针”问题就是没有一个确切的适应度函数表征个体好坏的问题，使得算法的进化失去导向。\n对于任何一个具体的优化问题，调节遗传算法的参数可能会有利于更好更快收敛，这些参数包括个体数目、交叉率和变异率。例如太大的变异率会导致丢失最优解，而过小的变异率会导致算法过早的收敛于局部最优点。对于这些参数的选择，现在还没有实用的上下限。\n适应度函数对于算法的速度和效果也很重要。\n\n\n== 变量 ==\n最简单的遗传算法将染色体表示为一个数位串，数值变量也可以表示成整数，或者实数（浮点数）。算法中的杂交和突变都是在字节串上进行的，所以所谓的整数或者实数表示也一定要转化为数位形式。例如一个变量的形式是实数，其范围是0～1，而要求的精度是0.001，那么可以用10个数位表示：0000000000表示0，1111111111表示1。那么0110001110就代表0.398。\n在遗传算法里，精英选择是一种非常成功的产生新个体的策略，它是把最好的若干个个体作为精英直接带入下一代个体中，而不经过任何改变。\n通过并行计算实现遗传算法一般有两种，一种是所谓粗糙并行遗传算法，即一个计算单元包含一个种群；而另一种是所谓精细并行遗传算法，每一个计算单元处理一个染色体个体。\n遗传算法有时候还引入其他变量，例如在实时优化问题中，可以在适应度函数中引入时间相关性和干扰。\n\n\n== 适用的问题 ==\n遗传算法擅长解决的问题是全局最优化问题，例如，解决时间表安排问题就是它的一个特长，很多安排时间表的软件都使用遗传算法，遗传算法还经常被用于解决实际工程问题。\n跟传统的爬山算法相比，遗传算法能够跳出局部最优而找到全局最优点。而且遗传算法允许使用非常复杂的适应度函数（或者叫做目标函数），并对变量的变化范围可以加以限制。而如果是传统的爬山算法，对变量范围进行限制意味着复杂的多的解决过程，这方面的介绍可以参看受限优化问题和非受限优化问题。\n\n\n== 发展历史 ==\n遗传算法由密歇根大学的约翰·霍兰德和他的同事于二十世纪六十年代在对细胞自动机（英文：cellular automata）进行研究时率先提出。在二十世纪八十年代中期之前，对于遗传算法的研究还仅仅限于理论方面，直到在匹兹堡召开了第一届世界遗传算法大会。随着计算机计算能力的发展和实际应用需求的增多，遗传算法逐渐进入实际应用阶段。1989年，纽约时报作者约翰·马科夫写了一篇文章描述第一个商业用途的遗传算法--进化者（英文：Evolver）。之后，越来越多种类的遗传算法出现并被用于许多领域中，财富杂志500强企业中大多数都用它进行时间表安排、数据分析、未来趋势预测、预算、以及解决很多其他组合优化问题。\n\n\n== 应用领域 ==\n\n计算机自动設計 （CAD, Computer-Automated Design）\n工业工程与运作管理\n物流系统设计\n生产调度\n制造系统控制\n系统优化设计\n汽车设计，包括材料选择、多目标汽车组件设计、减轻重量等。\n机电系统设计。\n分布计算机网络的拓扑结构。\n电路设计，此类用途的遗传算法叫做进化电路。\n电子游戏设计，例如计算平衡解决方案。\n机器智能设计和机器人学习。\n模糊控制系统的训练。\n移动通讯优化结构。\n时间表安排，例如为一个大学安排不冲突的课程时间表。\n旅行推销员问题。\n神经网络的训练，也叫做神经进化。\n\n\n== 相关技术 ==\n遗传程序是John Koza与遗传算法相关的一个技术，在遗传程序中，并不是参数优化，而是计算机程序优化。遗传程序一般采用树型结构表示计算机程序用于进化，而不是遗传算法中的列表或者数组。一般来说，遗传程序比遗传算法慢，但同时也可以解决一些遗传算法解决不了的问题。\n交互式遗传算法是利用人工评价进行操作的遗传算法，一般用于适应度函数无法得到的情况，例如，对于图像、音乐、艺术的设计和“优化”，或者对运动员的训练等。\n模拟退火是解决全局优化问题的另一个可能选择。它是通过一个解在搜索空间的随机变动寻找最优点的方法：如果某一阶段的随机变动增加适应度，则总是被接受，而降低适应度的随机变动根据一定的概率被有选择的接受。这个概率由当时的退火温度和适应度恶化的程度决定，而退火温度按一定速度降低。从模拟退火算法看，最优化问题的解是通过寻找最小能量点找到的，而不是寻找最佳适应点找到的。模拟退火也可以用于标准遗传算法里，只要把突变率随时间逐渐降低就可以了。\n\n\n== 参见 ==\n演化策略\n遗传编程\n演化规划\n模拟退火\n禁忌搜索\n旅行推销员问题\n最优化\n交叉 (遗传算法)\n突變 (遺傳演算法)\n\n\n== 参考文献 ==\nGoldberg, David E (1989), 遗传算法：搜索、优化和机器学习，Kluwer Academic Publishers, Boston, MA.\nGoldberg, David E (2002), 创新的设计：竞争遗传算法课程，Addison-Wesley, Reading, MA.\nHarvey, Inman (1992), 物种适应和遗传算法持续进行的基础 in 'Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', F.J. Varela and P. Bourgine (eds.), MIT Press/Bradford Books, Cambridge, MA, pp. 346-354.\nKoza, John (1992), 遗传算法：通过自然选择编写计算机程序\nMichalewicz, Zbigniew (1999), 遗传算法+数据结构=进化程序，Springer-Verlag.\nMitchell, Melanie, (1996), 遗传算法概论，MIT Press, Cambridge, MA.\nPoli, R., Langdon, W. B., McPhee, N. F. A Field Guide to Genetic Programming. Lulu.com, freely available from the internet. 2008. ISBN 978-1-4092-0073-4. \nSchmitt, Lothar M (2001), 遗传算法理论，Theoretical Computer Science (259), pp. 1-61\nSchmitt, Lothar M (2004), 遗传算法理论（二），Theoretical Computer Science (310), pp. 181-231\nVose, Michael D (1999), 简单遗传算法：基础和理论，MIT Press, Cambridge, MA.\n\n\n== 外部链接 ==\nhttps://web.archive.org/web/20160507233728/http://userweb.eng.gla.ac.uk/yun.li/ga_demo/ - 格拉斯哥大学的在线交互式演示与学习GA_demo\nhttps://web.archive.org/web/20050903002901/http://cs.felk.cvut.cz/~xobitko/ga/ - 用Java语言编写的遗传算法在线介绍程序。\n伊利诺斯遗传算法实验室 - 可以下载技术报告和程序源代码。\nGlobal Optimization Algorithms - Theory and Application （页面存档备份，存于互联网档案馆）", "泊松回归": "在统计学上，泊松回归（英語：Poisson regression）是用来为计数资料和列联表建模的一种回归分析。泊松回归假设因变量（英语：response variable）Y是泊松分布，并假设它期望值的对数可由一组未知参数进行线性表达。当其用于列联表分析时，泊松回归模型也被称作对数-线性模型。\n泊松回归模型是广义线性模型（GLM）的一种，以对数变化作为连接函数（link function），该模型的假设之一是其被解释变量服从泊松分布。\n\n\n== 泊松回归模型 ==\n\n  \n    \n      \n        \n          x\n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{n}}\n   代表由一组相互独立的变量组成的向量，其泊松回归的模型形式为:\n\n  \n    \n      \n        log\n        ⁡\n        (\n        E\n        ⁡\n        (\n        Y\n        ∣\n        \n          x\n        \n        )\n        )\n        =\n        α\n        +\n        \n          \n            β\n          \n          ′\n        \n        \n          x\n        \n        ,\n      \n    \n    {\\displaystyle \\log(\\operatorname {E} (Y\\mid \\mathbf {x} ))=\\alpha +\\mathbf {\\beta } '\\mathbf {x} ,}\n   \n  \n    \n      \n        α\n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle \\alpha \\in \\mathbb {R} }\n  ， \n  \n    \n      \n        \n          β\n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\beta } \\in \\mathbb {R} ^{n}}\n  .\n亦可简洁表示为：\n  \n    \n      \n        log\n        ⁡\n        (\n        E\n        ⁡\n        (\n        Y\n        ∣\n        \n          x\n        \n        )\n        )\n        =\n        \n          \n            θ\n          \n          ′\n        \n        \n          x\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\log(\\operatorname {E} (Y\\mid \\mathbf {x} ))={\\boldsymbol {\\theta }}'\\mathbf {x} ,\\,}\n  \n此处，\n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   是 n+1维的向量，由n个独立变量（自变量向量）一个常向量（元素取值全为1）构成，用一个θ 代表第一个表达式当中的 α 和 β。\n因此，当已知泊松回归模型当中的 θ和解释变量 \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  , 其满足泊松分布的被解释变量的期望值可以由下式来预测：\n\n  \n    \n      \n        E\n        ⁡\n        (\n        Y\n        ∣\n        \n          x\n        \n        )\n        =\n        \n          e\n          \n            \n              \n                θ\n              \n              ′\n            \n            \n              x\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid \\mathbf {x} )=e^{{\\boldsymbol {\\theta }}'\\mathbf {x} }.\\,}\n  Yi 是被解释变量的观测值，相应的解释变量为 xi ，可由极大似然估计（MLE）的方法来估计参数θ。 极大似然估计不能通过解析表达式获得解析解，是由其对数似然函数为凸函数的特性，可通过Newton–Raphson或其他基于梯度下降的思想方法来进行参数估计。\n\n\n== 极大似然估计 ==\n如上所述，已知泊松回归模型当中的 θ和解释变量 \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  , 其回归表达式为：\n\n  \n    \n      \n        E\n        ⁡\n        (\n        Y\n        ∣\n        x\n        )\n        =\n        \n          e\n          \n            \n              θ\n              ′\n            \n            x\n          \n        \n        \n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid x)=e^{\\theta 'x}\\,}\n  ,泊松分布的概率密度函数为：\n\n  \n    \n      \n        p\n        (\n        y\n        ∣\n        x\n        ;\n        θ\n        )\n        =\n        \n          \n            \n              [\n              E\n              ⁡\n              (\n              Y\n              ∣\n              x\n              )\n              \n                ]\n                \n                  y\n                \n              \n              ×\n              \n                e\n                \n                  −\n                  E\n                  ⁡\n                  (\n                  Y\n                  ∣\n                  x\n                  )\n                \n              \n            \n            \n              y\n              !\n            \n          \n        \n        =\n        \n          \n            \n              \n                e\n                \n                  y\n                  \n                    θ\n                    ′\n                  \n                  x\n                \n              \n              \n                e\n                \n                  −\n                  \n                    e\n                    \n                      \n                        θ\n                        ′\n                      \n                      x\n                    \n                  \n                \n              \n            \n            \n              y\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle p(y\\mid x;\\theta )={\\frac {[\\operatorname {E} (Y\\mid x)]^{y}\\times e^{-\\operatorname {E} (Y\\mid x)}}{y!}}={\\frac {e^{y\\theta 'x}e^{-e^{\\theta 'x}}}{y!}}}\n  现已知解释变量的观测值为由 m个向量组成 \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n            +\n            1\n          \n        \n        ,\n        \n        i\n        =\n        1\n        ,\n        …\n        ,\n        m\n      \n    \n    {\\displaystyle x_{i}\\in \\mathbb {R} ^{n+1},\\,i=1,\\ldots ,m}\n  , 对应 m 个被解释变量的观测值，\n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle y_{1},\\ldots ,y_{m}\\in \\mathbb {R} }\n  .  若同时已知θ, 则该组观测值所对应的联合概率可由下式表达：\n\n  \n    \n      \n        p\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n        ∣\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            m\n          \n        \n        ;\n        θ\n        )\n        =\n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              \n                e\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  \n                    θ\n                    ′\n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n              \n              \n                e\n                \n                  −\n                  \n                    e\n                    \n                      \n                        θ\n                        ′\n                      \n                      \n                        x\n                        \n                          i\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                y\n                \n                  i\n                \n              \n              !\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle p(y_{1},\\ldots ,y_{m}\\mid x_{1},\\ldots ,x_{m};\\theta )=\\prod _{i=1}^{m}{\\frac {e^{y_{i}\\theta 'x_{i}}e^{-e^{\\theta 'x_{i}}}}{y_{i}!}}.}\n  极大似然方法估计 θ的核心思想是，去找到能使得基于当前观测值的联合概率尽可能达到最大的θ。（可理解为：变量的取值当前观测值，与取值为其他任何数值相比，是发生概率最高的事件）。 既然目标是寻找到最优的θ，可以先将上式的等号左边简单表达为关于θ 的表达式：\n\n  \n    \n      \n        L\n        (\n        θ\n        ∣\n        X\n        ,\n        Y\n        )\n        =\n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          \n            \n              \n                e\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  \n                    θ\n                    ′\n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n              \n              \n                e\n                \n                  −\n                  \n                    e\n                    \n                      \n                        θ\n                        ′\n                      \n                      \n                        x\n                        \n                          i\n                        \n                      \n                    \n                  \n                \n              \n            \n            \n              \n                y\n                \n                  i\n                \n              \n              !\n            \n          \n        \n      \n    \n    {\\displaystyle L(\\theta \\mid X,Y)=\\prod _{i=1}^{m}{\\frac {e^{y_{i}\\theta 'x_{i}}e^{-e^{\\theta 'x_{i}}}}{y_{i}!}}}\n  .注意等号右边的表达式并未改写，但通常难于付诸计算，因而采用其对数变化后的表达式（ log-likelihood）即：\n\n  \n    \n      \n        ℓ\n        (\n        θ\n        ∣\n        X\n        ,\n        Y\n        )\n        =\n        log\n        ⁡\n        L\n        (\n        θ\n        ∣\n        X\n        ,\n        Y\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          (\n          \n            \n              y\n              \n                i\n              \n            \n            \n              θ\n              ′\n            \n            \n              x\n              \n                i\n              \n            \n            −\n            \n              e\n              \n                \n                  θ\n                  ′\n                \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            −\n            log\n            ⁡\n            (\n            \n              y\n              \n                i\n              \n            \n            !\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\ell (\\theta \\mid X,Y)=\\log L(\\theta \\mid X,Y)=\\sum _{i=1}^{m}\\left(y_{i}\\theta 'x_{i}-e^{\\theta 'x_{i}}-\\log(y_{i}!)\\right)}\n  .由于 θ 仅出现在似然函数的前两项，因而在极大化似然函数的运算过程中，可以只考虑前两项。可以删去第三项yi!，待优化的似然函数可以简洁表达为：\n\n  \n    \n      \n        ℓ\n        (\n        θ\n        ∣\n        X\n        ,\n        Y\n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          (\n          \n            \n              y\n              \n                i\n              \n            \n            \n              θ\n              ′\n            \n            \n              x\n              \n                i\n              \n            \n            −\n            \n              e\n              \n                \n                  θ\n                  ′\n                \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\ell (\\theta \\mid X,Y)=\\sum _{i=1}^{m}\\left(y_{i}\\theta 'x_{i}-e^{\\theta 'x_{i}}\\right)}\n  .\n为了找到极大值，需要求解方程：\n\n  \n    \n      \n        \n          \n            \n              ∂\n              ℓ\n              (\n              θ\n              ∣\n              X\n              ,\n              Y\n              )\n            \n            \n              ∂\n              θ\n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial \\ell (\\theta \\mid X,Y)}{\\partial \\theta }}=0}\n    \n可以通过对其似然函数取负值 （negative log-likelihood）, \n  \n    \n      \n        −\n        ℓ\n        (\n        θ\n        ∣\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle -\\ell (\\theta \\mid X,Y)}\n  是一个凸函数, 标准的凸优化方法可以考虑来求解 θ的最优值。统一的方法是Newton-Raphson 与Iterative Weighted Least Square（IWLS）算法。 给θ一组初始值，IWLS 是通过多次迭代更新直到θ 收敛。\n\n\n== 泊松回归的应用 ==\n泊松回归常用于被解释变量为计数（Count）形式时，包括事件发生的次数，比如：客服中心接到的电话次数。其满足相互独立的假设。在此例子中，即为：拨打客服电话的人们之间不存在相互关联。不会因为甲拨打了客服，而影响乙拨打的可能性。但在建模时，需要考虑统计该事件发生的时期，比如目标变量统计的是一天接到的电话次数，还是一个星期，或者一个月。这个时期的数据作为回归模型中的抵消值，在下面解释。\n\n\n=== \"曝光量\"（Exposure） 与 偏移量 (trade off) ===\n泊松分布也可以适用于比率数据，即事件发生次数与其测量时间或测量范围的比值。比如生物学家测量某森林中树木种类的数目， 比率变量即为每平方千米的树木种类数。人口学家关注的是每个人口年（person-year）的人口死亡数。通常来说，比率变量表达的是单位时间内该事件发生的次数。这些例子中，平方米”，“人口年”这些变量就是所谓的\"曝光量\"（Exposure）。泊松回归中将其视为偏移量放在等式右边。\n\n  \n    \n      \n        log\n        ⁡\n        \n          (\n          (\n          E\n          ⁡\n          (\n          Y\n          ∣\n          x\n          )\n          \n            /\n          \n          (\n          \n            exposure\n          \n          )\n          )\n        \n        =\n        \n          θ\n          ′\n        \n        x\n      \n    \n    {\\displaystyle \\log {((\\operatorname {E} (Y\\mid x)/({\\text{exposure}}))}=\\theta 'x}\n  which implies\n\n  \n    \n      \n        log\n        ⁡\n        \n          (\n          E\n          ⁡\n          (\n          Y\n          ∣\n          x\n          )\n          )\n        \n        −\n        log\n        ⁡\n        \n          (\n          \n            exposure\n          \n          )\n        \n        =\n        log\n        ⁡\n        \n          \n            (\n            \n              \n                \n                  E\n                  ⁡\n                  (\n                  Y\n                  ∣\n                  x\n                  )\n                \n                exposure\n              \n            \n            )\n          \n        \n        =\n        \n          θ\n          ′\n        \n        x\n      \n    \n    {\\displaystyle \\log {(\\operatorname {E} (Y\\mid x))}-\\log {({\\text{exposure}})}=\\log {\\left({\\frac {\\operatorname {E} (Y\\mid x)}{\\text{exposure}}}\\right)}=\\theta 'x}\n  在R中运行广义线性模型时，可用offset()来指定表示“曝光量”的变量:\n\n\n=== 过度离势和零膨胀 ===\n服从泊松分布的变量,具有期望与方差相等的特征。若观测样本的方差远大于期望值的时，则认为存在过度离势，当前的模型不合理。其常见的原因是缺失重要的解释变量。解决该问题的方法，通常采用准似然估计（quasi-likelihood） 或者负二项分布来估计。泊松回归的另一个常见的问题是零膨胀zero-inflated model。标准的泊松分布其定义域为非负整数，被解释变量y取值为0的概率为：\n\n  \n    \n      \n        p\n        (\n        y\n        =\n        0\n        ∣\n        x\n        ;\n        θ\n        )\n        =\n        \n          e\n          \n            −\n            \n              e\n              \n                \n                  θ\n                  ′\n                \n                x\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p(y=0\\mid x;\\theta )=e^{-e^{\\theta 'x}}}\n  但如果观测样本中添加大量的0，则取值为0的频率远大于理论概率，此时不适宜直接采用泊松回归。比如观测一组人在一小时内的吸烟情况，目标变量是每人吸了多少根烟。但当观测人群中有大量的非吸烟者，就会有过多的目标变量为0， 这就是零膨胀。可以采用其他的广义线性模型，比如负二项分布负二项分布来建模，或者零膨胀模型zero-inflated model 来解决。\n\n\n== 參考文獻 ==", "样本": "樣本（英語：sample）是统计学术语，指从全体中随机抽取的个体。通过对样本的调查，可以大概的了解全体的情况。抽樣时抽取样本来进行调查，而普查时则需要调查每一个个体。樣本統計學是罗纳德·费希尔博士提出的。\n樣本統計學並不是將整個母體拿去調查，所以必有誤差。\n樣本統計學的內容有統計結果、可信度和誤差，不同的統計公司會統計出不同的統計內容。\n\n\n== 樣本的規定 ==\n樣本，從母體抽樣時，應符合下列條件，才可表現出資料母體的型態：\n\n母體的分佈需很平均，在抽樣時才不會只抽到相同而不正確的結果。\n抽樣量不可太少，否則會影響對資料的準確度。", "众数 (数学)": "众数（英語：mode）指一组数据中出现次数最多的数据值。例如{8，7，7，8，6，5，5，8，8，8}中，出現最多的是8，因此眾數是8，众数可能是一個數（數據值），但也可能是多個數（數據值）。若數據的數據值出現次數相同且無其他數據值時，則不存在眾數。例如{5，2，8，2，5，8}中，2、5、8出現次數相同且沒有其他數，因此此數據不存在眾數。\n在離散概率分布中，众数是指概率质量函数有最大值的數據，也就是最容易取様到的數據。在連續概率分布中，众数是指機率密度函數有最大值的數據（峰值）。\n在統計學上，众数和平均數、中位數類似，都是总体或随机变量有關集中趨勢的重要資訊。在高斯分佈（正態分佈）中，眾數為峰值所在的位置，和平均數、中位數相同。但若分佈是高度偏斜分佈，眾數可能會和平均數、中位數有很大的差異。\n分佈中的众数不一定只有一個，若概率质量函数或機率密度函數在x1, x2……等多個點都有最大值，就會有多個众数，最極端的情形是離散型均勻分佈，所有的點概率都相同，所有的點都是眾數。若機率密度函數有數個局部最大值，一般會將這幾個極值都稱為众数，此連續機率分佈會稱為多峰分布（和單峰性相反）。\n若是對稱的單峰分布（例如正態分佈），眾數和平均數、中位數會重合。若一随机变量是由對稱的总体中產生，可以用取樣的平均值來估計總體的眾數。\n\n\n== 特征 ==\n\n用众数代表一组数据，适合于数据量较多时使用，且众数不受极端数据的影响，并且求法简便。在一组数据中，如果个别数据有很大的变动，选择中位数表示这组数据的“集中趋势”就比较适合。\n當數值或被觀察者沒有明顯次序（常發生於非數值性資料）時特別有用，由於可能無法良好定義算術平均數和中位數。例子：{蘋果，蘋果，香蕉，橙，橙，橙，桃}的眾數是橙。\n\n\n== 使用 ==\n主要用于分类数据，也可用于顺序数据和数值型数据。\n\n\n== 歷史 ==\n众数的英文mode最早是由卡尔·皮尔逊在1895年開始使用。\n\n\n== 参见 ==\n集中趨勢：平均數、中位數\n常態分佈\n多峰分布\n描述统计学\n矩 (數學)\n概括统计量\n\n\n== 参考文献 ==", "線性回歸": "在统计学中，线性回归（英語：linear regression）是利用称为线性回归方程的最小平方函數对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归（multivariable linear regression）。在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。\n线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。\n线性回归有很多实际用途。分为以下两大类：\n\n如果目标是预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y值。\n给定一个变量y和一些变量\n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle X_{1}}\n  ,...,\n  \n    \n      \n        \n          X\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle X_{p}}\n  ，这些变量有可能与y相关，线性回归分析可以用来量化y与Xj之间相关性的强度，评估出与y不相关的\n  \n    \n      \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{j}}\n  ，并识别出哪些\n  \n    \n      \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{j}}\n  的子集包含了关于y的冗余信息。线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚。相反，最小二乘逼近可以用来拟合那些非线性的模型。因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。\n\n\n== 簡介 ==\n\n\n=== 理論模型 ===\n給一個随機樣本\n  \n    \n      \n        (\n        \n          Y\n          \n            i\n          \n        \n        ,\n        \n          X\n          \n            i\n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            i\n            p\n          \n        \n        )\n        ,\n        \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle (Y_{i},X_{i1},\\ldots ,X_{ip}),\\,i=1,\\ldots ,n}\n  ，一個線性回歸模型假設回歸子\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n  和回歸量\n  \n    \n      \n        \n          X\n          \n            i\n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            i\n            p\n          \n        \n      \n    \n    {\\displaystyle X_{i1},\\ldots ,X_{ip}}\n  之間的關係是除了X的影響以外，還有其他的變數存在。我們加入一個誤差項\n  \n    \n      \n        \n          ε\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}}\n  （也是一個随機變量）來捕獲除了\n  \n    \n      \n        \n          X\n          \n            i\n            1\n          \n        \n        ,\n        …\n        ,\n        \n          X\n          \n            i\n            p\n          \n        \n      \n    \n    {\\displaystyle X_{i1},\\ldots ,X_{ip}}\n  之外任何對\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n  的影響。所以一個多變量線性回歸模型表示為以下的形式：\n\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        \n          β\n          \n            0\n          \n        \n        +\n        \n          β\n          \n            1\n          \n        \n        \n          X\n          \n            i\n            1\n          \n        \n        +\n        \n          β\n          \n            2\n          \n        \n        \n          X\n          \n            i\n            2\n          \n        \n        +\n        …\n        +\n        \n          β\n          \n            p\n          \n        \n        \n          X\n          \n            i\n            p\n          \n        \n        +\n        \n          ε\n          \n            i\n          \n        \n        ,\n        \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle Y_{i}=\\beta _{0}+\\beta _{1}X_{i1}+\\beta _{2}X_{i2}+\\ldots +\\beta _{p}X_{ip}+\\varepsilon _{i},\\qquad i=1,\\ldots ,n}\n  其他的模型可能被認定成非線性模型。一個線性回歸模型不需要是自變量的線性函數。線性在這裡表示\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}}\n  的條件均值在參數\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  裡是線性的。例如：模型\n  \n    \n      \n        \n          Y\n          \n            i\n          \n        \n        =\n        \n          β\n          \n            1\n          \n        \n        \n          X\n          \n            i\n          \n        \n        +\n        \n          β\n          \n            2\n          \n        \n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n        +\n        \n          ε\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle Y_{i}=\\beta _{1}X_{i}+\\beta _{2}X_{i}^{2}+\\varepsilon _{i}}\n  在\n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n  和\n  \n    \n      \n        \n          β\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta _{2}}\n  裡是線性的，但在\n  \n    \n      \n        \n          X\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle X_{i}^{2}}\n  裡是非線性的，它是\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  的非線性函數。\n\n\n=== 數據和估計 ===\n區分随機變量和這些變量的觀測值是很重要的。通常來說，觀測值或數據（以小寫字母表記）包括了n個值 \n  \n    \n      \n        (\n        \n          y\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            i\n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            i\n            p\n          \n        \n        )\n        ,\n        \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle (y_{i},x_{i1},\\ldots ,x_{ip}),\\,i=1,\\ldots ,n}\n  .\n我們有\n  \n    \n      \n        p\n        +\n        1\n      \n    \n    {\\displaystyle p+1}\n  個參數\n  \n    \n      \n        \n          β\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          β\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\beta _{0},\\ldots ,\\beta _{p}}\n  需要決定，為了估計這些參數，使用矩陣表記是很有用的。\n\n  \n    \n      \n        Y\n        =\n        X\n        β\n        +\n        ε\n        \n      \n    \n    {\\displaystyle Y=X\\beta +\\varepsilon \\,}\n  其中Y是一個包括了觀測值\n  \n    \n      \n        \n          Y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          Y\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle Y_{1},\\ldots ,Y_{n}}\n  的列向量，\n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  包括了未觀測的随機成份\n  \n    \n      \n        \n          ε\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          ε\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{1},\\ldots ,\\varepsilon _{n}}\n  以及回歸量的觀測值矩陣\n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ：\n\n  \n    \n      \n        X\n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      11\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    x\n                    \n                      1\n                      p\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      21\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    x\n                    \n                      2\n                      p\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    x\n                    \n                      n\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    x\n                    \n                      n\n                      p\n                    \n                  \n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle X={\\begin{pmatrix}1&x_{11}&\\cdots &x_{1p}\\\\1&x_{21}&\\cdots &x_{2p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{np}\\end{pmatrix}}}\n  X通常包括一個常數項。\n如果X列之間存在線性相關，那麽參數向量\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  就不能以最小二乘法估計除非\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  被限制，比如要求它的一些元素之和為0。\n\n\n=== 古典假設 ===\n樣本是在母體之中随機抽取出來的。\n應變量Y在實直線上是連續的，\n殘差項是獨立且相同分佈的(iid)，也就是說，殘差是独立随机的，且服從高斯分佈。這些假設意味著殘差項不依賴自變量的值，所以\n  \n    \n      \n        \n          ε\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}}\n  和自變量X（预測變量）之間是相互獨立的。\n在這些假設下，建立一個顯式線性回歸作為條件预期模型的簡單線性回歸，可以表示為：\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        \n          Y\n          \n            i\n          \n        \n        ∣\n        \n          X\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        α\n        +\n        β\n        \n          x\n          \n            i\n          \n        \n        \n      \n    \n    {\\displaystyle {\\mbox{E}}(Y_{i}\\mid X_{i}=x_{i})=\\alpha +\\beta x_{i}\\,}\n  \n\n\n== 最小二乘法分析 ==\n\n\n=== 最小二乘法估計 ===\n回歸分析的最初目的是估計模型的參數以便達到對數據的最佳拟合。在決定一個最佳拟合的不同標準之中，最小二乘法是非常優越的。這種估計可以表示為：\n\n  \n    \n      \n        \n          \n            \n              β\n              ^\n            \n          \n        \n        =\n        (\n        \n          X\n          \n            T\n          \n        \n        X\n        \n          )\n          \n            −\n            1\n          \n        \n        \n          X\n          \n            T\n          \n        \n        y\n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}=(X^{T}X)^{-1}X^{T}y\\,}\n  \n\n\n=== 迴歸推論 ===\n對於每一個\n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ldots ,n}\n  ，我們用\n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  代表誤差項\n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  的方差。一個無偏誤的估計是：\n\n  \n    \n      \n        \n          \n            \n              \n                σ\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            S\n            \n              n\n              −\n              p\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}^{2}={\\frac {S}{n-p}},}\n  其中\n  \n    \n      \n        S\n        :=\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            \n              \n                ε\n                ^\n              \n            \n          \n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S:=\\sum _{i=1}^{n}{\\hat {\\varepsilon }}_{i}^{2}}\n  是誤差平方和（殘差平方和）。估計值和實際值之間的關係是：\n\n  \n    \n      \n        \n          \n            \n              \n                σ\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        ⋅\n        \n          \n            \n              n\n              −\n              p\n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        ∼\n        \n          χ\n          \n            n\n            −\n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}^{2}\\cdot {\\frac {n-p}{\\sigma ^{2}}}\\sim \\chi _{n-p}^{2}}\n  其中\n  \n    \n      \n        \n          χ\n          \n            n\n            −\n            p\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\chi _{n-p}^{2}}\n  服從卡方分佈，自由度是\n  \n    \n      \n        n\n        −\n        p\n      \n    \n    {\\displaystyle n-p}\n  \n對普通方程的解可以冩為：\n\n  \n    \n      \n        \n          \n            \n              β\n              ^\n            \n          \n        \n        =\n        (\n        \n          \n            X\n            \n              T\n            \n          \n          X\n          \n            )\n            \n              −\n              1\n            \n          \n          \n            X\n            \n              T\n            \n          \n          y\n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X^{T}X)^{-1}X^{T}y} .}\n  這表示估計項是因變量的線性組合。進一步地說，如果所觀察的誤差服從正態分佈。參數的估計值將服從聯合正態分佈。在當前的假設之下，估計的參數向量是精確分佈的。\n\n  \n    \n      \n        \n          \n            \n              β\n              ^\n            \n          \n        \n        ∼\n        N\n        (\n        β\n        ,\n        \n          σ\n          \n            2\n          \n        \n        (\n        \n          X\n          \n            T\n          \n        \n        X\n        \n          )\n          \n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\hat {\\beta }}\\sim N(\\beta ,\\sigma ^{2}(X^{T}X)^{-1})}\n  其中\n  \n    \n      \n        N\n        (\n        ⋅\n        )\n      \n    \n    {\\displaystyle N(\\cdot )}\n  表示多變量正態分佈。\n參數估計值的標準差是：\n\n  \n    \n      \n        \n          \n            \n              \n                σ\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        =\n        \n          \n            \n              \n                S\n                \n                  n\n                  −\n                  p\n                \n              \n            \n            \n              \n                [\n                \n                  \n                    (\n                    \n                      X\n                      \n                        T\n                      \n                    \n                    X\n                    )\n                  \n                  \n                    −\n                    1\n                  \n                \n                ]\n              \n              \n                j\n                j\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}_{j}={\\sqrt {{\\frac {S}{n-p}}\\left[\\mathbf {(X^{T}X)} ^{-1}\\right]_{jj}}}.}\n  參數\n  \n    \n      \n        \n          β\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\beta _{j}}\n  的\n  \n    \n      \n        100\n        (\n        1\n        −\n        α\n        )\n        %\n      \n    \n    {\\displaystyle 100(1-\\alpha )\\%}\n  置信區間可以用以下式子來計算：\n\n  \n    \n      \n        \n          \n            \n              \n                β\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        ±\n        \n          t\n          \n            \n              \n                α\n                2\n              \n            \n            ,\n            n\n            −\n            p\n          \n        \n        \n          \n            \n              \n                σ\n                ^\n              \n            \n          \n          \n            j\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\beta }}_{j}\\pm t_{{\\frac {\\alpha }{2}},n-p}{\\hat {\\sigma }}_{j}.}\n  誤差項可以表示為：\n\n  \n    \n      \n        \n          \n            \n              \n                r\n                ^\n              \n            \n          \n          =\n          y\n          −\n          X\n          \n            \n              \n                β\n                ^\n              \n            \n          \n          =\n          y\n          −\n          X\n          (\n          \n            X\n            \n              T\n            \n          \n          X\n          \n            )\n            \n              −\n              1\n            \n          \n          \n            X\n            \n              T\n            \n          \n          y\n        \n        .\n        \n      \n    \n    {\\displaystyle \\mathbf {{\\hat {r}}=y-X{\\hat {\\boldsymbol {\\beta }}}=y-X(X^{T}X)^{-1}X^{T}y} .\\,}\n  \n\n\n==== 單變量線性回歸 ====\n單變量線性回歸，又稱簡單線性回歸（simple linear regression, SLR），是最簡單但用途很廣的回歸模型。其回歸式為：\n\n  \n    \n      \n        Y\n        =\n        α\n        +\n        β\n        X\n        +\n        ε\n      \n    \n    {\\displaystyle Y=\\alpha +\\beta X+\\varepsilon }\n  為了從一組樣本\n  \n    \n      \n        (\n        \n          y\n          \n            i\n          \n        \n        ,\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (y_{i},x_{i})}\n  （其中\n  \n    \n      \n        i\n        =\n        1\n        ,\n         \n        2\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ 2,\\ldots ,n}\n  ）之中估計最合適（誤差最小）的\n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  和\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  ，通常採用最小二乘法，其計算目標為最小化殘差平方和：\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          ε\n          \n            i\n          \n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        −\n        α\n        −\n        β\n        \n          x\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}\\varepsilon _{i}^{2}=\\sum _{i=1}^{n}(y_{i}-\\alpha -\\beta x_{i})^{2}}\n  使用微分法求極值：將上式分别對\n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  和\n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  做一階偏微分，並令其等於0：\n\n  \n    \n      \n        \n          {\n          \n            \n              \n                \n                  n\n                   \n                  α\n                  +\n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                   \n                  β\n                  =\n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                \n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                   \n                  α\n                  +\n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    x\n                    \n                      i\n                    \n                    \n                      2\n                    \n                  \n                   \n                  β\n                  =\n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    x\n                    \n                      i\n                    \n                  \n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n            \n          \n          \n        \n      \n    \n    {\\displaystyle \\left\\{{\\begin{array}{lcl}n\\ \\alpha +\\sum \\limits _{i=1}^{n}x_{i}\\ \\beta =\\sum \\limits _{i=1}^{n}y_{i}\\\\\\sum \\limits _{i=1}^{n}x_{i}\\ \\alpha +\\sum \\limits _{i=1}^{n}x_{i}^{2}\\ \\beta =\\sum \\limits _{i=1}^{n}x_{i}y_{i}\\end{array}}\\right.}\n  此二元一次線性方程組可用克萊姆法則求解，得解\n  \n    \n      \n        \n          \n            \n              α\n              ^\n            \n          \n        \n        ,\n         \n        \n          \n            \n              β\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\alpha }},\\ {\\hat {\\beta }}}\n  ：\n\n  \n    \n      \n        \n          \n            \n              β\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              n\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                y\n                \n                  i\n                \n              \n            \n            \n              n\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              \n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        n\n                      \n                    \n                    \n                      x\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n              )\n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                x\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    x\n                    ¯\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle {\\hat {\\beta }}={\\frac {n\\sum \\limits _{i=1}^{n}x_{i}y_{i}-\\sum \\limits _{i=1}^{n}x_{i}\\sum \\limits _{i=1}^{n}y_{i}}{n\\sum \\limits _{i=1}^{n}x_{i}^{2}-\\left(\\sum \\limits _{i=1}^{n}x_{i}\\right)^{2}}}={\\frac {\\sum \\limits _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum \\limits _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}\\,}\n  \n  \n    \n      \n        \n          \n            \n              α\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n            \n            \n              n\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              \n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        n\n                      \n                    \n                    \n                      x\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        −\n        \n          \n            \n              x\n              ¯\n            \n          \n        \n        \n          \n            \n              β\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\alpha }}={\\frac {\\sum \\limits _{i=1}^{n}x_{i}^{2}\\sum \\limits _{i=1}^{n}y_{i}-\\sum \\limits _{i=1}^{n}x_{i}\\sum \\limits _{i=1}^{n}x_{i}y_{i}}{n\\sum \\limits _{i=1}^{n}x_{i}^{2}-\\left(\\sum \\limits _{i=1}^{n}x_{i}\\right)^{2}}}={\\bar {y}}-{\\bar {x}}{\\hat {\\beta }}}\n  \n  \n    \n      \n        S\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        −\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          y\n          \n            i\n          \n          \n            2\n          \n        \n        −\n        \n          \n            \n              n\n              (\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              +\n              (\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              2\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n            \n            \n              n\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              \n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        n\n                      \n                    \n                    \n                      x\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle S=\\sum \\limits _{i=1}^{n}(y_{i}-{\\hat {y}}_{i})^{2}=\\sum \\limits _{i=1}^{n}y_{i}^{2}-{\\frac {n(\\sum \\limits _{i=1}^{n}x_{i}y_{i})^{2}+(\\sum \\limits _{i=1}^{n}y_{i})^{2}\\sum \\limits _{i=1}^{n}x_{i}^{2}-2\\sum \\limits _{i=1}^{n}x_{i}\\sum \\limits _{i=1}^{n}y_{i}\\sum \\limits _{i=1}^{n}x_{i}y_{i}}{n\\sum \\limits _{i=1}^{n}x_{i}^{2}-\\left(\\sum \\limits _{i=1}^{n}x_{i}\\right)^{2}}}}\n  \n  \n    \n      \n        \n          \n            \n              \n                σ\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        =\n        \n          \n            S\n            \n              n\n              −\n              2\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\sigma }}^{2}={\\frac {S}{n-2}}.}\n  協方差矩陣是： \n\n  \n    \n      \n        \n          \n            1\n            \n              n\n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              \n                \n                  (\n                  \n                    \n                      ∑\n                      \n                        i\n                        =\n                        1\n                      \n                      \n                        n\n                      \n                    \n                    \n                      x\n                      \n                        i\n                      \n                    \n                  \n                  )\n                \n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  ∑\n                  \n                    x\n                    \n                      i\n                    \n                    \n                      2\n                    \n                  \n                \n                \n                  −\n                  ∑\n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n              \n              \n                \n                  −\n                  ∑\n                  \n                    x\n                    \n                      i\n                    \n                  \n                \n                \n                  n\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{n\\sum _{i=1}^{n}x_{i}^{2}-\\left(\\sum _{i=1}^{n}x_{i}\\right)^{2}}}{\\begin{pmatrix}\\sum x_{i}^{2}&-\\sum x_{i}\\\\-\\sum x_{i}&n\\end{pmatrix}}}\n  平均響應置信區間為：\n\n  \n    \n      \n        \n          y\n          \n            d\n          \n        \n        =\n        (\n        α\n        +\n        \n          \n            \n              β\n              ^\n            \n          \n        \n        \n          x\n          \n            d\n          \n        \n        )\n        ±\n        \n          t\n          \n            \n              \n                α\n                2\n              \n            \n            ,\n            n\n            −\n            2\n          \n        \n        \n          \n            \n              σ\n              ^\n            \n          \n        \n        \n          \n            \n              \n                1\n                n\n              \n            \n            +\n            \n              \n                \n                  (\n                  \n                    x\n                    \n                      d\n                    \n                  \n                  −\n                  \n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  ∑\n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  −\n                  \n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{d}=(\\alpha +{\\hat {\\beta }}x_{d})\\pm t_{{\\frac {\\alpha }{2}},n-2}{\\hat {\\sigma }}{\\sqrt {{\\frac {1}{n}}+{\\frac {(x_{d}-{\\bar {x}})^{2}}{\\sum (x_{i}-{\\bar {x}})^{2}}}}}}\n  預報響應置信區間為：\n\n  \n    \n      \n        \n          y\n          \n            d\n          \n        \n        =\n        (\n        α\n        +\n        \n          \n            \n              β\n              ^\n            \n          \n        \n        \n          x\n          \n            d\n          \n        \n        )\n        ±\n        \n          t\n          \n            \n              \n                α\n                2\n              \n            \n            ,\n            n\n            −\n            2\n          \n        \n        \n          \n            \n              σ\n              ^\n            \n          \n        \n        \n          \n            1\n            +\n            \n              \n                1\n                n\n              \n            \n            +\n            \n              \n                \n                  (\n                  \n                    x\n                    \n                      d\n                    \n                  \n                  −\n                  \n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  ∑\n                  (\n                  \n                    x\n                    \n                      i\n                    \n                  \n                  −\n                  \n                    \n                      \n                        x\n                        ¯\n                      \n                    \n                  \n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{d}=(\\alpha +{\\hat {\\beta }}x_{d})\\pm t_{{\\frac {\\alpha }{2}},n-2}{\\hat {\\sigma }}{\\sqrt {1+{\\frac {1}{n}}+{\\frac {(x_{d}-{\\bar {x}})^{2}}{\\sum (x_{i}-{\\bar {x}})^{2}}}}}}\n  \n\n\n=== 方差分析 ===\n在方差分析（ANOVA）中，總平方和分解為兩個或更多部分。\n總平方和SST (sum of squares for total) 是：\n\n  \n    \n      \n        \n          SST\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          y\n          \n            i\n          \n        \n        −\n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\text{SST}}=\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}\n  　，其中：　\n  \n    \n      \n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\bar {y}}={\\frac {1}{n}}\\sum _{i}y_{i}}\n  同等地：\n\n  \n    \n      \n        \n          SST\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          y\n          \n            i\n          \n          \n            2\n          \n        \n        −\n        \n          \n            1\n            n\n          \n        \n        \n          \n            (\n            \n              \n                ∑\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\text{SST}}=\\sum _{i=1}^{n}y_{i}^{2}-{\\frac {1}{n}}\\left(\\sum _{i}y_{i}\\right)^{2}}\n  回歸平方和SSReg (sum of squares for regression。也可寫做模型平方和，SSM，sum of squares for model) 是：\n\n  \n    \n      \n        \n          SSReg\n        \n        =\n        ∑\n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      y\n                      ^\n                    \n                  \n                \n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          \n            \n              \n                β\n                ^\n              \n            \n          \n          \n            T\n          \n        \n        \n          \n            X\n          \n          \n            T\n          \n        \n        \n          y\n        \n        −\n        \n          \n            1\n            n\n          \n        \n        \n          (\n          \n            \n              y\n              \n                T\n              \n            \n            u\n            \n              u\n              \n                T\n              \n            \n            y\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\text{SSReg}}=\\sum \\left({\\hat {y}}_{i}-{\\bar {y}}\\right)^{2}={\\hat {\\boldsymbol {\\beta }}}^{T}\\mathbf {X} ^{T}\\mathbf {y} -{\\frac {1}{n}}\\left(\\mathbf {y^{T}uu^{T}y} \\right),}\n  殘差平方和SSE (sum of squares for error) 是：\n\n  \n    \n      \n        \n          SSE\n        \n        =\n        \n          ∑\n          \n            i\n          \n        \n        \n          \n            \n              (\n              \n                \n                  y\n                  \n                    i\n                  \n                \n                −\n                \n                  \n                    \n                      \n                        y\n                        ^\n                      \n                    \n                  \n                  \n                    i\n                  \n                \n              \n              )\n            \n            \n              2\n            \n          \n        \n        =\n        \n          \n            y\n            \n              T\n            \n          \n          y\n          −\n          \n            \n              \n                \n                  β\n                  ^\n                \n              \n            \n            \n              T\n            \n          \n          \n            X\n            \n              T\n            \n          \n          y\n        \n        .\n      \n    \n    {\\displaystyle {\\text{SSE}}=\\sum _{i}{\\left({y_{i}-{\\hat {y}}_{i}}\\right)^{2}}=\\mathbf {y^{T}y-{\\hat {\\boldsymbol {\\beta }}}^{T}X^{T}y} .}\n  總平方和SST又可寫做SSReg和SSE的和：\n\n  \n    \n      \n        \n          SST\n        \n        =\n        \n          ∑\n          \n            i\n          \n        \n        \n          \n            (\n            \n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          \n            y\n            \n              T\n            \n          \n          y\n        \n        −\n        \n          \n            1\n            n\n          \n        \n        \n          (\n          \n            \n              y\n              \n                T\n              \n            \n            u\n            \n              u\n              \n                T\n              \n            \n            y\n          \n          )\n        \n        =\n        \n          SSReg\n        \n        +\n        \n          SSE\n        \n        .\n      \n    \n    {\\displaystyle {\\text{SST}}=\\sum _{i}\\left(y_{i}-{\\bar {y}}\\right)^{2}=\\mathbf {y^{T}y} -{\\frac {1}{n}}\\left(\\mathbf {y^{T}uu^{T}y} \\right)={\\text{SSReg}}+{\\text{SSE}}.}\n  回歸係數R2是：\n\n  \n    \n      \n        \n          R\n          \n            2\n          \n        \n        =\n        \n          \n            SSReg\n            SST\n          \n        \n        =\n        1\n        −\n        \n          \n            SSE\n            SST\n          \n        \n        .\n      \n    \n    {\\displaystyle R^{2}={\\frac {\\text{SSReg}}{\\text{SST}}}=1-{\\frac {\\text{SSE}}{\\text{SST}}}.}\n  \n\n\n== 其他方法 ==\n\n\n=== 廣義最小二乘法 ===\n廣義最小二乘法可以用在當觀測誤差具有異方差或者自相關的情況下。\n\n\n=== 總體最小二乘法 ===\n總體最小二乘法用於當自變量有誤時。\n\n\n=== 廣義線性模式 ===\n廣義線性模式應用在當誤差分佈函數不是正態分佈時。比如指數分佈，伽瑪分佈，逆高斯分佈，泊松分佈，二項式分佈等。\n\n\n=== 穩健回歸 ===\n將平均絕對誤差最小化，不同於在線性回歸中是將均方誤差最小化。\n\n\n== 線性回歸的應用 ==\n\n\n=== 趨勢線 ===\n一條趨勢線代表著時間序列數據的長期走勢。它告訴我們一組特定數據（如GDP、石油價格和股票價格）是否在一段時期内增長或下降。雖然我們可以用肉眼觀察數據點在坐標系的位置大體畫出趨勢線，更恰當的方法是利用線性回歸計算出趨勢線的位置和斜率。\n\n\n=== 流行病学 ===\n有关吸烟对死亡率和发病率影响的早期证据来自采用了回归分析的观察性研究。为了在分析观测数据时减少伪相关，除最感兴趣的变量之外,通常研究人员还会在他们的回归模型里包括一些额外变量。例如，假设有一个回归模型，在这个回归模型中吸烟行为是我们最感兴趣的独立变量，其相关变量是经数年观察得到的吸烟者寿命。研究人员可能将社会经济地位当成一个额外的独立变量，已确保任何经观察所得的吸烟对寿命的影响不是由于教育或收入差异引起的。然而，我们不可能把所有可能混淆结果的变量都加入到实证分析中。例如，某种不存在的基因可能会增加人死亡的几率，还会让人的吸烟量增加。因此，比起采用观察数据的回归分析得出的结论，随机对照试验常能产生更令人信服的因果关系证据。当可控实验不可行时，回归分析的衍生，如工具变量回归，可尝试用来估计观测数据的因果关系。\n\n\n=== 金融 ===\n資本資產定價模型利用線性回歸以及Beta係數的概念分析和計算投資的系統風險。這是從聯繫投資回報和所有風險性資產回報的模型Beta係數直接得出的。\n\n\n=== 经济学 ===\n线性回归是经济学的主要实证工具。例如，它是用来预测消费支出，固定投资支出，存货投资，一国出口产品的购买，进口支出，要求持有流动性资产，劳动力需求、劳动力供给。\n\n\n== 参考文献 ==\n\n\n=== 引用 ===\n\n\n=== 来源 ===\n书籍\n刊物文章\n\n\n== 延伸阅读 ==\n\n\n== 参见 ==\n\n\n== 外部連結 ==\nLeast-Squares Regression （页面存档备份，存于互联网档案馆）, PhET Interactive simulations, University of Colorado at Boulder", "条件独立": "在概率论和統計學中，两事件R和B在给定的另一事件Y发生时条件独立，類似於統計獨立性，就是指当事件Y发生时，R发生与否和B发生与否就条件概率分布而言是独立的。换句话讲，R和B在给定Y发生时条件独立，当且仅当已知Y发生时，知道R发生与否无助于知道B发生与否，同样知道B发生与否也无助于知道R发生与否。\n\n\n== 定義 ==\n\nR和B在给定Y发生时条件独立，用概率论的标准记号表示为\n\n  \n    \n      \n        Pr\n        (\n        R\n        ∩\n        B\n        ∣\n        Y\n        )\n        =\n        Pr\n        (\n        R\n        ∣\n        Y\n        )\n        Pr\n        (\n        B\n        ∣\n        Y\n        )\n        \n      \n    \n    {\\displaystyle \\Pr(R\\cap B\\mid Y)=\\Pr(R\\mid Y)\\Pr(B\\mid Y)\\,}\n  也可以等价地表示为\n\n  \n    \n      \n        Pr\n        (\n        R\n        ∣\n        B\n        ∩\n        Y\n        )\n        =\n        Pr\n        (\n        R\n        ∣\n        Y\n        )\n        .\n        \n      \n    \n    {\\displaystyle \\Pr(R\\mid B\\cap Y)=\\Pr(R\\mid Y).\\,}\n  因为当事件Y发生时，R发生与否和B发生与否就条件概率分布而言是独立的。\n两个随机变量X和Y在给定第三个随机变量Z的情况下条件独立当且仅当它们在给定Z时的条件概率分布互相独立，也就是说，给定Z的任一值，X的概率分布和Y的值无关，Y的概率分布也和X的值无关。\n\n\n== 法则 ==\n從基本定義可導出一套描述條件獨立的重要法则。因這些推论在任何機率空間中都成立，因此也对所有变量关于另一变量的条件概率分布成立，只需考慮相应子空间即可。譬如說\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        Y\n        ⇒\n        Y\n        ⊥\n        \n        \n        \n        ⊥\n        X\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\Rightarrow Y\\perp \\!\\!\\!\\perp X}\n  也就意味着\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        Y\n        ∣\n        K\n        ⇒\n        Y\n        ⊥\n        \n        \n        \n        ⊥\n        X\n        ∣\n        K\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\mid K\\Rightarrow Y\\perp \\!\\!\\!\\perp X\\mid K}\n  。\n注：位於算式下方的逗號意为“和”。\n\n\n=== 對稱性 ===\n\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        Y\n        \n        ⇒\n        \n        Y\n        ⊥\n        \n        \n        \n        ⊥\n        X\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp Y\\quad \\Rightarrow \\quad Y\\perp \\!\\!\\!\\perp X}\n  \n\n\n=== 分解 ===\n\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        A\n        ,\n        B\n        \n        ⇒\n        \n        \n           and \n        \n        \n          \n            {\n            \n              \n                \n                  X\n                  ⊥\n                  \n                  \n                  \n                  ⊥\n                  A\n                \n              \n              \n                \n                  X\n                  ⊥\n                  \n                  \n                  \n                  ⊥\n                  B\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B\\quad \\Rightarrow \\quad {\\text{ and }}{\\begin{cases}X\\perp \\!\\!\\!\\perp A\\\\X\\perp \\!\\!\\!\\perp B\\end{cases}}}\n  證明：\n\n  \n    \n      \n        \n          p\n          \n            X\n            ,\n            A\n            ,\n            B\n          \n        \n        (\n        x\n        ,\n        a\n        ,\n        b\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n            ,\n            B\n          \n        \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle p_{X,A,B}(x,a,b)=p_{X}(x)p_{A,B}(a,b)}\n        (\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        A\n        ,\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B}\n  的定义)\n\n  \n    \n      \n        \n          ∫\n          \n            B\n          \n        \n        \n        \n          p\n          \n            X\n            ,\n            A\n            ,\n            B\n          \n        \n        (\n        x\n        ,\n        a\n        ,\n        b\n        )\n        =\n        \n          ∫\n          \n            B\n          \n        \n        \n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n            ,\n            B\n          \n        \n        (\n        a\n        ,\n        b\n        )\n      \n    \n    {\\displaystyle \\int _{B}\\!p_{X,A,B}(x,a,b)=\\int _{B}\\!p_{X}(x)p_{A,B}(a,b)}\n        (对B积分以消去B)\n\n  \n    \n      \n        \n          p\n          \n            X\n            ,\n            A\n          \n        \n        (\n        x\n        ,\n        a\n        )\n        =\n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n        \n          p\n          \n            A\n          \n        \n        (\n        a\n        )\n      \n    \n    {\\displaystyle p_{X,A}(x,a)=p_{X}(x)p_{A}(a)}\n       同理可证X和B條件獨立。\n\n\n=== 微弱的聯合 ===\n\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        A\n        ,\n        B\n        \n        ⇒\n        \n        \n           and \n        \n        \n          \n            {\n            \n              \n                \n                  X\n                  ⊥\n                  \n                  \n                  \n                  ⊥\n                  A\n                  ∣\n                  B\n                \n              \n              \n                \n                  X\n                  ⊥\n                  \n                  \n                  \n                  ⊥\n                  B\n                  ∣\n                  A\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A,B\\quad \\Rightarrow \\quad {\\text{ and }}{\\begin{cases}X\\perp \\!\\!\\!\\perp A\\mid B\\\\X\\perp \\!\\!\\!\\perp B\\mid A\\end{cases}}}\n  證明：\n\n藉由定義\n  \n    \n      \n        Pr\n        (\n        X\n        )\n        =\n        Pr\n        (\n        X\n        ∣\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X)=\\Pr(X\\mid A,B)}\n  \n由於分解的屬性\n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp B}\n  , \n  \n    \n      \n        Pr\n        (\n        X\n        )\n        =\n        Pr\n        (\n        X\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X)=\\Pr(X\\mid B)}\n  \n結合兩個等式得\n  \n    \n      \n        Pr\n        (\n        X\n        ∣\n        B\n        )\n        =\n        Pr\n        (\n        X\n        ∣\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle \\Pr(X\\mid B)=\\Pr(X\\mid A,B)}\n  ，其中確認 \n  \n    \n      \n        X\n        ⊥\n        \n        \n        \n        ⊥\n        A\n        ∣\n        B\n      \n    \n    {\\displaystyle X\\perp \\!\\!\\!\\perp A\\mid B}\n  第二個條件可以類似地被證明。\n\n\n== 註釋 ==\n\n\n== 參考資料 ==\n\n\n== 參見 ==\n条件期望", "描述统计学": "描述統計（descriptive statistics），是统计学中，來描绘或总结观察量的基本情况的統計总称。其与統計推論相对应。\n\n\n== 内容 ==\n研究者可以透過對數據資料的图像化处理，將資料摘要变為圖表，以直观了解整體資料分布的情況。通常會使用的工具是频数分布表與圖示法，如多邊圖、直方圖、饼图、散点圖等。\n研究者也可以透過分析數據資料，以了解各變量內的观察值集中與分散的情況。運用的工具有：集中量數，如平均數、中位數、眾數、幾何平均數、調和平均數。與變異量數，如全距、平均差、標準差、相對差、四分差。\n在推論統計中，測量樣本的集中量數與變異量數都是变量的不偏估計值，但是以平均數、變異數、標準差的有效性最高。\n數據的次數分配情況，往往會呈現正態分布。為了表示測量數據與正態分布偏離的情況，會使用偏度、峰度這兩種統計數據。\n為了解個別觀察值在整體中所佔的位置，會需要將觀察值轉換為相對量數，如百分等級、標準分數、四分位数等。\n\n\n== 描述统计学的應用 ==\n描述統計学為測量樣本和有關的內容提供簡單的總結。這樣的總結可能是量化的，例如統計數據，或以簡單易懂的圖表來表示。這些摘要可以為統計分析數據的一部分的描述的基礎上形成一個更廣泛的內容。\n例如，在籃球的投籃命中率是一個描述性統計分析，總結了一個球員或球隊的性能。\n在商業世界中，描述统计学為證券回報提供有用的總結分析，因為它提供以往證券回報的行為以供參考。\n\n\n== 外部链接 ==\nDescriptive Statistics Lecture: University of Pittsburgh Supercourse （页面存档备份，存于互联网档案馆）", "离散程度": "在统计学里，离散程度（英語：statistical dispersion，scatter，spread）或离散度，又稱统计变异性（statistical variability），简称 變異、變差（variation）、变率，是指一个分布或随机变量的拉伸或压缩程度。习惯上，“离散”常用来描述数据分布，而“變異”（指：變異數、方差）更常用来描述随机变量的变异程度。用以描述离散程度或變異的量主要有方差、標準差、變異系数和四分位距等。\n离散程度与集中趋势相对，因此，离散度就是指各个变量值与集中趋势的偏离程度。\n\n\n== 衡量 ==\n衡量离散程度的值，通常是非负实数：当衡量值取零时，表示分布集中在同一个值上；随着衡量值的增加，随机变量的取值越来越分散。\n部分描述离散程度的量是带单位的，并且，这些量的单位与随机变量本身的单位相同。也就是说，如果随机变量的单位是米或秒，则这些量的单位也是米或秒。这些量举例如下：\n\n标准差\n四分位距\n全距\n平均绝对偏差\n绝对差中位数\n平均差\n间隔关系此外，也有一些无量纲量：\n\n變異係數\n四分位離散係數\n基尼系数\n熵另外，还有一些带单位的量，但是他们的单位和随机变量本身的单位不同：\n\n方差\n离散指数\n\n\n== 可解释性 ==\n变差的可解释性，通常是对于一个随机变量而言的。当观测到随机变量的一些取值（例如训练集中的标签可视作是一个随机变量的一些观测值），需要推断随机变量服从的分布时，就会遇到这个问题。一般而言，推断有限观测值的随机变量服从的分布的过程，即是建立模型的过程。\n假设有随机变量\n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n  及其服从的真实分布\n  \n    \n      \n        \n          X\n        \n        ∼\n        D\n      \n    \n    {\\displaystyle \\mathbf {X} \\sim D}\n  。则对于该随机变量的观测值，可计算其变差（以方差表示）\n  \n    \n      \n        \n          \n            SS\n          \n          \n            total\n          \n        \n        :=\n        \n          Var\n        \n        (\n        \n          X\n        \n        )\n      \n    \n    {\\displaystyle {\\text{SS}}_{\\text{total}}:={\\text{Var}}(\\mathbf {X} )}\n  ；对于分布，亦可计算其变差\n  \n    \n      \n        \n          \n            SS\n          \n          \n            distribution\n          \n        \n        :=\n        \n          Var\n        \n        (\n        D\n        )\n      \n    \n    {\\displaystyle {\\text{SS}}_{\\text{distribution}}:={\\text{Var}}(D)}\n  。则\n  \n    \n      \n        \n          \n            SS\n          \n          \n            distribution\n          \n        \n      \n    \n    {\\displaystyle {\\text{SS}}_{\\text{distribution}}}\n  是相对该随机变量的可解释變異（英语：explainable variation），其余的部分则是不可解释變異（英语：unexplainable variation）。为了衡量不可解释變異，可引入不可解释變異分数（英语：fraction of unexplainable variation）\n  \n    \n      \n        \n          FUV\n        \n        :=\n        1\n        −\n        \n          \n            \n              \n                \n                  SS\n                \n                \n                  distribution\n                \n              \n              \n                \n                  SS\n                \n                \n                  total\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{FUV}}:=1-{\\tfrac {{\\text{SS}}_{\\text{distribution}}}{{\\text{SS}}_{\\text{total}}}}}\n  。不可解释變異亦称为统计噪声。\n假设\n  \n    \n      \n        \n          D\n          ′\n        \n      \n    \n    {\\displaystyle D'}\n  是模型给出的随机变量的分布。则对于该预测分布，我们可以计算器變異（以方差表示）\n  \n    \n      \n        \n          \n            SS\n          \n          \n            model\n          \n        \n        :=\n        \n          Var\n        \n        (\n        \n          D\n          ′\n        \n        )\n      \n    \n    {\\displaystyle {\\text{SS}}_{\\text{model}}:={\\text{Var}}(D')}\n  。则\n  \n    \n      \n        \n          \n            SS\n          \n          \n            model\n          \n        \n      \n    \n    {\\displaystyle {\\text{SS}}_{\\text{model}}}\n  是该模型相对该随机变量的已解释變異（英语：explained variation），其余部分则是未解释變異（英语：unexplained variation）。同样，为了衡量未解释變異，可引入未解释變異分数（英语：fraction of unexplained variation）\n  \n    \n      \n        \n          FUV\n        \n        :=\n        1\n        −\n        \n          \n            \n              \n                \n                  SS\n                \n                \n                  model\n                \n              \n              \n                \n                  SS\n                \n                \n                  total\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{FUV}}:=1-{\\tfrac {{\\text{SS}}_{\\text{model}}}{{\\text{SS}}_{\\text{total}}}}}\n  。\n\n\n== 参考资料 ==", "自助法": "在统计学中，自助法（Bootstrap Method，Bootstrapping，或自助抽樣法、拔靴法）是一种从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。自助法由Bradley Efron于1979年在《Annals of Statistics》上發表。當樣本來自能以正态分布來描述的总体，其抽樣分布為正态分布；但當樣本來自的总體無法以正态分布來描述，則以漸進分析法、自助法等來分析。採用隨機可置換抽樣（random sampling with replacement）。对于小数据集，自助法效果很好。\n\n\n== .632自助法 ==\n最常用的一种是.632自助法，假设给定的数据集包含d个样本。该数据集有放回地抽样d次，产生d个样本的训练集。这样原数据样本中的某些样本很可能在该样本集中出现多次。没有进入该训练集的样本最终形成检验集（测试集）。\n显然每个样本被选中的概率是1/d，因此未被选中的概率就是(1-1/d)，这样一个样本在训练集中没出现的概率就是d次都未被选中的概率，即(1-1/d)d。当d趋于无穷大时，这一概率就将趋近于e-1=0.368，所以留在训练集中的样本大概就占原来数据集的63.2%。\n\n\n== 参见 ==\n刀切法\n\n\n== 参考文献 ==\n范明/孟小峰. 数据挖掘：概念与技术. 机械工业出版社. 2012年8月: 241. ISBN 978-7-111-39140-1 （中文）.", "最小描述長度": "最小描述長度原則是將奧卡姆剃刀形式化後的一種結果。其想法是，在給予假說的集合的情況下，能產生最多資料壓縮效果的那個假說是最好的。它是在1978年由Jorma Rissanen所引入的。在資訊理論和計算機學習理論中，最小描述長度原則是個重要觀念。\n任一資料集都可以由一有限（譬如說，二進位制的）字母集內符號所成的字串來表示。\"最小描述長度原則背後的基本想法是：在任一給定的資料集內的任何規律性都可用來壓縮。亦即在描述資料時，與逐字逐句來描述資料的方式相比，能使用比所需還少的符號\"（Grünwald, 1998。見底下的鏈結）。既然我們希望選取到的假說能抓到資料中最多的規律，於是我們則尋找壓縮效果最佳的假說。\n若要如此，我們首先要決定一個用來壓縮資料的程式代碼。最一般的方式是選一個（具有圖靈完全特性的）計算機程式語言。我們接著以這個語言撰寫一個會輸出某筆資料的電腦程式。於是這個程式則能代表此筆資料。而能輸出此資料而又最短的程式，其長度被稱為此項資料的柯氏複雜性。這是Ray Solomonoff的核心概念，一個將歸納推論理想化後的理論。\n然而，其想法在數學上並不提供一個實際的推論方法。最重要的理由如下：\n\n柯氏複雜度在計算理論中是不可計算的：不存在一個電腦程式，能在給予任意序列的資料情況下，輸出最短而又可以產生此筆資料的程式。即使我們偶遇一個最短的程式，一般情況下也無法確知它是最短的。\n柯氏複雜度與使用什麼電腦語言來描述程式有關。使用什麼語言是可任意選擇的，但一些額外的常數項會影響複雜度。基於這個理由，柯氏複雜度理論中傾向不理會常數項。但實務上，通常只有資料中的很小一部分總量是可得的，如此，則常數通常可以對於推論結果有很大的影響：好的結果不能保證能適用於有限的資料。而最小描述長度原則則藉由以下方式來試圖補救上述問題：\n\n限制所能使用代碼的集合。使得對於所允許的代碼而言，尋找某筆資料最短的碼長變成可行（可計算）的。\n選擇一種代碼，使得不論手上有什麼樣的資料，此代碼在都是相當有效率的。這點是有些獨特的（exclusive），這方面則有許多研究在進行中。比起\"程式\"，在最小描述長度理論的領域中，比較常用的是侯選假說、模型或代碼。允許使用的代碼集合則被稱為模型類別。（有些作者則模型類別指為模型，這會令人混淆）於是代碼描述及借助於這代碼的資料描述的加總是最小的那個代碼會被選取。\n最小描述長度的一個重要特性是，它提供了一個避免過適現象的保護裝置。這是因為它在假說複雜度和已知假說下的資料複雜度之間做了取捨。要理解為什麼這是對的，可以考慮以下的例子。假設你拋擲一個銅板一千次，且你觀察到了正反面的個數。我們考慮兩種模式類別：第一種是對每個結果，以0表正面及1表反面的方式來編碼。這代碼所代表的假說即為這銅板是公平。根據這個編碼方式所得的代碼長度總是1000位元。對於一個有偏向的銅板，第二個模型類別則是，所有有效率的代碼。其代表的假說即為這個銅板是不公正的。譬如，我們觀察到510個正面和490個反面。於是，根據第二個模型中最佳碼的所求得的碼長，應少於一千位元。因為這理由，一個天真的統計方法可能提出第二個假說應是對資料較好的解釋。然而，在一個最小描述長度策略中，我們必須基於假說建造一個 單一碼，我們不能只是用最好的那一個。一個簡單的方式是，使用兩部分的代碼。我們先指定模型中哪一個假說擁有最佳的表現。然後指定使用這個代碼的資料。我們須要不少位元去指定哪一個代碼要使用。所以基於第二個模型的總碼長將大於一千位元。所以，如果你服從最小描述長度策略，其結論將是，即使第二個模型類別的最佳資料可以更適應資料，仍然沒有足夠的證據可以支持銅板是偏向的假說。\n最小描述長度核心是代碼長度函數和機率分佈之間的一對一且映成（牽涉到的引理為克拉夫特不等式）。\n對於任意機率分佈\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  ，去建造一代碼\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  ，其長度為\n  \n    \n      \n        C\n        (\n        x\n        )\n      \n    \n    {\\displaystyle C(x)}\n  等於\n  \n    \n      \n        −\n        l\n        o\n        \n          g\n          \n            2\n          \n        \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle -log_{2}P(x)}\n  是可行的；這代碼最小化預期的碼長。反過來說，給予一個編碼\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  ，則可以建造一個機率分佈\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  ，保持同樣的結果。（在這裡忽略rounding issues）。換句話說，搜尋一有效代碼被化簡化搜尋一個好的機率分佈，反過來說亦如是。\n\n\n== 相關概念 ==\n透過上述的程式碼與機率分佈之間的相似點，最小描述長度原理和機率論及統計學是有很密切的關連。這使得有些研究員將最小描述長度看成等同於Bayesian inference。模型的代碼長度和模型及資的料代碼長度則分別相當於Baysian架構中的prior probability和marginal likelihood。這觀點可用David MacKay的Information Theory, Inference, and Learning Algorithms中的例子來說明。（見下面鏈結）然而，當用Bayesian機器建造有效率 最小描述長度 程式碼有用時，最小描述長度 架構也包含其它非Bayesian的程式碼。一個例子是Shtarkov的'normalized maximum likelihood code'，在目前 最小描述長度 理論中扮演一個核心角色。但不等於Bayesian推論。更進一步，Rissanen強調，對於真實資料產生過程，我們應該不做假設：實務上，一個model class在傳統上是真實的減化，所以不包含任何客觀角度都為真的程式碼或機率分佈，根據 最小描述長度 哲學，如果Bayesian方法是基於對某於可能的資料產生過會導致不好結果的不安全的priors，我們應該除去。從 最小描述長度 的觀點來看，可接受的priors，通常傾向所謂的objvective Bayesian分析；然而，其動機通常是不同的。\n最小描述長度並非第一個資訊理論來學習的策略。早在1968年，Wallace和Boulton即提倡一類似概念，稱作最小訊息長度。最小描述長度和最小訊息長度的不同一直是讓學界及百科編撰者困惑的來源。表面上來說，這些方法大致看似相同，但有一些主要不同，特別是在解釋方法上：\n\n最小訊息長度是一個完全面向Bayesian策略：它從這個想法開始：一假說代表其在關於資料產生過程，以prior分佈表示的可信度。最小描述長度公開地避免任何關於資料產生過程的假設（但請面上面關於選擇一個\"合理\"代碼的困難）。\n兩種方法都使用了兩部分代碼：第一部分總是代表一人試圖去學習的資訊，如模型類別的索引（model selection）或參數值（估計理論）第二部分則是一種資料的編碼，在給予第一部分資訊的情況下。其不同是在於，在 最小描述長度 中，其建議，我們不想學到的參數應被移到第二部分的碼，其中他們可以使用one-part code來和資料在一起。這通常比two-part code更有效率。在 最小訊息長度 原始描述，所有的參數是在第一部分被編碼，所以會學到所有參數。\n\n\n== 外部連結 ==\nMinimum Description Length on the Web，by the University of Helsinki. Features readings, demonstrations, events and links to MDL researchers.Homepage of Jorma Rissanen，containing lecture notes and other recent material on MDL.Homepage of Peter Grünwald （页面存档备份，存于互联网档案馆），containing his very good tutorial on MDL.P. Grunwald, M. A. Pitt and I. J. Myung (eds.), Advances in Minimum Description Length: Theory and Applications （页面存档备份，存于互联网档案馆），M.I.T. Press (MIT Press), April 2005, ISBN0-262-07262-9 （页面存档备份，存于互联网档案馆）。On-line textbook: Information Theory, Inference, and Learning Algorithms （页面存档备份，存于互联网档案馆），by David MacKay，has many chapters on Bayesian methods, including examples illustrating the intimate connections between Bayesian inference and data compression。A short introduction （页面存档备份，存于互联网档案馆） to Model Selection, Kolmogorov Complexity and Minimum Description Length.I. O. Kyrgyzov, O. O. Kyrgyzov, H. Maître and M. Campedel. Kernel MDL to Determine the Number of Clusters，MLDM, pp. 203-217, 2007。", "测量精度": "在测量学中，测量精度（measuring accuracy）或精準度，是衡量测量结果的真实性与可靠性的指标，通常包含精密度（precision，或译精确度）、准确度（accuracy）、正确度（trueness）及公差（tolerance）等含义。\n上述中，“准确度”被认为是由正确度和精密度组合而成，用于衡量观测结果与其真值之间的接近程度；“正确度”指测量值的数学期望与真实值之间的接近程度，反映了测量过程中系统误差的大小；“精确度”指测量值与其数学期望之间的离散程度，反映了测量过程中偶然误差的大小。因此，准确度反映了偶然误差和系统误差的联合影响。\n在中文语境下，「精度」常被用于指精密度或是精确度，「準度」则通常指准确度或是正确度的简称，「精準度」则是两者複合的含糊用语。精度和准度的具体含意应根据语境进行判别，规范性文件则通常会回避对“精度”的使用以免造成歧义。\n\n\n== 基本概念 ==\n\n\n=== ISO 5725 ===\n在1994年国际标准化组织发布的关于测量精度概念的规范文件ISO 5725及其所对应的中华人民共和国国家标准GB/T 6379-2004 《测量方法与结果的准确度（正确度与精密度）》中，对测量精度的描述被分为准确度、正确度和精密度三个概念。该规范性文件的第一部分给出了对这三个概念的定义：\n\n准确度（英語：accuracy）：测试结果与接受参照值间的一致程度\n正确度（英語：trueness）：由大量测试结果得到的平均数与接受参照值间的一致程度\n精确度（英語：precision）：在规定条件下，独立测试结果间的一致程度与之相关的还有偏倚、重复性、再现性的概念：\n\n偏倚（英語：bias）：测试结果的期望与接受参照值之差\n重复性（英語：repeatability）：在重复性条件下的精密度\n再现性（英語：reproducibility）：在再现性条件下的精密度另外，对于准确度，ISO 5725注明“当用于一组测试结果时，由随机误差分量和系统误差即偏倚分量组成”；对于重复性的注明是“正确度的度量通常用术语偏倚表示”以及“准确度曾被称为‘平均数的准确度’，这种用法不被推荐”；对于精密度的注明则是“精密度仅仅依赖于随机误差的分布而与真值或规定值无关”“ 精密度的度量通常以不精密度表达，其量值用测试结果的标准差来表示，精密度越低，标准差越大”。除GB/T 6379-2004以外，中华人民共和国国家计量技术规范JJF 1001-2001 《通用计量术语及定义》中亦以相近的描述定义准确度、正确度和精密度。\n\n\n=== 测绘学 ===\n中国大陆使用的测绘学领域规范性文件GB/T 14911-2008 《测绘基本术语》中仅定义了“准确度”与“精密度”：\n准确度（英語：accuracy）：在一定测量条件下，对某一次的多次测量中，测量值的估值与其真值的偏离程度\n精密度（英語：precision）：在一定测量条件下，对某一次的多次测量中，各测量值间的离散程度可见，测绘学中的“精密度”与ISO 5725及GB/T 6379-2004的概念相近，但前者的“准确度”则更接近于后者“正确度”的概念。而对于后者的“准确度”，测绘学有使用“精确度”一词来代称的情况。另外，测绘学中的“精度指标”通常是指平均误差、中误差、极限误差与相对误差等衡量精密度的指标。在不存在系统误差时，测绘学中的“精确度”即可由“精度（精密度）”代称；而存在系统误差时，测绘学中的“精确度”则应由“精度（精密度）”和“准确度（正确度）”共同衡量。\n\n\n== 精度指标 ==\n\n假设某一观测量的真实值为 \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n   ，对其进行 \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   次观测，可以得到由 \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   个观测值组成的观测向量\n\n  \n    \n      \n        X\n        =\n        \n          \n            \n              [\n              \n                \n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                  \n                  \n                    \n                      X\n                      \n                        2\n                      \n                    \n                  \n                  \n                    ⋯\n                  \n                  \n                    \n                      X\n                      \n                        n\n                      \n                    \n                  \n                \n              \n              ]\n            \n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle X={\\begin{bmatrix}X_{1}&X_{2}&\\cdots &X_{n}\\end{bmatrix}}^{T}}\n  这些观测量的测量误差 \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n   是其真实值与观测值之差：\n\n  \n    \n      \n        Δ\n        =\n        \n          \n            \n              X\n              ~\n            \n          \n        \n        −\n        X\n      \n    \n    {\\displaystyle \\Delta ={\\tilde {X}}-X}\n  以概率论中的中心极限定理为依据，测量误差通常被视作是数学期望为 \n  \n    \n      \n        E\n        ⁡\n        [\n        Δ\n        ]\n      \n    \n    {\\displaystyle \\operatorname {E} [\\Delta ]}\n  ，标准差为 \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n   的随机变量，并且服从于相应的正态分布：\n\n  \n    \n      \n        f\n        (\n        Δ\n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                \n                  2\n                  π\n                \n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  (\n                  Δ\n                  −\n                  E\n                  ⁡\n                  [\n                  Δ\n                  ]\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(\\Delta )={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {(\\Delta -\\operatorname {E} [\\Delta ])^{2}}{2\\sigma ^{2}}}}}\n  基于这一假设，可以采用统计学的方法构造各类指标对测量误差的分布情况进行分析，以评价测量结果的准确度、精密度和正确度。又由于偶然误差和系统误差具有不同的统计特性，即偶然误差的数学期望为零，但系统误差不然。因此在进行测量结果的分析时，也常会将偶然误差与系统误差分别分析，即选用不同的精度指标来评价精密度和正确度。\n\n\n=== 偶然误差 ===\n偶然误差是指在大小和符号上表现出偶然性，但总体上符合一定统计规律的误差，其数学期望为零。精密度即是对偶然误差统计的描述。\n\n\n==== 方差与中误差 ====\n根据 \n  \n    \n      \n        E\n        ⁡\n        [\n        Δ\n        ]\n        =\n        0\n      \n    \n    {\\displaystyle \\operatorname {E} [\\Delta ]=0}\n   的特性，可以得出偶然误差的中误差为：\n\n  \n    \n      \n        σ\n        =\n        \n          \n            E\n            ⁡\n            [\n            \n              Δ\n              \n                2\n              \n            \n            ]\n            −\n            E\n            ⁡\n            [\n            Δ\n            \n              ]\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            E\n            ⁡\n            [\n            \n              Δ\n              \n                2\n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle \\sigma ={\\sqrt {\\operatorname {E} [\\Delta ^{2}]-\\operatorname {E} [\\Delta ]^{2}}}={\\sqrt {\\operatorname {E} [\\Delta ^{2}]}}}\n  其估计值由下列公式计算\n\n  \n    \n      \n        \n          \n            \n              σ\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  Δ\n                  \n                    2\n                  \n                \n              \n              n\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\sigma }}={\\sqrt {\\frac {\\sum _{i=1}^{n}\\Delta ^{2}}{n}}}}\n  通过方差是中误差的平方的关系，亦可得到偶然误差的方差及其估计值。\n\n\n==== 极限误差 ====\n对于正态分布，误差分布于与平均值距离一倍及二倍、三倍中误差之间的概率分别为\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  Pr\n                  ⁡\n                  (\n                  −\n                  σ\n                  <\n                  Δ\n                  <\n                  +\n                  σ\n                  )\n                  =\n                  68.3\n                  %\n                \n              \n              \n                \n                  Pr\n                  ⁡\n                  (\n                  −\n                  2\n                  σ\n                  <\n                  Δ\n                  <\n                  +\n                  2\n                  σ\n                  )\n                  =\n                  95.5\n                  %\n                \n              \n              \n                \n                  Pr\n                  ⁡\n                  (\n                  −\n                  3\n                  σ\n                  <\n                  Δ\n                  <\n                  +\n                  3\n                  σ\n                  )\n                  =\n                  99.7\n                  %\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}\\operatorname {Pr} (-\\sigma <\\Delta <+\\sigma )=68.3\\%\\\\\\operatorname {Pr} (-2\\sigma <\\Delta <+2\\sigma )=95.5\\%\\\\\\operatorname {Pr} (-3\\sigma <\\Delta <+3\\sigma )=99.7\\%\\end{cases}}}\n  在远离平均值时，误差出现的概率相当接近于零，可以在假设检验中将其排除，而选定的排除“该误差是偶然误差”这一假设的极限值即为极限误差。在测量学中，常以二倍或三倍中误差作为极限误差。\n\n\n==== 平均误差 ====\n平均误差即平均绝对误差，对于一定观测条件下的某组独立的偶然误差来说，是其绝对值的数学期望：\n\n  \n    \n      \n        θ\n        =\n        E\n        ⁡\n        [\n        \n          |\n          Δ\n          |\n        \n        ]\n      \n    \n    {\\displaystyle \\theta =\\operatorname {E} [\\left\\vert \\Delta \\right\\vert ]}\n  相应的估计值为\n\n  \n    \n      \n        \n          \n            \n              θ\n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          |\n          Δ\n          |\n        \n      \n    \n    {\\displaystyle {\\hat {\\theta }}={\\frac {1}{n}}\\sum _{i=1}^{n}\\left\\vert \\Delta \\right\\vert }\n  根据正态分布的概率分布函数，可以得出平均误差 \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   与中误差 \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n   之间的数学关系：\n\n  \n    \n      \n        θ\n        =\n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            +\n            ∞\n          \n        \n        \n          |\n          Δ\n          |\n        \n        f\n        (\n        Δ\n        )\n        d\n        \n        Δ\n        =\n        \n          ∫\n          \n            0\n          \n          \n            +\n            ∞\n          \n        \n        2\n        Δ\n        f\n        (\n        Δ\n        )\n        d\n        \n        Δ\n        =\n        \n          \n            \n              2\n              π\n            \n          \n        \n        σ\n      \n    \n    {\\displaystyle \\theta =\\int _{-\\infty }^{+\\infty }\\left\\vert \\Delta \\right\\vert f(\\Delta )\\operatorname {d} \\!\\Delta =\\int _{0}^{+\\infty }2\\Delta f(\\Delta )\\operatorname {d} \\!\\Delta ={\\sqrt {\\frac {2}{\\pi }}}\\sigma }\n  即有\n\n  \n    \n      \n        θ\n        ≈\n        0.7979\n        σ\n      \n    \n    {\\displaystyle \\theta \\approx 0.7979\\sigma }\n  \n\n\n==== 或然误差 ====\n或然误差 \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n   是使区间 \n  \n    \n      \n        (\n        −\n        ρ\n        ,\n        +\n        ρ\n        )\n      \n    \n    {\\displaystyle (-\\rho ,+\\rho )}\n   内的累积概率分布为 \n  \n    \n      \n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 1/2}\n   的值，即：\n\n  \n    \n      \n        \n          ∫\n          \n            −\n            ρ\n          \n          \n            +\n            ρ\n          \n        \n        f\n        (\n        Δ\n        )\n        d\n        \n        Δ\n        =\n        \n          \n            1\n            2\n          \n        \n      \n    \n    {\\displaystyle \\int _{-\\rho }^{+\\rho }f(\\Delta )\\operatorname {d} \\!\\Delta ={\\frac {1}{2}}}\n  且可解得\n\n  \n    \n      \n        ρ\n        ≈\n        0.6745\n        σ\n      \n    \n    {\\displaystyle \\rho \\approx 0.6745\\sigma }\n  \n\n\n=== 系统误差 ===\n观测量 \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   中存在的系统误差是指观测量的真实值 \n  \n    \n      \n        \n          \n            \n              X\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {X}}}\n   与其数学期望 \n  \n    \n      \n        E\n        ⁡\n        [\n        X\n        ]\n      \n    \n    {\\displaystyle \\operatorname {E} [X]}\n   之间的差值：\n\n  \n    \n      \n        ε\n        =\n        \n          \n            \n              X\n              ~\n            \n          \n        \n        −\n        E\n        ⁡\n        [\n        X\n        ]\n      \n    \n    {\\displaystyle \\varepsilon ={\\tilde {X}}-\\operatorname {E} [X]}\n  \n\n\n=== 均方误差 ===\n观测量 \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   的均方误差 \n  \n    \n      \n        MSE\n        ⁡\n        [\n        X\n        ]\n      \n    \n    {\\displaystyle \\operatorname {MSE} [X]}\n    通过下列公式计算：\n\n  \n    \n      \n        MSE\n        ⁡\n        [\n        X\n        ]\n        =\n        E\n        ⁡\n        [\n        (\n        X\n        −\n        \n          \n            \n              X\n              ~\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        ]\n      \n    \n    {\\displaystyle \\operatorname {MSE} [X]=\\operatorname {E} [(X-{\\tilde {X}})^{2}]}\n  将其进行分解，可以得出以方差和系统误差的平方和表示的均方误差：\n\n  \n    \n      \n        \n          \n            \n              \n                MSE\n                ⁡\n                [\n                X\n                ]\n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                (\n                X\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                [\n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                )\n                +\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                )\n                \n                  ]\n                  \n                    2\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                +\n                2\n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                )\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                )\n                +\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                E\n                ⁡\n                [\n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                \n                  )\n                  \n                    2\n                  \n                \n                ]\n                +\n                2\n                E\n                ⁡\n                [\n                (\n                X\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                )\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                )\n                ]\n                +\n                E\n                ⁡\n                [\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n                ]\n              \n            \n            \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    X\n                  \n                  \n                    2\n                  \n                \n                +\n                2\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                E\n                ⁡\n                [\n                X\n                ]\n                )\n                (\n                E\n                ⁡\n                [\n                X\n                ]\n                −\n                \n                  \n                    \n                      X\n                      ~\n                    \n                  \n                \n                )\n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    X\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  ε\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\operatorname {MSE} [X]&=\\operatorname {E} [(X-{\\tilde {X}})^{2}]\\\\[4pt]&=\\operatorname {E} [[(X-\\operatorname {E} [X])+(\\operatorname {E} [X]-{\\tilde {X}})]^{2}]\\\\[4pt]&=\\operatorname {E} [(X-\\operatorname {E} [X])^{2}+2(X-\\operatorname {E} [X])(\\operatorname {E} [X]-{\\tilde {X}})+(\\operatorname {E} [X]-{\\tilde {X}})^{2}]\\\\[4pt]&=\\operatorname {E} [(X-\\operatorname {E} [X])^{2}]+2\\operatorname {E} [(X-\\operatorname {E} [X])(\\operatorname {E} [X]-{\\tilde {X}})]+\\operatorname {E} [(\\operatorname {E} [X]-{\\tilde {X}})^{2}]\\\\[4pt]&=\\sigma _{X}^{2}+2(\\operatorname {E} [X]-\\operatorname {E} [X])(\\operatorname {E} [X]-{\\tilde {X}})+\\varepsilon ^{2}\\\\[4pt]&=\\sigma _{X}^{2}+\\varepsilon ^{2}\\\\[4pt]\\end{aligned}}}\n  因此，均方误差被认为同时包含了对偶然误差和系统误差的定量描述，可以衡量测量学中的“精确度”。\n\n\n== 參見 ==\n\n不确定度\n测量平差\n测量误差\n假精確\n效度\n信度\n圆概率误差\n置信度\n\n\n== 注释 ==\n\n\n== 参考文献 ==\n\n\n== 拓展阅读 ==\n\n\n=== 书籍 ===\n武汉大学测绘学院测量平差学科组．误差理论与测量平差基础（第三版）．武汉：武汉大学出版社，2014．ISBN 978-7-307-12922-1．\nTaylor, John. Introduction to error analysis, the study of uncertainties in physical measurements. （页面存档备份，存于互联网档案馆） 1997. ISBN 0-935702-42-3.\n\n\n=== 规范 ===\nISO 5725-1:1994(en) Accuracy (trueness and precision) of measurement methods and results — Part 1: General principles and definitions （页面存档备份，存于互联网档案馆）（英文）\n中华人民共和国国家标准，GB/T 6379.1-2004 测量方法与结果的准确度（正确度与精密度） 第一部分：总则与定义\n中华人民共和国国家标准，GB/T 14911-2008  测绘基本术语 （页面存档备份，存于互联网档案馆）\n中华人民共和国国家计量技术规范，JJF 1001-2001 通用计量术语及定义 （页面存档备份，存于互联网档案馆）\n\n\n== 外部連結 ==\nBIPM - Guides in metrology （页面存档备份，存于互联网档案馆） - Guide to the Expression of Uncertainty in Measurement (GUM) and International Vocabulary of Metrology (VIM)\n\"Beyond NIST Traceability: What really creates accuracy\" - Controlled Environments magazine\nPrecision and Accuracy with Three Psychophysical Methods （页面存档备份，存于互联网档案馆）\nGuidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results, Appendix D.1: Terminology （页面存档备份，存于互联网档案馆）\nAccuracy and Precision （页面存档备份，存于互联网档案馆）\nAccuracy vs Precision （页面存档备份，存于互联网档案馆） — a brief, clear video by Matt Parker", "Bagging算法": "Bagging算法 （英語：Bootstrap aggregating，引導聚集算法），又稱裝袋算法，是機器學習領域的一種集成學習算法。最初由Leo Breiman於1994年提出。Bagging算法可與其他分類、回歸算法結合，提高其準確率、穩定性的同時，透過降低結果的變異數，避免過擬合的發生。\n\n\n== 算法步骤 ==\n给定一个大小为\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  的训练集\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  ，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  个大小为\n  \n    \n      \n        \n          n\n          ′\n        \n      \n    \n    {\\displaystyle n'}\n  的子集\n  \n    \n      \n        \n          D\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle D_{i}}\n  ，作为新的训练集。在这\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  个训练集上使用分类、回归等算法，则可得到\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  个模型，在透過取平均值、取多数票等方法，即可得到Bagging的结果。\n\n\n== 参考文献 ==\nBreiman, Leo. Bagging predictors (PDF). Technical Report No. 421. 1994  [2020-01-11]. （原始内容存档 (PDF)于2019-07-18）. \nBreiman, Leo. Bagging predictors. Machine Learning. 1996, 24 (2): 123–140. doi:10.1007/BF00058655. CiteSeerX: 10.1.1.32.9399. \nAlfaro, E., Gámez, M. and García, N. adabag: An R package for classification with AdaBoost.M1, AdaBoost-SAMME and Bagging. 2012  [2016-10-02]. （原始内容存档于2019-09-24）.", "爬山算法": "爬山算法是一种局部择优的方法，采用启發式方法，是对深度优先搜索的一种改进，它利用反馈信息帮助生成解的决策。\n爬山算法一般存在以下问题：\n\n局部最大\n高地：也称为平顶，搜索一旦到达高地，就无法确定搜索最佳方向，会产生随机走动，使得搜索效率降低。\n山脊：搜索可能会在山脊的两面来回震荡，前进步伐很小。解决方法：随机重启爬山算法", "最小二乘法": "最小二乘法（英語：least squares method），又称最小平方法，是一种數學優化建模方法。它通过最小化誤差的平方和尋找數據的最佳函數匹配。\n利用最小二乘法可以簡便的求得未知的數據，並使得求得的數據與實際數據之間誤差的平方和為最小。\n“最小平方法”是對線性方程組，即方程個數比未知數更多的方程組，以迴歸分析求得近似解的標準方法。在這整個解決方案中，最小平方法演算為每一方程式的結果中，將殘差平方和的總和最小化。\n最重要的應用是在曲線擬合上。最小平方所涵義的最佳擬合，即殘差（殘差為：觀測值與模型提供的擬合值之間的差距）平方總和的最小化。當問題在自變量（x變量）有重大不確定性時，那麼使用簡易迴歸和最小平方法會發生問題；在這種情況下，須另外考慮變量-誤差-擬合模型所需的方法，而不是最小平方法。\n最小平方問題分為兩種：線性或普通的最小平方法，和非線性的最小平方法，取決於在所有未知數中的殘差是否為線性。線性的最小平方問題發生在統計迴歸分析中；它有一個封閉形式的解決方案。非線性的問題通常經由迭代細緻化來解決；在每次迭代中，系統由線性近似，因此在這兩種情況下核心演算是相同的。\n最小平方法所得出的多項式，即以擬合曲線的函數來描述自變量與預計應變量的變異數關係。\n當觀測值來自指數族且滿足輕度條件時，最小平方估計和最大似然估計是相同的。最小平方法也能從動差法得出。\n以下討論大多是以線性函數形式來表示，但對於更廣泛的函數族，最小平方法也是有效和實用的。此外，迭代地將局部的二次近似應用於或然性（藉由費雪信息），最小平方法可用於擬合廣義線性模型。\n最小平方法通常歸功於高斯（Carl Friedrich Gauss，1795），但最小平方法是由阿德里安-马里·勒让德（Adrien-Marie Legendre）首先發表的。\n\n\n== 簡介 ==\n\n\n=== 歷史背景 ===\n最小平方法發展於天文學和大地測量學領域，科學家和數學家嘗試為大航海探索時期的海洋航行挑戰提供解決方案。準確描述天體的行為是船艦在大海洋上航行的關鍵，水手不能再依靠陸上目標導航作航行。\n這個方法是在十八世紀期間一些進步的集大成：\n\n不同觀測值的組合是真實值的最佳估計；多次觀測會減少誤差而不是增加，也許在1722年由Roger Cotes首先闡明。在相同條件下採取的不同觀察結果，與只嘗試記錄一次最精確的觀察結果是對立的。這個方法被稱為平均值方法。托馬斯·馬耶爾（Tobias Mayer）在1750年研究月球的天平動時，特別使用這種方法，而拉普拉斯（Pierre-Simon Laplace）在1788年他的工作成果中以此解釋木星和土星的運動差異。在不同條件下進行的不同觀測值組合。該方法被稱為最小絕對偏差法，出現在Roger Joseph Boscovich在1757年他對地球形體的著名作品，而拉普拉斯在1799年也表示了同樣的問題。評定對誤差達到最小的解決方案標準，拉普拉斯指明了誤差的概率密度的數學形式，並定義了誤差最小化的估計方法。為此，拉普拉斯使用了一雙邊對稱的指數分佈，現在稱為拉普拉斯分佈作為誤差分佈的模型，並將絕對偏差之和作為估計誤差。他認為這是他最簡單的假設，他期待得出算術平均值而成為最佳的估計。可相反地，他的估計是後驗中位數。\n\n\n=== 最小平方法 ===\n\n1801年，意大利天文學家朱塞普·皮亞齊發現了第一顆小行星谷神星。经过40天的追蹤觀測後，由於谷神星運行至太陽背後，使得皮亞齊失去了谷神星的位置。隨後全世界的科學家利用皮亞齊的觀測數據開始尋找谷神星，但是根據大多数人計算的结果来尋找谷神星都沒有结果。當年24歲的高斯也計算了谷神星的軌道。奥地利天文學家海因里希·奥伯斯根據高斯計算出来的軌道重新發現了谷神星。\n高斯使用的最小平方法的方法發表於1809年他的著作《天體運動論》中，而法國科學家勒壤得于1806年獨立發現“最小平方法”，但因不為世人所知而默默無聞。兩人曾為誰最早創立最小平方法原理發生爭執。\n1829年，高斯提供了最小平方法的優化效果強於其他方法的證明，見高斯-马尔可夫定理。\n\n\n== 示例 ==\n\n某次實驗得到了四個數據點 \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  ：\n  \n    \n      \n        (\n        1\n        ,\n        6\n        )\n      \n    \n    {\\displaystyle (1,6)}\n  、\n  \n    \n      \n        (\n        2\n        ,\n        5\n        )\n      \n    \n    {\\displaystyle (2,5)}\n  、\n  \n    \n      \n        (\n        3\n        ,\n        7\n        )\n      \n    \n    {\\displaystyle (3,7)}\n  、\n  \n    \n      \n        (\n        4\n        ,\n        10\n        )\n      \n    \n    {\\displaystyle (4,10)}\n  （右圖紅色的點）。我們希望找出一條和這四個點最匹配的直線 \n  \n    \n      \n        y\n        =\n        \n          β\n          \n            2\n          \n        \n        x\n        +\n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle y=\\beta _{2}x+\\beta _{1}}\n  ，即找出在某種「最佳情况」下能夠大致符合如下超定線性方程组的 \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   和 \n  \n    \n      \n        \n          β\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta _{2}}\n  ：\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  β\n                  \n                    1\n                  \n                \n                +\n                1\n                \n                  β\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                6\n              \n              \n            \n            \n              \n                \n                  β\n                  \n                    1\n                  \n                \n                +\n                2\n                \n                  β\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                5\n              \n              \n            \n            \n              \n                \n                  β\n                  \n                    1\n                  \n                \n                +\n                3\n                \n                  β\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                7\n              \n              \n            \n            \n              \n                \n                  β\n                  \n                    1\n                  \n                \n                +\n                4\n                \n                  β\n                  \n                    2\n                  \n                \n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                10\n              \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{4}\\beta _{1}+1\\beta _{2}&&\\;=\\;&&6&\\\\\\beta _{1}+2\\beta _{2}&&\\;=\\;&&5&\\\\\\beta _{1}+3\\beta _{2}&&\\;=\\;&&7&\\\\\\beta _{1}+4\\beta _{2}&&\\;=\\;&&10&\\\\\\end{alignedat}}}\n  最小平方法採用的方法是盡量使得等號兩邊差的平方最小，也就是找出這個函數的最小值：\n\n  \n    \n      \n        \n          \n            \n              \n                S\n                (\n                \n                  β\n                  \n                    1\n                  \n                \n                ,\n                \n                  β\n                  \n                    2\n                  \n                \n                )\n                =\n              \n              \n                \n                  \n                    [\n                    \n                      6\n                      −\n                      (\n                      \n                        β\n                        \n                          1\n                        \n                      \n                      +\n                      1\n                      \n                        β\n                        \n                          2\n                        \n                      \n                      )\n                    \n                    ]\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  \n                    [\n                    \n                      5\n                      −\n                      (\n                      \n                        β\n                        \n                          1\n                        \n                      \n                      +\n                      2\n                      \n                        β\n                        \n                          2\n                        \n                      \n                      )\n                    \n                    ]\n                  \n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                +\n                \n                  \n                    [\n                    \n                      7\n                      −\n                      (\n                      \n                        β\n                        \n                          1\n                        \n                      \n                      +\n                      3\n                      \n                        β\n                        \n                          2\n                        \n                      \n                      )\n                    \n                    ]\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  \n                    [\n                    \n                      10\n                      −\n                      (\n                      \n                        β\n                        \n                          1\n                        \n                      \n                      +\n                      4\n                      \n                        β\n                        \n                          2\n                        \n                      \n                      )\n                    \n                    ]\n                  \n                  \n                    2\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}S(\\beta _{1},\\beta _{2})=&\\left[6-(\\beta _{1}+1\\beta _{2})\\right]^{2}+\\left[5-(\\beta _{1}+2\\beta _{2})\\right]^{2}\\\\&+\\left[7-(\\beta _{1}+3\\beta _{2})\\right]^{2}+\\left[10-(\\beta _{1}+4\\beta _{2})\\right]^{2}.\\\\\\end{aligned}}}\n  最小值可以通过对 \n  \n    \n      \n        S\n        (\n        \n          β\n          \n            1\n          \n        \n        ,\n        \n          β\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S(\\beta _{1},\\beta _{2})}\n   分别求 \n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta _{1}}\n   和 \n  \n    \n      \n        \n          β\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta _{2}}\n   的偏導數，然後使他們等於零得到。\n\n  \n    \n      \n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              \n                β\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        0\n        =\n        8\n        \n          β\n          \n            1\n          \n        \n        +\n        20\n        \n          β\n          \n            2\n          \n        \n        −\n        56\n      \n    \n    {\\displaystyle {\\frac {\\partial S}{\\partial \\beta _{1}}}=0=8\\beta _{1}+20\\beta _{2}-56}\n  \n\n  \n    \n      \n        \n          \n            \n              ∂\n              S\n            \n            \n              ∂\n              \n                β\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        0\n        =\n        20\n        \n          β\n          \n            1\n          \n        \n        +\n        60\n        \n          β\n          \n            2\n          \n        \n        −\n        154.\n      \n    \n    {\\displaystyle {\\frac {\\partial S}{\\partial \\beta _{2}}}=0=20\\beta _{1}+60\\beta _{2}-154.}\n  如此就得到了一个只有两个未知数的方程组，很容易就可以解出：\n\n  \n    \n      \n        \n          β\n          \n            1\n          \n        \n        =\n        3.5\n      \n    \n    {\\displaystyle \\beta _{1}=3.5}\n  \n\n  \n    \n      \n        \n          β\n          \n            2\n          \n        \n        =\n        1.4\n      \n    \n    {\\displaystyle \\beta _{2}=1.4}\n  也就是说直线 \n  \n    \n      \n        y\n        =\n        3.5\n        +\n        1.4\n        x\n      \n    \n    {\\displaystyle y=3.5+1.4x}\n   是最佳的。\n\n\n== 方法 ==\n人们对由某一变量\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   或多个变量\n  \n    \n      \n        \n          t\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle t_{1}}\n  ……\n  \n    \n      \n        \n          t\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle t_{n}}\n   构成的相关变量\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  感兴趣。如弹簧的形变与所用的力相关，一个企业的盈利与其营业额，投资收益和原始资本有关。为了得到这些变量同\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  之间的关系，便用不相关变量去构建\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ，使用如下函数模型\n\n  \n    \n      \n        \n          y\n          \n            m\n          \n        \n        =\n        f\n        (\n        \n          t\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          t\n          \n            q\n          \n        \n        ;\n        \n          b\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          b\n          \n            p\n          \n        \n        )\n      \n    \n    {\\displaystyle y_{m}=f(t_{1},\\dots ,t_{q};b_{1},\\dots ,b_{p})}\n  ,\n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  个獨立变量或\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  个係數去拟合。\n通常人们将一个可能的、对不相关变量t的构成都无困难的函数类型称作函数模型（如抛物线函数或指数函数）。参数b是为了使所选择的函数模型同观测值y相匹配。（如在测量弹簧形变时，必须将所用的力与弹簧的膨胀系数联系起来）。其目标是合适地选择参数，使函数模型最好的拟合观测值。一般情况下，观测值远多於所选择的参数。\n其次的问题是怎样判断不同拟合的质量。高斯和勒让德的方法是，假设测量误差的平均值为0。令每一个测量误差对应一个变量并与其它测量误差不相关（随机无关）。人们假设，在测量误差中绝对不含系统误差，它们应该是纯偶然误差(有固定的變異數)，围绕真值波动。除此之外，测量误差符合正态分布，这保证了偏差值在最后的结果y上忽略不计。\n确定拟合的标准应该被重视，并小心选择，较大误差的测量值应被赋予较小的權。并建立如下规则：被选择的参数，应该使算出的函数曲线与观测值之差的平方和最小。用函数表示为：\n\n  \n    \n      \n        \n          min\n          \n            \n              \n                b\n                →\n              \n            \n          \n        \n        \n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          (\n          \n            y\n            \n              m\n            \n          \n          −\n          \n            y\n            \n              i\n            \n          \n          \n            )\n            \n              2\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\min _{\\vec {b}}{\\sum _{i=1}^{n}(y_{m}-y_{i})^{2}}.}\n  \n用欧几里得度量表达为：\n\n  \n    \n      \n        \n          min\n          \n            \n              \n                b\n                →\n              \n            \n          \n        \n        ‖\n        \n          \n            \n              \n                y\n                →\n              \n            \n          \n          \n            m\n          \n        \n        (\n        \n          \n            \n              b\n              →\n            \n          \n        \n        )\n        −\n        \n          \n            \n              y\n              →\n            \n          \n        \n        \n          ‖\n          \n            2\n          \n          \n            2\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\min _{\\vec {b}}\\|{\\vec {y}}_{m}({\\vec {b}})-{\\vec {y}}\\|_{2}^{2}\\ .}\n  \n又因为\n  \n    \n      \n        ‖\n        \n          \n            \n              \n                y\n                →\n              \n            \n          \n          \n            m\n          \n        \n        (\n        \n          \n            \n              b\n              →\n            \n          \n        \n        )\n        −\n        \n          \n            \n              y\n              →\n            \n          \n        \n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\|{\\vec {y}}_{m}({\\vec {b}})-{\\vec {y}}\\|_{2}}\n    ≥0,\n所以也可以表示为\n  \n    \n      \n        \n          min\n          \n            \n              \n                b\n                →\n              \n            \n          \n        \n        ‖\n        \n          \n            \n              \n                y\n                →\n              \n            \n          \n          \n            m\n          \n        \n        (\n        \n          \n            \n              b\n              →\n            \n          \n        \n        )\n        −\n        \n          \n            \n              y\n              →\n            \n          \n        \n        \n          ‖\n          \n            2\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle \\min _{\\vec {b}}\\|{\\vec {y}}_{m}({\\vec {b}})-{\\vec {y}}\\|_{2}\\ .}\n   \n最小化问题的精度，依赖于所选择的函数模型。\n\n\n== 线性函数模型 ==\n典型的一类函数模型是线性函数模型。最简单的线性式是\n  \n    \n      \n        y\n        =\n        \n          b\n          \n            0\n          \n        \n        +\n        \n          b\n          \n            1\n          \n        \n        t\n      \n    \n    {\\displaystyle y=b_{0}+b_{1}t}\n  ，写成矩陣式，为\n\n  \n    \n      \n        \n          min\n          \n            \n              b\n              \n                0\n              \n            \n            ,\n            \n              b\n              \n                1\n              \n            \n          \n        \n        \n          \n            ‖\n            \n              \n                \n                  (\n                  \n                    \n                      \n                        1\n                      \n                      \n                        \n                          t\n                          \n                            1\n                          \n                        \n                      \n                    \n                    \n                      \n                        ⋮\n                      \n                      \n                        ⋮\n                      \n                    \n                    \n                      \n                        1\n                      \n                      \n                        \n                          t\n                          \n                            n\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n              \n                \n                  (\n                  \n                    \n                      \n                        \n                          b\n                          \n                            0\n                          \n                        \n                      \n                    \n                    \n                      \n                        \n                          b\n                          \n                            1\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n              −\n              \n                \n                  (\n                  \n                    \n                      \n                        \n                          y\n                          \n                            1\n                          \n                        \n                      \n                    \n                    \n                      \n                        ⋮\n                      \n                    \n                    \n                      \n                        \n                          y\n                          \n                            n\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        =\n        \n          min\n          \n            b\n          \n        \n        ‖\n        A\n        b\n        −\n        Y\n        \n          ‖\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\min _{b_{0},b_{1}}\\left\\|{\\begin{pmatrix}1&t_{1}\\\\\\vdots &\\vdots \\\\1&t_{n}\\end{pmatrix}}{\\begin{pmatrix}b_{0}\\\\b_{1}\\end{pmatrix}}-{\\begin{pmatrix}y_{1}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}\\right\\|_{2}=\\min _{b}\\|Ab-Y\\|_{2}.}\n  直接给出该式的参数解：\n\n  \n    \n      \n        \n          b\n          \n            1\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                t\n                \n                  i\n                \n              \n              \n                y\n                \n                  i\n                \n              \n              −\n              n\n              ⋅\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                t\n                \n                  i\n                \n                \n                  2\n                \n              \n              −\n              n\n              ⋅\n              (\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle b_{1}={\\frac {\\sum _{i=1}^{n}t_{i}y_{i}-n\\cdot {\\bar {t}}{\\bar {y}}}{\\sum _{i=1}^{n}t_{i}^{2}-n\\cdot ({\\bar {t}})^{2}}}}\n   和 \n  \n    \n      \n        \n          b\n          \n            0\n          \n        \n        =\n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        −\n        \n          b\n          \n            1\n          \n        \n        \n          \n            \n              t\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle b_{0}={\\bar {y}}-b_{1}{\\bar {t}}}\n  其中\n  \n    \n      \n        \n          \n            \n              t\n              ¯\n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          t\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\bar {t}}={\\frac {1}{n}}\\sum _{i=1}^{n}t_{i}}\n  ，为t值的算术平均值。也可解得如下形式：\n\n  \n    \n      \n        \n          b\n          \n            1\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                t\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n              )\n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                t\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle b_{1}={\\frac {\\sum _{i=1}^{n}(t_{i}-{\\bar {t}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}^{n}(t_{i}-{\\bar {t}})^{2}}}}\n  \n\n\n=== 简单线性模型 y = b0 + b1t 的例子 ===\n随机选定10艘战舰，并分析它们的长度与宽度，寻找它们长度与宽度之间的关系。由下面的描点图可以直观地看出，一艘战舰的长度（t）与宽度（y）基本呈线性关系。散点图如下：\n\n以下图表列出了各战舰的数据，随后步骤是采用最小二乘法确定两变量间的线性关系。\n\n仿照上面给出的例子\n\n  \n    \n      \n        \n          \n            \n              t\n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                t\n                \n                  i\n                \n              \n            \n            n\n          \n        \n        =\n        \n          \n            1678\n            10\n          \n        \n        =\n        167\n        \n          .\n        \n        8\n      \n    \n    {\\displaystyle {\\bar {t}}={\\frac {\\sum _{i=1}^{n}t_{i}}{n}}={\\frac {1678}{10}}=167{.}8}\n   并得到相应的\n  \n    \n      \n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        =\n        18\n        \n          .\n        \n        41\n      \n    \n    {\\displaystyle {\\bar {y}}=18{.}41}\n  .\n然后确定b1\n\n  \n    \n      \n        \n          b\n          \n            1\n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                t\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              )\n              (\n              \n                y\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    y\n                    ¯\n                  \n                \n              \n              )\n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              (\n              \n                t\n                \n                  i\n                \n              \n              −\n              \n                \n                  \n                    t\n                    ¯\n                  \n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle b_{1}={\\frac {\\sum _{i=1}^{n}(t_{i}-{\\bar {t}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}^{n}(t_{i}-{\\bar {t}})^{2}}}}\n  \n  \n    \n      \n        =\n        \n          \n            \n              3287\n              \n                .\n              \n              820\n            \n            \n              20391\n              \n                .\n              \n              60\n            \n          \n        \n        =\n        0\n        \n          .\n        \n        1612\n        \n        ,\n      \n    \n    {\\displaystyle ={\\frac {3287{.}820}{20391{.}60}}=0{.}1612\\;,}\n  可以看出，战舰的长度每变化1m，相对应的宽度便要变化16cm。并由下式得到常数项b0：\n\n  \n    \n      \n        \n          b\n          \n            0\n          \n        \n        =\n        \n          \n            \n              y\n              ¯\n            \n          \n        \n        −\n        \n          b\n          \n            1\n          \n        \n        \n          \n            \n              t\n              ¯\n            \n          \n        \n        =\n        18\n        \n          .\n        \n        41\n        −\n        0\n        \n          .\n        \n        1612\n        ⋅\n        167\n        \n          .\n        \n        8\n        =\n        −\n        8\n        \n          .\n        \n        6394\n        \n        ,\n      \n    \n    {\\displaystyle b_{0}={\\bar {y}}-b_{1}{\\bar {t}}=18{.}41-0{.}1612\\cdot 167{.}8=-8{.}6394\\;,}\n  在这里随机理论不加阐述。可以看出点的拟合非常好，长度和宽度的相关性大约为96.03％。\n利用Matlab得到拟合直线：\n\n\n=== 一般线性情况 ===\n若含有更多不相关模型变量\n  \n    \n      \n        \n          t\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          t\n          \n            q\n          \n        \n      \n    \n    {\\displaystyle t_{1},...,t_{q}}\n  ，可如组成线性函数的形式\n\n  \n    \n      \n        y\n        (\n        \n          t\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          t\n          \n            q\n          \n        \n        ;\n        \n          b\n          \n            0\n          \n        \n        ,\n        \n          b\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          b\n          \n            q\n          \n        \n        )\n        =\n        \n          b\n          \n            0\n          \n        \n        +\n        \n          b\n          \n            1\n          \n        \n        \n          t\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          b\n          \n            q\n          \n        \n        \n          t\n          \n            q\n          \n        \n      \n    \n    {\\displaystyle y(t_{1},\\dots ,t_{q};b_{0},b_{1},\\dots ,b_{q})=b_{0}+b_{1}t_{1}+\\cdots +b_{q}t_{q}}\n  即线性方程组\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  b\n                  \n                    0\n                  \n                \n                +\n                \n                  b\n                  \n                    1\n                  \n                \n                \n                  t\n                  \n                    11\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    j\n                  \n                \n                \n                  t\n                  \n                    1\n                    j\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    q\n                  \n                \n                \n                  t\n                  \n                    1\n                    q\n                  \n                \n                =\n                \n                  y\n                  \n                    1\n                  \n                \n              \n            \n            \n              \n                \n                  b\n                  \n                    0\n                  \n                \n                +\n                \n                  b\n                  \n                    1\n                  \n                \n                \n                  t\n                  \n                    21\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    j\n                  \n                \n                \n                  t\n                  \n                    2\n                    j\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    q\n                  \n                \n                \n                  t\n                  \n                    2\n                    q\n                  \n                \n                =\n                \n                  y\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                ⋮\n              \n            \n            \n              \n                \n                  b\n                  \n                    0\n                  \n                \n                +\n                \n                  b\n                  \n                    1\n                  \n                \n                \n                  t\n                  \n                    i\n                    1\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    j\n                  \n                \n                \n                  t\n                  \n                    i\n                    j\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    q\n                  \n                \n                \n                  t\n                  \n                    i\n                    q\n                  \n                \n                =\n                \n                  y\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                ⋮\n              \n            \n            \n              \n                \n                  b\n                  \n                    0\n                  \n                \n                +\n                \n                  b\n                  \n                    1\n                  \n                \n                \n                  t\n                  \n                    n\n                    1\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    j\n                  \n                \n                \n                  t\n                  \n                    n\n                    j\n                  \n                \n                +\n                ⋯\n                +\n                \n                  b\n                  \n                    q\n                  \n                \n                \n                  t\n                  \n                    n\n                    q\n                  \n                \n                =\n                \n                  y\n                  \n                    n\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{matrix}b_{0}+b_{1}t_{11}+\\cdots +b_{j}t_{1j}+\\cdots +b_{q}t_{1q}=y_{1}\\\\b_{0}+b_{1}t_{21}+\\cdots +b_{j}t_{2j}+\\cdots +b_{q}t_{2q}=y_{2}\\\\\\vdots \\\\b_{0}+b_{1}t_{i1}+\\cdots +b_{j}t_{ij}+\\cdots +b_{q}t_{iq}=y_{i}\\\\\\vdots \\\\b_{0}+b_{1}t_{n1}+\\cdots +b_{j}t_{nj}+\\cdots +b_{q}t_{nq}=y_{n}\\end{matrix}}}\n  通常人们将tij记作数据矩阵 A，参数bj记做参数向量b，观测值yi记作Y，则线性方程组又可写成：\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  \n                    t\n                    \n                      11\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      1\n                      j\n                    \n                  \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      1\n                      q\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    t\n                    \n                      21\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      2\n                      j\n                    \n                  \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      2\n                      q\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    t\n                    \n                      i\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      i\n                      j\n                    \n                  \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      i\n                      q\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    t\n                    \n                      n\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      n\n                      j\n                    \n                  \n                  ⋯\n                \n                \n                  \n                    t\n                    \n                      n\n                      q\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        ⋅\n        \n          \n            (\n            \n              \n                \n                  \n                    b\n                    \n                      0\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    b\n                    \n                      j\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    b\n                    \n                      q\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    y\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    y\n                    \n                      n\n                    \n                  \n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\begin{pmatrix}1&t_{11}&\\cdots &t_{1j}\\cdots &t_{1q}\\\\1&t_{21}&\\cdots &t_{2j}\\cdots &t_{2q}\\\\\\vdots \\\\1&t_{i1}&\\cdots &t_{ij}\\cdots &t_{iq}\\\\\\vdots \\\\1&t_{n1}&\\cdots &t_{nj}\\cdots &t_{nq}\\end{pmatrix}}\\cdot {\\begin{pmatrix}b_{0}\\\\b_{1}\\\\b_{2}\\\\\\vdots \\\\b_{j}\\\\\\vdots \\\\b_{q}\\end{pmatrix}}={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{i}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}}\n    即  \n  \n    \n      \n        A\n        b\n        =\n        Y\n      \n    \n    {\\displaystyle Ab=Y}\n  上述方程运用最小二乘法导出为线性平方差计算的形式为：\n\n  \n    \n      \n        \n          min\n          \n            b\n          \n        \n        ‖\n        A\n        b\n        −\n        Y\n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\min _{b}\\|Ab-Y\\|_{2}}\n  。\n\n\n=== 最小二乘法的解 ===\n\n  \n    \n      \n        \n          min\n          \n            b\n          \n        \n        \n          \n            ‖\n            \n              \n                A\n                b\n              \n              −\n              \n                Y\n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        ,\n        \n          A\n        \n        ∈\n        \n          \n            C\n          \n          \n            n\n            ×\n            m\n          \n        \n        ,\n        \n          Y\n        \n        ∈\n        \n          \n            C\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\min _{b}\\left\\|{\\boldsymbol {Ab}}-{\\boldsymbol {Y}}\\right\\|_{2},{\\boldsymbol {A}}\\in \\mathbf {C} ^{n\\times m},{\\boldsymbol {Y}}\\in \\mathbf {C} ^{n}}\n  \n的特解为A的广义逆矩阵与Y的乘积，这同时也是二范数极小的解，其通解为特解加上A的零空间。证明如下：\n先将Y拆成A的值域及其正交补两部分\n\n  \n    \n      \n        \n          Y\n        \n        =\n        \n          \n            Y\n          \n          \n            1\n          \n        \n        +\n        \n          \n            Y\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Y}}={\\boldsymbol {Y}}_{1}+{\\boldsymbol {Y}}_{2}}\n  \n\n  \n    \n      \n        \n          \n            Y\n          \n          \n            1\n          \n        \n        =\n        \n          A\n        \n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n        ∈\n        R\n        \n          (\n          \n            A\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Y}}_{1}={\\boldsymbol {A}}{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}\\in R\\left({\\boldsymbol {A}}\\right)}\n  \n\n  \n    \n      \n        \n          \n            Y\n          \n          \n            2\n          \n        \n        =\n        \n          (\n          \n            \n              I\n            \n            −\n            \n              A\n            \n            \n              \n                A\n              \n              \n                †\n              \n            \n          \n          )\n        \n        \n          Y\n        \n        ∈\n        R\n        \n          \n            (\n            \n              A\n            \n            )\n          \n          \n            ⊥\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Y}}_{2}=\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}{\\boldsymbol {A}}^{\\dagger }\\right){\\boldsymbol {Y}}\\in R\\left({\\boldsymbol {A}}\\right)^{\\bot }}\n  所以\n  \n    \n      \n        \n          A\n          b\n        \n        −\n        \n          \n            Y\n          \n          \n            1\n          \n        \n        ∈\n        R\n        \n          (\n          \n            A\n          \n          )\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Ab}}-{\\boldsymbol {Y}}_{1}\\in R\\left({\\boldsymbol {A}}\\right)}\n  ，可得\n\n  \n    \n      \n        \n          \n            ‖\n            \n              \n                A\n                b\n              \n              −\n              \n                Y\n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        =\n        \n          \n            ‖\n            \n              \n                A\n                b\n              \n              −\n              \n                \n                  Y\n                \n                \n                  1\n                \n              \n              +\n              \n                (\n                \n                  −\n                  \n                    \n                      Y\n                    \n                    \n                      2\n                    \n                  \n                \n                )\n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        =\n        \n          \n            ‖\n            \n              \n                A\n                b\n              \n              −\n              \n                \n                  Y\n                \n                \n                  1\n                \n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        +\n        \n          \n            ‖\n            \n              \n                Y\n              \n              \n                2\n              \n            \n            ‖\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\left\\|{\\boldsymbol {Ab}}-{\\boldsymbol {Y}}\\right\\|^{2}=\\left\\|{\\boldsymbol {Ab}}-{\\boldsymbol {Y}}_{1}+\\left(-{\\boldsymbol {Y}}_{2}\\right)\\right\\|^{2}=\\left\\|{\\boldsymbol {Ab}}-{\\boldsymbol {Y}}_{1}\\right\\|^{2}+\\left\\|{\\boldsymbol {Y}}_{2}\\right\\|^{2}}\n  故当且仅当\n  \n    \n      \n        \n          b\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {b}}}\n  是\n  \n    \n      \n        \n          A\n          b\n        \n        =\n        \n          \n            Y\n          \n          \n            1\n          \n        \n        =\n        \n          A\n        \n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Ab}}={\\boldsymbol {Y}}_{1}={\\boldsymbol {A}}{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}}\n  解时，\n  \n    \n      \n        \n          b\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {b}}}\n  即为最小二乘解，即\n  \n    \n      \n        \n          b\n        \n        =\n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n        =\n        \n          \n            \n              (\n              \n                \n                  \n                    \n                      \n                        A\n                      \n                    \n                    \n                      H\n                    \n                  \n                \n                \n                  \n                    A\n                  \n                \n              \n              )\n            \n            \n              −\n              1\n            \n          \n        \n        \n          \n            \n              \n                A\n              \n            \n            \n              H\n            \n          \n        \n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {b}}={\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}={\\left({{{\\mathbf {A} }^{H}}{\\mathbf {A} }}\\right)^{-1}}{{\\mathbf {A} }^{H}}{\\mathbf {Y} }}\n  。\n又因为\n\n  \n    \n      \n        N\n        \n          (\n          \n            A\n          \n          )\n        \n        =\n        N\n        \n          (\n          \n            \n              \n                A\n              \n              \n                †\n              \n            \n            \n              A\n            \n          \n          )\n        \n        =\n        R\n        \n          (\n          \n            \n              I\n            \n            −\n            \n              \n                A\n              \n              \n                †\n              \n            \n            \n              A\n            \n          \n          )\n        \n        =\n        \n          {\n          \n            \n              (\n              \n                \n                  I\n                \n                −\n                \n                  \n                    A\n                  \n                  \n                    †\n                  \n                \n                \n                  A\n                \n              \n              )\n            \n            \n              h\n            \n            :\n            \n              h\n            \n            ∈\n            \n              \n                C\n              \n              \n                n\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle N\\left({\\boldsymbol {A}}\\right)=N\\left({\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right)=R\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right)=\\left\\{\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}:{\\boldsymbol {h}}\\in \\mathbf {C} ^{n}\\right\\}}\n  故\n  \n    \n      \n        \n          A\n          b\n        \n        =\n        \n          A\n        \n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {Ab}}={\\boldsymbol {A}}{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}}\n  的通解为\n\n  \n    \n      \n        \n          b\n        \n        =\n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n        +\n        \n          (\n          \n            \n              I\n            \n            −\n            \n              \n                A\n              \n              \n                †\n              \n            \n            \n              A\n            \n          \n          )\n        \n        \n          h\n        \n        :\n        \n          h\n        \n        ∈\n        \n          \n            C\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {b}}={\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}+\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}:{\\boldsymbol {h}}\\in \\mathbf {C} ^{n}}\n  因为\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    ‖\n                    \n                      \n                        \n                          A\n                        \n                        \n                          †\n                        \n                      \n                      \n                        Y\n                      \n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                <\n                \n                  \n                    ‖\n                    \n                      \n                        \n                          A\n                        \n                        \n                          †\n                        \n                      \n                      \n                        Y\n                      \n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  \n                    ‖\n                    \n                      \n                        (\n                        \n                          \n                            I\n                          \n                          −\n                          \n                            \n                              A\n                            \n                            \n                              †\n                            \n                          \n                          \n                            A\n                          \n                        \n                        )\n                      \n                      \n                        h\n                      \n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    ‖\n                    \n                      \n                        \n                          A\n                        \n                        \n                          †\n                        \n                      \n                      \n                        Y\n                      \n                      +\n                      \n                        (\n                        \n                          \n                            I\n                          \n                          −\n                          \n                            \n                              A\n                            \n                            \n                              †\n                            \n                          \n                          \n                            A\n                          \n                        \n                        )\n                      \n                      \n                        h\n                      \n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n                ,\n                \n                  (\n                  \n                    \n                      I\n                    \n                    −\n                    \n                      \n                        A\n                      \n                      \n                        †\n                      \n                    \n                    \n                      A\n                    \n                  \n                  )\n                \n                \n                  h\n                \n                ≠\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\left\\|{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}\\right\\|^{2}&<\\left\\|{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}\\right\\|^{2}+\\left\\|\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}\\right\\|^{2}\\\\&=\\left\\|{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}+\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}\\right\\|^{2},\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}\\neq {\\boldsymbol {0}}\\\\\\end{aligned}}}\n  所以\n  \n    \n      \n        \n          \n            A\n          \n          \n            †\n          \n        \n        \n          Y\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}}\n  又是二范数极小的最小二乘解。\n\n\n== 参考文献 ==\n\n\n== 外部链接 ==\n3種統計學點估計的理論推演:動差法,最小平方法,最大概似估計法 （页面存档备份，存于互联网档案馆）\nhttp://www.physics.csbsju.edu/stats/least_squares.html （页面存档备份，存于互联网档案馆）\nhttps://web.archive.org/web/20200218162523/http://www.zunzun.com/\nhttp://www.orbitals.com/self/least/least.htm （页面存档备份，存于互联网档案馆）\n最小二乘法. PlanetMath.", "信息檢索": "資訊檢索（英語：Information Retrieval）是从信息资源集合获得与信息需求相关的信息资源的活动。搜索可以基于全文或其他基于内容的索引。\n自动信息检索系统用于减少所谓的“資訊超載”。许多大學和公共图书馆使用IR系统提供图书、期刊和其他文件的访问。Web搜索引擎是最常见的IR应用程序。\n\n\n== 概述 ==\n当用户向系统输入查询时，信息检索过程开始。查询是信息需求的正式声明，例如在Web搜索引擎中的搜索字符串。在信息检索中，查询不会唯一地标识集合中的单个对象。相反可以有不止一个对象匹配查询，它们可能具有不同程度的相关性。\n对象是由内容集合或数据库中的信息表示的实体。用户查询要与数据库信息进行匹配。然而，与数据库的经典SQL查询相反，在信息检索中，返回的结果可能匹配或不匹配查询，因此结果通常被排名。这种结果排名是信息检索搜索与数据库搜索相比的关键区别。根据应用，数据对象可以是文本文档、图像、音频、思维导图或视频等。通常文档本身不保存或直接存储在IR系统中，而是以文献替代或元数据在系统中表示。\n大多数IR系统对数据库中的每个对象与查询匹配的程度计算数值分数，并根据此值对对象进行排名。然后向用户显示排名靠前的对象。如果用户希望细化查询，则可以重复该过程。\n\n\n== 信息检索的类型 ==\n按照检索手段，可分为：\n\n传统信息检索（手工检索）和\n现代信息检索（计算机检索）；按照检索内容，分为：\n\n书目检索、\n数据检索、\n事实检索、\n全文检索、\n图像检索：例如：Google images\n多媒体检索：例如：SoundHound（聲頻檢索）。\n\n\n== 信息检索的主要技术指标 ==\n傳統的指標：\n\n齊全率\n準確率\n检索速度常用的指標代號：\n\n  \n    \n      \n        X\n        ∩\n        Y\n      \n    \n    {\\displaystyle X\\cap Y}\n  ：兩個檢索的交集\n\n  \n    \n      \n        \n          |\n        \n        X\n        \n          |\n        \n      \n    \n    {\\displaystyle |X|}\n  ：檢索結果的數量\n\n  \n    \n      \n        ∫\n      \n    \n    {\\displaystyle \\int }\n  ：積分\n\n  \n    \n      \n        ∑\n      \n    \n    {\\displaystyle \\sum }\n  ：求和\n\n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  ：对称差\n\n\n== 检索系统 ==\n運用一定的方法從某種資訊媒介上（包括書、硬碟、光碟等） 的資料中查找所需要情報的系統。一般可區分為手工情報檢索系統（檢索卡）、機械情報檢索系統（微縮卷）和計算機情報檢索系統三大類。\nProQuest是目前最大及歷史最悠久的情報檢索服務供應商，從1938年起就開始為學校把期刊製成微縮膠卷來儲存\n。這些膠卷在數碼以後，繼續以光碟陣及網上服務的形式為學校提供過期期刊內容的存取服務。\n以下為市面上比較常見的情報系統：\n\nDIALOG\nOvum\nEmerald\nABI\n\n\n== 參考文獻 ==\n\n\n== 參見 ==\n\n搜索引擎\n网络搜索引擎\n跨語檢索\n文本信息检索\n图像搜索", "直方图": "在统计学中，直方图（英語：histogram）是一种对数据分布情况的图形表示，是一种二维統計圖表，它的两个坐标分别是统计样本和该样本对应的某个属性的度量，以長條圖（bar）的形式具體表現。因為直方圖的長度及寬度很適合用來表現數量上的變化，所以較容易解讀差異小的數值。直方图是品质管理七大工具之一。\n\n\n== 语源 ==\n英語：histogram（直方图）源自希臘語：histos「竖立」（如船的桅杆），和希臘語：gramma 「描绘、记录」。这一术语由英国统计学家卡尔·皮尔逊于1895年创立。\n\n\n== 归一化直方图 ==\n把直方图上每个属性的计数除以所有属性的计数之和，就得到了归一化直方图。之所以叫“归一”，是因为归一化直方图的所有属性的计数之和为1，也就是说，每个属性对应计数都是0到1之间的一个数（百分比）。\n\n\n== 多维直方图 ==\n直方图通常是二维的，但可以扩展到更高维度。例如，在一个社区中做人口统计，可以以性别和身高为属性做直方图。这个直方图就是三维直方图，因为它统计了性别和身高两个属性（另一个维度是计数）。这个人口统计直方图或许具有如下数据：\n\n身高为160cm的男性有100人（(160,男)=>100）；\n身高为160cm的女性有120人（(160,女)=>120）；\n身高为161cm的男性有98人（(161,男)=>98）；\n身高为161cm的女性有115人（(161,女)=>115）；\n身高为162cm的男性有103人（(162,男)=>103）；\n身高为162cm的女性有108人（(162,女)=>108）；\n身高为163cm的男性有111人（(163,男)=>111）；\n身高为163cm的女性有107人（(163,女)=>107）；\n……\n\n\n== 应用 ==\n\n\n=== 图像直方图 ===\n\n图像直方图（英語：image histogram）是用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素数。可以借助观察该直方图了解需要如何调整亮度分布。这种直方图中，横坐标的左侧为纯黑、较暗的区域，而右侧为较亮、纯白的区域。因此，一张较暗图片的图像直方图中的数据多集中于左侧和中间部分；而整体明亮、只有少量阴影的图像则相反。\n很多数码相机提供图像直方图功能，拍摄者可以通过观察图像直方图了解到当前图像是否过分曝光或者曝光不足。\n计算机视觉领域常借助图像直方图来实现图像的二值化。\n\n\n=== 颜色直方图 ===\n在图像处理和摄影领域中，颜色直方图（英語：color histogram）指图像中颜色分布的图形表示。数字图像的颜色直方图覆盖该图像的整个色彩空间，标绘各个颜色区间中的像素数。\n颜色直方图本身可以针对任意色彩空间使用，但这一术语通常只用在诸如 RGB 和 HSV 的三维色彩空间，而针对灰度图像时常使用亮度直方图（英語：intensity histogram）这一术语。\n\n\n=== 质量直方图 ===\n在质量管理领域中，质量分布图是根据从生产过程中收集来的质量数据分布情况，画成以组距为底边、以频数为高度的一系列连接起来的直方图。\n\n\n=== 堆疊直方圖 ===\n適合將數量上的變化趨勢以「堆疊」的方式比較，堆疊直方圖呈現各項目的總累積數值。\n\n\n== 參考 ==\n\n\n== 內部連結 ==\nQC7手法\n\n\n== 外部連結 ==\n（英文） 直方图区间优化 Archive.is的存檔，存档日期2012-07-07", "模型选择": "模型选择（英語：Model selection）是在给定数据的情况下，在一组候选模型中选定最优模型的过程。在最简单的情形之下，给定数据可以是已存在的数据。不过，在复杂的情形下，模型选择也可能牵涉到实验设计，以便能够收集数据来进行模型选择。诸多候选模型的预测或解释能力相近時，根据奥卡姆剃刀原则，最简单的模型往往是最好的选择，这有助于避免过拟合。\n在做决策时，或是在不确定条件下进行优化时，模型选择也可以指代从大量计算模型中选取少数代表性模型的问题。\n\n\n== 参考文献 ==", "知识库": "知识库（Knowledge base）是用于知识管理的一种特殊的数据库，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。\n\n\n== 表示形式 ==\n测试框架\n规则\n语义网络\n\n\n=== 特点 ===\n主动性\n动态性\n变态性\n静态性\n问天下\n\n\n== 验证 ==\n\n\n=== 知识库的不完整性 ===\n悬挂条件如果该规则的任意前提条件都不出现在数据库中，也不出现在所有规则的结论部分，则该规则永远不会被激活。无用结论如果一个规则结论部分的谓词没有在知识库中任何规则的前提条件中出现，该谓词称为无用条件。孤立规则如果一个规则前提部分的谓词都是悬挂条件，并且其结论部分的谓词都是无用结论，则称该规则为孤立的。\n\n\n=== 知识库的不一致性 ===\n冗余规则\n包含规则\n循环规则\n冲突规则\n\n\n== 参见 ==", "交叉驗證": "交叉验证，有時亦稱循環估計  ，\n是一種統計學上將数据樣本切割成較小子集的實用方法。於是可以先在一個子集上做分析，而其它子集則用來做後續對此分析的確認及驗證。一開始的子集被稱為訓練集。而其它的子集則被稱為驗證集或測試集。交叉验证的目的，是用未用来给模型作训练的新数据，测试模型的性能，以便減少诸如过拟合和选择偏差等問題，并给出模型如何在一个独立的数据集上通用化（即，一个未知的数据集，如实际问题中的数据）。\n交叉驗證的理論是由Seymour Geisser所開始的。它對於防範根据数据建议的测试假设是非常重要的，特別是當後續的樣本是危險、成本過高或科学上不适合时去搜集。\n\n\n== 交叉验证的使用 ==\n假设有个未知模型具有一个或多个待定的参数，且有一个数据集能够反映该模型的特征属性（训练集）。适应的过程是对模型的参数进行调整，以使模型尽可能反映训练集的特征。如果从同一个训练样本中选择独立的样本作为验证集合，当模型因训练集过小或参数不合适而产生过拟合时，验证集的测试予以反映。\n交叉验证是一种预测模型拟合性能的方法。\n\n\n== 常見的交叉驗證形式 ==\n\n\n=== Holdout 驗證 ===\n常識來說，Holdout 驗證並非一種交叉驗證，因為数据並沒有交叉使用。\n隨機從最初的樣本中選出部分，形成交叉驗證数据，而剩餘的就當做訓練数据。\n一般來說，少於原本樣本三分之一的数据被選做驗證数据。\n\n\n=== k折交叉验证 ===\nk折交叉验证（英語：k-fold cross-validation），将训练集分割成k个子样本，一个单独的子样本被保留作为验证模型的数据，其他k − 1个样本用来训练。交叉验证重复k次，每个子样本验证一次，平均k次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。\n\n\n=== 留一驗證 ===\n正如名稱所建議，留一驗證（英語：leave-one-out cross-validation, LOOCV）意指只使用原本樣本中的一項來當做驗證資料，而剩餘的則留下來當做訓練資料。這個步驟一直持續到每個樣本都被當做一次驗證資料。\n事實上，這等同於k折交叉验证，其中k為原本樣本個數。\n在某些情況下是存在有效率的演算法，如使用kernel regression 和吉洪诺夫正则化。\n\n\n== 誤差估計 ==\n可以計算估計誤差。常見的誤差衡量標準是均方差和方根均方差，\n分別為交叉驗證的方差和標準差。\n\n\n== 另見 ==\n重抽样\n提升方法\n引导聚集算法\n\n\n== 參考文獻 ==\n\n\n== 外部連結 ==\nNaive Bayes implementation with cross-validation in Visual Basic (includes executable and source code)\nA generic k-fold cross-validation implementation (free open source; includes a distributed version that can utilize multiple computers and in principle can speed up the running time by several orders of magnitude.)", "概率模型": "機率模型（Statistical Model，也稱為Probabilistic Model）是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的機率关系。从数学上讲，该模型通常被表达为\n  \n    \n      \n        (\n        Y\n        ,\n        P\n        )\n      \n    \n    {\\displaystyle (Y,P)}\n  ，其中\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  是观测集合用来描述可能的观测结果，\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  是\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  对应的機率分布函数集合。若使用機率模型，一般而言需假设存在一个确定的分布\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  生成观测数据\n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  。因此通常使用统计推断的办法确定集合\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  中谁是数据产生的原因。\n大多数统计检验都可以被理解为一种機率模型。例如，一个比较两组数据均值的司徒頓t檢定可以被认为是对该機率模型母數是否为0的檢定。此外，检验与模型的另一个共同点则是两者都需要提出假设并且误差在模型中常被假设为正态分布。\n\n\n== 定义 ==\n概率模型\n  \n    \n      \n        \n          \n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {P}}}\n  是一个概率分布函数或密度函数的集合。可分为参数模型，无参数和半参数模型。\n参数模型是一组由有限维参数构成的分布集合\n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          \n            P\n          \n          \n            θ\n          \n        \n        :\n        θ\n        ∈\n        Θ\n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{\\mathbb {P} _{\\theta }:\\theta \\in \\Theta \\}}\n  。其中\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  是参数，而\n  \n    \n      \n        Θ\n        ⊆\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\Theta \\subseteq \\mathbb {R} ^{d}}\n  是其可行欧几里得子空间。概率模型可被用来描述一组可产生已知采样数据的分布集合。例如，假设数据产生于唯一参数的高斯分布，则我们可假设该概率模型为\n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          P\n        \n        (\n        x\n        ;\n        μ\n        ,\n        σ\n        )\n        =\n        \n          \n            1\n            \n              \n                \n                  2\n                  π\n                \n              \n              σ\n            \n          \n        \n        exp\n        ⁡\n        \n          {\n          \n            −\n            \n              \n                1\n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n            (\n            x\n            −\n            μ\n            \n              )\n              \n                2\n              \n            \n          \n          }\n        \n        :\n        μ\n        ∈\n        \n          R\n        \n        ,\n        σ\n        >\n        0\n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{\\mathbb {P} (x;\\mu ,\\sigma )={\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left\\{-{\\frac {1}{2\\sigma ^{2}}}(x-\\mu )^{2}\\right\\}:\\mu \\in \\mathbb {R} ,\\sigma >0\\}}\n  。\n无参数模型则是一组由无限维参数构成的概率分布函数集合，可被表示为\n  \n    \n      \n        \n          \n            P\n          \n        \n        =\n        {\n        \n          all distributions\n        \n        }\n      \n    \n    {\\displaystyle {\\mathcal {P}}=\\{{\\text{all distributions}}\\}}\n  。\n相比于无参数模型和参数模型，半参数模型也由无限维参数构成，但其在分布函数空间内并不紧密。例如，一组混叠的高斯模型。确切的说，如果\n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  是参数的维度，\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  是数据点的大小，如果随着\n  \n    \n      \n        d\n        →\n        ∞\n      \n    \n    {\\displaystyle d\\rightarrow \\infty }\n  和\n  \n    \n      \n        n\n        →\n        ∞\n      \n    \n    {\\displaystyle n\\rightarrow \\infty }\n  则\n  \n    \n      \n        d\n        \n          /\n        \n        n\n        →\n        0\n      \n    \n    {\\displaystyle d/n\\rightarrow 0}\n  ，则我们称之为半参数模型。", "資料類型": "在程式設計的型別系統中，数据类型（英語：Data type），又稱資料型態、資料型別，是用來約束数据的解釋。在程式語言中，常見的数据类型包括原始类型（如：整數、浮點數或字元）、多元組、記錄單元、代數資料類型、抽象数据类型、參考型別、类以及函式型別。資料類型描述了數值的表示法、解釋和結構，並以演算法操作，或是物件在記憶體中的儲存區，或者其它儲存裝置。\n\n\n== 機器中的資料類型 ==\n所有在電腦中，基於數位電子學的底層資料，都是以位元（0 或 1）表示。其中資料的最小的定址單位，稱為位元組（通常是八位元，以八個位元為一組）。機器碼指令處理的單位，稱作字長（至 2007 年止，一般為 32 或 64 位元）大部分對字長的指令解譯，主要以二進制為主，如一個 32 位元的字長，可以表示從 0 至 \n  \n    \n      \n        \n          2\n          \n            32\n          \n        \n        −\n        1\n      \n    \n    {\\displaystyle 2^{32}-1}\n   的無符號整數值，或者表示從 \n  \n    \n      \n        −\n        \n          2\n          \n            31\n          \n        \n      \n    \n    {\\displaystyle -2^{31}}\n   至 \n  \n    \n      \n        \n          2\n          \n            31\n          \n        \n        −\n        1\n      \n    \n    {\\displaystyle 2^{31}-1}\n   的有符號整數值。由於有了二的補數，機器語言和機器大多不需要區分無符號和有符號資料類型。存在著特殊的算術指令，對字長中的位元使用不同的解釋，以此作為浮點數。\n\n\n== 原始資料類型 ==\n\n程式語言提供若干原始数据型別，以作為程式以及專用化複合類型的建立基礎。典型的原始資料類型包含各種整數、浮點數以及字串型別。儘管這些建立基礎：陣列、記錄單元以及參考所聯繫的資料，可能未包括在基本型別，但仍可將其視為若干原始值的聚集。\n\n\n== 複合型別 ==\n\n這部分可包括以下內容（最終仍取決於程式語言）：\n\nRECORD一組變數型別，例子：資料庫表格中的一行TABLE資料庫中的索引欄位NESTED TABLE任意的單一複合型別的一維陣列VARRAY同一型別變數、且固定大小的收集處\n\n\n== 數值範圍 ==\n每一個資料類型都有一個數值上的最大和最小值，稱作數值範圍。了解數值的範圍是很重要的，尤其是當使用較小的型別時，你就只能儲存範圍之內的數值。試圖儲存一個超出其範圍的數值，可能會導致編譯或執行錯誤，或者不正確的計算結果（因為被截斷）。\n一個變數的範圍，是基於用以保存數值的位元組數目，而且整數資料類型通常能夠儲存 \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n   數值（此處的 \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   是指位元）。對於其它的資料類型（例如，浮點數），其數值範圍更為複雜，且幾乎取決於所使用的儲存方法。還有一些不用完全部的位元，例如，布林只需一個位元，且表示一個二進制值（雖然在實踐中，通常會用完剩餘的 7 個位元）。某些程式語言也允許反向決定，程式設計者定義解決問題所需的範圍和精度，然後由編譯器自動選擇合適的整數或浮點數。\n下表列出常見的資料類型，及其數值範圍：\n\n\n== 資料結構 ==\n\n\n== 抽象類型 ==\n\n\n== 註釋 ==\n\n\n== 参考文献 ==\n\nLuca Cardelli, Peter Wegner. On Understanding Types, Data Abstraction, and Polymorphism, [1] （页面存档备份，存于互联网档案馆） from Computing Surveys (December, 1985).\n\n\n== 参见 ==\n\n类型理論，關於类型的數學模型。\n型別系統，關於在程式語言型別中的選擇差異。", "明氏距离": "明氏距离又叫做明可夫斯基距离，是欧氏空间中的一种测度，被看做是欧氏距离和曼哈顿距离的一种推广。\n\n\n== 定义 ==\n两点\n\n  \n    \n      \n        P\n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \n           and \n        \n        Q\n        =\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle P=(x_{1},x_{2},\\ldots ,x_{n}){\\text{ and }}Q=(y_{1},y_{2},\\ldots ,y_{n})\\in \\mathbb {R} ^{n}}\n  之间的明氏距离公式为：\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                |\n              \n              \n                x\n                \n                  i\n                \n              \n              −\n              \n                y\n                \n                  i\n                \n              \n              \n                \n                  |\n                \n                \n                  p\n                \n              \n            \n            )\n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left(\\sum _{i=1}^{n}|x_{i}-y_{i}|^{p}\\right)^{1/p}.}\n  p取1或2时的明氏距离是最为常用的，p=2即为欧氏距离，而p=1时则为曼哈顿距离。当p取无穷时的极限情况下，可以得到切比雪夫距离：\n\n  \n    \n      \n        \n          lim\n          \n            p\n            →\n            ∞\n          \n        \n        \n          \n            \n              (\n              \n                \n                  ∑\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    n\n                  \n                \n                \n                  |\n                \n                \n                  x\n                  \n                    i\n                  \n                \n                −\n                \n                  y\n                  \n                    i\n                  \n                \n                \n                  \n                    |\n                  \n                  \n                    p\n                  \n                \n              \n              )\n            \n            \n              \n                1\n                p\n              \n            \n          \n        \n        =\n        \n          max\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          |\n        \n        \n          x\n          \n            i\n          \n        \n        −\n        \n          y\n          \n            i\n          \n        \n        \n          |\n        \n        .\n        \n      \n    \n    {\\displaystyle \\lim _{p\\to \\infty }{\\left(\\sum _{i=1}^{n}|x_{i}-y_{i}|^{p}\\right)^{\\frac {1}{p}}}=\\max _{i=1}^{n}|x_{i}-y_{i}|.\\,}", "数据建模": "在软件工程中，数据建模是运用正式的数据建模技术，建立信息系统的数据模型的过程。\n\n\n== 概述 ==\n数据建模是一种用于定义和分析数据的要求和其需要的相应支持的訊息系统的过程。因此，数据建模的过程中，涉及到的专业数据建模工作，与企业的利益和用户的訊息系统密切相关。\n从需求到实际的数据库，有三种不同的类型  。用于訊息系统的数据模型作为一个概念数据模型，本质上是一组记录数据要求的最初的规范技术。数据首先用于讨论适合企业的最初要求，然后被转变为一个逻辑数据模型，该模型可以在数据库中的数据结构概念模型中实现。一个概念数据模型的实现可能需要多个逻辑数据模型。数据建模中的最后一步是确定逻辑数据模型到物理数据模型中到对数据，访问，性能和存储的具体要求。数据建模定义的不只是数据元素，也包括它们的结构和它们之间的关系。\n\n\n== 参考文献 ==", "非線性系統": "在物理科學中，如果描述某個系統的方程其輸入（自變數）與輸出（應變數）不成正比，則稱為非線性系統。由於自然界中大部分的系統本質上都是非線性的，因此許多工程師、物理學家、數學家和其他科學家對於非線性問題的研究都極感興趣。非線性系統和線性系統最大的差別在於，非線性系統可能會導致混沌、不可預測，或是不直觀的結果。\n一般來說，非線性系統的行為可以用一組非線性聯立方程來描述。非線性方程裡含有由未知數構成的非線性函數；換句話說，一個非線性方程並不能寫成其未知數的線性組合。非線性微分方程，則是指方程裡含有未知函數及其導函數的乘冪不等於一的項。在判定一個方程是線性或非線性時，只需考慮未知數（或未知函數）的部分，不需要檢查方程中是否有已知的非線性項。例如在微分方程中，若所有的未知函數、未知導函數皆為一次，即使出現由某個已知變數所構成的非線性函數，仍稱它是線性微分方程。\n由於非線性方程非常難解，因此常常需要以線性方程來近似一個非線性系統（線性近似）。這種近似對某範圍內的輸入值（自變數）是很準確的，但線性近似之後反而會無法解釋許多有趣的現象，例如孤波、混沌和奇點。這些奇特的現象，也常常讓非線性系統的行為看起來違反直覺、不可預測，或甚至混沌。雖然「混沌的行為」和「隨機的行為」感覺很相似，但兩者絕對不能混為一談；也就是說，一個混沌系統的行為絕對不是隨機的。\n舉例來說，許多天氣系統就是混沌的，微小的擾動即可導致整個系統產生各種不同的複雜結果。就目前的科技而言，這種天氣的非線性特性即成了長期天氣預報的絆腳石。\n某些書的作者以非線性科學來代指非線性系統的研究，但也有人不以為然：\n\n「在科學領域裡使用『非線性科學』這個詞，就如同把動物學裡大部分的研究對象稱作『非大象動物』一樣可笑。」\n\n\n== 定義 ==\n在數學上，一個線性函數（映射）\n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   擁有以下兩個性質：\n\n疊加性：\n  \n    \n      \n        \n          f\n          (\n          x\n          +\n          y\n          )\n           \n          =\n          f\n          (\n          x\n          )\n           \n          +\n          f\n          (\n          y\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x+y)\\ =f(x)\\ +f(y)}\n  ；\n齊次：\n  \n    \n      \n        \n          f\n          (\n          α\n          x\n          )\n           \n          =\n          α\n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(\\alpha x)\\ =\\alpha f(x)}\n  。在 α 是有理數的情況下，一個可疊加函數必定是齊次函數（在討論線性與否時，齊次函數專指一次齊次函數）；若 \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   是連續函數，則只要 α 是任意實數，就可以從疊加性推出齊次。然而在推廣至任意複數 α 時，疊加性便再也無法導出齊次了。也就是說，在複數的世界裡存在一種反線性映射，它滿足疊加性，但卻非齊次。疊加性和齊次這兩個條件常會被合併在一起，稱之為疊加原理：\n\n  \n    \n      \n        f\n        (\n        α\n        x\n        +\n        β\n        y\n        )\n        =\n        α\n        f\n        (\n        x\n        )\n        +\n        β\n        f\n        (\n        y\n        )\n        \n      \n    \n    {\\displaystyle f(\\alpha x+\\beta y)=\\alpha f(x)+\\beta f(y)\\,}\n  。對於一個表示為\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        C\n        \n      \n    \n    {\\displaystyle f(x)=C\\,}\n  的方程，如果 \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   是一個線性映射，則稱為線性方程，反之則稱為非線性方程。另外，如果 \n  \n    \n      \n        C\n        =\n        0\n      \n    \n    {\\displaystyle C=0}\n  ，則稱此方程齊次（齊次在函數和方程上的定義不同，齊次方程指方程內沒有和 x 無關的項 C，即任何項皆和 x 有關）。\n這裡 \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        C\n      \n    \n    {\\displaystyle f(x)=C}\n   的定義是很一般性的，\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   可為任何數字、向量、函數等，而 \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   可以指任意映射，例如有條件限制（給定初始值或邊界值）的微分或積分運算。如果 \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   內含有對 \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   的微分運算，此方程即是一個微分方程。\n\n\n== 非線性代數方程 ==\n\n代數方程又稱為多項式方程。令某多項式等於零可得一個多項式方程，例如：\n\n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        x\n        −\n        1\n        =\n        0\n        \n      \n    \n    {\\displaystyle x^{2}+x-1=0\\,}\n  。利用勘根法可以找出某個代數方程的解；但若是代數方程組則較為複雜，有時候甚至很難確定一個代數方程組是否具有複數解（見希爾伯特零點定理）。即使如此，對於一些具有有限個複數解的多項式方程組而言，我們已經找到解的方法，並且也已充分了解這種系統的行為。代數方程組的研究是代數幾何裡重要的一環，而代數幾何正是現代數學裡的其中一個分枝。\n\n\n== 非線性遞迴關係 ==\n若將一個序列前項和後項之間的關係定義成某個非線性映射，則稱為非線性遞迴關係，例如單峰映射和侯世達數列。由非線性遞迴關係構成的非線性離散模型，在實際應用中包括 NARMAX（Nonlinear AutoRegressive Moving Average with eXogenous inputs，外部輸入非線性自迴歸移動平均）模型、非線性系統辨識和分析程序等。這些方法可以用來分析時域、頻域和時空域（spatio-temporal domains）裡複雜的非線性行為。\n\n\n== 非線性微分方程 ==\n若描述一個系統的微分方程是非線性的，則稱此系統為非線性系統。含有非線性微分方程的問題，系統彼此間的表現差異極大，而每個問題的解法或是分析方法也都不一樣。非線性微分方程的例子如流體力學的納維-斯托克斯方程，以及生物學的洛特卡－沃爾泰拉方程。\n解非線性問題最大的難處在於找出未知的解：一般來說，我們無法用已知的解來拼湊出其他滿足微分方程的未知解；而在線性的系統裡，卻可以利用一組線性獨立的解，透過疊加原理組合出此系統的通解。例如滿足狄利克雷邊界條件的一維熱傳導問題，其解（時間的函數）可以寫成許多不同頻率之正弦函數的線性組合，而這也讓它的解很彈性、具有很大的變化空間。通常我們可以找到非線性微分方程的特解，但由於此時疊加原理並不適用，故無法利用這些特解來建構出其他新的解。\n\n\n=== 常微分方程 ===\n一階常微分方程常常可以利用分離變數法來解，特別是自守方程\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        f\n        (\n        u\n        )\n        \n      \n    \n    {\\displaystyle {\\frac {du}{dx}}=f(u)\\,}\n  。例如\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        −\n        \n          u\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle {\\frac {du}{dx}}=-u^{2}\\,}\n  這個方程式的通解為 \n  \n    \n      \n        u\n        =\n        \n          \n            1\n            \n              x\n              +\n              C\n            \n          \n        \n      \n    \n    {\\displaystyle u={\\frac {1}{x+C}}}\n  ，特解為 u = 0（即通解在 C 趨近於無限大時的極限）。此方程是非線性的，因為它可以被改寫為\n\n  \n    \n      \n        \n          \n            \n              d\n              u\n            \n            \n              d\n              x\n            \n          \n        \n        +\n        \n          u\n          \n            2\n          \n        \n        =\n        0\n        \n      \n    \n    {\\displaystyle {\\frac {du}{dx}}+u^{2}=0\\,}\n  ，而等號左邊並不是 u 的線性映射。若把此式的 u2 換成 u，則會變成線性方程（指数衰减）。\n二階和高階非線性常微分方程組的解幾乎無法表示成解析解，反而較常表為隐函数或非初等函数積分的形式。\n分析常微分方程常用的方法包括：\n\n檢查是否有任何守恆量（特別是在處理哈密頓系統的時候）。\n檢查有沒有類似守恆量的耗散量（見李亞普諾夫函數）。\n利用泰勒展開式作線性近似。\n利用變數變換法，改寫成較易分析的方程。\n分岔理論。\n微擾法（也可應用在代數方程上）。\n\n\n=== 偏微分方程 ===\n\n研究非線性偏微分方程最常見也最基礎的方法就是變數變換，變換以後的方程會較簡單，甚至有可能會變成線性方程。有時候，變數變換後的方程可能會變成一個或兩個以上的常微分方程（如同用分離變數法解偏微分方程），不管這些常微分方程可不可解，都能幫助我們了解這個系統的行為。\n另一個流體力學和熱力學裡常用的方法（但數學性較低），是利用尺度分析來簡化一個較一般性的方程，使它僅適用在某個特定的邊界條件上。例如，在描述一個圓管內一維層流的暫態時，我們可以把非線性的納維-斯托克斯方程簡化成一個線性偏微分方程；這時候尺度分析提供了兩個特定的邊界條件：一維和層流。\n其他分析非線性偏微分方程的方法還有特徵線法，以及上述分析常微分方程時常用的方法。\n\n\n=== 單擺 ===\n\n非線性問題的一個典型的例子，就是重力作用之下單擺的運動。單擺的運動可由以下的方程來描述（用拉格朗日力學可以證明）：\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        sin\n        ⁡\n        (\n        θ\n        )\n        =\n        0\n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\sin(\\theta )=0\\,}\n  。這是一個非線性且無因次的方程，\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   是單擺和它靜止位置所夾的角度，如動畫所示。此方程的一個解法是將 \n  \n    \n      \n        \n          \n            \n              d\n              θ\n            \n            \n              d\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d\\theta }{dt}}}\n   視為積分因子，積分以後得\n\n  \n    \n      \n        ∫\n        \n          \n            \n              d\n              θ\n            \n            \n              \n                C\n                \n                  0\n                \n              \n              +\n              2\n              cos\n              ⁡\n              (\n              θ\n              )\n            \n          \n        \n        =\n        t\n        +\n        \n          C\n          \n            1\n          \n        \n        \n      \n    \n    {\\displaystyle \\int {\\frac {d\\theta }{\\sqrt {C_{0}+2\\cos(\\theta )}}}=t+C_{1}\\,}\n  。上述的解是隱解的形式，同時也包含了橢圓積分。這個解通常沒有什麼用，因為非初等函數積分（即使 \n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle C_{0}=0}\n   仍然是非初等函數）把解的各種特性隱藏了起來，使我們不易看出單擺系統的行為。\n另一個解法是把這個非線性方程作線性近似：利用泰勒展開式將非線性的 sine 函數線性化，並在某些特定的點附近討論解的情形。例如，若在 \n  \n    \n      \n        θ\n        =\n        0\n      \n    \n    {\\displaystyle \\theta =0}\n   的點附近作線性近似（又稱小角度近似），\n  \n    \n      \n        θ\n        ≈\n        0\n      \n    \n    {\\displaystyle \\theta \\approx 0}\n   時，\n  \n    \n      \n        sin\n        ⁡\n        (\n        θ\n        )\n        ≈\n        θ\n      \n    \n    {\\displaystyle \\sin(\\theta )\\approx \\theta }\n  ，故原方程可以改寫為\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        θ\n        =\n        0\n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\theta =0\\,}\n  。近似後的方程變成了簡諧振盪，因此當單擺運動到底部附近時，可以對應到一個簡諧振子。而若在 \n  \n    \n      \n        θ\n        =\n        π\n      \n    \n    {\\displaystyle \\theta =\\pi }\n  （即當單擺運動到圓弧的最高點時）附近作線性近似，\n  \n    \n      \n        sin\n        ⁡\n        (\n        θ\n        )\n        =\n        sin\n        ⁡\n        (\n        π\n        −\n        θ\n        )\n        ≈\n        π\n        −\n        θ\n      \n    \n    {\\displaystyle \\sin(\\theta )=\\sin(\\pi -\\theta )\\approx \\pi -\\theta }\n  ，故原方程可以改寫為\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        π\n        −\n        θ\n        =\n        0\n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+\\pi -\\theta =0\\,}\n  。這個方程的解含有雙曲正弦函數，因此和小角度近似不同，這個近似是不穩定的，也就是說 \n  \n    \n      \n        \n          |\n        \n        θ\n        \n          |\n        \n      \n    \n    {\\displaystyle |\\theta |}\n   會無限制地增加（但此近似方程的解也可能是有界的）。當我們把解對應回單擺系統後，就可以了解為什麼單擺在圓弧的最高點時不能達到穩定平衡，也就是說，單擺在最高點時是不穩定的狀態。\n另一個有趣的線性近似是在 \n  \n    \n      \n        θ\n        =\n        \n          \n            π\n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {\\pi }{2}}}\n   附近，此時 \n  \n    \n      \n        sin\n        ⁡\n        (\n        θ\n        )\n        ≈\n        1\n      \n    \n    {\\displaystyle \\sin(\\theta )\\approx 1}\n  ，故原方程可以改寫為\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              θ\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        1\n        =\n        0\n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\theta }{dt^{2}}}+1=0\\,}\n  ，這個近似後的方程可以對應到自由落體。\n若把以上線性近似的結果合在一起看，就能大致了解單擺的運動情形。利用其他解非線性微分方程的方法，可以進一步幫助我們找到更精確的相圖，或是估算單擺的週期。\n\n\n== 非線性表現（列舉） ==\n古典混沌（和量子混沌相對）—— 指系統裡無法預測的行為。\n多穩態 —— 指系統在兩個或多個互斥的狀態之間切換。\n非周期振盪 —— 指一個函數在任何周期上都不會固定重複其函數值（也稱作混沌振盪）。\n振幅死亡 —— 指系統內的某振盪因系統的自回饋或受其他系統影響而停止的現象。\n孤波 —— 指行進中能自我增強而不消散的孤立波。\n\n\n== 非線性方程（列舉） ==\n\n\n== 分析非線性系統 ==\ninteralg （页面存档备份，存于互联网档案馆） —— OpenOpt 和 FuncDesigner 架構下的求解器，可用來檢查一個非線性代數方程系統是否有任何解，或甚至找出其所有解。\n非線性模型及其模擬展示（連結至蒙納許大學的虛擬實驗室）\nFyDiK （页面存档备份，存于互联网档案馆） —— 可模擬非線性動態系統的軟體。\n\n\n== 參見 ==\n\n\n== 參考資料 ==\n\n\n== 延伸閱讀 ==\n\n\n== 外部連結 ==\n命令與控制研究計畫（Command and Control Research Program, CCRP） （页面存档备份，存于互联网档案馆）\n新英格蘭複雜系統研究所 —— 複雜系統的概念（Concepts: Linear and Nonlinear） （页面存档备份，存于互联网档案馆）\n麻省理工開放式課程 （页面存档备份，存于互联网档案馆） —— 非線性動力學一：混沌（Nonlinear Dynamics I: Chaos） （页面存档备份，存于互联网档案馆）\n非線性模型 （页面存档备份，存于互联网档案馆） —— 物理系統的非線性模型資料庫（MATLAB）\n洛斯阿拉莫斯國家實驗室的非線性研究中心（The Center for Nonlinear Studies） （页面存档备份，存于互联网档案馆）", "数据处理": "数据处理通常是指“收集和操作数据项以产生有意义的信息。”英语中数据处理“Data Processing (DP)”一词也指组织中负责操作数据处理应用程序的部门。 \n\n\n== 数据处理功能 ==\n数据处理可能涉及各种过程，包括：\n\n验证 – 确保提供的数据正确且相关。\n排序 – “按某些顺序，以不同的集合排列数据项”。\n汇总 – 从具体数据中提取要点。\n聚合 – 合并多块数据。\n分析 – “对数据的收集，组织，分析，解释和表达”。\n报告 – 列出详细信息，摘要数据或计算得到的信息。\n分类 – 将数据分为各种类别。\n\n\n== 历史 ==\n美国普查局的历史说明了数据处理方式从手动到电子程序的演变。\n\n\n=== 手动数据处理 ===\n尽管英语中术语“数据处理”的广泛使用仅始于20世纪50年代， 但是数据处理工作已由人类手动执行了数千年。例如，记账涉及诸如过帐交易和制作资产负债表和现金流量表等。随着机械计算器和电子计算器的出现，工作效率得以提升。一个从事手动或用计算器计算工作的人称为“计算员”。\n1850年到1880年，美国人口普查局采用了“计数系统，由于所需分类的组合数量增加，这个系统变得越来越复杂。一次计数只能记录有限数量的组合，因此必须处理五六遍表格。” “1880年人口普查的结果用了7年的时间才发布” 。\n\n\n=== 自动数据处理 ===\n1890年美国人口普查中使用了赫尔曼·何乐礼打孔卡设备。 “人口普查局使用何乐礼的打孔卡设备，可以在两三年内完成1890年人口普查数据的大部分制表工作，作为对比，1880年人口普查则花了七八年。据估计，使用何乐礼的系统节省了约500万美元的处理成本“ （以1890年的美元计算)，即便1890年的调查问题数量是1880年的两倍。\n\n\n=== 电子数据处理 ===\n使用计算机进行数据处理代表了后来的发展。1950年美国人口普查中，人口普查局开始有限度地使用电子计算机：1951年交付UNIVAC I系统 。\n\n\n== 应用领域 ==\n\n\n=== 商业数据处理 ===\n商业数据处理涉及大量输入输出数据，计算操作则相对较少。例如，一家保险公司需要保存大量保单记录，打印并邮寄账单，以及接收付款和过帐。\n\n\n== 参考資料 =="}